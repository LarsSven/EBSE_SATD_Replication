project,pull_number,pull_type,id,text,classification,indicator
pulsar,3019,review,243864237,"yes, I think earlier code was not formatted with formatter that caused this reformatting change.",code_debt,low_quality_code
spark,8093,review,36782853,"Hmm... you'll also need to change `taskEndReasonFromJson`, otherwise the history server won't see the new information. Also, when adding new properties to serialized classes, we generally use `Option[Foo]` (see calls to `Utils.jsonOption` in this class).",code_debt,low_quality_code
spark,13565,review,66319324,"Pretty sure that such a long duration isn't really necessary, but I don't think it hurts to make it longer just in case.",code_debt,low_quality_code
flink,4364,review,128512830,"In very rare cases, it might. I want to change the `Execution` a bit on the `master` to make this unnecessary.
However, that is too much surgery in a critical part for a bugfix release, so I decided to be conservative in the runtime code and rather pay this price in the tests.",code_debt,complex_code
trafficserver,6379,review,373984797,i think this is already defined in #6317 so it should not be here again ?,code_debt,low_quality_code
spark,8531,review,48378751,I'm thinking of taking over this PR and am considering dropping this logic since it adds complexity and might not be a huge performance issue in practice. Let me know if you disagree.,code_debt,complex_code
superset,13210,review,582436382,"`scalar` in linear algebra, relative to `vector`. refer to SQLAlachemy and Pandas variable naming. 
`isArray` more straightforward, I changed this variable name.",code_debt,low_quality_code
spark,11620,review,56418491,indentation,code_debt,low_quality_code
calcite,2286,review,534475582,"What about using `rules(RelTrait in, JdbcConvention out, RelBuilderFactory relBuilderFactory)` here, but with null `in` value and adding if statement in that other overloaded method?
It allows slightly avoid duplication of code",code_debt,duplicated_code
attic-stratos,192,review,23697450,I think its better to use org.wso2.carbon.utils.CarbonUtils.getCarbonHome() here.,code_debt,low_quality_code
spark,23982,review,263647807,nit: `left_anti`,code_debt,low_quality_code
tvm,1403,review,201886571,"this warning seems can be a bit frequent, instead of this, do
Do CHECK_GE(sorted_order_.size(), threads_.size());",code_debt,low_quality_code
phoenix,516,review,292095594,Do you mean I should remove the `addPKColumns` variable and change the if statement to something like the following:,code_debt,low_quality_code
beam,10455,review,362682454,Is it possible to reverse this (`SqlTypeFamily.BINARY.equals(family)`)? That is the preferred style and also eliminates the need for a null check.,code_debt,low_quality_code
spark,5760,review,29388845,"OK, yeah I see that in the docs, though it's not set up that way in CDH (at least, maybe my installation never needed to configure that file a certain way, dunno). It seems bad to silently ignore unreadable files, so at least log it maybe? then... should it be a warning? because it sounds like there's one file that could reasonably be expected to be unreadable. Do we special case it and warn on anything else? fail on anything else?  i'd rather tighten this up in some way from doing this silently.",code_debt,low_quality_code
spark,204,review,11469526,"Very minor - but I'm guessing the first error condition will happen way more often than the second - and getting a message that says it's ""invalid"" is a lot less clear than just saying it doesn't exist. What about breaking these out and including more specific error messages in either case? Also it might be nice to print the value of `fileSystem` in the error message, so it knows what filesystem we inferred the path to be from.",code_debt,low_quality_code
flink,6236,review,241972720,Format the code to be consistent with old functions,code_debt,low_quality_code
spark,23747,review,269640058,Nit: Redundant conversion,code_debt,complex_code
cloudstack,1578,review,77754965,"Per our coding standards, please wrap all `if` blocks in curly braces.",code_debt,low_quality_code
jena,337,review,159239355,"Doing just one operation for fluent seems inconsistent. `Context` manipulation isn't (shouldn't) be that common an operation.
See also the quite recent `Context.mergeCopy`.",code_debt,low_quality_code
trafodion,1036,review,109077021,Would this change be needed if we touch the package name in T2 source to include apache ? Should we change the package name (org.apache.trafodion...) in the source and the file layout  to make it consistent ?,code_debt,low_quality_code
spark,1484,review,23672440,It might be easier to read if we use multiple lines:,code_debt,low_quality_code
arrow,5451,review,329113827,"This is ""bad"", according to the tidyverse style guide, which I believed we were trying to follow: https://style.tidyverse.org/functions.html#long-lines-1
I can get used to whatever style conventions we decide, just want to make sure we're in agreement.",code_debt,low_quality_code
kafka,1791,review,80134212,Rather than using `!muted` I think it would be much clearer to alias this to a boolean that is named: `guaranteeExpirationOrder` similar to the `guaranteeMessageOrder` boolean in `Sender`,code_debt,low_quality_code
spark,25811,review,324969028,I've just renamed this as it made me confused - I imagined DB as LevelDB but there's separate suite for LevelDB. KVStore sounds better to me.,code_debt,low_quality_code
spark,29992,review,504358081,Quick question: can we avoid catching `NullPointerException`? It's a bit odd that we catch `NullPointerException`. We could just switch to if-else I guess.,code_debt,low_quality_code
drill,996,review,146815141,Could you please factor out this logic in a separate method?,code_debt,low_quality_code
incubator-mxnet,13549,review,246983078,indent.,code_debt,low_quality_code
flink,7123,review,236347738,missing space **...setup (data ...**,code_debt,low_quality_code
flink,6178,review,196555570,maybe more descriptive name than `get`.,code_debt,low_quality_code
spark,16706,review,98121245,"nit: remove ""of bytes"".",code_debt,low_quality_code
airflow,5343,review,288918215,"This overall feels like it makes the code cleaner, but I do wonder if it's useful to keep this config setting (so that people can tweak the behaviour of the Airflow without _having_ to go in to the UI and change behaviours.
If we do decide to delete these two config values we should add a note to UPDATING.md about it.",code_debt,low_quality_code
spark,25823,review,330702021,"This is a single class name, variable name should reflect that.",code_debt,low_quality_code
incubator-pinot,5853,review,476001577,Recommend inline these 2 methods for readability (as the existing code). Current way is not as readable,code_debt,low_quality_code
druid,7758,review,287539672,but why did you add the space between `undefined` and `)`,code_debt,low_quality_code
beam,10367,review,364531963,"Yes, here:
It would be nice if we could get rid of this case somehow, because by making this optional we have to deal with the possibility of `pvalue.pipeline` being `None` throughout the code base.  I went back and forth on whether to make the arg optional or simply ignore the error in the method above, but I think I decided that the method above was a common case and thus we needed the protection against None-values throughout the code.",code_debt,low_quality_code
incubator-dolphinscheduler,4896,review,584050628,keep old-style maybe better,code_debt,low_quality_code
airflow,14219,review,584775664,"Ideally you should get the token by calling a python method directly -- by calling this endpoint you are ""retesting"" the login endpoint, which it would be better to avoid.",code_debt,low_quality_code
hudi,1274,review,370818776, I thought having all metadata constants in one place would make it simpler. This is used in reading archived commit. I can move the constant to ArchivedTimeline if you think thats a better place.,code_debt,low_quality_code
reef,1473,review,198332781,"I did -- so setting SkipMessage=null has the same effect as the SkipTestException since our version of XUnit does not support it. But I think the comment of ""// Use null to run tests"" could be a bit more explicit. This is more what I was thinking:",code_debt,low_quality_code
incubator-heron,1969,review,122385823,"There was an intentional optimization to reduce one  `System.nanoTime()` call per tuple execution. In the new commit this optimization is removed to simplify the reasoning. 
Now it is much easier to understand.",code_debt,complex_code
calcite,1584,review,347069363,"BTW, this code `this.getRoot().getCluster().getMetadataQuerySupplier().get()` confused me a lot, can you please explain why we need a fresh new `RelMetadataQuery` instance here ? Couldn't we use the instance already existing there with `RelOptCluster#getMetadataQuery` ? (I have checked that almost each `isValid` case is in the `RelOptRuleCall` circle).
Even we have to got a fresh new instance here, why we just add a new interface `RelOptCluster#getMetadataQuerySupplier` just for debugging ? Can you refactor that out ?",code_debt,low_quality_code
spark,23943,review,264538235,"Since we don't need the old index, shall we remove the obsolete indexes?",code_debt,dead_code
spark,16944,review,103047268,useless import?,code_debt,low_quality_code
flink,10454,review,355092999,also useless,code_debt,complex_code
druid,4754,review,152706384,"I think it should be computed on higher level, producing two different classes. The lambda below is capturing, i. e. it's an allocation on each iteration",code_debt,low_quality_code
flink,11403,review,392897158,nit: indent,code_debt,low_quality_code
airflow,1830,review,90948950,"I find the else to be more readable, but happy to drop it and the indent...",code_debt,low_quality_code
arrow,898,review,130206222,I would prefer doing this change differently. Maybe by allowing allocator to return an empty buffer even if closed. This is because it makes bugs/issues much easier to understand than getting an NPE.,code_debt,low_quality_code
arrow,2623,review,220653441,This automatic fix isn't my favorite though I see that it is necessary to not have a braking change in the api for  ParquetDataset (with the filters argument). Perhaps though it would be better to throw an error here and have this fix in that specific case instead of allowing a wrong nesting level in all cases.,code_debt,low_quality_code
beam,2330,review,112816993,.toString() is unnecessary,code_debt,complex_code
kafka,10039,review,571271791,"Feels slightly odd to pass in the set of topics to load here, but I can't think of a good way to avoid it. Perhaps we could pass MetadataCache into LogManager and let startup call MetadataCache#getAllTopics? That might be more risky though since it changes the startup order in KafkaServer, maybe we can look into this as a follow-up.
Besides that, the name here seems strange. Maybe something like ""topicsToLoad""?",code_debt,low_quality_code
pulsar,6196,review,374325392,Instead of `3.7` we should use `PYTHON_VERSION` variable,code_debt,low_quality_code
lucene-solr,2423,review,581247428,"what is this? is it doing a union? For a union, it would be better to use BasicOperations.union on Automaton objects rather than mess around with regexes as strings",code_debt,low_quality_code
arrow,66,review,64469901,"This is actually somewhat problematic, since two arrays might be unequal but their unequal parts are ""masked"" by the parent bitmap. For example
so the data is technically equal, even though the children are different when you examine them without the ""mask"" of the parent bitmap
this suggests that the `Equals` method should accept an inclusion bitmap, which adds a lot of complexity. @emkornfield what do you think?",code_debt,complex_code
iceberg,1128,review,443691512,Why is `name` needed? The `MessageType` has a name that can be used instead of passing it in separately.,code_debt,complex_code
helix,478,review,325376537,"Let's pass the map only, Map<GlobalRebalancePreferenceKey, Integer>.
ClusterConfig is too much for the rebalancer.",code_debt,complex_code
beam,12938,review,550847398,nitpick - there should be blank line above the comment like before,code_debt,low_quality_code
nifi,4753,review,561119292,"For consistency, `Zookeeper` should be replaced with `ZooKeeper` to match the official naming.",code_debt,low_quality_code
spark,22878,review,241273157,nit: use the api using `jsonFormatSchema`,code_debt,low_quality_code
hawq,392,review,54617226,indentation is off the hook,code_debt,low_quality_code
hbase,1627,review,418984035,Is this really needed? Isn't BULK_LOAD_HFILES_BY_FAMILY loaded as false by default if not set on LoadIncrementalHFiles#195? Or perhaps you can move this for a separate method that could be overridden by  TestLoadIncrementalHFilesByFamily to avoid duplicating whole setUpBeforeClass in the child class?,code_debt,complex_code
spark,29728,review,487921098,"Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?",code_debt,low_quality_code
beam,6707,review,229053783,Is this a debugging println? Should this line be removed?,code_debt,low_quality_code
kafka,2492,review,100844023,No sure -- we also have upgrade info for 0.9. and 0.10 -- maybe this need a general clean-up (so not part of this PR)?,code_debt,low_quality_code
nifi,1918,review,122836798,I knew that code should have been simpler :) updated it to use the readValueAsTree,code_debt,complex_code
spark,15172,review,83513326,nit: space after `if`,code_debt,low_quality_code
geode-native,288,review,185342808,variable name could be better perhaps,code_debt,low_quality_code
flink,6170,review,195824665,indentation,code_debt,low_quality_code
trafficserver,6609,review,419565115,indentation here is not multiple of four,code_debt,low_quality_code
kafka,9998,review,569278561,Nit: seems a bit redundant. Can we not assign the size in the line below?,code_debt,complex_code
spark,17459,review,118818160,"Nit: can the last two args simply be m, n for clarity?",code_debt,low_quality_code
spark,19495,review,144992379,nit: extra line,code_debt,low_quality_code
flink,2363,comment,240791329,"Thanks for your contribution @zentol. I've gone over the code and made some inline comments. My main concern/question is actually the representation of metric's type and hierarchy information. I think that encoding it in a string and then re-parsing it on the receiver side to reconstruct the information is rather fragile and error-prone especially wrt maintainability. Maybe you can give me some background why you decided to do it so.
Apart from that, I think the code contains many tests, which I really like :-)",design_debt,non-optimal_design
accumulo,332,comment,368365644,"Agreed.
You may have gotten that impression because I'm against bundling. But, I'm also against using Hadoop's bundled libs for same reason I'm against bundling our own. I'm in favor of intentional and thoughtful dependency convergence, as a downstream activity. I'd actually prefer we not ship any binary tarball packaging... but since we do, we might as well do it in a way that works well for most users. In any case, I agree with you, this can be improved once we get the basics in. :smile_cat:",design_debt,non-optimal_design
spark,2685,comment,61052515,"Okay I think the issue is pretty tough. Unfortunately hive is directly using the shaded objenesis classes. However, Spark needs Kryo 2.21 which depends on the original objenesis classes.
Here is the hive code that uses it:
https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L186
So we can't just remove kryo that hive uses. This is pretty ugly. One solution might be to update chill in Spark so that Spark is using the same Kryo version as Hive.",design_debt,non-optimal_design
phoenix,936,comment,724872173,"1. Not sure about it but we can introduce additional constraints like the total size of scanned bytes as you suggested to further improve this feature later. 
2. This is correct. By itself, it does not eliminate. However, the client can wait for all the page operation to complete or fail before returning to the application, as an additional improvement. This will further reduce the race conditions. I think we have to enforce the client side timestamp to make the race almost impossible.
3. I expect this feature will improve the overall performance and availability since paging limits the memory usage and the time to hold server resources. My experience with paging on a real cluster is very positive.  I have not seen any negative impact yet as long as the page size is not very small (e.g., less than 1000).",design_debt,non-optimal_design
incubator-heron,1728,comment,281558138,"@objmagic - Thanks for reviewing. I'm agree with you. At first, I think putting `ByteAmountUnit` inside of `ByteAmount` is a better approach and using enum to instead static `MB` and `GB` inside of `ByteAmount` seems nice too. But It might trigger a large refactor in `ByteAmount`. So, I decided to put `ByteAmountUnit` outside of `ByteAmount` temporarily.",design_debt,non-optimal_design
storm,1406,comment,222930875,"@abhishekagarwal87 @knusbaum 
Not at all. It shouldn't hurt much so let's apply this as it is. We can address broader considerations from another issues.
One thing I'd like to see is the status of backpressure for each queue. We can show the percentage, or just show whether this queue meets condition to trigger backpressure or not.",design_debt,non-optimal_design
helix,1472,comment,710682396,"This PR is ready to be merged, approved by @kaisun2000 
There is connection leakage in CustomRestClientImpl and causes timeout waiting for connection.
Fix this issue by consuming entity and releasing the content stream and connection.",design_debt,non-optimal_design
hadoop,840,comment,499279608,The implementation is different from the description.  It would be easier to call it docker instead of docker-build to make the command less characters to type.,design_debt,non-optimal_design
thrift,448,comment,183605715,"@nsuke, I switched it so we now consider a `long` like two `int`s (the high bits and low bits) and we combine their values like we combine `hashCodes` (just with a different multiplicative factor). For `double`s we get the `long` represented by their bytes and treat as before. Since we're changing the values, I also took the chance to pick arbitrary constants to combine with.
For what it's worth, though, the previous `hashCode` helper functions are pretty much what are described in Effective Java. Anyway, I'll rebase and squash after you have a look.
@jsirois That's a good idea. Unfortunately, I don't have the time at the moment to do that :(",design_debt,non-optimal_design
cloudstack,1740,comment,353858935,"@yvsubhash you are right... My problem is not related to this one here. The snapshots in XenServer is working as you said. However, sometimes if exceptions happen during copy of the snapshot to the secondary storage, snapshots in the primary storage are not cleaned. I am still investigating and trying to find a way to solve this problem.
BTW thanks for the reply!",design_debt,non-optimal_design
cloudstack,1705,summary,0,Made the changes to improve logging.,code_debt,low_quality_code
spark,13729,summary,0,[SPARK-16008][ML] Remove unnecessary serialization in logistic regression,code_debt,complex_code
usergrid,180,summary,0,[USERGRID-415] Pushing changes to make duplicate properties more readabl...,code_debt,duplicated_code
airflow,10346,summary,0,Breeze was slightly too chatty when there was no dirs created,code_debt,low_quality_code
dubbo,4655,summary,0,optimize some code styles,code_debt,low_quality_code
phoenix,508,review,285231247,"if all is null --> code will use indexes list
if all is not null --> code will get all the indexes on that table. 
will be implemented in the upcoming PR.",requirement_debt,requirement_partially_implemented
carbondata,240,review,84475433,Map is not supported yet so thrown unsupported exception for Map,requirement_debt,requirement_partially_implemented
spark,16280,review,92527834,"oh i know why. The `InMemoryExternalCatalog` hasn't implemented all the interfaces(e.g. some partition related ones), so we did it intentionally.",requirement_debt,requirement_partially_implemented
druid,3956,comment,281423634,"Well, it's definitely sketchy to be calling aggregate and get concurrently. HyperLogLogCollector isn't thread safe. It's possible that you'll get bizarre values from time to time, like if the offset of an HLLC is in the process of being incremented in one thread while it's being read in another thread. So I think this PR has value.",requirement_debt,non-functional_requirements_not_fully_satisfied
airflow,1352,comment,309399231,"I'm sorry for commenting on such an old (and closed) PR.
Is this feature completely implemented?  It seems that the SchedulerJob class uses a completely different method for loading DAGs than the webserver etc. and it will not schedule tasks from a packaged dag since it only considers raw python files.",requirement_debt,requirement_partially_implemented
groovy,480,comment,275655612,"I think no documentation and no test. I extracted the example from the code you changed actually to show you what your change will affect. In my opinion the get/set path needs to be removed, but that is another topic. The trouble is that get/set in the MOP is in general very badly documented. So it might or it might not be, that your change is a breaking one. Well no, it is a breaking change in terms of semantics, but if real world examples will be badly affected by this? no idea. But I have another example:
    public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
the last three asserts will all fail.",test_debt,lack_of_tests
zeppelin,3497,comment,548180234,"Thanks for the contribution, @amakaur  Could you add unit test  ?",test_debt,lack_of_tests
flink,966,comment,126706999,Ah yes. I'll update them in a while. There's actually some problem with the unit test I've written too. Travis fails sporadically.,test_debt,flaky_test
camel-quarkus,1526,comment,672675824,"The test in
org.apache.camel.quarkus.core.CoreTest#testLookupRoutes
has a TODO as the 2nd route from RouteBuilderConfigurer is not discovered.
It uses @Produces annotation from JEE but Camel cannot discover it. Not sure how we can make this possible.
The regular RouteBuilder classes are discovered via jandex index and added during recorder magic.
I would assume a @Produces annotation from CDI/JEE would also work. But since the bean is not injected somewhere then arc may not trigger it. So maybe we need some jandex magic to discover all methods that returns a RouteBuilderConfigurer and are annotated with @Produces should then record the method, or whatever needs to be done.",test_debt,low_coverage
brooklyn-server,144,comment,222313156,"Worth adding test case(s).
Also note the related discussion the mailing list. An alternative suggestion is that we pick up _all_ files with the given name on the classpath (rather than all those in a given directory). I personally prefer the approach of all files in the directory. That would allow us to more easily incrementally add things (e.g. have separate files for upgrading between versions).
---
I wonder about a nicer package name than `org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames`. I imagine many people will just put this in their `./conf/` directory, so we don't want them to have to create a really deep nested directory.
---
Another thing we could add (in the future?) is if there are conflicting changes - e.g. A is renamed to B in the first file, and B is renamed to C in the second file. Currently, the result would depend on the other the files were processed: i.e. it could be ""B"" or ""C"". It would be good to be more predictable.
I'm fine with that being deferred for now.",test_debt,lack_of_tests
spark,1285,comment,47998047,Good catch! Can you add a unit test for queue input stream? It could be in the InputStreamsSuite.scala.,test_debt,lack_of_tests
thrift,1039,comment,238129818,"The `thrift` directory makes sense.
The test # 4 was relying on `sleep` in test script that is inherently flaky.
I've removed a double pclose in parent process code so we'll see if it fixes the problem in #368 result.
Another problem is that Appveyor CI is failing due to include error.
Not sure why it's only happening on Windows.",test_debt,flaky_test
spark,9773,comment,157865029,I will add a test case.,test_debt,lack_of_tests
spark,3246,review,22343430,"There are _12_ new overloads of `createStream` on top of the existing 4. This seems like big overkill. There should be one version in Java/Scala that takes all arguments, one each that takes minimal arguments, and any others needed to retain binary compatibility. The rest seem superfluous.",design_debt,non-optimal_design
incubator-pinot,6004,review,496927468,I tried and don't see a better way to organize the code. We need to have slightly different logic for each primitive type,design_debt,non-optimal_design
shardingsphere,3158,review,329463176,"We may not add a class only for convenience, may need more design or inline into original class",design_debt,non-optimal_design
flink,705,review,30817476,Actually no. I think it's safe to have it fixed. We only need to adjust the values for tests. What is the easiest way to only allow internal configuration? The problem is that for integration tests it's hard to set configuration values for runtime components otherwise.,design_debt,non-optimal_design
beam,1278,review,86889887,"Fair. Added that. Still not perfect, but at least it will be perfect in pipelines without bundle failures, which is, hopefully, most pipelines.",design_debt,non-optimal_design
spark,2933,review,19377630,"I think it's better to keep this internal, it's a tradeoff between 1.0 and 1.1, most of the users do need to touch this.
We could document it later if user really need it.",design_debt,non-optimal_design
incubator-brooklyn,910,review,41485190,"No particularly strong feelings from me, but I'd lean towards including the no-arg constructor for two reasons:
1. It's consistent with the other enrichers (we could change them all, but having some of each pattern seems more confusing).
2. The contract for enrichers/policies/entities/locations to be instantiated through the `EnricherSpec` etc is that the class must have a no-arg constructor. We don't expect people to call this constructor directly.
The second point means we probably should include the constructor with a comment. We could maybe even change `InternalPolicyFactory` etc so that it can handle calling protected constructors, which would enforce that more.
Anyway, I'm happy to ignore it in this PR.",design_debt,non-optimal_design
camel-quarkus,570,review,360695073,"I think we really need to consider option 1, eventually we may need to clean up a bit the camel's bom but I think that as long term we'll end up supporting most of the component camel supports and the chance to hit this issue again is in my opinion high",design_debt,non-optimal_design
flink,9748,review,328404835,"Add comment for this in this commit otherwise, other people would be confused.",documentation_debt,low_quality_documentation
spark,3099,review,20111063,"Doc needed!  Something like:
""A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer.  When [[fit()]] and [[transform()]] are called, the stages are executed in order, and each stage may modify the dataset before it is passed to the next stage.""
Maybe say something about what happens when there are no stages too.",documentation_debt,outdated_documentation
arrow,8648,review,567582052,"1.  Input first,
2. Input output next.
3. Output variables.",documentation_debt,outdated_documentation
flink,4143,review,123715389,"In general, I would not use the ""kleene"" term, as we do not use it throughout the rest of the docs.
You can say sth like ""looping"" or simply oneOrMore.
In addition, it would also be more consistent to not use ""operator"" but ""pattern"". In the docs we have ""pattern sequences"" composed of ""patterns"" so we should stick to that.
So ""Kleene Operator"" -> ""looping pattern"" or ""oneOrMore pattern""",documentation_debt,low_quality_documentation
incubator-brooklyn,910,review,39962111,"Worth having some javadoc (even though the method is small, it's behaviour is non-obvious). It wasn't clear until looking carefully why would increment the iterator sometimes (i.e. call `next()`) and not other times.",documentation_debt,low_quality_documentation
tinkerpop,1308,review,512632352,"nit: Capital ""A"" in ""Authorization"" please since it's a title. 
nit: There's a bit more formatting to do in the text like enclosing class names in backticks.
I think it would be worth adding some note here to providers to say that while Gremlin Server supports this authorization feature it is not a feature that TinkerPop requires of graph providers as part of the agreement between client and server. Graph providers may choose to implement their own methods for authorization in the manner they see fit.  I would say a similar ""IMPORTANT"" callout box should probably be added to the reference documentation to alert users to this notion. Finally, as you draw closer to a final body of work, this is a neat new feature that should have upgrade documentation. (and perhaps more user facing documentation?))
UPDATE: I read a bit further on and saw you linked from the user documentation to this page....that could suffice, but if I'm thinking of this feature right I sense that users will write these authorizors and i think it could be a popular feature which means more front facing documentation.",documentation_debt,low_quality_documentation
helix,1208,review,464529958,"Typo.
Also, why do you ever need to set the task version after-the-fact?",documentation_debt,low_quality_documentation
kafka,6762,review,286572482,"The comment above doesn't seem to fit the case, perhaps the result of a refactor somewhere along the way. Shall we move it below to the actual `Dead` case?",documentation_debt,outdated_documentation
cloudstack,2699,review,194376281,"Well, I did not see anything yet in the Github. However, if you add just a snippet of comments explaining why we need such method, I am fine with it.",documentation_debt,low_quality_documentation
beam,9730,review,331592994,"use `re.search()`.  also, I don't think this needs to be private, so remove the leading underscore.
maybe add a comment like:  ""we don't use json.loads to test validity because we don't want to propagate json syntax errors downstream to the runner""",documentation_debt,outdated_documentation
arrow,5213,review,318643171,"It would be nice to have some javadocs for the new methods, although they are fairly self-explanatory",documentation_debt,outdated_documentation
zookeeper,684,review,238789004,"Yes, I think we randomly chose one, it depends on how much data you have, we'll update the doc for the best practice. Also there is metric to show the cache hit rate, if it's too low, maybe we need to raise the cache size.",documentation_debt,outdated_documentation
tinkerpop,141,review,44267313,"Typo.""the the"" => ""then the""",documentation_debt,low_quality_documentation
spark,4214,review,25109930,can you add a quick comment of what `true` means here:,documentation_debt,low_quality_documentation
trafficserver,866,comment,240594556,"I'd say it is not sufficient.
As a workaround for most cases, it works, and personally I'm OK with it as is. However, I think the behavior of `TSStringPercentEncode` shouldn't be changed because it's just a workaround for logging issue.
So, if the API keeps current behavior, then I'm fine with landing this change.",code_debt,low_quality_code
flink,1052,comment,134567455,"Really good work @r-pogalz. I had only some minor comments concerning style and test cases. 
I like your approach to split the implementation of FLINK-687 into multiple parts. This makes it far easier to review. Concerning the description of FLINK-2106, you haven't integrated the outer sort merge join into the optimizer and the API, yet. I guess this will happen as a next step. Maybe you can update the description of FLINK-2106 accordingly.
Other than that, the PR looks good to me :-)",code_debt,low_quality_code
spark,4105,comment,103159431,"This is reasonable, but it doesn't handle `Error`. I don't think you need a new 'inner' method? it also duplicates the job cleanup code.",code_debt,low_quality_code
tajo,308,comment,67804082,"Hi @sirpkt ,
+1
The patch looks good to me. I have one suggestion. Each test method name should have the prefix 'test'. For example, `lastValue1` should be `testLastValue1`. It's trivial, so you can immediately commit the patch after fixing them.",code_debt,low_quality_code
tvm,7021,comment,797529527,"@ANSHUMAN87 Hi, thanks for trying this! Could you be more specific about your setting? The adaptive evaluator works in the occasion when the repeat/number of the measure_option is a big number (like 500), and according to our experiment results in the paper, the search efficiency outperforms than the base AutoTVM in the Transformer encoder tuning case.",code_debt,slow_algorithm
systemds,1023,comment,728301245,"LGTM - thanks for the cleanup @Shafaq-Siddiqi. The scenario with 10 components was still failing, but after some debugging it turned out this was due to Kmeans not converging. During the merge I fixed the hard-coded maximum iterations for Kmeans, some formatting issues, and vectorized part of the cholesky computation. With those changes it ran fine.",code_debt,low_quality_code
pulsar,9246,comment,775491410,"sorry for the late reply. @sijie 
I just notice this change and am wondering the necessity of exposing the whole admin client from functions to users. Maybe we can have some discussion when you are available",code_debt,low_quality_code
hadoop,2674,comment,781282914,"+1 LGTM , @cyrus-jackson  please to take care of the checkstyle too",code_debt,low_quality_code
arrow,5508,comment,535832777,"@emkornfield I created a new benchmark to evaluate the performance of consumer directly. The improvement is not significant:
after: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  77326.747 ± 218.829  ns/op
before: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  79007.087 ± 63.994  ns/op
I think there are two reasons for this:
1. in our benchmark, the jdbc implementation is based on h2, and for this library the wasNull method implementation was simple: 
    @Override
    public boolean wasNull() throws SQLException {
        try {
            debugCodeCall(""wasNull"");
            checkClosed();
            return wasNull;
        } catch (Exception e) {
            throw logAndConvert(e);
        }
    }
It can be seen that the implementation is based on a simple flag, plus some simple checks. For other implementations with other RDBs, the implementation can be more heavy. For example, the following code gives the implementation of MySQL JDBC, which may involve megamorphic virtual calls:
  public boolean wasNull() throws SQLException {
    try {
      return this.thisRow.wasNull();
    } catch (CJException var2) {
      throw SQLExceptionsMapping.translateException(var2, this.getExceptionInterceptor());
    }
  }
2. The time for wasNull method was insignificant compared with the Arrow set methods.",code_debt,slow_algorithm
arrow,4322,comment,493617041,"For posterity, I came across this discussion around the original hard-coding of the rpath: https://github.com/apache/arrow/pull/2489#discussion_r215664651.",code_debt,low_quality_code
spark,11006,comment,178279347,"LGTM, other than the naming issues (StandingQuery, etc. in the code)",code_debt,low_quality_code
flink,11794,comment,619004787,@godfreyhe I think that this is a valid tradeoff for now. In the future we may have an exception that says that `CollectionEnviroment not allowed to be used with TableEnvironment. Please use XXXX instead`. For `DataSet` we do not have this problem because the execution logic is hardcoded in the environment.,code_debt,low_quality_code
lucene-solr,1802,comment,683374849,"I ran a quick benchmark of just the javadoc task of `master` vs `LUCENE-9215` branch:
`./gradlew clean compileJava && time ./gradlew javadoc`
master: BUILD SUCCESSFUL in 3m 21s
LUCENE-9215: BUILD SUCCESSFUL in 3m 22s
So this check pass is effectively free and doesn't slow down javadoc processing at all (the hard part is already done), and I think it is a lot easier dealing with the errors directly from `gradlew javadoc`: things like filenames and line numbers really help. Plus we remove the previous python script which was recursively parsing HTML, and that thing was never instant.",code_debt,low_quality_code
airflow,5847,comment,523869438,"It is, but the less load we can place on the reviewers the better. If it's possible without lots of effort anyway.
Everything in `tests/utils` is badly named btw -- they are utils _for_ tests, not tests themselves.",code_debt,low_quality_code
hudi,360,comment,377576044,"@n3nash : Thanks. Added both Unused and Redundant Imports in checkstyle and corresponding code-style. If there are any other rules missing, we can add them in future PRs.",code_debt,low_quality_code
carbondata,432,summary,0,"Carbon-core module should not depend on spark, this PR removes this dependency",build_debt,under-declared_dependencies
druid,1535,summary,0,Update alphanumeric sort docs + more tests / examples,test_debt,low_coverage
kafka,7908,summary,0,KAFKA-9068: Fix incorrect JavaDocs for `Stores.xxxSessionStore(...)`,documentation_debt,low_quality_documentation
spark,7046,summary,0,[SPARK-8639] [Docs] Fixed Minor Typos in Documentation,documentation_debt,low_quality_documentation
ignite,2941,summary,0,"ignite-6774 Java doc is broken: ""LUDecomposition.java:40: warning - T…",documentation_debt,low_quality_documentation
trafodion,294,comment,178076645,Looks good to me. Thanks for fixing all those typos.,documentation_debt,low_quality_documentation
storm,1783,comment,263910453,"@ambud 
The code looks good except what @vesense commented. 
Two things more to address:
1. It would be better to document new configurations. Without documentation, end-users have no idea about added feature. `external/storm-hbase/README.md` and `docs/storm-hbase.md`.
2. The code already uses JDK 8 API (Map.getOrDefault()), so can't get it as it is for 1.x. Could you provide a new PR for 1.x branch?
It would be also great if you can test it (with Caffeine) on JRE7 (expected to not work but we can document the precondition for JRE version) and JRE8 (expected to work).
cc. @ben-manes Is my expectation right?
Thanks in advance!",documentation_debt,outdated_documentation
fineract,810,comment,623127986,"@maektwain just iteratively follow the process described on https://github.com/apache/fineract#pull-requests ... :smiling_imp: (so add that to this PR as well). If this could be written more clearly in the README, then improve it. smiley_cat",documentation_debt,low_quality_documentation
incubator-brooklyn,83,comment,49760955,"A few failing tests, and a couple of (very minor) comments, but other than that, looks good (once tests are passing again :-))",documentation_debt,low_quality_documentation
madlib,425,comment,519652224,We also need to add user docs along with examples for the new function,documentation_debt,outdated_documentation
trafficserver,6001,comment,539771774,"Can you add a doc ?
Also can you update the release notes if we add this new feature to the master - https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html ?
How about remap plugin? can you check the corresponding symbols for remap plugin as well?
What other features are you planning for this ?",documentation_debt,outdated_documentation
hadoop,2556,summary,0,HDFS-15731. Reduce threadCount for unit tests to reduce the memory usage,design_debt,non-optimal_design
couchdb,2426,description,0,"Design doc writes could fail on the target when replicating if with non-admin
credentials. Typically the replicator will skip over them and bump the
`doc_write_failures` counter. However, that relies on the POST request
returning a `200 OK` response. If the authentication scheme is implemented such
that the whole request fails if some docs don't have enough permission to be
written, then the replication job ends up crashing with an ugly exception and
gets stuck retrying forever. In order to accomodate that scanario write _design
docs in their separate requests just like we write attachments.
Fixes: #2415",code_debt,low_quality_code
spark,15272,description,0,"Jira : https://issues.apache.org/jira/browse/SPARK-17698
`ExtractEquiJoinKeys` is incorrectly using filter predicates as the join condition for joins. `canEvaluate` [0] tries to see if the an `Expression` can be evaluated using output of a given `Plan`. In case of filter predicates (eg. `a.id='1'`), the `Expression` passed for the right hand side (ie. '1' ) is a `Literal` which does not have any attribute references. Thus `expr.references` is an empty set which theoretically is a subset of any set. This leads to `canEvaluate` returning `true` and `a.id='1'` is treated as a join predicate. While this does not lead to incorrect results but in case of bucketed + sorted tables, we might miss out on avoiding un-necessary shuffle + sort. See example below:
[0] : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L91
eg.
BEFORE: This is doing shuffle + sort over table scan outputs which is not needed as both tables are bucketed and sorted on the same columns and have same number of buckets. This should be a single stage job.
AFTER :
- Added a new test case for this scenario : `SPARK-17698 Join predicates should not contain filter clauses`
- Ran all the tests in `BucketedReadSuite`",code_debt,complex_code
trafficcontrol,4821,description,0,"There are a couple of CDN-in-a-Box services that take much longer to build than they ought to because of how long it takes to send the build context to the daemon - but don't actually need a context of that size. This reduces the services' contexts to only what is necessary for them to build.
- CDN in a Box
Build CDN-in-a-Box",code_debt,slow_algorithm
gobblin,1862,description,0,"This PR makes various changes to improve how throttling server uses config library.
* Add a policies endpoint to throttling server to query the policy for a particular resource.
* Add expiring resources to broker so policies can be reloaded (to always get the newest one from config library).
* Refactor config library client to make loading of few configs more efficient.
* Refactor Hadoop fs config store to make it less confusing.",code_debt,low_quality_code
flink,8136,description,0,"*This work is a preparation for FLINK-11726.*
*In `SingleInputGate#create`, we could remove unused parameter `ExecutionAttemptID`.
And for the constructor of `SingleInputGate`, we could remove unused parameter `TaskIOMetricGroup`.
Then we introduce `createSingleInputGate` for reusing the process of creating `SingleInputGate` in related tests.*
This change is a trivial rework / code cleanup without any test coverage.",code_debt,dead_code
tvm,7314,description,0,"This PR improves the implementation of GPU `argwhere` added in https://github.com/apache/tvm/pull/6868, using exclusive scan (see https://github.com/apache/tvm/pull/7303). 
The current implementation of `argwhere` is very inefficient, because it uses atomic to update the write location. Since all threads compete for the single location, this effectively makes it a sequential kernel. Moreover, since the output indices need to be lexicographically sorted, the current implementation involves sorting along each axis.
Since `argwhere` is literally an instance of stream compaction, this is a perfect application of exclusive scan. Now, `argwhere` simply consists of 
* A single call to exclusive scan on a boolean flag array to compute the write indices.
* Compaction using the write indices (just copying elements with nonzero condition).
both of which are highly parallel operation. Thus, both atomic and sort are gone, vastly simplifying the implementation. Moreover, it also brings huge speed up, as shown below.
All numbers in milli sec
please review @zhiics @Laurawly @mbrookhart @tkonolige @anijain2305 @trevor-m",code_debt,slow_algorithm
jmeter,337,description,0,"Fixed bug whereby calling `registerError` with the following data set `[""A"", ""B"", ""C"", ""D"", ""E"", ""F""]` would return `[[""A"", 1], [null, null], [null, null], [null, null], [null, null]]` instead of `[[""A"", 1], [""B"", 1], [""C"", 1], [""D"", 1], [""E"", 1]]`.
Improved JavaDoc for `registerError`
Also removed JavaDoc which did not add anything to the method names.
Made the code more readable and at the same time fixed a subtle error.
On my spock branch:
- Bug fix (non-breaking change which fixes an issue)
[style-guide]: https://wiki.apache.org/jmeter/CodeStyleGuidelines",code_debt,low_quality_code
airflow,7085,description,0,"Most of the executors run local task jobs by running `airflow tasks run ...`. This is achieved by passing  `['airflow', 'tasks', 'run', ...]` object to subprocess.check_call. 
This is very limiting when creating new executors that do not necessarily want to start a new process when starting a local task job, e.g. fork a process instead of create.
We could achieve a similar effect if we process back the argument list, but this is an ugly and hack solution, so I did refactor the code and now the executor passes the LocalTaskJobDeferredRun object that contains all the detailed information. A particular executor could create a command if it needs it.
This will facilitate the development of other executors:
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-29%3A+AWS+Fargate+Executor (@aelzeiny)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option (@dazza-codes)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-25+The+Knative+Executor (@dimberman 
https://github.com/apache/airflow/pull/6750 (@nuclearpinguin )
This also made the DebugExecutor code simpler. (@nuclearpinguin )
---
Issue link: [AIRFLOW-6334](https://issues.apache.org/jira/browse/AIRFLOW-6334)
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.",design_debt,non-optimal_design
incubator-mxnet,11076,description,0,"this pull request is based on https://github.com/apache/incubator-mxnet/pull/10804
with the following further changes:
1. reduce ident changes
2. prefer cudnn depthwise convolution over mxnet implementation
still use the explicit #if #else #endif statement over
the new variable effective_num_group solution for backward code path compability
because the new variable effective_num_group may confuse readers with standard group convolution",design_debt,non-optimal_design
trafficserver,7279,description,0,"Some messages get excessively noisy under high traffic conditions if
something about their mechanism goes wrong. The pipe logging feature,
for instance, will emit warning and error messages on every single log
event if the reader goes down or the pipe buffer fills up. This can
result in thousands of log messages being emitted per second, which
makes reading the logs difficult and causes disk space issues.
This commit addresses this issue by adding throttled versions of the
common logging messages so they only emit a message on some set
interval (60 seconds as a default). The following functions are added:
SiteThrottledStatus
SiteThrottledNote
SiteThrottledWarning
SiteThrottledError
SiteThrottledFatal
SiteThrottledAlert
SiteThrottledEmergency
As a bonus, these are implemented using a generic Throttler class which may also be
useful in other applications where throttling is desired.",design_debt,non-optimal_design
druid,4815,review,146068538,Can you add some tests around early publishing and loading sequence data from disk?,test_debt,lack_of_tests
spark,24043,review,292472811,We should at least add a test case for this new option,test_debt,lack_of_tests
spark,18887,review,140699378,The test cases in `kvstore` are just unit test cases. We also need integration tests for ensuring they work as expected.,test_debt,lack_of_tests
incubator-mxnet,13654,review,241911517,"Can you add comment saying why we have excluded these test cases?
Do we have custom tests for these somewhere?",test_debt,lack_of_tests
drill,159,review,39804781,"please add a test where both _SYSTEM_ and _SESSION_ options are changed, and confirm the reset is working as expected.",test_debt,lack_of_tests
arrow,4515,description,0,"At this point the function is not exported or documented and threads are always used, users would need to set `options(arrow.use_threads)` to turn them off.",documentation_debt,outdated_documentation
airflow,2816,description,0,"Dear Airflow maintainers,
    - https://issues.apache.org/jira/browse/AIRFLOW-1848
Dataflow Python operator takes in a filename without `.py` extension, which was incorrectly documented previously.
N/A, just a doc change.
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood (""add"", not ""adding"")
    5. Body wraps at 72 characters
    6. Body explains ""what"" and ""why"", not ""how""",documentation_debt,low_quality_documentation
spark,29402,description,0,"This PR proposes to `include` `_images` and `_sources` directories, generated from Sphinx, in Jekyll build.
**For `_images` directory,**
After SPARK-31851, now we add some images to use within the pages built by Sphinx. It copies and images into `_images` directory. Later, when Jekyll builds, the underscore directories are ignored by default which ends up with missing image in the main doc.
Before:
After:
**For `_sources` directory,**
To show the images correctly in PySpark documentation.
No, only in unreleased branches.
Manually tested via:",documentation_debt,low_quality_documentation
camel,4496,description,0,"#4490 does not cover all the cases. While performing end-to-end tests with this snapshot, I found that traces were still not aggregating correctly. This PR fixes the problem.
I am not sure why the tests did not catch this issue, so unfortunately cannot provide better automatic coverage.
Below are the contribution guidelines:
https://github.com/apache/camel/blob/master/CONTRIBUTING.md",test_debt,low_coverage
airflow,730,review,85289615,We already have a test plugin in `tests/plugins/`. This is less complete than that one and should probably not be added here. We can consider moving the other one (in a separate PR) to serve as a sample.,architecture_debt,violation_of_modularity
trafficserver,3322,review,176559777,Do me a favor and move both `MemSpan` and `MemArena` so the test source files are in alphabetically order.,architecture_debt,violation_of_modularity
