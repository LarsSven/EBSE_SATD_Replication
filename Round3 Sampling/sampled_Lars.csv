project,pull_number,pull_type,id,text,classification,indicator
systemds,204,review,73768209,Would you want to check if sc is a SparkContext instance to raise a more meaningful error?,code_debt,low_quality_code
incubator-heron,81,review,51090091,Can we get rid of this?,code_debt,low_quality_code
spark,8744,review,39581040,`en` is unused.,code_debt,dead_code
spark,27612,review,380444937,"~nit. If possible, can we have additional test statements for `minute` and `second` together below line 743 to make it sure?~
Never mind. I missed the other PR which landed already on `branch-2.4`.",code_debt,low_quality_code
flink,6173,review,196042694,"We have unmodifiable map alternatives in the JDK. If there is not a good reason why we need guava here, I would suggest to solve this without this import. Even in that case, I would use import and not fully qualified classname.",code_debt,low_quality_code
guacamole-client,122,review,161413980,"If this is an upper limit, I would suggest something like `radius-max-retries` to make this more clear.",code_debt,low_quality_code
spark,15963,review,88906249,Nit: the second brace and its match are redundant,code_debt,complex_code
kafka,3662,review,132859349,"I tried breaking up the new test method in `WorkerSinkTaskTest` into multiple methods, but it actually just made it harder to read.",code_debt,low_quality_code
arrow,2890,review,230019538,There's some other cruft here that ought to be removed. I'll make some additional changes and then merge this,code_debt,dead_code
flink,7759,review,258458285,"consider converting the following conditions to methods, this would assign a name to the condition, and possibly would make it a bit easier to test.",code_debt,low_quality_code
spark,30204,review,516533952,nit: Can we unify this with `createYarnResourceForResourceProfile` in YarnAllocator ?,code_debt,low_quality_code
beam,1706,review,95843283,"This worries me.
I'm ok with having the tests run with `Parameterized`, but if we do so we should still be able to determine which case we're in and set the expected exception per-test within the test based on the value of the parameters, rather than inspecting the name of the test. For example, `testConflict_runnableOnServiceAnnotation_expectIllegalStateException` doesn't demonstrate by reading the test code directly that it expects an exception.",code_debt,low_quality_code
incubator-pinot,5700,review,459633576,"Unused, please remove",code_debt,dead_code
airflow,12096,review,517611582,"It's a more efficient way to get the last item from a generator - no need to iterate over every item from the generator just to get to the last one.
It might make a difference in case of lots of events.",code_debt,slow_algorithm
hadoop,1881,review,388858948,nit: this line is long enough its time to split,code_debt,low_quality_code
reef,1054,review,68419655,This looks 100% the same as in the local runtime. Can we reuse that code instead of duplicating it?,code_debt,duplicated_code
spark,23062,review,234404871,Spacing here. Is this simpler with .forall?,code_debt,low_quality_code
spark,158,review,11264777,"Instead of doing this, just prevent users from creating a StorageLevel with offHeap = true and replication = 1. Add a check in the StorageLevel constructor and throw an exception if they make one. Otherwise nobody will understand why this code was added here.",code_debt,low_quality_code
spark,1218,review,15828037,"@mridulm yes I'm aware of that. But before, SparkContext was using reflection to instantiate two different classes in the yarn package, and then connect them manually. I removed one of those (see that there's still reflection code to load `YarnClusterScheduler`) because it seemed unnecessary.",code_debt,dead_code
incubator-mxnet,15371,review,302817980,Get rid of dead code or un-comment to add coverage.,code_debt,dead_code
nifi-minifi-cpp,1040,review,605509404,"minor, but `org::apache::nifi::minifi::` is not needed
in a few other places, too, `org::apache::nifi::minifi::utils::StringUtils::toBool(...)` could be shortened to `utils::StringUtils::toBool(...)`",code_debt,dead_code
spark,25626,review,320095359,"nit: we can use multiline string, e.g.",code_debt,low_quality_code
systemds,740,review,172401453,use `new MatrixBlock(((ScalarObject) newOutput).getDoubleValue())` instead.,code_debt,low_quality_code
flink,11047,review,379996777,Can we introduce a `TableSinkFactoryContextImpl` class to reduce so many anonymous classes?,code_debt,low_quality_code
beam,13137,review,522334301,Maybe you also need to add null checks in process... up to you.,code_debt,low_quality_code
nifi,1036,review,81411506,"Not an incredibly big deal, but getSupportedPropertyDescriptors() is called very often (most often in validation during a UI Refresh) which causes lots of ArrayLists to be created.  I prefer the way DistributeLoad handles this, by creating one ArrayList and reusing it, while also using an AtomicBoolean to know when it should be recreated.",code_debt,low_quality_code
hive,550,review,343202348,yeah...this seems to be one place where this tablename git confusing ... :+1:,code_debt,low_quality_code
carbondata,3584,review,389603171,"Suppose to be && instead of || ?
line 123: 
boolean usePartitionInfoForDataMapPruning = table.isHivePartitionTable() && filter != null && !filter.isEmpty() && partitions != null
and use the same flag to reset in line 138, it is confusing now",code_debt,low_quality_code
brooklyn-server,835,review,142912453,"TL;DR: I agree this is fine. Below are some notes from digging around in the synchronization code.
Looking at the other synchronization blocks, `emitInternal` will synchronize on `EntityManagementSupport` instance (when it calls `getSubscriptionContext()`. It will then call `publish` which will synchronize on `LocalSubscriptionManager` instance (in `getSubscriptionsForEntitySensor`).
However, I think we can trust both the `EntityManagementSupport` and the `LocalSubscriptionManager` to not call out to alien code while holding a lock on itself. Therefore we should be safe in that respect.
The code in `AbstractEntity` and `AbstractGroupImpl` looks a bit scary, where it first gets the lock on this `values` object and then on either `abstractGroup.members` or `abstractEntity.children` (but it's kind-of understandable why it does that and how that pattern avoids it; it would just be nicer if instead we didn't call out to alient code while holding the lock on `values` - but that's a bigger discussion than for this PR):",code_debt,low_quality_code
spark,15627,review,85951050,"If we can cleanup the variables names above I think it would help a lot, the test is confusing.  I know you just copy and pasted but would be nice to clean up.
Also can we have 3 tests or 3 asserts,  
- one for same file in --files
- one for same file in --archives
- one for same file in --files and --archives",code_debt,low_quality_code
ignite,8048,review,480182091,Bad formatting.,code_debt,low_quality_code
spark,1222,review,15478958,"We shouldn't remove this... This is intended to be a general purpose class for logging that handles compression and buffering and other logic. I notice that you replaced this with PrintWriter elsewhere. Even with the new naming scheme you can still use this class for event logging, and it will simplify `EventLoggingListener` a bunch.",code_debt,low_quality_code
spark,31296,review,562868528,"This sounds a little risky to me, `A process is invoked even for empty partitions`.
This may cause a hang situation if the command is expecting input.
For example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.
If we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",code_debt,low_quality_code
dubbo,3162,review,245860102,"Personally think that this place is worth discussing. Most of the exceptions have been handled in connect and disconnect, and there is a log. I think the log here can be removed.",code_debt,low_quality_code
spark,22154,review,234385204,"The suggested change is only for making this test suite cleaner, right? In that case I'd +1 with the suggestion of being able to clearly check we're catching the exception we know we're throwing.
Would you like to submit a PR for it?
The intent is never to actually reach the null-return, but always cause an exception to be thrown at `CodeGenerator.compile()` and abruptly return to the caller with the exception. To make the compiler happy you'll have to have some definite-returning statement to end the function, so a useless null-return would probably have to be there anyway (since the compiler can't tell you'll always be throwing an exception unless you do a throw inline)",code_debt,low_quality_code
ignite,7851,review,430399878,"It is not correct javadoc ""list"", it should be written in HTML style like
So, it is a test though... who cares.",code_debt,low_quality_code
flink,12900,review,454415036,"Is this interface really necessary? Especially with `@PublicEvolving` annotation? How are users supposed to use it? If I understand it correctly you need it for internal operations. Moreover you need it because the `WrapperTypeInfo` is in `blink-runtime`, right?
Can't we move the `WrapperTypeInfo` to the `table-common` instead?  The class itself has no runtime dependencies. Only the factory methods need some runtime classes.",code_debt,complex_code
flink,12176,review,426165664,do not need `ifPresent`,code_debt,complex_code
gobblin,1106,review,70877402,"Is there any reason to `catch Throwable`, instead of specific exceptions?",code_debt,low_quality_code
spark,20194,review,161000312,"Impala is the only reference I can find https://www.cloudera.com/documentation/enterprise/5-10-x/topics/impala_show.html
To be consistent with the other SHOW function, we can make LIKE optional?",code_debt,low_quality_code
spark,21476,review,192476723,"This is so specific to the way YARN runs things that I don't think it would be useful anywhere else. If at some point it becomes useful, the code can be moved.
I think the tests I added are better than just unit testing this function, since that way the code is actually being run through YARN and bash.",code_debt,low_quality_code
openwhisk,4031,review,255882825,Note that the `namespace` and `action` would remain same for all test. Though we do a cleanup after each run it may be better to use different name for each test run,code_debt,low_quality_code
incubator-pinot,4777,review,344013968,"try to make comparisons more specific. Since you know there are only 2 parts and parts cannot be negative, avoid the ""less than"" comparison and instead use `parts.length != 2`",code_debt,low_quality_code
druid,3928,review,106563776,"formatting, new lines would be easier to read",code_debt,low_quality_code
ozone,1503,review,506851310,Can we reuse an existed method OzoneFSUtils#addTrailingSlashIfNeeded(keyName) instead of?,code_debt,low_quality_code
spark,28038,review,402528402,"Yes. Talking with @dbtsai he wanted to add a lock on the blocks inside of `doCleanupShuffle`, but given that the only price is duplicated messages to the executors I'm not sure its worth the overhead of keeping track of that many locks.",code_debt,low_quality_code
spark,4690,review,25098634,"I think we can back that out if there's any question to keep this limited. @JoshRosen Yes that's where we ended up again, now that it's clear that there are several ways and several places this can happen. It's easiest just to ignore the exception.",code_debt,low_quality_code
trafficcontrol,3601,review,297263939,"maybe just add a ""finally"" instead of a ""then""?",code_debt,low_quality_code
apisix-dashboard,979,review,543388564,hard code is not a good way,code_debt,low_quality_code
spark,5355,review,28042923,`np.uint64` -> `np.int32` (to be consistent with Scala implementation),code_debt,low_quality_code
incubator-heron,1820,review,113760959,Is this the implementation class? If so can we use `class` instead of type to be more explicit?,code_debt,low_quality_code
beam,92,review,58796896,"This particular test (and probably others in this file) could be made runner-agnostic. That would make them much better.
However, this is an underlying problem here. Ok to ignore for now.",code_debt,low_quality_code
flink,14734,review,569501903,nit: one step further would be to store `CheckpointBrief` in `PendingCheckpoint` instead of collections. But that's probably out of scope of this PR,code_debt,low_quality_code
kafka,5068,review,191512386,Do we actually want this to be `Long.MAX_VALUE`? Seems error prone since the normal thing to do with the TTL is add it to the current time and that will cause overflow. Should we have some sentinel value for infinite TTL?,code_debt,low_quality_code
spark,9193,review,42587435,"nit: this is kinda hard to read with the double negatives. consider:
?",code_debt,low_quality_code
incubator-doris,5219,review,559096882,better to add the value of max journal id and image id in response msg for easy debugging.,code_debt,low_quality_code
drill,1892,review,363140288,"You probably *really* don't want to do this. Converting the response to a string causes the client to download the **entire** response to memory, then convert it to a Java string. Doing so completely negates the disk-based buffering that seems to be implemented below.",code_debt,low_quality_code
calcite,2369,review,593096896,"Makes sense. I removed these assertions completely, as they are unrelated to the fix. Also added comments to the tests.",code_debt,low_quality_code
hadoop,802,review,291163702,nit: stick at the top (line 20) with the gab above the org.apache stuff. We can at least try to not make import ordering worse,code_debt,low_quality_code
kafka,3325,review,128649545,"Instead of using this separate class and do the `instanceof` check on each call (which maybe expensive), maybe we could just have a `WrappedBatchingStateRestoreCallback` which only takes the non-batching `StateRestoreCallback` in constructor and then in `restoreAll` always do the for-loop, and in places that we need it (seems we only have two callers) we can do sth. like
Just once.",code_debt,low_quality_code
spark,15971,review,89209660,"This is pretty hacky. It makes assumptions about how things are formatted in the event log.
The previous assert should be enough (ignoring my previous comment about changing this test).",code_debt,complex_code
lucene-solr,1303,review,387662567,"This is fine with me but FWIW I wouldn't even bother defining it.  It has no value set aside like this; I doubt any user code would want to refer to it.  If we want to document what the default cost is, we should say so in cost()'s javadoc.  I know some devs like to make static constants for everything but IMO it's sometimes wasted ceremony.",code_debt,complex_code
superset,10728,review,479692093,"nit: commented out code bothers me. It's in a storybook, though, so I won't strongly object.",code_debt,low_quality_code
spark,19911,review,155335791,"@dongjoon-hyun to be clear, I think there are 2 problems:
1. The PostgresDialect indicates that `CASCADE` is enabled by default for Postgres. This isn't the case as the Postgres docs show. 
2. As you correctly mention (this is what in my previous comment), Spark doesn't use `CASCADE` at all, which, especially considering the method this PR edits, is a bit odd I think. I plan to open a different JIRA ticket for this, and add it. This will be more work, and is outside the scope of the current JIRA.",code_debt,low_quality_code
druid,1265,review,27927153,"Aren't 1, 2 and 4 the same as the 1, Shorts.BYTES and Ints.BYTES from the part above?
This code is really confusing me...",code_debt,low_quality_code
cassandra,711,review,472502571,"nit: better to do `node1.nodetoolResult(""disableautocompaction"", ""netstats_test"").asserts().success();` rather than exec into the instance.",code_debt,low_quality_code
ozone,1233,review,482676002,"need space before ""+""",code_debt,low_quality_code
druid,3939,review,102355893,Use log message formatting `%d`,code_debt,low_quality_code
spark,29795,review,493190061,nit: use local variable if possible,code_debt,low_quality_code
druid,5418,review,179293634,"Removed `reportParseExceptions` here, it should always be true (always thrown, handling depends on config)",code_debt,low_quality_code
kafka,7984,review,372683286,As above. Simplify to:,code_debt,complex_code
spark,19763,review,152914613,"After rethinking about this, I think it is better to indicate this threshold also determines the number of threads in parallelism. So it should not be set to zero or negative number.",code_debt,low_quality_code
kafka,2929,review,113856373,remove. this import already exists.,build_debt,over-declared_dependencies
spark,228,comment,39641216,"Hi, @pwendell , Thank you for your comments, here is my reply
First, ""whether accumulator value should be subject to change in the case of failure""...I think no, though during a long period, Spark runs in this way (this patch is actually resolving a very old TODO in DAGScheduler.scala)...I think accumulator is usually used to take the task which is more complicate than rdd.count/rdd.filter.count (Maybe I'm wrong), e.g. counting the sum of distance from the points to a potential center in K-means (see mllib), I think in this case, the health status of the cluster should be transparent to the user, i.e. the final result of K-means should be irrelevant to whether executor is lost, etc....
Second, Good point, I can understand what the use scenario is, but do you mind providing more details like how to implement this in Spark? I guess this can be solved by providing a approximateValue API in Accumulator or SparkContext....
Third, actually, this patch ensures that the value of the accumulator in a stage will only be available when this stage becomes independent (means that no job needs it any more, https://github.com/apache/spark/pull/228/files#diff-6a9ff7fb74fd490a50462d45db2d5e11L400), if a job finishes, and the other job still needs the certain stage, the accumulator value calculated in that stage will not be counted...",design_debt,non-optimal_design
beam,10227,comment,558882339,"We should still get rid of setup_requires, but this just might not be the _complete_ solution.",design_debt,non-optimal_design
storm,1480,comment,226404728,"@abhishekagarwal87: Ok, didn't spot that. Was only looking for ""guava"" relocations.
So, the dependency could be additionally included relocated into storm-redis. Or is it possible to use the already relocated package which is provided via storm-core? This would save some resources. I'm not very experienced with the `maven-shade-plugin` yet, unfortunately.
What is preferred?",design_debt,non-optimal_design
incubator-heron,3479,comment,626232335,"One ""short term workaround""™ for getting the `pex_pytest` to work was to change it so the pex binaries+tests defaulted to be non-zip safe so they are extracted. There were still failures later, which I think is from pex_library consumption. I'll see if I can patch that too, then hopefully come up with a neater solution for all of the issues",design_debt,non-optimal_design
druid,2231,comment,173443345,"Actually , looking at com.alibaba.rocketmq.common.ServiceThread from http://grepcode.com/file/repo1.maven.org/maven2/com.alibaba.rocketmq/rocketmq-common/3.2.6/com/alibaba/rocketmq/common/ServiceThread.java/ it looks like ServiceThread does a lot of synchronization and notifying on itself. That makes this impl much harder to review without an intimate understanding of ServiceThread.
Regarding testing, The general recommendation if the framework does not have a unit-test friendly tool, is to use EasyMock to force the behaviors you are testing for.",design_debt,non-optimal_design
spark,25651,comment,534813380,"@rdblue yea it's possible. In this PR, I try to adopt your suggestion to make it clear that `TableProvider.getTable` should take all the table metadata, so the method signature becomes
`TableProvider` has another `getTable` method which needs to infer schema/partitioning, and previously the method signature was
To make it consistent, I change it to use `properties: Map[String, String]`, also rename it to `loadTable` since we need to touch many files anyway.
We can still keep the old method signature with a TODO to change it later, so that this PR can be much smaller.",design_debt,non-optimal_design
spark,25785,comment,531497209,"I see, can that thread be a daemon? If System.exit is viable (i.e. immediately stopping daemon threads) then it should be. But if not, then yeah such a thread needs to be shut down cleanly somehow during the shutdown process. This could be a shutdown hook.",design_debt,non-optimal_design
spark,30392,comment,729168362,"I think so. In some cases, unnecessary executor-side reduce might invoke an additional map task although it just returns the single element. So this is just a minor concern for me.
The other thing is, `takeOrdered` is introduced into RDD API earlier than `treeReduce`. So I guess we may not consider to use `treeReduce` in this place before. For the nature of `takeOrdered` that sequences of elements are collected to the driver and reduced. It sounds a good fit for `treeReduce` as we can partially reduce before collecting to the driver. There is an overhead but sounds like a trade-off. Driver side usually a bottleneck for such case. The driver-side reduce is not parallel and also bound to local memory for all data.
I'm totally okay to close this if it doesn't sound good direction to go.",design_debt,non-optimal_design
cloudstack,4448,comment,724618330,@ravening I don't see the probably with requiring an internal DNS entry.  The operator can always enter the external DNS IP for it if they need to.  I can only see this causing pain elsewhere in the code.,design_debt,non-optimal_design
storm,1781,comment,262494323,"That should do it - the batching option has been removed from MapState, in favor of parallel processing with existing opaque and transactional logic to handle consistency. Cassandra batch statements are more trouble than they're worth.",design_debt,non-optimal_design
superset,12626,summary,0,docs: Updates to Superset Site for 1.0,code_debt,low_quality_code
arrow,6103,summary,0,ARROW-7473: [C++][Gandiva] Improve error message for locate function,code_debt,low_quality_code
carbondata,2878,summary,0,[CARBONDATA-3107] Optimize error/exception coding for better debugging,code_debt,low_quality_code
zookeeper,1328,summary,0,Extra horizontal lines,code_debt,low_quality_code
hbase,2167,summary,0,HBASE-24791 Improve HFileOutputFormat2 to avoid always call getTableRelativePath method,code_debt,low_quality_code
trafodion,115,summary,0,"Adding .rat-excludes, readme for rat and remove unneeded files lacking copyrights.Also hive/TEST009 fix.",code_debt,dead_code
tinkerpop,712,review,140818483,"Even though `Bindings` is based on a `ThreadLocal<T>` instance, I think the implementation is not thread-safe.
For example: Multiple tasks executed serially on the same thread, modifying the same bindings dictionary.
Besides not being thread-safe, it doesn't support defining a binding on 1 thread and adding the step on another, sample:",requirement_debt,non-functional_requirements_not_fully_satisfied
storm,2241,review,129494647,"Looks like you're removing max spout pending and also configuration for backpressure. Does this patch also touch the mechanism of backpressure?
And let's address this in patch or file an issue and remove TODO.",requirement_debt,requirement_partially_implemented
incubator-mxnet,9552,review,176825148,"Just tried. That function for FP32 convolution seems not implemented appropriately. The data member `layout` of struct `ConvolutionParam` is a `option<int>` type, but it was used with the assumption that `layout` contains a real value in that function. This will cause my test case to fail. Making that function properly implemented needs another PR. For now, I will just keep the shape inference for quantized_conv op. Already deleted the TODO comment.",requirement_debt,requirement_partially_implemented
nifi,71,comment,130314880,"Why is propertyMap marked volatile?  The value is only ever set once at construction time.
If the answer is because of thread safety, the contents of the HashMap are not ""protected"" just because the reference to the map is marked volatile.  puts/gets to the map do not inherit the memory barrier protections associated to the volatile reference.  c.f.  http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap
Maybe a review of the concurrency issues of this processor is in order before accepting this merge request?  I'm pretty sure that, even though the class will mostly behave correctly since values are set during OnScheduled and OnStopped, these are not ""safely published"" to the map.  While unlikely, other threads could potentially see stale values in this map.
Either this class should likely be using ConcurrentHashMap here, or it should republish an entirely fresh map by calling ""new HashMap()"" instead of ""clear()"".",requirement_debt,non-functional_requirements_not_fully_satisfied
flink,439,comment,76031299,"I like the idea - that means that the webfrontend now shows what values are actually used, even if they are not in the config.
To make it even nicer, it would be cool to:
- Show which values come from the config (top) and show for which ones what default is used (in a second section below)
- add a Unit test that checks that all keys from the `ConfigConstants` are converted. I think it is easy to check this via enumerating all fields in the class using reflection.",test_debt,lack_of_tests
helix,1447,comment,704707243,"minor: suggest to change the PR (commit) title to ""Replace Thread.sleep() with TestHelper.verify() to fix the flaky unit tests"".",test_debt,flaky_test
spark,19143,comment,327427666,"You haven't actually added tests, and that's not all this PR does. At the least, this doesn't match the intent you describe, and should be closed. I'd back up and describe the test you want in the JIRA.",test_debt,lack_of_tests
kafka,8402,comment,609450055,test this please,test_debt,lack_of_tests
flink,5901,comment,387376186,"By the way, the test still doesn't catch the bug that you're fixing (https://issues.apache.org/jira/browse/FLINK-8286). I think we need proper end-to-end tests that really test Flink-Kerberos integration on an actual YARN cluster. I have started looking onto using Docker Compose for that, i.e. bringing up a hadoop cluster in docker with Kerberos and then running Flink on that as an end-to-end test.",test_debt,low_coverage
druid,3788,comment,268051850,"@erikdubbelboer ok that makes sense.
Can you please:
1. Add a comment to the code as to why this does not use `_`
2. Add unit tests to verify functionality.",test_debt,lack_of_tests
incubator-heron,3032,comment,437498318,"Overall is ok for me. 
**IT** coverage will be useful so i will be addressing through #3106",test_debt,low_coverage
zeppelin,2706,comment,353205540,@zjffdu can u confirm it's a flaky test?,test_debt,flaky_test
incubator-pinot,5769,review,467200428,"I think both ways have some pros and cons. I personally think that using a single endpoint is easier for users?
* The main purpose of online AD endpoint is to provide a convenient way to run AD tasks so the endpoint should be as simple as possible. Using a single endpoint is more user-friendly. Using a separate CRUD endpoint does allow more flexibility but it will ask users to firstly register their data before using this service.
* Secondly, online service will not allow much too large size of data so in my sense, sending data in the request every time is not a bottleneck?
I think we could provide both ways. This is phase 1 for this feature. In phase 2, we probably could support another two endpoints to support what you suggested. 
Regarding the cleanup, I think currently in phase 1, it is just a one-call request so I mentioned this as stateless because users will not have a separate endpoint to retrieve anomalies afterwards and hence we could clean up them. In phase 2, another two endpoints will be provided and for those endpoints, we do not need to do the cleanup.
Thanks for your suggestions!",design_debt,non-optimal_design
spark,1270,review,17257206,"`RDD[(Set[Double], Set[Double])]` may be hard for Java users. We can ask users to input `RDD[(Array[Double], Array[Double])]`, requiring that the labels are ordered. It is easier for Java users and faster to compute intersection and other set operations.",design_debt,non-optimal_design
pulsar,5032,review,317796702,"This will require to update this file for each release which is not ideal. We could get the list of versions from `versions.json` though the maven install might take a while to complete.
Another option would be to just build the latest version and make sure we add the version tag, so that we just build 1 version (since the older docs will not change).",design_debt,non-optimal_design
incubator-mxnet,8015,review,141497578,"yes, let's worry about this later.",design_debt,non-optimal_design
kafka,764,review,50764320,"@apovzner Personally I think the timestamp should be accurate. Modifying the timestamp sounds very hacky and creates extra complexity. Please also notice that the timestamp index built by the followers will be purely depending on the timestamp in outer message of compressed messages. The followers will not even decompress the messages. If we play the trick here, the time index on follower will also be affected. 
If we want to make things right, then producer should be able to get the necessary topic configuration info from broker, either from TopicMetadataRequest or some other requests. So the producer can set the timestamp correctly to avoid server side recompression. But like you said this is a bigger change and it is unnecessary to block on that change.
I think the current solution is reasonably clean as of the moment.
Once the producer is able to get the topic configuration from broker, we can simply migrate to use that. Since everything is purely internal, the migration is very simple and transparent to users.",design_debt,non-optimal_design
samza,230,review,124421458,"@sborya I agree it is pretty convoluted how we are doing this. However, @prateekm and I have discussed this option extensively in the past when we are working on [SAMZA-1212](https://issues.apache.org/jira/browse/SAMZA-1212). 
As I explained before , it is not fully clear how to wire-in the source of the stop and cause of the stop of a streamprocessor across the components. I don't believe it is as straightforward as passing in a `Throwable`. It could work. It becomes particularly tricky for bounded jobs or jobs that decide to stop themselves using TaskCoordinator. The approach you are suggesting is equivalent to threading a needle , where we will end up passing around the state/source of the `stop` across the components' api and callbacks. 
The alternative and more straightforward solution, imo, is to clearly define a state model for each components and persist/manage the status in the streamprocessor. That way, it will make it easier for streamprocessor to take remediation steps.
We can discuss more on this. However, this PR is simply a bug-fix. We should scrutinize the refactoring/behavior of the APIs in a separate thread/JIRA. What do you think?",design_debt,non-optimal_design
spark,22847,review,228600757,"it will not be JITted; it also should not not be too small, otherwise there will be many function calls.",design_debt,non-optimal_design
spark,7626,review,35370290,Yea this is right now used by a lot of test code. Somewhat annoying to update those.,design_debt,non-optimal_design
incubator-mxnet,11502,review,200206922,Needs more explanation on what this is used for and how do users use this,documentation_debt,low_quality_documentation
beam,4185,review,153627791,Can you add a comment with an example of an actual value here?,documentation_debt,low_quality_documentation
incubator-pinot,4397,review,307084550,please add some javadoc,documentation_debt,outdated_documentation
flink,12260,review,429716465,"Could you add a short comment for this field to explain why we need this? The same to `CatalogManagerCalciteSchema`, `CatalogSchemaTable`, `DatabaseCalciteSchema`. For example:",documentation_debt,low_quality_documentation
hbase,1077,review,368823636,May be good to update doc for `hbase.coprocessor.regionserver.classes` in section `Restricting Coprocessor Usage` of cp.adoc?,documentation_debt,outdated_documentation
flink,6076,review,190841926,"+1 for getting this into the docs. I offer some grammatical improvements. Also, is it correct to describe ""operators are required to completely process a given watermark before forwarding it downstream"" as a general rule, meaning that it might have exceptions, or should we simply say ""operators are required ..."" without adding this caveat?
I changed behaviour to behavior because most of the docs seem to be using American spellings rather than English ones, but I'm not sure if we have a policy regarding this.",documentation_debt,low_quality_documentation
nifi,1553,review,103938291,Are these comments necessary?,documentation_debt,low_quality_documentation
kafka,241,review,40615287,"Typo. ""Unless the..."" Don't leave us hanging!",documentation_debt,low_quality_documentation
nifi,4369,review,452304677,Typo: configureSasToken(),documentation_debt,low_quality_documentation
incubator-mxnet,8302,review,155828922,The documentation need to be updated. Does copying from mkl-dnn array to sparse ndarray work??,documentation_debt,outdated_documentation
beam,12241,review,454528596,I would add a comment above to state the assumption. But it's minor though.,documentation_debt,low_quality_documentation
apisix,2036,review,487762501,why call `res_fun` twice?,documentation_debt,low_quality_documentation
kafka,158,review,38986690,"just to circle back here - I asked @benstopford to remove the test and doc and do it in a new PR, since they are a bit out of context here.",documentation_debt,outdated_documentation
gobblin,2788,review,351428413,typo: Gobblin config path,documentation_debt,low_quality_documentation
beam,5049,comment,379410687,"Note that this change is not Python 3 compatible since `long` was removed in Python 3.  Can you add something similar to the following?
https://github.com/apache/beam/blob/8d854d4ce0365a8e201b388618d7732f000c65b9/sdks/python/apache_beam/transforms/combiners.py#L39",code_debt,low_quality_code
airflow,3090,comment,370109428,"nit: i think line 68-70 can be removed (to avoid confusion), looks like abandoned legacy code:
thanks for fixing this!",code_debt,dead_code
storm,128,comment,45091287,"I finally made it through all of the code.  It looks good for the most part.  Just a few minor comments.  I also am wiling to maintain/support this code. I have a umber of customers who I know would be very interested in using this, so I would be on the hook for supporting it anyways :)",code_debt,low_quality_code
spark,2761,comment,58776141,"I also think, it's difficult to apply new style checker only to new codes. I cleaned up codes in origin/master for the style checker suggested in this PR.  So, if this PR is merged, then we can enforce the new style to developers and all developers have to do is to check the style of the code changed by them.",code_debt,low_quality_code
spark,9432,comment,154740151,"Oops my bad, also removed the unnecessary sbin/../ from the other tachyon paths",code_debt,complex_code
kafka,5221,comment,399157359,"here are the results on my workstation:
the update path is somewhat slower (because the update must start by 1st copying the latest snapshot and then applying the new data to the copy), but starvation is gone.
also note that metadata codepaths in general do an awful lot of copying - seems to me that converting the entire class to java (or at least making it use 100% java collections) would avoid multiple copies going to/from KafkaApi (all those toJava() calls).",code_debt,slow_algorithm
arrow,1804,comment,377119109,"I'm sorta ambivalent on the package name -- I looked at crates.io and there are some other ASF projects with packages that just use the Foo in Apache Foo. If ""arrow"" is shorter and sweeter, that's no problem",code_debt,low_quality_code
spark,1561,comment,50182962,"Looks good to me, this is definitely cleaner",code_debt,low_quality_code
activemq-artemis,3455,comment,779848243,"@michaelandrepearce this is just code cleanup. Nothing that would bring any value to a release notes...
The commit itself would be enough record of the change here.
If someone is creating a JIRA, it would be a task, as the is not an improvement, not a feature, not a bug fx.. no value on the release notes.",code_debt,low_quality_code
airflow,1435,comment,215288030,"Much thanks for the in-depth review!
Is the scenario you are worrying about (two workers running the same task instance) already possible? For example if a worker's communication with the DB gets interrupted, then the scheduler assigns the task instance to a new worker, and then the communication between the initial worker and the DB resumes.
This makes sense. I misspoke in the PR description though, SLAs should still be sent, the difference would be the SLA email would now omit task instances in the dagrun that didn't succeed for reasons other than depends_on_past not being met (e.g. a task that couldn't run because it's pool was full won't get reported in the email). I think I'm going to just include all tasks that don't have a successful status in the SLA miss email, even those stuck on depends_on_past to align with your criteria (if the task caused core_data to not be delivered by 9AM the task caused the DAG to miss it's SLA regardless of it's depends_on_past_dependency), plus is stops treating depends_on_past differently from the other dependencies like the pool being full. LMK what you think.
Agreed about the efficiency, was going to look into caching if this causes perf issues.
The newfound power of the force flag could be used instead of ignore_depends_on_past, but making ""force"" the default for every backfill could potentially be a bit dangerous as users could e.g. unintentionally force run over a large range of already successful tasks in a backfill or violate a pool constraint. If you have any ideas let me know.
Agreed about not passing in a bunch of different flags. There is actually a TODO above that part of the code in the PR to use a context parameter instead (it will be addressed in this PR).
For the flag upstream_failed I would prefer to leave the fix for another PR since it was an existing hack and the cope of this PR is already a bit dangerously large.",code_debt,low_quality_code
spark,28661,comment,635693433,"Well, I'd say it differently. A Python person may not know what a JVM stack trace means. Taking it away doesn't itself do much except shorten a big dump of output, which doesn't really simplify much. Taking away important information that perhaps someone _else_ can make sense of isn't making usage (debugging) harder. if this is only removing ""unhelpful"" stack traces from the console, I can see it.",code_debt,low_quality_code
kafka,6517,comment,484309340,"@hzxa21 Thanks for the code review.
1)	This consistently happens on Windows platform. I have seen few people complaining similar issues for some Linux flavors, but consensus is this is not an issue for most of the popular Linux flavors. This is a classic windows problem where if a handle is open for a file, NTFS does not allow it to be renamed / moved / deleted. All the file handles open on the file are required to be closed before renaming them. This is not much of an issue with Linux because of the way Linux file system is implemented. The way files are implemented in Linux, it allows to rename / move / delete files even if there are open handles to them.
The way I think about this patch is, before renaming any files / directories, we should always close the file channels. This is applicable to Linux as well. There are 2 fundamental scenarios. 
a)	Deletion of directories since the topic is deleted and deletion of segment files by cleaner thread. We believe, we should close the log file in case of log deletion and close the segment in case of segment file cleanup. Since no one should be reading these files, it is safe to close.
b)	 During swapping flow, where ‘.cleaned’ files are renamed to ‘.swap’ files and then they are renamed to actual log files. Here also we believe segment should be closed during renaming and re-opened after the swap.
Overall gist is, files should be closed before renaming them and should be opened back if required. This should be applicable to Linux as well
2)	Yes, initially we implemented the change in rename files method. But that change turned out to be 
ugly. Here is the abandoned pull request done such way.
https://github.com/Microsoft/kafka/pull/17/files
Problem was, there was no need to re-open ‘.deleted’ files. So we had put an if check to not to re-open ‘.deleted’ file. Also, it resulted in changes at many places.
Also, I felt like it was not a cleaner fix where we are not solving the root problem. Which is to close the segment before they are renamed / deleted. Also, we wanted this to be a generic fix, rather than specific to Windows.
3)	Yes, I can add some test cases. We wanted to get a initial feedback on the idea before adding tests.",code_debt,low_quality_code
spark,17582,comment,294933906,"so we should definitely fix the /api/v1/applications/<app-id>/logs to go through the acls.  It looks like it should be protected in ApiRootResource.java. You have the app id so it needs to do something like the withSparkUI to get the acls included in that application.
Like I mentioned the listing (/api/v1/applications) and /api/v1/applications/<app-id> (which is same info I believe as listing) were intentionally left open.  I don't really see a reason to change that but if other people have a use case for it then perhaps we should make which pages are protected by acls configurable.  
on the history server I would expect spark.acls.enable=false and spark.history.ui.acls.enable=true, I can see where that could be confusing, perhaps we should document this better. spark.acls.enable on the history UI really is protecting the root UI, not the app level ui's.  We could explicitly turn this off.",code_debt,low_quality_code
hive,1780,comment,761269576,Updated PR to improve readability (https://github.com/apache/hive/pull/1780/commits/37e707705c6cc7c025bbe7fb1b67682062dc84c7) and at the same time address HIVE-24646.,code_debt,low_quality_code
pulsar,2888,comment,441582678,"The ""data:"" parser you wrote would be a duplicate code because it's already implemented in the in-house URL class I wrote.  I'm not going to make a change request because I don't want to delay this feature, but either of the parsers should be removed eventually to eliminate duplicate code.
Also, because environment variables are general enough as data sources, I think ""env:"" parser should be added as a `URLStreamHandler` so that other places can use it too via the in-house URL class.
As for ""token:"", I'm ok with having it in this plugin as a special case. But if you renamed it to ""raw:"", it could be used on other places.",code_debt,duplicated_code
spark,23260,comment,446345304,"My understanding is that this allows pointing the Spark UI directly at the history server (old JHS or new ATS) instead of hardcoding the NM URL and relying on the NM redirecting you, since the NM may not exist later on.
That does open up some questions though. The code being modified is in the AM, which means that the user needs to opt into this when he submits the app, when perhaps if there was a way to hook this up on the Spark history server side only, that may be more useful.
I think someone tried that in the past but the SHS change was very YARN-specific, which made it kinda sub-optimal.",code_debt,low_quality_code
dubbo,2741,summary,0,Fixing flaky tests in PortTelnetHandlerTest,test_debt,flaky_test
ignite,6069,summary,0,ignite-11261: [ML] Flaky test(testNaiveBaggingLogRegression),test_debt,flaky_test
trafficserver,5380,summary,0,Fixes spelling in src,documentation_debt,low_quality_documentation
shardingsphere-elasticjob,1608,summary,0,For checkstyle & fix typo,documentation_debt,low_quality_documentation
flink,5663,comment,371969548,Currently in flink connector we are depending only on aws-sdk-kinesis and not on aws-java-sdk-bundle and also don't depend on kinesisvideo. So by default the dependency on kinesisvideo is not included in the connector which means we don't have to exclude any dependencies. I also verified that there is no unwanted netty dependencies by running mvn dependency:tree. The only instance of netty is this: `[INFO] |  +- org.apache.flink:flink-shaded-netty:jar:4.0.27.Final-2.0:provided` in accordance to the value in flink-parent pom.,build_debt,over-declared_dependencies
netbeans,968,comment,430955215,"@geertjanw The XSDs/DTDs are used to generate code and my interpretation of the reply from apache legal is, that the license transcends to the generated code. So for weblogic we would distribute unlicensed code (i.e. we have no right to distribute) and for jboss/wildfly we would distribute LGPL code, which is against the ASF rules.
Getting the XSDs/DTDs at runtime would be perfectly doable, but the modules would need to be reworked to no rely on code generated at build-time.
@vikasprabhakar the DTD files are present here:
https://www.jboss.org/j2ee/dtd/
The files are missing an explicit license and as such I would assume they are covered by the projects LGPL license. That at least is reflected in some of the XSDs I saw. Asking redhat would most probably reveal the same. That still leaves us with the ""wrong"" license.",documentation_debt,low_quality_documentation
druid,2683,comment,198480289,Are these documented?,documentation_debt,outdated_documentation
beam,539,comment,228871382,"My fault. I should open a JIRA issue before I start on this. I thought it is a quick change.
For the WordCount part, I like to move it to a top level class, and document why Spark runners need its own copy. (+@dhalperi for Read.from doesn't work for HDFS)
For the Tfidf part, I think I am close to run it with Spark runner in beam-examples module. #533 
I am having a spark dependency problem in the test (JavaSparkContext class not found):
https://builds.apache.org/job/beam_PreCommit_MavenVerify/1908/console
Could you help me to take a look? And, you can make a call on whether you want to include the Tfidf in this PR or left to me to handle in #533.",documentation_debt,outdated_documentation
flink,1813,comment,229693884,@subhankarb We should also add Redis Sink to the fault tolerance guarantee table for the connectors in the documentation. It can be found at `flink/docs/apis/streaming/fault_tolerance.md`.,documentation_debt,outdated_documentation
spark,16892,comment,280117833,Ok I added back the other test but improved the commenting there.,documentation_debt,low_quality_documentation
lucene-solr,133,summary,0,LUCENE-9069: Prevent memory leaks in PerFieldAnalyzerWrapper,design_debt,non-optimal_design
spark,24300,description,0,"This PR aims to clean up package name mismatches.
Pass the Jenkins.",code_debt,low_quality_code
spark,5511,description,0,"Even if we wrap column names in backticks like ``a#$b.c``,  we still handle the ""."" inside column name specially. I think it's fragile to use a special char to split name parts, why not put name parts in `UnresolvedAttribute` directly?",code_debt,low_quality_code
flink,10753,description,0,"-->
To complete finish the translation work of this page.
This page is about Flink's Table API & SQL. Mainly showing the usage of Table API & SQL, how Flink optimize SQL queries and the difference between Blink and Flink planner. Now it has been translate into Chinese.
This change is a trivial rework / code cleanup without any test coverage.",code_debt,low_quality_code
kafka,6680,description,0,"* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`",code_debt,complex_code
accumulo,221,description,0,"* Removed log4j config being done in Java but some remains
* Logging is now configured using standard log4j JVM property
  'log4.configuration' in accumulo-env.sh
* Tarball ships with less log4j config files (3 rather than 6)
  which are all log4j properties files.
* Log4j XML can still be used by editing accumulo-env.sh
* Moved AsyncSocketAppend to start module due to classpath issues
* Removed auditLog.xml and added audit log configuration to
  log4j-service & log4j-monitor properties files
* Accumulo conf/ directory no longer has an examples/ directory.
  Configuration files ship in conf/ and are used by default.
* Accumulo monitor by default will bind to 0.0.0.0 but will
  advertise hostname looked up in Java for log forwarding
* Shortened names of logging system properties
* Removed MonitorLoggingIT as it is now difficult to setup log
  forwarding using MiniAccumuloCluster.",code_debt,low_quality_code
kafka,4691,description,0,A few small logging improvements which help debugging replication issues.,code_debt,low_quality_code
zeppelin,3897,description,0,"Travis-CI:
 - remove all environment variables from the build matrix, to use the same cache for all jobs
 - activate the mysql service only when this service is needed
 - activate the xvfb service, when is necessary and possible
 - removed Bower Caching to remove too many complicated lines in Travis-ci
 - Giving the test names
 - Installing R only once with conda, previously it was installed twice with 'testing/install_R.sh' and 'testing/install_external_dependencies.sh
 - remove the R-Cache, because the installation with conda is quite fast
 - Delete 'test/install_R.sh' because it is no longer used
Other:
 - Ignore the tests in 'HeliumApplicationFactoryTest.java' to get the JUnit tests running in the IDE + remove exclude in 'travis.yml
 - Remove deprecation warning in maven-surefire-plugin
 - Helium works better with an absolute path, because a relative path in PATH, is not a good idea for local testing
 - Remove JVM language dependent asserts
Improvement
* https://issues.apache.org/jira/browse/ZEPPELIN-5024
* Travis-CI: https://travis-ci.org/github/Reamer/zeppelin/builds/723116251",code_debt,dead_code
hudi,1664,description,0,"Default value of number of delta commits required to do inline compaction (hoodie.compact.inline.max.delta.commits) is currently 1. Because of this by default every delta commit to MERGE ON READ table is doing inline compaction as well automatically.
I think default value of 1 is little overkill and also it will make MERGE ON READ work like COPY ON WRITE with compaction on every run. I am propsing to increaset the value to old default of 10.
This pull request is a trivial rework / code cleanup without any test coverage.",design_debt,non-optimal_design
spark,26138,description,0,"What changes were proposed in this pull request?
Some of the columns of JDBC/ODBC tab  Session info in Web UI are hard to understand.
Add tool tip for Start time, finish time , Duration and Total Execution
Why are the changes needed?
To improve the understanding of the WebUI
Does this PR introduce any user-facing change?
No
How was this patch tested?
manual test",design_debt,non-optimal_design
kafka,4138,description,0,"Even though this class is internal, it's widely
used by other projects and it's better to avoid
breaking them until we have a publicly supported
test library.",design_debt,non-optimal_design
arrow,7885,review,465480283,Added more test cases for negative numbers and boundary conditions.,test_debt,low_coverage
spark,28841,review,463960973,"Could you add tests for multiple file cases? Probably, I think you might be able to use `(new File(""/tmp/file.csv"")).setLastModified(xxx)` to control timestamp.",test_debt,lack_of_tests
flink,8438,review,287425587,I think we don't need this test here.,test_debt,expensive_tests
flink,9977,review,338932359,"Add a UT test for these coders. For example, we can add a test in `pyflink/fn_execution/tests`.",test_debt,lack_of_tests
spark,29403,review,496341174,"Also, if I'm correct, it would be good to include a test case for this case.  What happens if you actually do operations on the null value stored as an enum?  Is the nullability of the resulting schema correct. Do operations like `.isNotNull` work correctly?",test_debt,lack_of_tests
samza,347,review,180646742,"I haven't measured how much time increase it causes. The purpose of this change is to make each test independent of each other. If we use BeforeClass and AfterClass tags, then these methods need to be static and the variables instantiated (i.e. producer, systemAdmin) needs to be static as well. These variables will then be instantiated exactly once for the test suite and used by all unit tests in this test suite. Thus kind allows interference between tests and make make test development difficult.
During development of this patch the interference must have caused some problem which required me to make this change. I don't exactly remember what that problem is now. But in general it seems like good practice to make unit test independent of each other. What do you think?",test_debt,expensive_tests
spark,6593,description,0,"This also helps us get rid of the sparkr-docs maven profile as docs are now built by just using -Psparkr when the roxygen2 package is available
Related to discussion in #6567 
cc @pwendell @srowen -- Let me know if this looks better",documentation_debt,low_quality_documentation
pulsar,7601,description,0,"-->
Fixes #<xyz>
Master Issue: #<xyz>
This mr adds an api to check if the worker is ready to serve requests.
*Describe the modifications you've done.*
This change is a trivial rework / code cleanup without any test coverage.
This change is already covered by existing tests, such as *(please describe tests)*.
This change added tests and can be verified as follows:
  - If a feature is not applicable for documentation, explain why?
  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation",documentation_debt,low_quality_documentation
kafka,6475,description,0,"Fix flaky test cases in `WorkerTest` by mocking the `ExecutorService` in `Worker`.  Previously, when using a real thread pool executor, the task may or may not have been run by the executor until the end of the test.
Related JIRA issues:
Ran all tests (`./gradlew test`).
Ran unit tests in `connect/runtime` repeatedly.",test_debt,flaky_test
dubbo,1906,description,0,"#1682: Enhance the test coverage part-4 : dubbo-common/src/main/java/com/alibaba/dubbo/common/status(store|threadpoolutils) modules
dubbo-common/src/test/java/com/alibaba/dubbo/common/utils/UrlUtilsTest.java
XXXXX",test_debt,low_coverage
kafka,9001,review,462716040,Could be moved to `UpdateFeaturesResponse` as a utility.,architecture_debt,violation_of_modularity
airflow,13728,comment,761947640,Temprary failures - it's good to go.,defect_debt,uncorrected_known_defects
activemq-artemis,2241,comment,412575863,"IMO, the JMS pool from 5.x should not be migrated to Artemis.  It belongs in it's own project with it's own release cycle.  Also, it makes sense for it to *not* be in the ActiveMQ project to make clear that the pool is generic and isn't tied to any ActiveMQ broker.
The pool on messaginghub has JMS 2.0 support.",architecture_debt,violation_of_modularity
