diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertex.java
index 9d93d04a190c0..407683f16b45c 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertex.java
@@ -24,12 +24,11 @@
 import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
 import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
 
-import javax.xml.ws.Provider;
-
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
+import java.util.function.Supplier;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
@@ -46,17 +45,17 @@ public class DefaultExecutionVertex implements SchedulingExecutionVertex {
 
 	private final InputDependencyConstraint inputDependencyConstraint;
 
-	private final Provider<ExecutionState> stateProvider;
+	private final Supplier<ExecutionState> stateSupplier;
 
 	public DefaultExecutionVertex(
 			ExecutionVertexID executionVertexId,
 			List<SchedulingResultPartition> producedPartitions,
 			InputDependencyConstraint dependencyConstraint,
-			Provider<ExecutionState> stateProvider) {
+			Supplier<ExecutionState> stateSupplier) {
 		this.executionVertexId = checkNotNull(executionVertexId);
 		this.inputDependencyConstraint = checkNotNull(dependencyConstraint);
 		this.consumedPartitions = new ArrayList<>();
-		this.stateProvider = checkNotNull(stateProvider);
+		this.stateSupplier = checkNotNull(stateSupplier);
 		this.producedPartitions = checkNotNull(producedPartitions);
 	}
 
@@ -67,7 +66,7 @@ public ExecutionVertexID getId() {
 
 	@Override
 	public ExecutionState getState() {
-		return stateProvider.invoke(null);
+		return stateSupplier.get();
 	}
 
 	@Override
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java
index 6e58529b998cf..ab47800909e83 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java
@@ -29,9 +29,6 @@
 import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
 import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
 import org.apache.flink.runtime.scheduler.strategy.SchedulingTopology;
-import org.apache.flink.util.Preconditions;
-
-import javax.xml.ws.Provider;
 
 import java.util.ArrayList;
 import java.util.Collections;
@@ -39,6 +36,9 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Optional;
+import java.util.function.Supplier;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
  * Adapter of {@link ExecutionGraph} to {@link SchedulingTopology}.
@@ -52,7 +52,7 @@ public class ExecutionGraphToSchedulingTopologyAdapter implements SchedulingTopo
 	private final Map<IntermediateResultPartitionID, DefaultResultPartition> resultPartitions;
 
 	public ExecutionGraphToSchedulingTopologyAdapter(ExecutionGraph graph) {
-		Preconditions.checkNotNull(graph, "execution graph can not be null");
+		checkNotNull(graph, "execution graph can not be null");
 
 		final int totalVertexCnt = graph.getTotalNumberOfVertices();
 		int totalPartitionCnt = 0;
@@ -78,7 +78,7 @@ public ExecutionGraphToSchedulingTopologyAdapter(ExecutionGraph graph) {
 				new ExecutionVertexID(vertex.getJobvertexId(), vertex.getParallelSubtaskIndex()),
 				schedulingPartitions,
 				vertex.getInputDependencyConstraint(),
-				new ExecutionStateProvider(vertex));
+				new ExecutionStateSupplier(vertex));
 			this.executionVertices.put(scheduleVertex.getId(), scheduleVertex);
 			verticesList.add(scheduleVertex);
 			executionVertexMap.put(vertex, scheduleVertex);
@@ -120,16 +120,16 @@ public Optional<SchedulingResultPartition> getResultPartition(IntermediateResult
 		return Optional.ofNullable(resultPartitions.get(intermediateResultPartitionId));
 	}
 
-	private static class ExecutionStateProvider implements Provider<ExecutionState> {
+	private static class ExecutionStateSupplier implements Supplier<ExecutionState> {
 
 		private final ExecutionVertex executionVertex;
 
-		ExecutionStateProvider(ExecutionVertex vertex) {
-			executionVertex = vertex;
+		ExecutionStateSupplier(ExecutionVertex vertex) {
+			executionVertex = checkNotNull(vertex);
 		}
 
 		@Override
-		public ExecutionState invoke(ExecutionState request) {
+		public ExecutionState get() {
 			return executionVertex.getExecutionState();
 		}
 	}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertexTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertexTest.java
index 7746238509c99..41546633dd9e8 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertexTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultExecutionVertexTest.java
@@ -23,97 +23,83 @@
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
-import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
 import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
 import org.apache.flink.util.TestLogger;
 
 import org.junit.Before;
 import org.junit.Test;
 
-import javax.xml.ws.Provider;
-
-import java.util.ArrayList;
-import java.util.Collection;
 import java.util.Collections;
-import java.util.List;
-import java.util.stream.Collectors;
+import java.util.function.Supplier;
 
 import static org.apache.flink.api.common.InputDependencyConstraint.ALL;
 import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.BLOCKING;
-import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertThat;
 
 /**
  * Unit tests for {@link DefaultExecutionVertex}.
  */
 public class DefaultExecutionVertexTest extends TestLogger {
 
-	private final ExecutionStateProviderTest stateProvider = new ExecutionStateProviderTest();
+	private final TestExecutionStateSupplier stateSupplier = new TestExecutionStateSupplier();
+
+	private DefaultExecutionVertex producerVertex;
 
-	private List<SchedulingExecutionVertex> schedulingExecutionVertices;
+	private DefaultExecutionVertex consumerVertex;
 
 	private IntermediateResultPartitionID intermediateResultPartitionId;
 
 	@Before
 	public void setUp() throws Exception {
 
-		schedulingExecutionVertices = new ArrayList<>(2);
 		intermediateResultPartitionId = new IntermediateResultPartitionID();
 
 		DefaultResultPartition schedulingResultPartition = new DefaultResultPartition(
 			intermediateResultPartitionId,
 			new IntermediateDataSetID(),
 			BLOCKING);
-		DefaultExecutionVertex schedulingVertex1 = new DefaultExecutionVertex(
+		producerVertex = new DefaultExecutionVertex(
 			new ExecutionVertexID(new JobVertexID(), 0),
 			Collections.singletonList(schedulingResultPartition),
 			ALL,
-			stateProvider);
-		schedulingResultPartition.setProducer(schedulingVertex1);
-		DefaultExecutionVertex schedulingVertex2 = new DefaultExecutionVertex(
+			stateSupplier);
+		schedulingResultPartition.setProducer(producerVertex);
+		consumerVertex = new DefaultExecutionVertex(
 			new ExecutionVertexID(new JobVertexID(), 0),
 			Collections.emptyList(),
 			ALL,
-			stateProvider);
-		schedulingVertex2.addConsumedPartition(schedulingResultPartition);
-		schedulingExecutionVertices.add(schedulingVertex1);
-		schedulingExecutionVertices.add(schedulingVertex2);
+			stateSupplier);
+		consumerVertex.addConsumedPartition(schedulingResultPartition);
 	}
 
 	@Test
 	public void testGetExecutionState() {
-		final ExecutionState[] states = ExecutionState.values();
-		for (ExecutionState state : states) {
-			for (SchedulingExecutionVertex srp : schedulingExecutionVertices) {
-				stateProvider.setExecutionState(state);
-				assertEquals(state, srp.getState());
-			}
+		for (ExecutionState state : ExecutionState.values()) {
+			stateSupplier.setExecutionState(state);
+			assertEquals(state, producerVertex.getState());
 		}
 	}
 
 	@Test
 	public void testGetProducedResultPartitions() {
-		Collection<IntermediateResultPartitionID> partitionIds1 =  schedulingExecutionVertices
-			.get(0).getProducedResultPartitions().stream().map(SchedulingResultPartition::getId)
-			.collect(Collectors.toList());
-		List<IntermediateResultPartitionID> partitionIds2 = Collections.singletonList(intermediateResultPartitionId);
-		assertThat(partitionIds1, containsInAnyOrder(partitionIds2.toArray()));
+		IntermediateResultPartitionID partitionIds1 = producerVertex
+			.getProducedResultPartitions().stream().findAny().map(SchedulingResultPartition::getId)
+			.orElseThrow(() -> new IllegalArgumentException("can not find result partition"));
+		assertEquals(partitionIds1, intermediateResultPartitionId);
 	}
 
 	@Test
 	public void testGetConsumedResultPartitions() {
-			Collection<IntermediateResultPartitionID> partitionIds1 = schedulingExecutionVertices
-				.get(1).getConsumedResultPartitions().stream().map(SchedulingResultPartition::getId)
-				.collect(Collectors.toList());
-			List<IntermediateResultPartitionID> partitionIds2 = Collections.singletonList(intermediateResultPartitionId);
-			assertThat(partitionIds1, containsInAnyOrder(partitionIds2.toArray()));
+		IntermediateResultPartitionID partitionIds1 = consumerVertex
+			.getConsumedResultPartitions().stream().findAny().map(SchedulingResultPartition::getId)
+			.orElseThrow(() -> new IllegalArgumentException("can not find result partition"));
+		assertEquals(partitionIds1, intermediateResultPartitionId);
 	}
 
 	/**
-	 * A simple implementation of {@link Provider} for testing.
+	 * A simple implementation of {@link Supplier} for testing.
 	 */
-	public static class ExecutionStateProviderTest implements Provider<ExecutionState> {
+	static class TestExecutionStateSupplier implements Supplier<ExecutionState> {
 
 		private ExecutionState executionState;
 
@@ -122,7 +108,7 @@ void setExecutionState(ExecutionState state) {
 		}
 
 		@Override
-		public ExecutionState invoke(ExecutionState request) {
+		public ExecutionState get() {
 			return executionState;
 		}
 	}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartitionTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartitionTest.java
index 702e8f527dbc0..a745ada3b776b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartitionTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultResultPartitionTest.java
@@ -30,11 +30,7 @@
 import org.junit.Before;
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Collection;
 import java.util.Collections;
-import java.util.List;
-import java.util.stream.Collectors;
 
 import static org.apache.flink.api.common.InputDependencyConstraint.ALL;
 import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.BLOCKING;
@@ -42,63 +38,59 @@
 import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.EMPTY;
 import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.PRODUCING;
 import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.RELEASED;
-import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertThat;
 
 /**
  * Unit tests for {@link DefaultResultPartition}.
  */
 public class DefaultResultPartitionTest extends TestLogger {
 
-	private final DefaultExecutionVertexTest.ExecutionStateProviderTest stateProvider = new DefaultExecutionVertexTest.ExecutionStateProviderTest();
+	private final DefaultExecutionVertexTest.TestExecutionStateSupplier stateProvider =
+		new DefaultExecutionVertexTest.TestExecutionStateSupplier();
 
-	private List<SchedulingExecutionVertex> schedulingExecutionVertices;
+	private SchedulingExecutionVertex producerVertex;
+
+	private SchedulingExecutionVertex consumerVertex;
 
 	private DefaultResultPartition resultPartition;
 
 	@Before
 	public void setUp() {
-		schedulingExecutionVertices = new ArrayList<>(2);
 		resultPartition = new DefaultResultPartition(
 			new IntermediateResultPartitionID(),
 			new IntermediateDataSetID(),
 			BLOCKING);
 
-		DefaultExecutionVertex vertex1 = new DefaultExecutionVertex(
+		producerVertex = new DefaultExecutionVertex(
 			new ExecutionVertexID(new JobVertexID(), 0),
 			Collections.singletonList(resultPartition),
 			ALL,
 			stateProvider);
-		resultPartition.setProducer(vertex1);
-		DefaultExecutionVertex vertex2 = new DefaultExecutionVertex(
+		resultPartition.setProducer(producerVertex);
+		consumerVertex = new DefaultExecutionVertex(
 			new ExecutionVertexID(new JobVertexID(), 0),
 			java.util.Collections.emptyList(),
 			ALL,
 			stateProvider);
-		resultPartition.addConsumer(vertex2);
-		schedulingExecutionVertices.add(vertex1);
-		schedulingExecutionVertices.add(vertex2);
+		resultPartition.addConsumer(consumerVertex);
 	}
 
 	@Test
 	public void testGetConsumers() {
-		Collection<ExecutionVertexID> schedulingConsumers = resultPartition.getConsumers()
-			.stream().map(SchedulingExecutionVertex::getId).collect(Collectors.toList());
-
-		List<ExecutionVertexID> executionConsumers = Collections.singletonList(schedulingExecutionVertices.get(1).getId());
-		assertThat(schedulingConsumers, containsInAnyOrder(executionConsumers.toArray()));
+		ExecutionVertexID schedulingConsumers = resultPartition.getConsumers()
+			.stream().findAny().map(SchedulingExecutionVertex::getId)
+			.orElseThrow(() -> new IllegalArgumentException("can not find vertex"));
+		assertEquals(schedulingConsumers, consumerVertex.getId());
 	}
 
 	@Test
 	public void testGetProducer() {
-		assertEquals(resultPartition.getProducer().getId(), schedulingExecutionVertices.get(0).getId());
+		assertEquals(resultPartition.getProducer().getId(), producerVertex.getId());
 	}
 
 	@Test
 	public void testGetPartitionState() {
-		final ExecutionState[] states = ExecutionState.values();
-		for (ExecutionState state : states) {
+		for (ExecutionState state : ExecutionState.values()) {
 			stateProvider.setExecutionState(state);
 			SchedulingResultPartition.ResultPartitionState partitionState = resultPartition.getState();
 			switch (state) {
