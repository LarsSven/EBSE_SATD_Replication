diff --git a/flink-python/pyflink/table/environment_settings.py b/flink-python/pyflink/table/environment_settings.py
index cbc19b0877125..d6fda402d32a2 100644
--- a/flink-python/pyflink/table/environment_settings.py
+++ b/flink-python/pyflink/table/environment_settings.py
@@ -45,7 +45,6 @@ class Builder(object):
         def __init__(self):
             gateway = get_gateway()
             self._j_builder = gateway.jvm.EnvironmentSettings.Builder()
-            self._is_blink_planner = False
 
         def use_old_planner(self):
             """
@@ -57,7 +56,6 @@ def use_old_planner(self):
             :rtype: EnvironmentSettings.Builder
             """
             self._j_builder = self._j_builder.useOldPlanner()
-            self._is_blink_planner = False
             return self
 
         def use_blink_planner(self):
@@ -69,7 +67,6 @@ def use_blink_planner(self):
             :rtype: EnvironmentSettings.Builder
             """
             self._j_builder = self._j_builder.useBlinkPlanner()
-            self._is_blink_planner = True
             return self
 
         def use_any_planner(self):
@@ -153,11 +150,10 @@ def build(self):
             :return: an immutable instance of EnvironmentSettings.
             :rtype: EnvironmentSettings
             """
-            return EnvironmentSettings(self._j_builder.build(), self._is_blink_planner)
+            return EnvironmentSettings(self._j_builder.build())
 
-    def __init__(self, j_environment_settings, is_blink_planner):
+    def __init__(self, j_environment_settings):
         self._j_environment_settings = j_environment_settings
-        self._is_blink_planner = is_blink_planner
 
     def get_built_in_catalog_name(self):
         """
@@ -189,15 +185,6 @@ def is_streaming_mode(self):
         """
         return self._j_environment_settings.isStreamingMode()
 
-    def is_blink_planner(self):
-        """
-        Tells if the :class:`~pyflink.table.TableEnvironment` works in blink planner.
-
-        :return: True if the TableEnvironment works in blink planner, false otherwise.
-        :rtype: bool
-        """
-        return self._is_blink_planner
-
     @staticmethod
     def new_instance():
         """
diff --git a/flink-python/pyflink/table/table_environment.py b/flink-python/pyflink/table/table_environment.py
index 934a9eb7f6173..dccbd82a5b850 100644
--- a/flink-python/pyflink/table/table_environment.py
+++ b/flink-python/pyflink/table/table_environment.py
@@ -47,9 +47,12 @@ class TableEnvironment(object):
 
     __metaclass__ = ABCMeta
 
-    def __init__(self, j_tenv, is_blink_planner=False, serializer=PickleSerializer()):
+    def __init__(self, j_tenv, serializer=PickleSerializer()):
         self._j_tenv = j_tenv
-        self._is_blink_planner = is_blink_planner
+        j_planner_class = j_tenv.getPlanner().getClass()
+        j_blink_planner_class = get_java_class(
+            get_gateway().jvm.org.apache.flink.table.planner.delegation.PlannerBase)
+        self._is_blink_planner = j_blink_planner_class.isAssignableFrom(j_planner_class)
         self._serializer = serializer
 
     def from_table_source(self, table_source):
@@ -737,9 +740,9 @@ def _get_execution_config(self, filename, schema):
 
 class StreamTableEnvironment(TableEnvironment):
 
-    def __init__(self, j_tenv, is_blink_planner=False):
+    def __init__(self, j_tenv):
         self._j_tenv = j_tenv
-        super(StreamTableEnvironment, self).__init__(j_tenv, is_blink_planner)
+        super(StreamTableEnvironment, self).__init__(j_tenv)
 
     def _get_execution_config(self, filename, schema):
         return self._j_tenv.execEnv().getConfig()
@@ -827,7 +830,6 @@ def create(stream_execution_environment, table_config=None, environment_settings
                              "'environment_settings' cannot be used at the same time")
 
         gateway = get_gateway()
-        is_blink_planner = False
         if table_config is not None:
             j_tenv = gateway.jvm.StreamTableEnvironment.create(
                 stream_execution_environment._j_stream_execution_environment,
@@ -839,18 +841,17 @@ def create(stream_execution_environment, table_config=None, environment_settings
             j_tenv = gateway.jvm.StreamTableEnvironment.create(
                 stream_execution_environment._j_stream_execution_environment,
                 environment_settings._j_environment_settings)
-            is_blink_planner = environment_settings.is_blink_planner()
         else:
             j_tenv = gateway.jvm.StreamTableEnvironment.create(
                 stream_execution_environment._j_stream_execution_environment)
-        return StreamTableEnvironment(j_tenv, is_blink_planner)
+        return StreamTableEnvironment(j_tenv)
 
 
 class BatchTableEnvironment(TableEnvironment):
 
-    def __init__(self, j_tenv, is_blink_planner=False):
+    def __init__(self, j_tenv):
         self._j_tenv = j_tenv
-        super(BatchTableEnvironment, self).__init__(j_tenv, is_blink_planner)
+        super(BatchTableEnvironment, self).__init__(j_tenv)
 
     def _get_execution_config(self, filename, schema):
         gateway = get_gateway()
@@ -986,4 +987,4 @@ def create(execution_environment=None, table_config=None, environment_settings=N
                                  "set to batch mode.")
             j_tenv = gateway.jvm.TableEnvironment.create(
                 environment_settings._j_environment_settings)
-            return BatchTableEnvironment(j_tenv, environment_settings.is_blink_planner())
+            return BatchTableEnvironment(j_tenv)
diff --git a/flink-python/pyflink/testing/test_case_utils.py b/flink-python/pyflink/testing/test_case_utils.py
index b8ffe0c02b759..af1bbb45f555a 100644
--- a/flink-python/pyflink/testing/test_case_utils.py
+++ b/flink-python/pyflink/testing/test_case_utils.py
@@ -151,7 +151,7 @@ def collect(self, table):
 
 class PyFlinkBlinkStreamTableTestCase(PyFlinkTestCase):
     """
-    Base class for stream unit tests.
+    Base class for stream unit tests of blink planner.
     """
 
     def setUp(self):
@@ -164,6 +164,9 @@ def setUp(self):
 
 
 class PyFlinkBlinkBatchTableTestCase(PyFlinkTestCase):
+    """
+    Base class for batch unit tests of blink planner.
+    """
 
     def setUp(self):
         super(PyFlinkBlinkBatchTableTestCase, self).setUp()
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
index 284aab7068833..668cdf60e1e8b 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
@@ -19,15 +19,18 @@
 package org.apache.flink.table.planner.plan.nodes.common
 
 import org.apache.calcite.plan.RelOptCluster
-import org.apache.calcite.rex.{RexCall, RexInputRef, RexLiteral, RexNode, RexProgram}
+import org.apache.calcite.rel.`type`.RelDataType
+import org.apache.calcite.rex._
 import org.apache.calcite.sql.`type`.SqlTypeName
 import org.apache.flink.api.dag.Transformation
 import org.apache.flink.streaming.api.operators.OneInputStreamOperator
 import org.apache.flink.streaming.api.transformations.OneInputTransformation
+import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.dataformat.BaseRow
 import org.apache.flink.table.functions.FunctionLanguage
 import org.apache.flink.table.functions.python.{PythonFunction, PythonFunctionInfo, SimplePythonFunction}
-import org.apache.flink.table.planner.calcite.{FlinkTypeFactory, FlinkTypeSystem}
+import org.apache.flink.table.planner.calcite.FlinkTypeFactory
+import org.apache.flink.table.planner.codegen.{CalcCodeGenerator, CodeGeneratorContext}
 import org.apache.flink.table.planner.functions.utils.ScalarSqlFunction
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.PYTHON_SCALAR_FUNCTION_OPERATOR_NAME
 import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
@@ -43,7 +46,7 @@ trait CommonPythonCalc {
     clazz.getMethod("convertLiteralToPython", classOf[RexLiteral], classOf[SqlTypeName])
   }
 
-  private[flink] def extractPythonScalarFunctionInfos(
+  private def extractPythonScalarFunctionInfos(
       rexCalls: Array[RexCall]): (Array[Int], Array[PythonFunctionInfo]) = {
     // using LinkedHashMap to keep the insert order
     val inputNodes = new mutable.LinkedHashMap[RexNode, Integer]()
@@ -56,7 +59,7 @@ trait CommonPythonCalc {
     (udfInputOffsets, pythonFunctionInfos)
   }
 
-  private[flink] def createPythonScalarFunctionInfo(
+  private def createPythonScalarFunctionInfo(
       rexCall: RexCall,
       inputNodes: mutable.Map[RexNode, Integer]): PythonFunctionInfo = rexCall.getOperator match {
     case sfc: ScalarSqlFunction if sfc.scalarFunction.getLanguage == FunctionLanguage.PYTHON =>
@@ -91,7 +94,7 @@ trait CommonPythonCalc {
       new PythonFunctionInfo(pythonFunction, inputs.toArray)
   }
 
-  private[flink] def getPythonScalarFunctionOperator(
+  private def getPythonScalarFunctionOperator(
       inputRowTypeInfo: BaseRowTypeInfo,
       outputRowTypeInfo: BaseRowTypeInfo,
       udfInputOffsets: Array[Int],
@@ -113,10 +116,10 @@ trait CommonPythonCalc {
       .asInstanceOf[OneInputStreamOperator[BaseRow, BaseRow]]
   }
 
-  private [flink] def generatePythonOneInputStream(
-    inputTransform: Transformation[BaseRow],
-    calcProgram: RexProgram,
-    name: String) = {
+  private def createPythonOneInputTransformation(
+      inputTransform: Transformation[BaseRow],
+      calcProgram: RexProgram,
+      name: String) = {
     val pythonRexCalls = calcProgram.getProjectList
       .map(calcProgram.expandLocalRef)
       .filter(_.isInstanceOf[RexCall])
@@ -147,14 +150,17 @@ trait CommonPythonCalc {
 
     val inputLogicalTypes =
       inputTransform.getOutputType.asInstanceOf[BaseRowTypeInfo].getLogicalTypes
-    val pythonOperatorInputTypeInfo = new BaseRowTypeInfo(inputLogicalTypes: _*)
+    val pythonOperatorInputTypeInfo = inputTransform.getOutputType.asInstanceOf[BaseRowTypeInfo]
     val pythonOperatorResultTyeInfo = new BaseRowTypeInfo(
       forwardedFields.map(inputLogicalTypes(_)) ++
         pythonRexCalls.map(node => FlinkTypeFactory.toLogicalType(node.getType)): _*)
 
     val pythonOperator = getPythonScalarFunctionOperator(
-      pythonOperatorInputTypeInfo, pythonOperatorResultTyeInfo,
-      pythonUdfInputOffsets, pythonFunctionInfos, forwardedFields)
+      pythonOperatorInputTypeInfo,
+      pythonOperatorResultTyeInfo,
+      pythonUdfInputOffsets,
+      pythonFunctionInfos,
+      forwardedFields)
 
     val pythonInputTransform = new OneInputTransformation(
       inputTransform,
@@ -166,16 +172,70 @@ trait CommonPythonCalc {
     (pythonInputTransform, pythonOperatorResultTyeInfo, resultProjectList)
   }
 
-  private [flink] def createProjectionRexProgram(
-     inputRowType: RowType,
-     outputRowType: RowType,
-     projectList: mutable.Buffer[RexNode],
-     cluster: RelOptCluster) = {
-    val factory = new FlinkTypeFactory(new FlinkTypeSystem)
+  private def createProjectionRexProgram(
+      inputRowType: RowType,
+      outputRelData: RelDataType,
+      projectList: mutable.Buffer[RexNode],
+      cluster: RelOptCluster) = {
+    val factory = cluster.getTypeFactory.asInstanceOf[FlinkTypeFactory]
     val inputRelData = factory.createFieldTypeFromLogicalType(inputRowType)
-    val outputRelData = factory.createFieldTypeFromLogicalType(outputRowType)
     RexProgram.create(inputRelData, projectList, null, outputRelData, cluster.getRexBuilder)
   }
+
+  protected def createOneInputTransformation(
+      inputTransform: Transformation[BaseRow],
+      inputsContainSingleton: Boolean,
+      calcProgram: RexProgram,
+      name: String,
+      config : TableConfig,
+      ctx : CodeGeneratorContext,
+      cluster: RelOptCluster,
+      rowType: RelDataType,
+      opName: String): OneInputTransformation[BaseRow, BaseRow] = {
+    val (pythonInputTransform, pythonOperatorResultTyeInfo, resultProjectList) =
+      createPythonOneInputTransformation(inputTransform, calcProgram, name)
+
+    if (inputsContainSingleton) {
+      pythonInputTransform.setParallelism(1)
+      pythonInputTransform.setMaxParallelism(1)
+    }
+
+    val onlyFilter = resultProjectList.zipWithIndex.forall { case (rexNode, index) =>
+      rexNode.isInstanceOf[RexInputRef] && rexNode.asInstanceOf[RexInputRef].getIndex == index
+    }
+
+    if (onlyFilter) {
+      pythonInputTransform
+    } else {
+      val outputType = FlinkTypeFactory.toLogicalRowType(rowType)
+      val rexProgram = createProjectionRexProgram(
+        pythonOperatorResultTyeInfo.toRowType, rowType, resultProjectList, cluster)
+      val substituteOperator = CalcCodeGenerator.generateCalcOperator(
+        ctx,
+        cluster,
+        pythonInputTransform,
+        outputType,
+        config,
+        rexProgram,
+        None,
+        retainHeader = true,
+        opName
+      )
+
+      val ret = new OneInputTransformation(
+        pythonInputTransform,
+        name,
+        substituteOperator,
+        BaseRowTypeInfo.of(outputType),
+        pythonInputTransform.getParallelism)
+
+      if (inputsContainSingleton) {
+        ret.setParallelism(1)
+        ret.setMaxParallelism(1)
+      }
+      ret
+    }
+  }
 }
 
 object CommonPythonCalc {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalcBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalcBase.scala
index f684f6d73fb82..47f3fa7954498 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalcBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalcBase.scala
@@ -46,8 +46,8 @@ abstract class BatchExecCalcBase(
     calcProgram: RexProgram,
     outputRowType: RelDataType)
   extends CommonCalc(cluster, traitSet, inputRel, calcProgram)
-    with BatchPhysicalRel
-    with BatchExecNode[BaseRow] {
+  with BatchPhysicalRel
+  with BatchExecNode[BaseRow] {
 
   override def deriveRowType(): RelDataType = outputRowType
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecPythonCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecPythonCalc.scala
index d5e8083cd493d..7adf0ee9cc9cd 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecPythonCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecPythonCalc.scala
@@ -22,16 +22,12 @@ import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.core.Calc
-import org.apache.calcite.rex.{RexInputRef, RexProgram}
+import org.apache.calcite.rex.RexProgram
 import org.apache.flink.api.dag.Transformation
-import org.apache.flink.streaming.api.transformations.OneInputTransformation
 import org.apache.flink.table.dataformat.BaseRow
-import org.apache.flink.table.planner.calcite.{FlinkTypeFactory, FlinkTypeSystem}
-import org.apache.flink.table.planner.codegen.{CalcCodeGenerator, CodeGeneratorContext}
+import org.apache.flink.table.planner.codegen.CodeGeneratorContext
 import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc
-import org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
-import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
 
 /**
   * Batch physical RelNode for Python ScalarFunctions.
@@ -57,40 +53,17 @@ class BatchExecPythonCalc(
   override protected def translateToPlanInternal(planner: BatchPlanner): Transformation[BaseRow] = {
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[BaseRow]]
-
-    val (pythonInputTransform, pythonOperatorResultTyeInfo, resultProjectList) =
-      generatePythonOneInputStream(inputTransform, calcProgram, getRelDetailedDescription)
-
-    val onlyFilter = resultProjectList.zipWithIndex.forall { case (rexNode, index) =>
-      rexNode.isInstanceOf[RexInputRef] && rexNode.asInstanceOf[RexInputRef].getIndex == index
-    }
-
-    if (onlyFilter) {
-      pythonInputTransform
-    } else {
-      val config = planner.getTableConfig
-      val ctx = CodeGeneratorContext(config).setOperatorBaseClass(
-        classOf[AbstractProcessStreamOperator[BaseRow]])
-      val outputType = FlinkTypeFactory.toLogicalRowType(getRowType)
-      val rexProgram = createProjectionRexProgram(
-        pythonOperatorResultTyeInfo.toRowType, outputType, resultProjectList, cluster)
-      val operator = CalcCodeGenerator.generateCalcOperator(
-        ctx,
-        cluster,
-        pythonInputTransform,
-        outputType,
-        config,
-        rexProgram,
-        None,
-        opName = "BatchCalc"
-      )
-
-      new OneInputTransformation(
-        pythonInputTransform,
-        getRelDetailedDescription,
-        operator,
-        BaseRowTypeInfo.of(outputType),
-        inputTransform.getParallelism)
-    }
+    val config = planner.getTableConfig
+    val ctx = CodeGeneratorContext(config)
+    createOneInputTransformation(
+      inputTransform,
+      inputsContainSingleton = false,
+      calcProgram,
+      getRelDetailedDescription,
+      config,
+      ctx,
+      cluster,
+      getRowType,
+    "BatchExecCalc")
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecPythonCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecPythonCalc.scala
index d91c6b8c1b57d..0bebc5ea36909 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecPythonCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecPythonCalc.scala
@@ -22,16 +22,13 @@ import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.core.Calc
-import org.apache.calcite.rex.{RexInputRef, RexProgram}
+import org.apache.calcite.rex.RexProgram
 import org.apache.flink.api.dag.Transformation
-import org.apache.flink.streaming.api.transformations.OneInputTransformation
 import org.apache.flink.table.dataformat.BaseRow
-import org.apache.flink.table.planner.calcite.FlinkTypeFactory
-import org.apache.flink.table.planner.codegen.{CalcCodeGenerator, CodeGeneratorContext}
+import org.apache.flink.table.planner.codegen.CodeGeneratorContext
 import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc
 import org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
-import org.apache.flink.table.runtime.typeutils.BaseRowTypeInfo
 
 /**
   * Stream physical RelNode for Python ScalarFunctions.
@@ -58,48 +55,18 @@ class StreamExecPythonCalc(
       planner: StreamPlanner): Transformation[BaseRow] = {
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[BaseRow]]
-
-    val (pythonInputTransform, pythonOperatorResultTyeInfo, resultProjectList) =
-      generatePythonOneInputStream(inputTransform, calcProgram, getRelDetailedDescription)
-
-    val onlyFilter = resultProjectList.zipWithIndex.forall { case (rexNode, index) =>
-      rexNode.isInstanceOf[RexInputRef] && rexNode.asInstanceOf[RexInputRef].getIndex == index
-    }
-
-    if (inputsContainSingleton()) {
-      pythonInputTransform.setParallelism(1)
-      pythonInputTransform.setMaxParallelism(1)
-    }
-
-    if (onlyFilter) {
-      pythonInputTransform
-    } else {
-      val config = planner.getTableConfig
-      val ctx = CodeGeneratorContext(config).setOperatorBaseClass(
-        classOf[AbstractProcessStreamOperator[BaseRow]])
-      val outputType = FlinkTypeFactory.toLogicalRowType(getRowType)
-      val rexProgram = createProjectionRexProgram(
-        pythonOperatorResultTyeInfo.toRowType, outputType, resultProjectList, cluster)
-      val substituteStreamOperator = CalcCodeGenerator.generateCalcOperator(
-        ctx,
-        cluster,
-        pythonInputTransform,
-        outputType,
-        config,
-        rexProgram,
-        None,
-        retainHeader = true,
-        "StreamExecCalc"
-      )
-
-      val ret = new OneInputTransformation(
-        pythonInputTransform,
-        getRelDetailedDescription,
-        substituteStreamOperator,
-        BaseRowTypeInfo.of(outputType),
-        inputTransform.getParallelism)
-
-      ret
-    }
+    val config = planner.getTableConfig
+    val ctx = CodeGeneratorContext(config).setOperatorBaseClass(
+      classOf[AbstractProcessStreamOperator[BaseRow]])
+    createOneInputTransformation(
+      inputTransform,
+      inputsContainSingleton(),
+      calcProgram,
+      getRelDetailedDescription,
+      config,
+      ctx,
+      cluster,
+      getRowType,
+      "StreamExecCalc")
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/utils/JavaUserDefinedScalarFunctions.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/utils/JavaUserDefinedScalarFunctions.java
index f3284d0142418..afb992b3e8255 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/utils/JavaUserDefinedScalarFunctions.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/utils/JavaUserDefinedScalarFunctions.java
@@ -18,9 +18,12 @@
 
 package org.apache.flink.table.planner.runtime.utils;
 
+import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.functions.FunctionContext;
+import org.apache.flink.table.functions.FunctionLanguage;
 import org.apache.flink.table.functions.ScalarFunction;
 
 import java.util.Arrays;
@@ -154,4 +157,63 @@ public boolean isDeterministic() {
 		}
 	}
 
+	/**
+	 * Test for Python Scalar Function.
+	 */
+	public static class PythonScalarFunction extends ScalarFunction {
+		private final String name;
+
+		public PythonScalarFunction(String name) {
+			this.name = name;
+		}
+
+		public int eval(int i, int j) {
+			return i + j;
+		}
+
+		@Override
+		public TypeInformation<?> getResultType(Class<?>[] signature) {
+			return BasicTypeInfo.INT_TYPE_INFO;
+		}
+
+		@Override
+		public FunctionLanguage getLanguage() {
+			return FunctionLanguage.PYTHON;
+		}
+
+		@Override
+		public String toString() {
+			return name;
+		}
+	}
+
+	/**
+	 * Test for Python Scalar Function.
+	 */
+	public static class BooleanPythonScalarFunction extends ScalarFunction {
+		private final String name;
+
+		public BooleanPythonScalarFunction(String name) {
+			this.name = name;
+		}
+
+		public boolean eval(int i, int j) {
+			return i + j > 1;
+		}
+
+		@Override
+		public TypeInformation<?> getResultType(Class<?>[] signature) {
+			return BasicTypeInfo.BOOLEAN_TYPE_INFO;
+		}
+
+		@Override
+		public FunctionLanguage getLanguage() {
+			return FunctionLanguage.PYTHON;
+		}
+
+		@Override
+		public String toString() {
+			return name;
+		}
+	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.xml
new file mode 100644
index 0000000000000..07c422385df06
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.xml
@@ -0,0 +1,55 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to you under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<Root>
+	<TestCase name="testPythonFunctionAsInputOfJavaFunction">
+		<Resource name="sql">
+			<![CDATA[SELECT pyFunc1(a, b) + 1 FROM MyTable]]>
+		</Resource>
+		<Resource name="planBefore">
+			<![CDATA[
+LogicalProject(EXPR$0=[+(pyFunc1($0, $1), 1)])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+		</Resource>
+		<Resource name="planAfter">
+			<![CDATA[
+Calc(select=[+(f0, 1) AS EXPR$0])
++- PythonCalc(select=[pyFunc1(a, b) AS f0])
+   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+]]>
+		</Resource>
+	</TestCase>
+	<TestCase name="testPythonFunctionMixedWithJavaFunction">
+		<Resource name="sql">
+			<![CDATA[SELECT pyFunc1(a, b), c + 1 FROM MyTable]]>
+		</Resource>
+		<Resource name="planBefore">
+			<![CDATA[
+LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+		</Resource>
+		<Resource name="planAfter">
+			<![CDATA[
+Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1])
++- PythonCalc(select=[c, pyFunc1(a, b) AS f0])
+   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+]]>
+		</Resource>
+	</TestCase>
+</Root>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.xml
similarity index 63%
rename from flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.xml
rename to flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.xml
index 3c0f6ddc19976..66a06cd9f5469 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.xml
@@ -6,9 +6,7 @@ this work for additional information regarding copyright ownership.
 The ASF licenses this file to you under the Apache License, Version 2.0
 (the "License"); you may not use this file except in compliance with
 the License.  You may obtain a copy of the License at
-
 http://www.apache.org/licenses/LICENSE-2.0
-
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@@ -28,9 +26,9 @@ LogicalProject(EXPR$0=[+(pyFunc1($0, $1), 1)])
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[+(f0, 1) AS EXPR$0])
-+- PythonCalc(select=[pyFunc1(a, b) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[+(f0, 1) AS EXPR$0])
++- FlinkLogicalCalc(select=[pyFunc1(a, b) AS f0])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
@@ -46,9 +44,9 @@ LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1])
-+- PythonCalc(select=[c, pyFunc1(a, b) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1])
++- FlinkLogicalCalc(select=[c, pyFunc1(a, b) AS f0])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
@@ -65,28 +63,28 @@ LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1], where=[>(f1, 0)])
-+- PythonCalc(select=[c, pyFunc1(a, b) AS f0, pyFunc2(a, c) AS f1])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1], where=[>(f1, 0)])
++- FlinkLogicalCalc(select=[c, pyFunc1(a, b) AS f0, pyFunc2(a, c) AS f1])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
 	<TestCase name="testPythonFunctionInWhereClause">
 		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc2(a, c)]]>
+			<![CDATA[SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc4(a, c)]]>
 		</Resource>
 		<Resource name="planBefore">
 			<![CDATA[
 LogicalProject(EXPR$0=[pyFunc1($0, $1)])
-+- LogicalFilter(condition=[pyFunc2($0, $2)])
++- LogicalFilter(condition=[pyFunc4($0, $2)])
    +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
 ]]>
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[f0 AS EXPR$0], where=[f1])
-+- PythonCalc(select=[pyFunc1(a, b) AS f0, pyFunc2(a, c) AS f1])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[f0 AS EXPR$0], where=[f1])
++- FlinkLogicalCalc(select=[pyFunc1(a, b) AS f0, pyFunc4(a, c) AS f1])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
@@ -102,10 +100,10 @@ LogicalProject(EXPR$0=[pyFunc3(pyFunc2(+($0, pyFunc1($0, $2)), $1), $2)])
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-PythonCalc(select=[pyFunc3(pyFunc2(f0, b), c) AS EXPR$0])
-+- Calc(select=[b, c, +(a, f0) AS f0])
-   +- PythonCalc(select=[b, c, a, pyFunc1(a, c) AS f0])
-      +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[pyFunc3(pyFunc2(f0, b), c) AS EXPR$0])
++- FlinkLogicalCalc(select=[b, c, +(a, f0) AS f0])
+   +- FlinkLogicalCalc(select=[b, c, a, pyFunc1(a, c) AS f0])
+      +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
@@ -121,45 +119,45 @@ LogicalProject(EXPR$0=[pyFunc1($0, $1)])
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-PythonCalc(select=[pyFunc1(a, b) AS EXPR$0])
-+- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[pyFunc1(a, b) AS EXPR$0])
++- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
 	<TestCase name="testOnlyOnePythonFunctionInWhereClause">
 		<Resource name="sql">
-			<![CDATA[SELECT a, b FROM MyTable WHERE pyFunc1(a, c)]]>
+			<![CDATA[SELECT a, b FROM MyTable WHERE pyFunc4(a, c)]]>
 		</Resource>
 		<Resource name="planBefore">
 			<![CDATA[
 LogicalProject(a=[$0], b=[$1])
-+- LogicalFilter(condition=[pyFunc1($0, $2)])
++- LogicalFilter(condition=[pyFunc4($0, $2)])
    +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
 ]]>
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[a, b], where=[f0])
-+- PythonCalc(select=[a, b, pyFunc1(a, c) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+FlinkLogicalCalc(select=[a, b], where=[f0])
++- FlinkLogicalCalc(select=[a, b, pyFunc4(a, c) AS f0])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
 		</Resource>
 	</TestCase>
 	<TestCase name="testFieldNameUniquify">
 		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable]]>
+			<![CDATA[SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable2]]>
 		</Resource>
 		<Resource name="planBefore">
 			<![CDATA[
 LogicalProject(EXPR$0=[pyFunc1($1, $2)], EXPR$1=[+($0, 1)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(f0, f1, f2)]]])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable2, source: [TestTableSource(f0, f1, f2)]]])
 ]]>
 		</Resource>
 		<Resource name="planAfter">
 			<![CDATA[
-Calc(select=[f00 AS EXPR$0, +(f0, 1) AS EXPR$1])
-+- PythonCalc(select=[f0, pyFunc1(f1, f2) AS f00])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
+FlinkLogicalCalc(select=[f00 AS EXPR$0, +(f0, 1) AS EXPR$1])
++- FlinkLogicalCalc(select=[f0, pyFunc1(f1, f2) AS f00])
+   +- FlinkLogicalTableSourceScan(table=[[default_catalog, default_database, MyTable2, source: [TestTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
 ]]>
 		</Resource>
 	</TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.xml
new file mode 100644
index 0000000000000..07c422385df06
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.xml
@@ -0,0 +1,55 @@
+<?xml version="1.0" ?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one or more
+contributor license agreements.  See the NOTICE file distributed with
+this work for additional information regarding copyright ownership.
+The ASF licenses this file to you under the Apache License, Version 2.0
+(the "License"); you may not use this file except in compliance with
+the License.  You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+-->
+<Root>
+	<TestCase name="testPythonFunctionAsInputOfJavaFunction">
+		<Resource name="sql">
+			<![CDATA[SELECT pyFunc1(a, b) + 1 FROM MyTable]]>
+		</Resource>
+		<Resource name="planBefore">
+			<![CDATA[
+LogicalProject(EXPR$0=[+(pyFunc1($0, $1), 1)])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+		</Resource>
+		<Resource name="planAfter">
+			<![CDATA[
+Calc(select=[+(f0, 1) AS EXPR$0])
++- PythonCalc(select=[pyFunc1(a, b) AS f0])
+   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+]]>
+		</Resource>
+	</TestCase>
+	<TestCase name="testPythonFunctionMixedWithJavaFunction">
+		<Resource name="sql">
+			<![CDATA[SELECT pyFunc1(a, b), c + 1 FROM MyTable]]>
+		</Resource>
+		<Resource name="planBefore">
+			<![CDATA[
+LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
++- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+		</Resource>
+		<Resource name="planAfter">
+			<![CDATA[
+Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1])
++- PythonCalc(select=[c, pyFunc1(a, b) AS f0])
+   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+]]>
+		</Resource>
+	</TestCase>
+</Root>
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.xml
deleted file mode 100644
index 3c0f6ddc19976..0000000000000
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.xml
+++ /dev/null
@@ -1,166 +0,0 @@
-<?xml version="1.0" ?>
-<!--
-Licensed to the Apache Software Foundation (ASF) under one or more
-contributor license agreements.  See the NOTICE file distributed with
-this work for additional information regarding copyright ownership.
-The ASF licenses this file to you under the Apache License, Version 2.0
-(the "License"); you may not use this file except in compliance with
-the License.  You may obtain a copy of the License at
-
-http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
--->
-<Root>
-	<TestCase name="testPythonFunctionAsInputOfJavaFunction">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b) + 1 FROM MyTable]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[+(pyFunc1($0, $1), 1)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[+(f0, 1) AS EXPR$0])
-+- PythonCalc(select=[pyFunc1(a, b) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testPythonFunctionMixedWithJavaFunction">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b), c + 1 FROM MyTable]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1])
-+- PythonCalc(select=[c, pyFunc1(a, b) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testPythonFunctionMixedWithJavaFunctionInWhereClause">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b), c + 1 FROM MyTable WHERE pyFunc2(a, c) > 0]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc1($0, $1)], EXPR$1=[+($2, 1)])
-+- LogicalFilter(condition=[>(pyFunc2($0, $2), 0)])
-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[f0 AS EXPR$0, +(c, 1) AS EXPR$1], where=[>(f1, 0)])
-+- PythonCalc(select=[c, pyFunc1(a, b) AS f0, pyFunc2(a, c) AS f1])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testPythonFunctionInWhereClause">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc2(a, c)]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc1($0, $1)])
-+- LogicalFilter(condition=[pyFunc2($0, $2)])
-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[f0 AS EXPR$0], where=[f1])
-+- PythonCalc(select=[pyFunc1(a, b) AS f0, pyFunc2(a, c) AS f1])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testChainingPythonFunction">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc3(pyFunc2(a + pyFunc1(a, c), b), c) FROM MyTable]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc3(pyFunc2(+($0, pyFunc1($0, $2)), $1), $2)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-PythonCalc(select=[pyFunc3(pyFunc2(f0, b), c) AS EXPR$0])
-+- Calc(select=[b, c, +(a, f0) AS f0])
-   +- PythonCalc(select=[b, c, a, pyFunc1(a, c) AS f0])
-      +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testOnlyOnePythonFunction">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(a, b) FROM MyTable]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc1($0, $1)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-PythonCalc(select=[pyFunc1(a, b) AS EXPR$0])
-+- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testOnlyOnePythonFunctionInWhereClause">
-		<Resource name="sql">
-			<![CDATA[SELECT a, b FROM MyTable WHERE pyFunc1(a, c)]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(a=[$0], b=[$1])
-+- LogicalFilter(condition=[pyFunc1($0, $2)])
-   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[a, b], where=[f0])
-+- PythonCalc(select=[a, b, pyFunc1(a, c) AS f0])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
-]]>
-		</Resource>
-	</TestCase>
-	<TestCase name="testFieldNameUniquify">
-		<Resource name="sql">
-			<![CDATA[SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable]]>
-		</Resource>
-		<Resource name="planBefore">
-			<![CDATA[
-LogicalProject(EXPR$0=[pyFunc1($1, $2)], EXPR$1=[+($0, 1)])
-+- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(f0, f1, f2)]]])
-]]>
-		</Resource>
-		<Resource name="planAfter">
-			<![CDATA[
-Calc(select=[f00 AS EXPR$0, +(f0, 1) AS EXPR$1])
-+- PythonCalc(select=[f0, pyFunc1(f1, f2) AS f00])
-   +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(f0, f1, f2)]]], fields=[f0, f1, f2])
-]]>
-		</Resource>
-	</TestCase>
-</Root>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.scala
deleted file mode 100644
index 5ac4a957fd68d..0000000000000
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/BatchPythonScalarFunctionSplitRuleTest.scala
+++ /dev/null
@@ -1,136 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.planner.plan.batch.table
-
-import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
-import org.apache.flink.api.scala._
-import org.apache.flink.table.api.scala._
-import org.apache.flink.table.functions.{FunctionLanguage, ScalarFunction}
-import org.apache.flink.table.planner.utils.TableTestBase
-import org.junit.Test
-
-class BatchPythonScalarFunctionSplitRuleTest extends TableTestBase {
-
-  @Test
-  def testPythonFunctionAsInputOfJavaFunction(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionMixedWithJavaFunction(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionMixedWithJavaFunctionInWhereClause(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new PythonScalarFunction("pyFunc2"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable WHERE pyFunc2(a, c) > 0"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionInWhereClause(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new BooleanPythonScalarFunction("pyFunc2"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc2(a, c)"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testChainingPythonFunction(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new PythonScalarFunction("pyFunc2"))
-    util.addFunction("pyFunc3", new PythonScalarFunction("pyFunc3"))
-
-    val sqlQuery = "SELECT pyFunc3(pyFunc2(a + pyFunc1(a, c), b), c) FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testOnlyOnePythonFunction(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testOnlyOnePythonFunctionInWhereClause(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new BooleanPythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT a, b FROM MyTable WHERE pyFunc1(a, c)"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testFieldNameUniquify(): Unit = {
-    val util = batchTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'f0, 'f1, 'f2)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-}
-
-class PythonScalarFunction(name: String) extends ScalarFunction {
-  def eval(i: Int, j: Int): Int = i + j
-
-  override def getResultType(signature: Array[Class[_]]): TypeInformation[_] =
-    BasicTypeInfo.INT_TYPE_INFO
-
-  override def getLanguage: FunctionLanguage = FunctionLanguage.PYTHON
-
-  override def toString: String = name
-
-}
-
-class BooleanPythonScalarFunction(name: String) extends ScalarFunction {
-  def eval(i: Int, j: Int): Boolean = i + j > 1
-
-  override def getResultType(signature: Array[Class[_]]): TypeInformation[_] =
-    BasicTypeInfo.BOOLEAN_TYPE_INFO
-
-  override def getLanguage: FunctionLanguage = FunctionLanguage.PYTHON
-
-  override def toString: String = name
-}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.scala
new file mode 100644
index 0000000000000..94ecaa28e905f
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/batch/table/PythonCalcTest.scala
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.batch.table
+
+import org.apache.flink.api.scala._
+import org.apache.flink.table.api.scala._
+import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.PythonScalarFunction
+import org.apache.flink.table.planner.utils.TableTestBase
+import org.junit.{Before, Test}
+
+class PythonCalcTest extends TableTestBase {
+  private val util = batchTestUtil()
+
+  @Before
+  def setup(): Unit = {
+    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
+    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
+  }
+
+  @Test
+  def testPythonFunctionAsInputOfJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b) + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testPythonFunctionMixedWithJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.scala
new file mode 100644
index 0000000000000..75e3528f7d331
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/rules/logical/PythonScalarFunctionSplitRuleTest.scala
@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.rules.logical
+
+import org.apache.calcite.plan.hep.HepMatchOrder
+import org.apache.flink.api.scala._
+import org.apache.flink.table.api.scala._
+import org.apache.flink.table.planner.plan.nodes.FlinkConventions
+import org.apache.flink.table.planner.plan.optimize.program._
+import org.apache.flink.table.planner.plan.rules.{FlinkBatchRuleSets, FlinkStreamRuleSets}
+import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.{BooleanPythonScalarFunction, PythonScalarFunction}
+import org.apache.flink.table.planner.utils.TableTestBase
+import org.junit.{Before, Test}
+
+/**
+  * Test for [[PythonScalarFunctionSplitRule]].
+  */
+class PythonScalarFunctionSplitRuleTest extends TableTestBase {
+
+  private val util = batchTestUtil()
+
+  @Before
+  def setup(): Unit = {
+    val programs = new FlinkChainedProgram[BatchOptimizeContext]()
+    programs.addLast(
+      "table_ref",
+      FlinkHepRuleSetProgramBuilder.newBuilder
+        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
+        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
+        .add(FlinkBatchRuleSets.TABLE_REF_RULES)
+        .build())
+    programs.addLast(
+      "logical",
+      FlinkVolcanoProgramBuilder.newBuilder
+        .add(FlinkBatchRuleSets.LOGICAL_OPT_RULES)
+        .setRequiredOutputTraits(Array(FlinkConventions.LOGICAL))
+        .build())
+    programs.addLast(
+      "logical_rewrite",
+      FlinkHepRuleSetProgramBuilder.newBuilder
+        .setHepRulesExecutionType(HEP_RULES_EXECUTION_TYPE.RULE_SEQUENCE)
+        .setHepMatchOrder(HepMatchOrder.BOTTOM_UP)
+        .add(FlinkStreamRuleSets.LOGICAL_REWRITE)
+        .build())
+    util.replaceBatchProgram(programs)
+
+    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
+    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
+    util.addFunction("pyFunc2", new PythonScalarFunction("pyFunc2"))
+    util.addFunction("pyFunc3", new PythonScalarFunction("pyFunc3"))
+    util.addFunction("pyFunc4", new BooleanPythonScalarFunction("pyFunc4"))
+  }
+
+  @Test
+  def testPythonFunctionAsInputOfJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b) + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testPythonFunctionMixedWithJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testPythonFunctionMixedWithJavaFunctionInWhereClause(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable WHERE pyFunc2(a, c) > 0"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testPythonFunctionInWhereClause(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc4(a, c)"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testChainingPythonFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc3(pyFunc2(a + pyFunc1(a, c), b), c) FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testOnlyOnePythonFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testOnlyOnePythonFunctionInWhereClause(): Unit = {
+    val sqlQuery = "SELECT a, b FROM MyTable WHERE pyFunc4(a, c)"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testFieldNameUniquify(): Unit = {
+    util.addTableSource[(Int, Int, Int)]("MyTable2", 'f0, 'f1, 'f2)
+    val sqlQuery = "SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable2"
+    util.verifyPlan(sqlQuery)
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.scala
new file mode 100644
index 0000000000000..23430611a2901
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/PythonCalcTest.scala
@@ -0,0 +1,47 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.planner.plan.stream.table
+
+import org.apache.flink.api.scala._
+import org.apache.flink.table.api.scala._
+import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.PythonScalarFunction
+import org.apache.flink.table.planner.utils.TableTestBase
+import org.junit.{Before, Test}
+
+class PythonCalcTest extends TableTestBase {
+  private val util = streamTestUtil()
+
+  @Before
+  def setup(): Unit = {
+    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
+    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
+  }
+
+  @Test
+  def testPythonFunctionAsInputOfJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b) + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+
+  @Test
+  def testPythonFunctionMixedWithJavaFunction(): Unit = {
+    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable"
+    util.verifyPlan(sqlQuery)
+  }
+}
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.scala
deleted file mode 100644
index b87fc88ba5cbb..0000000000000
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/table/StreamingPythonScalarFunctionSplitRuleTest.scala
+++ /dev/null
@@ -1,136 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.planner.plan.stream.table
-
-import org.apache.flink.api.common.typeinfo.{BasicTypeInfo, TypeInformation}
-import org.apache.flink.api.scala._
-import org.apache.flink.table.api.scala._
-import org.apache.flink.table.functions.{FunctionLanguage, ScalarFunction}
-import org.apache.flink.table.planner.utils.TableTestBase
-import org.junit.Test
-
-class StreamingPythonScalarFunctionSplitRuleTest extends TableTestBase {
-
-  @Test
-  def testPythonFunctionAsInputOfJavaFunction(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionMixedWithJavaFunction(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionMixedWithJavaFunctionInWhereClause(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new PythonScalarFunction("pyFunc2"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b), c + 1 FROM MyTable WHERE pyFunc2(a, c) > 0"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testPythonFunctionInWhereClause(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new BooleanPythonScalarFunction("pyFunc2"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable WHERE pyFunc2(a, c)"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testChainingPythonFunction(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-    util.addFunction("pyFunc2", new PythonScalarFunction("pyFunc2"))
-    util.addFunction("pyFunc3", new PythonScalarFunction("pyFunc3"))
-
-    val sqlQuery = "SELECT pyFunc3(pyFunc2(a + pyFunc1(a, c), b), c) FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testOnlyOnePythonFunction(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(a, b) FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testOnlyOnePythonFunctionInWhereClause(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'a, 'b, 'c)
-    util.addFunction("pyFunc1", new BooleanPythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT a, b FROM MyTable WHERE pyFunc1(a, c)"
-    util.verifyPlan(sqlQuery)
-  }
-
-  @Test
-  def testFieldNameUniquify(): Unit = {
-    val util = streamTestUtil()
-    util.addTableSource[(Int, Int, Int)]("MyTable", 'f0, 'f1, 'f2)
-    util.addFunction("pyFunc1", new PythonScalarFunction("pyFunc1"))
-
-    val sqlQuery = "SELECT pyFunc1(f1, f2), f0 + 1 FROM MyTable"
-    util.verifyPlan(sqlQuery)
-  }
-}
-
-class PythonScalarFunction(name: String) extends ScalarFunction {
-  def eval(i: Int, j: Int): Int = i + j
-
-  override def getResultType(signature: Array[Class[_]]): TypeInformation[_] =
-    BasicTypeInfo.INT_TYPE_INFO
-
-  override def getLanguage: FunctionLanguage = FunctionLanguage.PYTHON
-
-  override def toString: String = name
-
-}
-
-class BooleanPythonScalarFunction(name: String) extends ScalarFunction {
-  def eval(i: Int, j: Int): Boolean = i + j > 1
-
-  override def getResultType(signature: Array[Class[_]]): TypeInformation[_] =
-    BasicTypeInfo.BOOLEAN_TYPE_INFO
-
-  override def getLanguage: FunctionLanguage = FunctionLanguage.PYTHON
-
-  override def toString: String = name
-}
