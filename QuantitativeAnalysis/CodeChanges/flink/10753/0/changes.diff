diff --git a/docs/dev/table/common.zh.md b/docs/dev/table/common.zh.md
index 6c7a3ed67124c..2d3454ec75e59 100644
--- a/docs/dev/table/common.zh.md
+++ b/docs/dev/table/common.zh.md
@@ -1,5 +1,5 @@
 ---
-title: "Concepts & Common API"
+title: "概念与通用 API"
 nav-parent_id: tableapi
 nav-pos: 0
 ---
@@ -22,50 +22,51 @@ specific language governing permissions and limitations
 under the License.
 -->
 
-The Table API and SQL are integrated in a joint API. The central concept of this API is a `Table` which serves as input and output of queries. This document shows the common structure of programs with Table API and SQL queries, how to register a `Table`, how to query a `Table`, and how to emit a `Table`.
+Table API 和 SQL 集成在同一套 API 中。该 API 的中心概念是`表`（Table），用作查询的输入和输出。本文介绍了 Table API 和 SQL 查询程序的通用结构、如何注册`表`、如何查询`表`以及如何发出`表`。
 
 * This will be replaced by the TOC
 {:toc}
 
-Main Differences Between the Two Planners
+两种计划器（Planner）的主要区别
 -----------------------------------------
 
-1. Blink treats batch jobs as a special case of streaming. As such, the conversion between Table and DataSet is also not supported, and batch jobs will not be translated into `DateSet` programs but translated into `DataStream` programs, the same as the streaming jobs.
-2. The Blink planner does not support `BatchTableSource`, use bounded `StreamTableSource` instead of it.
-3. The Blink planner only support the brand new `Catalog` and does not support `ExternalCatalog` which is deprecated.
-4. The implementations of `FilterableTableSource` for the old planner and the Blink planner are incompatible. The old planner will push down `PlannerExpression`s into `FilterableTableSource`, while the Blink planner will push down `Expression`s.
-5. String based key-value config options (Please see the documentation about [Configuration]({{ site.baseurl }}/dev/table/config.html) for details) are only used for the Blink planner.
-6. The implementation(`CalciteConfig`) of `PlannerConfig` in two planners is different.
-7. The Blink planner will optimize multiple-sinks into one DAG (supported only on `TableEnvironment`, not on `StreamTableEnvironment`). The old planner will always optimize each sink into a new DAG, where all DAGs are independent of each other.
-8. The old planner does not support catalog statistics now, while the Blink planner does.
+1. Blink 将批处理作业视作流处理的一种特例。严格来说，表和批数据之间的转换并不受支持，并且批处理作业也不会转换成`批处理`程序而是转换成流处理程序，`流处理`作业也一样。
+2. Blink 计划器不支持  `BatchTableSource`，而是使用有界的  `StreamTableSource` 来替代。
+3. Blink 计划器仅支持全新的 `Catalog` 并且不支持被弃用的 `ExternalCatalog`。
+4. 原版计划器和 Blink 计划器中 `FilterableTableSource` 的实现是不兼容的。原版计划器会将 `PlannerExpression` 下推至 `FilterableTableSource`，而 Blink 计划器则是将 `Expression` 下推。
+5. 基于字符串的键值配置选项仅在 Blink 计划器中使用。（详情参见 [配置]({{ site.baseurl }}/zh/dev/table/config.html) ）
+6. `PlannerConfig` 在两种计划器中的实现（`CalciteConfig`）是不同的。
+7. Blink 计划器会将多sink（multiple-sinks）优化成一张有向无环图（DAG）（仅支持 `TableEnvironment`，不支持 `StreamTableEnvironment`）。原版计划器总是将每个sink都优化成一个新的有向无环图，且所有图相互独立。
+8. 原版计划器目前不支持 catalog 统计，而 Blink 支持。
 
 
-Structure of Table API and SQL Programs
+Table API 和 SQL 程序的结构
 ---------------------------------------
 
-All Table API and SQL programs for batch and streaming follow the same pattern. The following code example shows the common structure of Table API and SQL programs.
+所有用于批处理和流处理的 Table API 和 SQL 程序都遵循相同的模式。下面的代码示例展示了 Table API 和 SQL 程序的通用结构。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
 
-// create a TableEnvironment for specific planner batch or streaming
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 为指定计划器的批处理或流处理作业创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// create a Table
-tableEnv.connect(...).createTemporaryTable("table1");
-// register an output Table
-tableEnv.connect(...).createTemporaryTable("outputTable");
+// 注册表
+tableEnv.registerTable("table1", ...)            // 或者
+tableEnv.registerTableSource("table2", ...);
+// 注册输出表
+tableEnv.registerTableSink("outputTable", ...);
 
-// create a Table object from a Table API query
-Table tapiResult = tableEnv.from("table1").select(...);
-// create a Table object from a SQL query
-Table sqlResult  = tableEnv.sqlQuery("SELECT ... FROM table1 ... ");
+// 根据 Table API 查询结果创建表
+Table tapiResult = tableEnv.scan("table1").select(...);
+// 根据 SQL 查询结果创建表
+Table sqlResult  = tableEnv.sqlQuery("SELECT ... FROM table2 ... ");
 
-// emit a Table API result Table to a TableSink, same for SQL result
+// 发送 Table API 的结果表至 TableSink，SQL 的情形也相同
 tapiResult.insertInto("outputTable");
 
-// execute
+// 执行
 tableEnv.execute("java_job");
 
 {% endhighlight %}
@@ -74,23 +75,24 @@ tableEnv.execute("java_job");
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
 
-// create a TableEnvironment for specific planner batch or streaming
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 为指定计划器的批处理或流处理作业创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// create a Table
-tableEnv.connect(...).createTemporaryTable("table1")
-// register an output Table
-tableEnv.connect(...).createTemporaryTable("outputTable")
+// 注册表
+tableEnv.registerTable("table1", ...)           // 或者
+tableEnv.registerTableSource("table2", ...)
+// 注册输出表
+tableEnv.registerTableSink("outputTable", ...);
 
-// create a Table from a Table API query
-val tapiResult = tableEnv.from("table1").select(...)
-// create a Table from a SQL query
-val sqlResult  = tableEnv.sqlQuery("SELECT ... FROM table1 ...")
+// 根据 Table API 查询结果创建表
+val tapiResult = tableEnv.scan("table1").select(...)
+// 根据 SQL 查询结果创建表
+val sqlResult  = tableEnv.sqlQuery("SELECT ... FROM table2 ...")
 
-// emit a Table API result Table to a TableSink, same for SQL result
+// 发送 Table API 的结果表至 TableSink，SQL 的情形也相同
 tapiResult.insertInto("outputTable")
 
-// execute
+// 执行
 tableEnv.execute("scala_job")
 
 {% endhighlight %}
@@ -99,61 +101,61 @@ tableEnv.execute("scala_job")
 <div data-lang="python" markdown="1">
 {% highlight python %}
 
-# create a TableEnvironment for specific planner batch or streaming
-table_env = ... # see "Create a TableEnvironment" section
+# 为指定计划器的批处理或流处理作业创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-# register a Table
-table_env.connect(...).create_temporary_table("table1")
+# 注册表
+table_env.register_table("table1", ...)           # 或者
+table_env.register_table_source("table2", ...)
 
-# register an output Table
-table_env.connect(...).create_temporary_table("outputTable")
+# 注册输出表
+table_env.register_table_sink("outputTable", ...);
 
-# create a Table from a Table API query
-tapi_result = table_env.from_path("table1").select(...)
-# create a Table from a SQL query
-sql_result  = table_env.sql_query("SELECT ... FROM table1 ...")
+# 根据 Table API 查询结果创建表
+tapi_result = table_env.scan("table1").select(...)
+# 根据 SQL 查询结果创建表
+sql_result  = table_env.sql_query("SELECT ... FROM table2 ...")
 
-# emit a Table API result Table to a TableSink, same for SQL result
+# 发送 Table API 的结果表至 TableSink，SQL 的情形也相同
 tapi_result.insert_into("outputTable")
 
-# execute
+# 执行
 table_env.execute("python_job")
 
 {% endhighlight %}
 </div>
 </div>
 
-**Note:** Table API and SQL queries can be easily integrated with and embedded into DataStream or DataSet programs. Have a look at the [Integration with DataStream and DataSet API](#integration-with-datastream-and-dataset-api) section to learn how DataStreams and DataSets can be converted into Tables and vice versa.
+**注释：** Table API 和 SQL 查询可以很容易地集成并嵌入到流处理或批处理程序中。 请参阅[与 DataStream 和 DataSet API 结合](#integration-with-datastream-and-dataset-api) 章节了解如何将数据流和数据集与表之间的相互转化。
 
 {% top %}
 
-Create a TableEnvironment
+创建 TableEnvironment
 -------------------------
 
-The `TableEnvironment` is a central concept of the Table API and SQL integration. It is responsible for:
+`TableEnvironment` 是 Table API 和 SQL 的核心概念。它负责:
 
-* Registering a `Table` in the internal catalog
-* Registering catalogs
-* Loading pluggable modules
-* Executing SQL queries
-* Registering a user-defined (scalar, table, or aggregation) function
-* Converting a `DataStream` or `DataSet` into a `Table`
-* Holding a reference to an `ExecutionEnvironment` or `StreamExecutionEnvironment`
+* 在内部的 catalog 中注册`表`
+* 注册外部的 catalog
+* 执行 SQL 查询
+* 注册自定义函数 （scalar、table 或 aggregation）
+* 将`流数据集`或`批数据集`转换成`表`
+* 引用  `ExecutionEnvironment` 或 `StreamExecutionEnvironment`
 
-A `Table` is always bound to a specific `TableEnvironment`. It is not possible to combine tables of different TableEnvironments in the same query, e.g., to join or union them.
+`表`总是绑定在确定的 `TableEnvironment` 上。不能在同一条查询中使用不同 TableEnvironment 中的表，例如，对它们进行 join 或 union 操作。
 
-A `TableEnvironment` is created by calling the static `BatchTableEnvironment.create()` or `StreamTableEnvironment.create()` method with a `StreamExecutionEnvironment` or an `ExecutionEnvironment` and an optional `TableConfig`. The `TableConfig` can be used to configure the `TableEnvironment` or to customize the query optimization and translation process (see [Query Optimization](#query-optimization)).
+`TableEnvironment` 可以通过静态方法 `BatchTableEnvironment.create()` 或者 `StreamTableEnvironment.create()` 在 `StreamExecutionEnvironment` 或者 `ExecutionEnvironment` 中创建，`TableConfig` 是可选项。`TableConfig`可用于配置`TableEnvironment`或自定义查询优化和转换过程(参见 [查询优化](#query-optimization))。
 
-Make sure to choose the specific planner `BatchTableEnvironment`/`StreamTableEnvironment` that matches your programming language.
+请确保选择与你的编程语言匹配的确定的计划器`BatchTableEnvironment`/`StreamTableEnvironment`。
 
-If both planner jars are on the classpath (the default behavior), you should explicitly set which planner to use in the current program.
+如果两种计划器的 jar 包都在 classpath 中（默认行为），你应该明确地设置要在当前程序中使用的计划器。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
 
 // **********************
-// FLINK STREAMING QUERY
+// FLINK 流式查询
 // **********************
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
@@ -162,10 +164,10 @@ import org.apache.flink.table.api.java.StreamTableEnvironment;
 EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();
 StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
 StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);
-// or TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);
+// 或者 TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);
 
 // ******************
-// FLINK BATCH QUERY
+// FLINK 批查询
 // ******************
 import org.apache.flink.api.java.ExecutionEnvironment;
 import org.apache.flink.table.api.java.BatchTableEnvironment;
@@ -174,7 +176,7 @@ ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();
 BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);
 
 // **********************
-// BLINK STREAMING QUERY
+// BLINK 流式查询
 // **********************
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.EnvironmentSettings;
@@ -183,10 +185,10 @@ import org.apache.flink.table.api.java.StreamTableEnvironment;
 StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
 EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
 StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
-// or TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);
+// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);
 
 // ******************
-// BLINK BATCH QUERY
+// BLINK 批查询
 // ******************
 import org.apache.flink.table.api.EnvironmentSettings;
 import org.apache.flink.table.api.TableEnvironment;
@@ -201,7 +203,7 @@ TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);
 {% highlight scala %}
 
 // **********************
-// FLINK STREAMING QUERY
+// FLINK 流式查询
 // **********************
 import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
 import org.apache.flink.table.api.EnvironmentSettings
@@ -210,10 +212,10 @@ import org.apache.flink.table.api.scala.StreamTableEnvironment
 val fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build()
 val fsEnv = StreamExecutionEnvironment.getExecutionEnvironment
 val fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings)
-// or val fsTableEnv = TableEnvironment.create(fsSettings)
+// 或者 val fsTableEnv = TableEnvironment.create(fsSettings)
 
 // ******************
-// FLINK BATCH QUERY
+// FLINK 批查询
 // ******************
 import org.apache.flink.api.scala.ExecutionEnvironment
 import org.apache.flink.table.api.scala.BatchTableEnvironment
@@ -222,7 +224,7 @@ val fbEnv = ExecutionEnvironment.getExecutionEnvironment
 val fbTableEnv = BatchTableEnvironment.create(fbEnv)
 
 // **********************
-// BLINK STREAMING QUERY
+// BLINK 流式查询
 // **********************
 import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
 import org.apache.flink.table.api.EnvironmentSettings
@@ -231,10 +233,10 @@ import org.apache.flink.table.api.scala.StreamTableEnvironment
 val bsEnv = StreamExecutionEnvironment.getExecutionEnvironment
 val bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build()
 val bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings)
-// or val bsTableEnv = TableEnvironment.create(bsSettings)
+// 或者 val bsTableEnv = TableEnvironment.create(bsSettings)
 
 // ******************
-// BLINK BATCH QUERY
+// BLINK 批查询
 // ******************
 import org.apache.flink.table.api.{EnvironmentSettings, TableEnvironment}
 
@@ -248,7 +250,7 @@ val bbTableEnv = TableEnvironment.create(bbSettings)
 {% highlight python %}
 
 # **********************
-# FLINK STREAMING QUERY
+# FLINK 流式查询
 # **********************
 from pyflink.datastream import StreamExecutionEnvironment
 from pyflink.table import StreamTableEnvironment, EnvironmentSettings
@@ -258,7 +260,7 @@ f_s_settings = EnvironmentSettings.new_instance().use_old_planner().in_streaming
 f_s_t_env = StreamTableEnvironment.create(f_s_env, environment_settings=f_s_settings)
 
 # ******************
-# FLINK BATCH QUERY
+# FLINK 批查询
 # ******************
 from pyflink.dataset import ExecutionEnvironment
 from pyflink.table import BatchTableEnvironment
@@ -267,7 +269,7 @@ f_b_env = ExecutionEnvironment.get_execution_environment()
 f_b_t_env = BatchTableEnvironment.create(f_b_env, table_config)
 
 # **********************
-# BLINK STREAMING QUERY
+# BLINK 流式查询
 # **********************
 from pyflink.datastream import StreamExecutionEnvironment
 from pyflink.table import StreamTableEnvironment, EnvironmentSettings
@@ -277,7 +279,7 @@ b_s_settings = EnvironmentSettings.new_instance().use_blink_planner().in_streami
 b_s_t_env = StreamTableEnvironment.create(b_s_env, environment_settings=b_s_settings)
 
 # ******************
-# BLINK BATCH QUERY
+# BLINK 批查询
 # ******************
 from pyflink.table import EnvironmentSettings, BatchTableEnvironment
 
@@ -288,294 +290,261 @@ b_b_t_env = BatchTableEnvironment.create(environment_settings=b_b_settings)
 </div>
 </div>
 
-**Note:** If there is only one planner jar in `/lib` directory, you can use `useAnyPlanner` (`use_any_planner` for python) to create specific `EnvironmentSettings`.
+**注释：** 如果`/lib`目录中只有一中计划器的 jar 包，则可以使用`useAnyPlanner`（python 使用`use any'u planner`）创建`EnvironmentSettings`。
+
 
 {% top %}
 
-Create Tables in the Catalog
+在 Catalog 中注册表
 -------------------------------
 
-A `TableEnvironment` maintains a map of catalogs of tables which are created with an identifier. Each
-identifier consists of 3 parts: catalog name, database name and object name. If a catalog or database is not
-specified, the current default value will be used (see examples in the [Table identifier expanding](#table-identifier-expanding) section).
-
-Tables can be either virtual (`VIEWS`) or regular (`TABLES`). `VIEWS` can be created from an
-existing `Table` object, usually the result of a Table API or SQL query. `TABLES` describe
-external data, such as a file, database table, or message queue.
-
-### Temporary vs Permanent tables.
-
-Tables may either be temporary, and tied to the lifecycle of a single Flink session, or permanent,
-and visible across multiple Flink sessions and clusters.
-
-Permanent tables require a [catalog]({{ site.baseurl }}/dev/table/catalogs.html) (such as Hive Metastore)
-to maintain metadata about the table. Once a permanent table is created, it is visible to any Flink
-session that is connected to the catalog and will continue to exist until the table is explicitly
-dropped.
-
-On the other hand, temporary tables are always stored in memory and only exist for the duration of
-the Flink session they are created within. These tables are not visible to other sessions. They are
-not bound to any catalog or database but can be created in the namespace of one. Temporary tables
-are not dropped if their corresponding database is removed.
+`TableEnvironment` 维护着一张按名称注册的表的 Catalog。有两种类型的表，*输入表* 和*输出表*。输入表可以被 Table API 和 SQL 查询应用并提供输入数据。输出表可以用于向外部系统发出 Table API 和 SQL 查询的查询结果。
 
-#### Shadowing
+有多种数据源可以被注册为输入表：
 
-It is possible to register a temporary table with the same identifier as an existing permanent
-table. The temporary table shadows the permanent one and makes the permanent table inaccessible as
-long as the temporary one exists. All queries with that identifier will be executed against the
-temporary table.
+* 一个已经存在的 `Table` 对象，一般是Table API 和 SQL 查询的查询结果。
+* `TableSource`，用于访问外部数据，例如文件、数据库或消息系统。
+*  由流处理（仅限流处理作业） 或者批处理 （仅限由原版计划器转换的批处理作业）程序得到的`DataStream` 或者 `DataSet`。 `DataStream` 或者 `DataSet` 的注册方法在[与 DataStream 和 DataSet API 结合](#integration-with-datastream-and-dataset-api) 章节中讨论。
 
-This might be useful for experimentation. It allows running exactly the same query first against a
-temporary table that e.g. has just a subset of data, or the data is obfuscated. Once verified that
-the query is correct it can be run against the real production table.
+输出表可以通过 `TableSink` 注册。
 
-### Create a Table
+### 注册表
 
-#### Virtual Tables
-
-A `Table` API object corresponds to a `VIEW` (virtual table) in a SQL terms. It encapsulates a logical
-query plan. It can be created in a catalog as follows:
+按下述流程可以将`表`注册到`TableEnvironment`中：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a TableEnvironment
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// table is the result of a simple projection query 
-Table projTable = tableEnv.from("X").select(...);
+// 表是简单投影查询的结果
+Table projTable = tableEnv.scan("X").select(...);
 
-// register the Table projTable as table "projectedTable"
-tableEnv.createTemporaryView("projectedTable", projTable);
+// 将表projTable注册为表“ projectedTable”
+tableEnv.registerTable("projectedTable", projTable);
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// table is the result of a simple projection query 
-val projTable: Table = tableEnv.from("X").select(...)
+// 表是简单投影查询的结果
+val projTable: Table = tableEnv.scan("X").select(...)
 
-// register the Table projTable as table "projectedTable"
-tableEnv.createTemporaryView("projectedTable", projTable)
+// 将表projTable注册为表“ projectedTable”
+tableEnv.registerTable("projectedTable", projTable)
 {% endhighlight %}
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-# get a TableEnvironment
-table_env = ... # see "Create a TableEnvironment" section
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-# table is the result of a simple projection query 
-proj_table = table_env.from_path("X").select(...)
+# 表是简单投影查询的结果
+proj_table = table_env.scan("X").select(...)
 
-# register the Table projTable as table "projectedTable"
+# 将表projTable注册为表“ projectedTable”
 table_env.register_table("projectedTable", proj_table)
 {% endhighlight %}
 </div>
 </div>
 
-**Note:** `Table` objects are similar to `VIEW`'s from relational database
-systems, i.e., the query that defines the `Table` is not optimized but will be inlined when another
-query references the registered `Table`. If multiple queries reference the same registered `Table`,
-it will be inlined for each referencing query and executed multiple times, i.e., the result of the
-registered `Table` will *not* be shared.
+**注释：** 已注册的`表`的处理方式与关系数据库系统中已知的`视图（view）`类似，即，定义`表`的查询未经过优化，但在另一个查询关联已注册的`表`时将内联（inline）。如果多个查询关联同一个已注册的`表`，则将为每个关联查询内联该查询并执行多次，即，已注册的`表`的结果将不会共享。
 
 {% top %}
 
-#### Connector Tables
+### 注册 TableSource
+
+`TableSource`可访问存储在存储系统中的外部数据，例如数据库（MySQL，HBase 等），具有特定编码的文件（CSV，Apache \[Parquet，Avro，ORC \]， ...）或消息传递系统（Apache Kafka，RabbitMQ 等）。
+
+Flink旨在为常见的数据格式和存储系统提供 TableSources。请参阅 [Table Sources 和 Sinks]({{ site.baseurl }}/zh/dev/table/sourceSinks.html) 获取有关受支持的 TableSources 的列表以及如何构建自定义 `TableSource` 的说明。
 
-It is also possible to create a `TABLE` as known from relational databases from a [connector]({{ site.baseurl }}/dev/table/connect.html) declaration.
-The connector describes the external system that stores the data of a table. Storage systems such as Apacha Kafka or a regular file system can be declared here.
+在 `TableEnvironment` 中注册 `TableSource` 的过程如下：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-tableEnvironment
-  .connect(...)
-  .withFormat(...)
-  .withSchema(...)
-  .inAppendMode()
-  .createTemporaryTable("MyTable")
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
+// 创建 TableSource
+TableSource csvSource = new CsvTableSource("/path/to/file", ...);
+
+// 将 TableSource 注册为表“ CsvTable”
+tableEnv.registerTableSource("CsvTable", csvSource);
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-tableEnvironment
-  .connect(...)
-  .withFormat(...)
-  .withSchema(...)
-  .inAppendMode()
-  .createTemporaryTable("MyTable")
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
+
+// 创建 TableSource
+val csvSource: TableSource = new CsvTableSource("/path/to/file", ...)
+
+// 将 TableSource 注册为表“ CsvTable”
+tableEnv.registerTableSource("CsvTable", csvSource)
 {% endhighlight %}
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-table_environment \
-    .connect(...) \
-    .with_format(...) \
-    .with_schema(...) \
-    .in_append_mode() \
-    .create_temporary_table("MyTable")
-{% endhighlight %}
-</div>
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-<div data-lang="DDL" markdown="1">
-{% highlight sql %}
-tableEnvironment.sqlUpdate("CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)")
+# 创建 TableSource
+csv_source = CsvTableSource("/path/to/file", ...)
+
+# 将 TableSource 注册为表“ CsvTable”
+table_env.register_table_source("csvTable", csv_source)
 {% endhighlight %}
 </div>
 </div>
 
-### Expanding Table identifiers
+**注释：** 对于 Blink 计划器，“TableEnvironment”只接受“StreamTableSource”、“LookupableTableSource”和“InputFormatTableSource”，用于批处理的“StreamTableSource”必须有界。
 
-Tables are always registered with a 3 part identifier consisting of catalog, database, and
-table name. The first two parts are optional and if they are not provided the set default values will
-be used. Identifiers follow SQL requirements which means that they can be escaped with a backtick character (`` ` ``).
-Additionally all SQL reserved keywords must be escaped.
+{% top %}
 
-<div class="codetabs" markdown="1">
-<div data-lang="java" markdown="1">
-{% highlight java %}
-TableEnvironment tEnv = ...;
-tEnv.useCatalog("custom_catalog");
-tEnv.useDatabase("custom_database");
+### 注册 Table Sink
 
-Table table = ...;
+注册的 `TableSink` 可以被用来向外部存储系统，例如数据库、键值存储、消息队列，或文件系统（以不同的编码方式，例如CSV、Apache \[Parquet、Avro、ORC\]，…）[发出 Table API 或 SQL 查询的结果](common.html#emit-a-table)。
 
-// register the view named 'exampleView' in the catalog named 'custom_catalog'
-// in the database named 'custom_database' 
-tableEnv.createTemporaryView("exampleView", table);
+Flink旨在为常见的数据格式和存储系统提供 TableSink。请参阅文档 [Table Sources 和 Sinks]({{ site.baseurl }}/zh/dev/table/sourceSinks.html)  获取关于可用 sink 的详细信息以及如何实现自定义 `TableSink` 的说明。
 
-// register the view named 'exampleView' in the catalog named 'custom_catalog'
-// in the database named 'other_database' 
-tableEnv.createTemporaryView("other_database.exampleView", table);
+在 `TableEnvironment` 中注册 `TableSink` 的过程如下：
 
-// register the view named 'View' in the catalog named 'custom_catalog' in the
-// database named 'custom_database'. 'View' is a reserved keyword and must be escaped.  
-tableEnv.createTemporaryView("`View`", table);
+<div class="codetabs" markdown="1">
+<div data-lang="java" markdown="1">
+{% highlight java %}
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// register the view named 'example.View' in the catalog named 'custom_catalog'
-// in the database named 'custom_database' 
-tableEnv.createTemporaryView("`example.View`", table);
+// 创建 TableSink
+TableSink csvSink = new CsvTableSink("/path/to/file", ...);
 
-// register the view named 'exampleView' in the catalog named 'other_catalog'
-// in the database named 'other_database' 
-tableEnv.createTemporaryView("other_catalog.other_database.exampleView", table);
+// 定义字段名称和类型
+String[] fieldNames = {"a", "b", "c"};
+TypeInformation[] fieldTypes = {Types.INT, Types.STRING, Types.LONG};
 
+// 将 TableSink 注册为表“CsvSinkTable”
+tableEnv.registerTableSink("CsvSinkTable", fieldNames, fieldTypes, csvSink);
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tEnv: TableEnvironment = ...;
-tEnv.useCatalog("custom_catalog")
-tEnv.useDatabase("custom_database")
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-val table: Table = ...;
+// 创建 TableSink
+val csvSink: TableSink = new CsvTableSink("/path/to/file", ...)
 
-// register the view named 'exampleView' in the catalog named 'custom_catalog'
-// in the database named 'custom_database' 
-tableEnv.createTemporaryView("exampleView", table)
+// 定义字段名称和类型
+val fieldNames: Array[String] = Array("a", "b", "c")
+val fieldTypes: Array[TypeInformation[_]] = Array(Types.INT, Types.STRING, Types.LONG)
 
-// register the view named 'exampleView' in the catalog named 'custom_catalog'
-// in the database named 'other_database' 
-tableEnv.createTemporaryView("other_database.exampleView", table)
+// 将 TableSink 注册为表“CsvSinkTable”
+tableEnv.registerTableSink("CsvSinkTable", fieldNames, fieldTypes, csvSink)
+{% endhighlight %}
+</div>
+
+<div data-lang="python" markdown="1">
+{% highlight python %}
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-// register the view named 'View' in the catalog named 'custom_catalog' in the
-// database named 'custom_database'. 'View' is a reserved keyword and must be escaped.  
-tableEnv.createTemporaryView("`View`", table)
+# 定义字段名称和类型
+field_names = ["a", "b", "c"]
+field_types = [DataTypes.INT(), DataTypes.STRING(), DataTypes.BIGINT()]
 
-// register the view named 'example.View' in the catalog named 'custom_catalog'
-// in the database named 'custom_database' 
-tableEnv.createTemporaryView("`example.View`", table)
+# 创建 TableSink
+csv_sink = CsvTableSink(field_names, field_types, "/path/to/file", ...)
 
-// register the view named 'exampleView' in the catalog named 'other_catalog'
-// in the database named 'other_database' 
-tableEnv.createTemporaryView("other_catalog.other_database.exampleView", table)
+# 将 TableSink 注册为表“CsvSinkTable”
+table_env.register_table_sink("CsvSinkTable", csv_sink)
 {% endhighlight %}
 </div>
-
 </div>
 
-Query a Table
+{% top %}
+
+查询表
 -------------
 
 ### Table API
 
-The Table API is a language-integrated query API for Scala and Java. In contrast to SQL, queries are not specified as Strings but are composed step-by-step in the host language. 
+Table API 是关于 Scala 和 Java 的集成语言式查询 API。与 SQL 相反，Table API 的查询不是由字符串指定，而是在宿主语言中逐步构成。
 
-The API is based on the `Table` class which represents a table (streaming or batch) and offers methods to apply relational operations. These methods return a new `Table` object, which represents the result of applying the relational operation on the input `Table`. Some relational operations are composed of multiple method calls such as `table.groupBy(...).select()`, where `groupBy(...)` specifies a grouping of `table`, and `select(...)` the projection on the grouping of `table`.
+Table API 是基于 `Table` 类的，该类类代表表（流或批处理），并提供使用关系操作的方法。这些方法返回一个新的 Table 对象，该对象表示对输入 Table 进行关系操作的结果。 一些关系操作由多个方法调用组成，例如 `table.groupBy(...).select()`，其中 `groupBy(...)` 指定 `table` 的分组，而 `select(...)` 在  `table` 分组上的投影。
 
-The [Table API]({{ site.baseurl }}/dev/table/tableApi.html) document describes all Table API operations that are supported on streaming and batch tables.
+文档 [Table API]({{ site.baseurl }}/zh/dev/table/tableApi.html) 说明了所有流处理和批处理表支持的 Table API 算子。
 
-The following example shows a simple Table API aggregation query:
+以下示例展示了一个简单的 Table API 聚合查询：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a TableEnvironment
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// register Orders table
+// 注册“Orders”表
 
-// scan registered Orders table
-Table orders = tableEnv.from("Orders");
-// compute revenue for all customers from France
+// 扫描注册的“Orders”表
+Table orders = tableEnv.scan("Orders");
+// 计算所有法国客户的收入
 Table revenue = orders
   .filter("cCountry === 'FRANCE'")
   .groupBy("cID, cName")
   .select("cID, cName, revenue.sum AS revSum");
 
-// emit or convert Table
-// execute query
+// 发出或转换表
+// 执行查询
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// register Orders table
+// 注册“Orders”表
 
-// scan registered Orders table
-val orders = tableEnv.from("Orders")
-// compute revenue for all customers from France
+// 扫描注册的“Orders”表
+val orders = tableEnv.scan("Orders")
+// 计算所有法国客户的收入
 val revenue = orders
   .filter('cCountry === "FRANCE")
   .groupBy('cID, 'cName)
   .select('cID, 'cName, 'revenue.sum AS 'revSum)
 
-// emit or convert Table
-// execute query
+// 发出或转换表
+// 执行查询
 {% endhighlight %}
 
-**Note:** The Scala Table API uses Scala Symbols, which start with a single tick (`'`) to reference the attributes of a `Table`. The Table API uses Scala implicits. Make sure to import `org.apache.flink.api.scala._` and `org.apache.flink.table.api.scala._` in order to use Scala implicit conversions.
+**注释：** Scala Table API使用该符号以单个刻度（`'`）开头的 Scala 符号以引用 Table 的属性。Table API 使用 Scala 隐式转换。请确保引入了 `org.apache.flink.api.scala._` 包和 `org.apache.flink.table.api.scala._` 包以使用 Scala 隐格式转换。
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-# get a TableEnvironment
-table_env = # see "Create a TableEnvironment" section
+# 创建 TableEnvironment
+table_env = # 参阅“创建 TableEnvironment ”章节
 
-# register Orders table
+# 注册“Orders”表
 
-# scan registered Orders table
-orders = table_env.from_path("Orders")
-# compute revenue for all customers from France
+# 扫描注册的“Orders”表
+orders = table_env.scan("Orders")
+# 计算所有法国客户的收入
 revenue = orders \
     .filter("cCountry === 'FRANCE'") \
     .group_by("cID, cName") \
     .select("cID, cName, revenue.sum AS revSum")
 
-# emit or convert Table
-# execute query
+# 发出或转换表
+# 执行查询
 {% endhighlight %}
 </div>
 </div>
@@ -584,21 +553,21 @@ revenue = orders \
 
 ### SQL
 
-Flink's SQL integration is based on [Apache Calcite](https://calcite.apache.org), which implements the SQL standard. SQL queries are specified as regular Strings.
+Flink SQL 是基于实现了SQL标准的 [Apache Calcite](https://Calcite.Apache.org) 的。SQL 查询由常规字符串指定。
 
-The [SQL]({{ site.baseurl }}/dev/table/sql/index.html) document describes Flink's SQL support for streaming and batch tables.
+文档 [SQL]({{ site.baseurl }}/zh/dev/table/sql.html) 描述了Flink对流处理和批处理表的SQL支持。
 
-The following example shows how to specify a query and return the result as a `Table`.
+下面的示例演示了如何指定查询并将结果作为 `Table` 对象返回。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a TableEnvironment
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// register Orders table
+// 注册“Orders”表
 
-// compute revenue for all customers from France
+// 计算所有法国客户的收入
 Table revenue = tableEnv.sqlQuery(
     "SELECT cID, cName, SUM(revenue) AS revSum " +
     "FROM Orders " +
@@ -606,19 +575,19 @@ Table revenue = tableEnv.sqlQuery(
     "GROUP BY cID, cName"
   );
 
-// emit or convert Table
-// execute query
+// 发出或转换表
+// 执行查询
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// register Orders table
+// 注册“Orders”表
 
-// compute revenue for all customers from France
+// 计算所有法国客户的收入
 val revenue = tableEnv.sqlQuery("""
   |SELECT cID, cName, SUM(revenue) AS revSum
   |FROM Orders
@@ -626,20 +595,20 @@ val revenue = tableEnv.sqlQuery("""
   |GROUP BY cID, cName
   """.stripMargin)
 
-// emit or convert Table
-// execute query
+// 发出或转换表
+// 执行查询
 {% endhighlight %}
 
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-# get a TableEnvironment
-table_env = ... # see "Create a TableEnvironment" section
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-# register Orders table
+# 注册“Orders”表
 
-# compute revenue for all customers from France
+# 计算所有法国客户的收入
 revenue = table_env.sql_query(
     "SELECT cID, cName, SUM(revenue) AS revSum "
     "FROM Orders "
@@ -647,24 +616,24 @@ revenue = table_env.sql_query(
     "GROUP BY cID, cName"
 )
 
-# emit or convert Table
-# execute query
+# 发出或转换表
+# 执行查询
 {% endhighlight %}
 </div>
 </div>
 
-The following example shows how to specify an update query that inserts its result into a registered table.
+下面的示例演示如何通过更新查询将其结果插入已注册表。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a TableEnvironment
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// register "Orders" table
-// register "RevenueFrance" output table
+// 注册"Orders"表
+// 注册"RevenueFrance"输出表
 
-// compute revenue for all customers from France and emit to "RevenueFrance"
+// 计算所有法国客户的收入并发送至"RevenueFrance"表
 tableEnv.sqlUpdate(
     "INSERT INTO RevenueFrance " +
     "SELECT cID, cName, SUM(revenue) AS revSum " +
@@ -673,19 +642,19 @@ tableEnv.sqlUpdate(
     "GROUP BY cID, cName"
   );
 
-// execute query
+// 执行查询
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// register "Orders" table
-// register "RevenueFrance" output table
+// 注册"Orders"表
+// 注册"RevenueFrance"输出表
 
-// compute revenue for all customers from France and emit to "RevenueFrance"
+// 计算所有法国客户的收入并发送至"RevenueFrance"表
 tableEnv.sqlUpdate("""
   |INSERT INTO RevenueFrance
   |SELECT cID, cName, SUM(revenue) AS revSum
@@ -694,20 +663,20 @@ tableEnv.sqlUpdate("""
   |GROUP BY cID, cName
   """.stripMargin)
 
-// execute query
+// 执行查询
 {% endhighlight %}
 
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-# get a TableEnvironment
-table_env = ... # see "Create a TableEnvironment" section
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
 
-# register "Orders" table
-# register "RevenueFrance" output table
+# 注册"Orders"表
+# 注册"RevenueFrance"输出表
 
-# compute revenue for all customers from France and emit to "RevenueFrance"
+# 计算所有法国客户的收入 and emit to "RevenueFrance"
 table_env.sql_update(
     "INSERT INTO RevenueFrance "
     "SELECT cID, cName, SUM(revenue) AS revSum "
@@ -716,110 +685,101 @@ table_env.sql_update(
     "GROUP BY cID, cName"
 )
 
-# execute query
+# 执行查询
 {% endhighlight %}
 </div>
 </div>
 
 {% top %}
 
-### Mixing Table API and SQL
+### 混用 Table API 和 SQL
 
-Table API and SQL queries can be easily mixed because both return `Table` objects:
+Table API 和 SQL 查询的混用非常简单因为它们都返回 `Table` 对象：
 
-* A Table API query can be defined on the `Table` object returned by a SQL query.
-* A SQL query can be defined on the result of a Table API query by [registering the resulting Table](#register-a-table) in the `TableEnvironment` and referencing it in the `FROM` clause of the SQL query.
+* 可以在 SQL 查询返回的 `Table` 对象上定义 Table API 查询。
+* 通过在 `TableEnvironment` 中注册[结果表](#register-a-table)并在 SQL 查询的 `FROM` 子句中引用它，可以在 Table API 查询的结果上定义 SQL 查询。
 
 {% top %}
 
-Emit a Table 
+发出表
 ------------
 
-A `Table` is emitted by writing it to a `TableSink`. A `TableSink` is a generic interface to support a wide variety of file formats (e.g. CSV, Apache Parquet, Apache Avro), storage systems (e.g., JDBC, Apache HBase, Apache Cassandra, Elasticsearch), or messaging systems (e.g., Apache Kafka, RabbitMQ). 
+`表`通过写入 `TableSink` 发出。`TableSink` 是一个通用接口，用于支持多种文件格式（如 CSV、Apache Parquet、Apache Avro）、存储系统（如 JDBC、Apache HBase、Apache Cassandra、Elasticsearch）或消息传递系统（如 Apache Kafka、RabbitMQ）。
 
-A batch `Table` can only be written to a `BatchTableSink`, while a streaming `Table` requires either an `AppendStreamTableSink`, a `RetractStreamTableSink`, or an `UpsertStreamTableSink`. 
+批处理`表`只能写入 `BatchTableSink`，而流处理`表`需要指定写入 `AppendStreamTableSink`，`RetractStreamTableSink` 或者 `UpsertStreamTableSink`。
 
-Please see the documentation about [Table Sources & Sinks]({{ site.baseurl }}/dev/table/sourceSinks.html) for details about available sinks and instructions for how to implement a custom `TableSink`.
+请参考文档 [Table Sources & Sinks]({{ site.baseurl }}/zh/dev/table/sourceSinks.html) 以获取更多关于可用 Sink 的信息以及如何自定义 `TableSink`。
 
-The `Table.insertInto(String tableName)` method emits the `Table` to a registered `TableSink`. The method looks up the `TableSink` from the catalog by the name and validates that the schema of the `Table` is identical to the schema of the `TableSink`. 
+方法 `Table.insertInto(String tableName)` 将`表`发送至已注册的 `TableSink`。该方法通过名称在 catalog 中查找 `TableSink` 并确认`Table` schema 和 `TableSink` schema 一致。
 
-The following examples shows how to emit a `Table`:
+下面的示例演示如何发出 `Table`：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a TableEnvironment
-TableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+TableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// create an output Table
-final Schema schema = new Schema()
-    .field("a", DataTypes.INT())
-    .field("b", DataTypes.STRING())
-    .field("c", DataTypes.LONG());
+// 创建 TableSink
+TableSink sink = new CsvTableSink("/path/to/file", fieldDelim = "|");
 
-tableEnv.connect(new FileSystem("/path/to/file"))
-    .withFormat(new Csv().fieldDelimiter('|').deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("CsvSinkTable");
+// 注册 TableSink 并指明 schema
+String[] fieldNames = {"a", "b", "c"};
+TypeInformation[] fieldTypes = {Types.INT, Types.STRING, Types.LONG};
+tableEnv.registerTableSink("CsvSinkTable", fieldNames, fieldTypes, sink);
 
-// compute a result Table using Table API operators and/or SQL queries
+// 通过 Table API 的算子或者 SQL 查询得出结果表
 Table result = ...
-// emit the result Table to the registered TableSink
+// 发送结果表至已注册的 TableSink
 result.insertInto("CsvSinkTable");
 
-// execute the program
+// 执行程序
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
-// create an output Table
-val schema = new Schema()
-    .field("a", DataTypes.INT())
-    .field("b", DataTypes.STRING())
-    .field("c", DataTypes.LONG())
+// 创建 TableSink
+val sink: TableSink = new CsvTableSink("/path/to/file", fieldDelim = "|")
 
-tableEnv.connect(new FileSystem("/path/to/file"))
-    .withFormat(new Csv().fieldDelimiter('|').deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("CsvSinkTable")
+// 注册 TableSink 并指明 schema
+val fieldNames: Array[String] = Array("a", "b", "c")
+val fieldTypes: Array[TypeInformation] = Array(Types.INT, Types.STRING, Types.LONG)
+tableEnv.registerTableSink("CsvSinkTable", fieldNames, fieldTypes, sink)
 
-// compute a result Table using Table API operators and/or SQL queries
+// 通过 Table API 的算子或者 SQL 查询得出结果表
 val result: Table = ...
 
-// emit the result Table to the registered TableSink
+// 发送结果表至已注册的 TableSink
 result.insertInto("CsvSinkTable")
 
-// execute the program
+// 执行程序
 {% endhighlight %}
 </div>
 
 <div data-lang="python" markdown="1">
 {% highlight python %}
-# get a TableEnvironment
-table_env = ... # see "Create a TableEnvironment" section
-
-# create a TableSink
-t_env.connect(FileSystem().path("/path/to/file")))
-    .with_format(Csv()
-                 .field_delimiter(',')
-                 .deriveSchema())
-    .with_schema(Schema()
-                 .field("a", DataTypes.INT())
-                 .field("b", DataTypes.STRING())
-                 .field("c", DataTypes.BIGINT()))
-    .create_temporary_table("CsvSinkTable")
-
-# compute a result Table using Table API operators and/or SQL queries
+# 创建 TableEnvironment
+table_env = ... # 参阅“创建 TableEnvironment ”章节
+
+field_names = ["a", "b", "c"]
+field_types = [DataTypes.INT(), DataTypes.STRING(), DataTypes.BIGINT()]
+
+# 创建 TableSink
+sink = CsvTableSink(field_names, field_types, "/path/to/file", "|")
+
+table_env.register_table_sink("CsvSinkTable", sink)
+
+# 通过 Table API 的算子或者 SQL 查询得出结果表
 result = ...
 
-# emit the result Table to the registered TableSink
+# 发送结果表至已注册的 TableSink
 result.insert_into("CsvSinkTable")
 
-# execute the program
+# 执行程序
 {% endhighlight %}
 </div>
 </div>
@@ -827,140 +787,140 @@ result.insert_into("CsvSinkTable")
 {% top %}
 
 
-Translate and Execute a Query
+解析与执行查询
 -----------------------------
 
-The behavior of translating and executing a query is different for the two planners.
+两种计划器解析和执行查询的方式是不同的。
 
 <div class="codetabs" markdown="1">
 <div data-lang="Old planner" markdown="1">
-Table API and SQL queries are translated into [DataStream]({{ site.baseurl }}/dev/datastream_api.html) or [DataSet]({{ site.baseurl }}/dev/batch) programs depending on whether their input is a streaming or batch input. A query is internally represented as a logical query plan and is translated in two phases:
+Table API 和 SQL 查询会被解析成 [流处理]({{ site.baseurl }}/zh/dev/datastream_api.html)或者[批处理]({{ site.baseurl }}/zh/dev/batch)程序， 这取决于它们的输入数据源是流式的还是批式的。查询在内部表示为逻辑查询计划，并被解析成两个阶段：
 
-1. Optimization of the logical plan
-2. Translation into a DataStream or DataSet program
+1. 优化逻辑执行计划
+2. 解析成流处理或批处理程序
 
-A Table API or SQL query is translated when:
+Table API 或者 SQL 查询在下列情况下会被解析：
 
-* a `Table` is emitted to a `TableSink`, i.e., when `Table.insertInto()` is called.
-* a SQL update query is specified, i.e., when `TableEnvironment.sqlUpdate()` is called.
-* a `Table` is converted into a `DataStream` or `DataSet` (see [Integration with DataStream and DataSet API](#integration-with-datastream-and-dataset-api)).
+* `表`被发送给 `TableSink`，即当调用 `Table.insertInto()` 时。
+* SQL 更新语句执行时，即，当调用 `TableEnvironment.sqlUpdate()` 时。
+* `表`被转换成`流数据`或者`批数据`时（参阅[与 DataStream 和 DataSet API 结合](#integration-with-datastream-and-dataset-api)）。
 
-Once translated, a Table API or SQL query is handled like a regular DataStream or DataSet program and is executed when `StreamExecutionEnvironment.execute()` or `ExecutionEnvironment.execute()` is called.
+解析完成后，Table API 或者 SQL 查询会被当做普通的流处理或批处理程序对待并且会在调用 `StreamExecutionEnvironment.execute()` 或 `ExecutionEnvironment.execute()` 的时候被执行。
 
 </div>
 
 <div data-lang="Blink planner" markdown="1">
-Table API and SQL queries are translated into [DataStream]({{ site.baseurl }}/dev/datastream_api.html) programs whether their input is streaming or batch. A query is internally represented as a logical query plan and is translated in two phases:
+Table API 和 SQL 查询会被转换成[流处理]({{ site.baseurl }}/zh/dev/datastream_api.html)程序不论它们的输入数据源是流式的还是批式的。查询在内部表示为逻辑查询计划，并被解析成两个阶段：
 
-1. Optimization of the logical plan,
-2. Translation into a DataStream program.
+1. 优化逻辑执行计划
+2. 解析成流处理程序
 
-The behavior of translating  a query is different for `TableEnvironment` and `StreamTableEnvironment`.
+TableEnvironment 和 StreamTableEnvironment 解析查询的方式不同。
 
-For `TableEnvironment`, A Table API or SQL query is translated when `TableEnvironment.execute()` is called, because `TableEnvironment` will optimize multiple-sinks into one DAG.
+对于 `TableEnvironment`，Table API 或者 SQL 查询会在调用 `TableEnvironment.execute()` 时被解析，因为 `TableEnvironment` 会将多 sink 优化成一张有向无环图。
 
-while for `StreamTableEnvironment`, A Table API or SQL query is translated when:
+而对于 `StreamTableEnvironment`，当下列情况发生时，Table API 或者 SQL 查询会被解析：
 
-* a `Table` is emitted to a `TableSink`, i.e., when `Table.insertInto()` is called.
-* a SQL update query is specified, i.e., when `TableEnvironment.sqlUpdate()` is called.
-* a `Table` is converted into a `DataStream`.
+* `表` 被发送至`TableSink`，即，当 `Table.insertInto()` 被调用时。
+* SQL 更新语句执行时，即，当调用 `TableEnvironment.sqlUpdate()` 时。
+* `表`被转换成`流数据`时。
 
-Once translated, a Table API or SQL query is handled like a regular DataStream program and is executed when `TableEnvironment.execute()` or `StreamExecutionEnvironment.execute()` is called.
+解析完成后，Table API 或者 SQL 查询会被当做普通的流处理程序对待并且会在调用 `TableEnvironment.execute()` 或者 `StreamExecutionEnvironment.execute()` 的时候被执行。
 
 </div>
 </div>
 
 {% top %}
 
-Integration with DataStream and DataSet API
+与 DataStream 和 DataSet API 结合
 -------------------------------------------
 
-Both planners on stream can integrate with the `DataStream` API. Only old planner can integrate with the `DataSet API`, Blink planner on batch could not be combined with both.
-**Note:** The `DataSet` API discussed below is only relevant for the old planner on batch.
+在流处理方面两种计划器都可以与 `DataStream` API 结合。只有原版计划器可以与 `DataSet API` 结合。在批处理方面，Blink 计划器不能同两种计划器中的任何一个结合。  
+**注释：** The `DataSet` API discussed below is only relevant for the old planner on batch.
 
-Table API and SQL queries can be easily integrated with and embedded into [DataStream]({{ site.baseurl }}/dev/datastream_api.html) and [DataSet]({{ site.baseurl }}/dev/batch) programs. For instance, it is possible to query an external table (for example from a RDBMS), do some pre-processing, such as filtering, projecting, aggregating, or joining with meta data, and then further process the data with either the DataStream or DataSet API (and any of the libraries built on top of these APIs, such as CEP or Gelly). Inversely, a Table API or SQL query can also be applied on the result of a DataStream or DataSet program.
+Table API 和 SQL 可以被很容易地集成并嵌入到[流处理]({{ site.baseurl }}/zh/dev/datastream_api.html)和[批处理]({{ site.baseurl }}/zh/dev/batch)程序中。例如，可以查询外部表（例如从 RDBMS），进行一些预处理，例如过滤，投影，聚合或与元数据 join，然后使用 DataStream 或 DataSet API（以及在这些 API 之上构建的任何库，例如 CEP 或 Gelly）。相反，也可以将 Table API 或 SQL 查询应用于流处理或批处理程序的结果。
 
-This interaction can be achieved by converting a `DataStream` or `DataSet` into a `Table` and vice versa. In this section, we describe how these conversions are done.
+这种交互可以通过`批数据集`或`流数据集`与`表`的相互转化实现。本节我们会介绍这些转化是如何实现的。
 
 ### Implicit Conversion for Scala
 
-The Scala Table API features implicit conversions for the `DataSet`, `DataStream`, and `Table` classes. These conversions are enabled by importing the package `org.apache.flink.table.api.scala._` in addition to `org.apache.flink.api.scala._` for the Scala DataStream API.
+Scala Table API 含有对 `DataSet`、`DataStream` 和 `Table` 类的隐式转换。 通过为 Scala DataStream API 导入 `org.apache.flink.table.api.scala._` 包以及 `org.apache.flink.api.scala._` 包，可以启用这些转换。
 
-### Create a View from a DataStream or DataSet
+### 将流数据集或批数据集注册成表
 
-A `DataStream` or `DataSet` can be registered in a `TableEnvironment` as a View. The schema of the resulting view depends on the data type of the registered `DataStream` or `DataSet`. Please check the section about [mapping of data types to table schema](#mapping-of-data-types-to-table-schema) for details.
-
-**Note:** Views created from a `DataStream` or `DataSet` can be registered as temporary views only.
+在 `TableEnvironment` 中可以将`批数据集或`流数据集`注册成表。结果表的 schema 取决于注册的`批数据集或`流数据集`的数据类型。请参阅文档 [数据类型到 table schema 的映射](#mapping-of-data-types-to-table-schema) for details.
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get StreamTableEnvironment
-// registration of a DataSet in a BatchTableEnvironment is equivalent
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment
+// 在 BatchTableEnvironment 中注册批数据集的方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
 DataStream<Tuple2<Long, String>> stream = ...
 
-// register the DataStream as View "myTable" with fields "f0", "f1"
-tableEnv.createTemporaryView("myTable", stream);
+// 将流数据集注册为具有字段 “f0”，“f1” 的表 “myTable”
+tableEnv.registerDataStream("myTable", stream);
 
-// register the DataStream as View "myTable2" with fields "myLong", "myString"
-tableEnv.createTemporaryView("myTable2", stream, "myLong, myString");
+// 将流数据集注册为具有字段 “myLong”，“myString” 的表 “myTable2”
+tableEnv.registerDataStream("myTable2", stream, "myLong, myString");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get TableEnvironment 
-// registration of a DataSet is equivalent
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+// 批数据集的注册方式相同
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[(Long, String)] = ...
 
-// register the DataStream as View "myTable" with fields "f0", "f1"
-tableEnv.createTemporaryView("myTable", stream)
+// 将流数据集注册为具有字段 “f0”，“f1” 的表 “myTable”
+tableEnv.registerDataStream("myTable", stream)
 
-// register the DataStream as View "myTable2" with fields "myLong", "myString"
-tableEnv.createTemporaryView("myTable2", stream, 'myLong, 'myString)
+// 将流数据集注册为具有字段 “myLong”，“myString” 的表 “myTable2”
+tableEnv.registerDataStream("myTable2", stream, 'myLong, 'myString)
 {% endhighlight %}
 </div>
 </div>
 
+**注释：** `流数据集`和`表`的名称不能使用 `^_DataStreamTable_[0-9]+` 的格式，而`批数据集`和`表`的名称不能使用 `^_DataSetTable_[0-9]+` 的格式。这些格式仅保留做内部使用。
+
 {% top %}
 
-### Convert a DataStream or DataSet into a Table
+### 将批数据集或流数据集转换成表
 
-Instead of registering a `DataStream` or `DataSet` in a `TableEnvironment`, it can also be directly converted into a `Table`. This is convenient if you want to use the Table in a Table API query. 
+除了在 `TableEnvironment` 中注册`流数据集`和`批数据集`的方法外，也可以将它们直接转换成`表`。如果你要在 Table API 查询中使用`表`，这将很方便。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get StreamTableEnvironment
-// registration of a DataSet in a BatchTableEnvironment is equivalent
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment
+// 在 BatchTableEnvironment 中注册批数据集的方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
 DataStream<Tuple2<Long, String>> stream = ...
 
-// Convert the DataStream into a Table with default fields "f0", "f1"
+// 将流数据集转换成具有默认字段 “f0”, “f1” 的表
 Table table1 = tableEnv.fromDataStream(stream);
 
-// Convert the DataStream into a Table with fields "myLong", "myString"
+// 将流数据集转换成具有字段 “myLong”, “myString” 的表
 Table table2 = tableEnv.fromDataStream(stream, "myLong, myString");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get TableEnvironment
-// registration of a DataSet is equivalent
-val tableEnv = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+// 批数据集的注册方式相同
+val tableEnv = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[(Long, String)] = ...
 
-// convert the DataStream into a Table with default fields '_1, '_2
+// 将流数据集转换成具有默认字段 '_1, '_2 的表
 val table1: Table = tableEnv.fromDataStream(stream)
 
-// convert the DataStream into a Table with fields 'myLong, 'myString
+// 将流数据集转换成具有字段 'myLong, 'myString 的表
 val table2: Table = tableEnv.fromDataStream(stream, 'myLong, 'myString)
 {% endhighlight %}
 </div>
@@ -968,52 +928,51 @@ val table2: Table = tableEnv.fromDataStream(stream, 'myLong, 'myString)
 
 {% top %}
 
-### Convert a Table into a DataStream or DataSet
+### 将表转换成流数据集或批数据集
 
-A `Table` can be converted into a `DataStream` or `DataSet`. In this way, custom DataStream or DataSet programs can be run on the result of a Table API or SQL query.
+`表`可以被转换成`流数据集`或`批数据集`。通过这种方式，自定义的批处理或流处理程序就可以在 Table API 或者 SQL 的查询结果上运行了。
 
-When converting a `Table` into a `DataStream` or `DataSet`, you need to specify the data type of the resulting `DataStream` or `DataSet`, i.e., the data type into which the rows of the `Table` are to be converted. Often the most convenient conversion type is `Row`. The following list gives an overview of the features of the different options:
+将`表`转换为`流数据集`或者`批数据集`时，你需要指定生成的`流数据集`或`批数据集`的数据类型，即，`表`的每行数据要转换成的数据类型。通常最方便的选择是转换成 `Row` 。以下列表概述了不同选项的功能：
 
-- **Row**: fields are mapped by position, arbitrary number of fields, support for `null` values, no type-safe access.
-- **POJO**: fields are mapped by name (POJO fields must be named as `Table` fields), arbitrary number of fields, support for `null` values, type-safe access.
-- **Case Class**: fields are mapped by position, no support for `null` values, type-safe access.
-- **Tuple**: fields are mapped by position, limitation to 22 (Scala) or 25 (Java) fields, no support for `null` values, type-safe access.
-- **Atomic Type**: `Table` must have a single field, no support for `null` values, type-safe access.
+- **Row**: 字段按位置映射，字段数量任意，支持 `null` 值，无类型安全（type-safe）检查。
+- **POJO**: 字段按名称映射（POJO 必须按`表`中字段名称命名），字段数量任意，支持 `null` 值，无类型安全检查。
+- **Case Class**: 字段按位置映射，不支持 `null` 值，有类型安全检查。
+- **Tuple**: 字段按位置映射，字段数量少于 22（Scala）或者 25（Java），不支持 `null` 值，无类型安全检查。
+- **Atomic Type**: `表`必须有一个字段，不支持 `null` 值，有类型安全检查。
 
-#### Convert a Table into a DataStream
+#### 将表转换成流数据集
 
-A `Table` that is the result of a streaming query will be updated dynamically, i.e., it is changing as new records arrive on the query's input streams. Hence, the `DataStream` into which such a dynamic query is converted needs to encode the updates of the table. 
+流式查询（streaming query）的结果表会动态更新，即，当新纪录到达查询的输入流时，查询结果会改变。因此，像这样将动态查询结果转换成流`数据集`需要对表的更新方式进行编码。
 
-There are two modes to convert a `Table` into a `DataStream`:
+将`表`转换为`流数据集`有两种模式：
 
-1. **Append Mode**: This mode can only be used if the dynamic `Table` is only modified by `INSERT` changes, i.e, it is append-only and previously emitted results are never updated.
-2. **Retract Mode**: This mode can always be used. It encodes `INSERT` and `DELETE` changes with a `boolean` flag.
+1. **Append Mode**: 仅当动态`表`仅通过`INSERT`更改进行修改时，才可以使用此模式，即，它仅是追加操作，并且之前发出的结果永远不会更新。
+2. **Retract Mode**: 任何情形都可以使用此模式。它使用 boolean 值对 `INSERT` 和 `DELETE` 操作的数据进行标记。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get StreamTableEnvironment. 
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment.
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// Table with two fields (String name, Integer age)
+// 有两个字段的表（String name, Integer age）
 Table table = ...
 
-// convert the Table into an append DataStream of Row by specifying the class
+// 通过指定类将表转换为 Row 的追加（append）流数据集
 DataStream<Row> dsRow = tableEnv.toAppendStream(table, Row.class);
 
-// convert the Table into an append DataStream of Tuple2<String, Integer> 
-//   via a TypeInformation
+// 通过 TypeInformation 将表转换为 Tuple2 <String，Integer> 的追加流数据集
 TupleTypeInfo<Tuple2<String, Integer>> tupleType = new TupleTypeInfo<>(
   Types.STRING(),
   Types.INT());
-DataStream<Tuple2<String, Integer>> dsTuple = 
+DataStream<Tuple2<String, Integer>> dsTuple =
   tableEnv.toAppendStream(table, tupleType);
 
-// convert the Table into a retract DataStream of Row.
-//   A retract stream of type X is a DataStream<Tuple2<Boolean, X>>. 
-//   The boolean field indicates the type of the change. 
-//   True is INSERT, false is DELETE.
-DataStream<Tuple2<Boolean, Row>> retractStream = 
+// 将表转换成 Row 的回收（retract）流数据集
+//   类型X的回收流就是 DataStream<Tuple2<Boolean, X>>。
+//   boolean 字段表示更改的类型。
+//   True 表示 INSERT，false 表示 DELETE
+DataStream<Tuple2<Boolean, Row>> retractStream =
   tableEnv.toRetractStream(table, Row.class);
 
 {% endhighlight %}
@@ -1021,69 +980,69 @@ DataStream<Tuple2<Boolean, Row>> retractStream =
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get TableEnvironment. 
-// registration of a DataSet is equivalent
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment.
+// 批数据集的注册方式相同
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
-// Table with two fields (String name, Integer age)
+// 有两个字段的表（String name, Integer age）
 val table: Table = ...
 
-// convert the Table into an append DataStream of Row
+// 将表转换成 Row 的追加流数据集
 val dsRow: DataStream[Row] = tableEnv.toAppendStream[Row](table)
 
-// convert the Table into an append DataStream of Tuple2[String, Int]
-val dsTuple: DataStream[(String, Int)] dsTuple = 
+// 将表转换成 Tuple2[String, Int] 的追加流数据集
+val dsTuple: DataStream[(String, Int)] dsTuple =
   tableEnv.toAppendStream[(String, Int)](table)
 
-// convert the Table into a retract DataStream of Row.
-//   A retract stream of type X is a DataStream[(Boolean, X)]. 
-//   The boolean field indicates the type of the change. 
-//   True is INSERT, false is DELETE.
+// 将表转换成 Row 的回收（retract）流数据集
+//   类型X的回收流就是 DataStream[(Boolean, X)]
+//   boolean 字段表示更改的类型。
+//   True 表示 INSERT，false 表示 DELETE
 val retractStream: DataStream[(Boolean, Row)] = tableEnv.toRetractStream[Row](table)
 {% endhighlight %}
 </div>
 </div>
 
-**Note:** A detailed discussion about dynamic tables and their properties is given in the [Dynamic Tables](streaming/dynamic_tables.html) document.
+**注释：** 文档[动态表](streaming/dynamic_tables.html)给出了有关动态表及其属性的详细讨论。
 
-#### Convert a Table into a DataSet
+#### 将表转换成批数据集
 
-A `Table` is converted into a `DataSet` as follows:
+将`表`转换成`批数据集`的过程如下：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get BatchTableEnvironment
+// 创建 BatchTableEnvironment
 BatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);
 
-// Table with two fields (String name, Integer age)
+// 有两个字段的表（String name, Integer age）
 Table table = ...
 
-// convert the Table into a DataSet of Row by specifying a class
+// 通过指定类将表转换成 Row 的批数据集
 DataSet<Row> dsRow = tableEnv.toDataSet(table, Row.class);
 
-// convert the Table into a DataSet of Tuple2<String, Integer> via a TypeInformation
+// 通过 TypeInformation 将表转换为 Tuple2<String，Integer> 的批数据集
 TupleTypeInfo<Tuple2<String, Integer>> tupleType = new TupleTypeInfo<>(
   Types.STRING(),
   Types.INT());
-DataSet<Tuple2<String, Integer>> dsTuple = 
+DataSet<Tuple2<String, Integer>> dsTuple =
   tableEnv.toDataSet(table, tupleType);
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get TableEnvironment 
-// registration of a DataSet is equivalent
+// 创建 TableEnvironment
+// 批数据集的注册方式相同
 val tableEnv = BatchTableEnvironment.create(env)
 
-// Table with two fields (String name, Integer age)
+// 有两个字段的表（String name, Integer age）
 val table: Table = ...
 
-// convert the Table into a DataSet of Row
+// 将表转换成 Row 的批数据集
 val dsRow: DataSet[Row] = tableEnv.toDataSet[Row](table)
 
-// convert the Table into a DataSet of Tuple2[String, Int]
+// 将表转换成 Tuple2[String, Int] 的批数据集
 val dsTuple: DataSet[(String, Int)] = tableEnv.toDataSet[(String, Int)](table)
 {% endhighlight %}
 </div>
@@ -1091,313 +1050,313 @@ val dsTuple: DataSet[(String, Int)] = tableEnv.toDataSet[(String, Int)](table)
 
 {% top %}
 
-### Mapping of Data Types to Table Schema
+### 数据类型到 Table Schema 的映射
 
-Flink's DataStream and DataSet APIs support very diverse types. Composite types such as Tuples (built-in Scala and Flink Java tuples), POJOs, Scala case classes, and Flink's Row type allow for nested data structures with multiple fields that can be accessed in table expressions. Other types are treated as atomic types. In the following, we describe how the Table API converts these types into an internal row representation and show examples of converting a `DataStream` into a `Table`.
+Flink 的 DataStream 和 DataSet APIs 支持多样的数据类型。例如 Tuple（Scala 内置以及Flink Java tuple）、POJO 类型、Scala case class 类型以及 Flink 的 Row 类型等允许嵌套且有多个可在表的表达式中访问的字段的复合数据类型。其他类型被视为原子类型。 Composite types such as Tuples (built-in Scala and Flink Java tuples), POJOs, Scala case classes, and Flink's Row type allow for nested data structures with multiple fields that can be accessed in table expressions. 下面，我们讨论 Table API 如何将这些数据类型类型转换为内部 row 表示形式，并提供将流数据集转换成表的样例。
 
-The mapping of a data type to a table schema can happen in two ways: **based on the field positions** or **based on the field names**.
+数据类型到 table schema 的映射有两种方式：**基于字段位置**或**基于字段名称**。
 
-**Position-based Mapping**
+**基于位置映射**
 
-Position-based mapping can be used to give fields a more meaningful name while keeping the field order. This mapping is available for composite data types *with a defined field order* as well as atomic types. Composite data types such as tuples, rows, and case classes have such a field order. However, fields of a POJO must be mapped based on the field names (see next section). Fields can be projected out but can't be renamed using an alias `as`.
+基于位置的映射可在保持字段顺序的同时为字段提供更有意义的名称。这种映射方式可用于*具有确定的字段顺序*的复合数据类型以及原子类型。如 tuple、row 以及 case class 这些复合数据类型都有这样的字段顺序。然而，POJO 类型的字段则必须通过名称映射（参见下一章）。可以将字段投影出来，但不能使用`as`重命名。
 
-When defining a position-based mapping, the specified names must not exist in the input data type, otherwise the API will assume that the mapping should happen based on the field names. If no field names are specified, the default field names and field order of the composite type are used or `f0` for atomic types. 
+定义基于位置的映射时，输入数据类型中一定不能存在指定的名称，否则 API 会假定应该基于字段名称进行映射。如果未指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 “ f0” 表示原子类型。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section;
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节;
 
 DataStream<Tuple2<Long, Integer>> stream = ...
 
-// convert DataStream into Table with default field names "f0" and "f1"
+// 将流数据集转换成有默认字段 “f0” 和 “f1” 的表
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with field "myLong" only
-Table table = tableEnv.fromDataStream(stream, "myLong");
+// 将流数据集转换成只有字段 “myLong” 的表
+Table table = tableEnv.fromDataStream(stream, “myLong”);
 
-// convert DataStream into Table with field names "myLong" and "myInt"
+// 将流数据集转换成有字段 “myLong” 和 “myInt” 的表
 Table table = tableEnv.fromDataStream(stream, "myLong, myInt");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[(Long, Int)] = ...
 
-// convert DataStream into Table with default field names "_1" and "_2"
+// 将流数据集转换成有默认字段 “_1” 和 “_2” 的表
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with field "myLong" only
+// 将流数据集转换成只有字段 “myLong” 的表
 val table: Table = tableEnv.fromDataStream(stream, 'myLong)
 
-// convert DataStream into Table with field names "myLong" and "myInt"
+// 将流数据集转换成有字段 “myLong” 和 “myInt” 的表
 val table: Table = tableEnv.fromDataStream(stream, 'myLong, 'myInt)
 {% endhighlight %}
 </div>
 </div>
 
-**Name-based Mapping**
+**基于名称的映射**
 
-Name-based mapping can be used for any data type including POJOs. It is the most flexible way of defining a table schema mapping. All fields in the mapping are referenced by name and can be possibly renamed using an alias `as`. Fields can be reordered and projected out.
+基于名称的映射适用于任何数据类型包括 POJO 类型。这是定义 table schema 映射最灵活的方式。映射中的所有字段均按名称引用，并且可以通过 `as` 重命名。字段可以被重新排序和映射。
 
-If no field names are specified, the default field names and field order of the composite type are used or `f0` for atomic types.
+若果没有指定任何字段名称，则使用默认的字段名称和复合数据类型的字段顺序，或者使用 `f0` 表示原子类型。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
 DataStream<Tuple2<Long, Integer>> stream = ...
 
-// convert DataStream into Table with default field names "f0" and "f1"
+// 将流数据集转换成有默认字段 “f0” 和 “f1” 的表
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with field "f1" only
-Table table = tableEnv.fromDataStream(stream, "f1");
+// 将流数据集转换成只有字段 “f1” 的表
+Table table = tableEnv.fromDataStream(stream, “f1”);
 
-// convert DataStream into Table with swapped fields
+// 将流数据集转换成表并交换字段顺序
 Table table = tableEnv.fromDataStream(stream, "f1, f0");
 
-// convert DataStream into Table with swapped fields and field names "myInt" and "myLong"
+// 将流数据集转换成表并交换字段顺序，命名为 “myInt” 和 “myLong”
 Table table = tableEnv.fromDataStream(stream, "f1 as myInt, f0 as myLong");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[(Long, Int)] = ...
 
-// convert DataStream into Table with default field names "_1" and "_2"
+// 将流数据集转换成有默认字段 “_1” 和 “_2” 的表
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with field "_2" only
+// 将流数据集转换成仅有字段 “_2” 的表
 val table: Table = tableEnv.fromDataStream(stream, '_2)
 
-// convert DataStream into Table with swapped fields
+// 将流数据集转换成表并交换字段顺序
 val table: Table = tableEnv.fromDataStream(stream, '_2, '_1)
 
-// convert DataStream into Table with swapped fields and field names "myInt" and "myLong"
+// 将流数据集转换成表并交换字段顺序，将字段命名为 “myInt” 和 “myLong”
 val table: Table = tableEnv.fromDataStream(stream, '_2 as 'myInt, '_1 as 'myLong)
 {% endhighlight %}
 </div>
 </div>
 
-#### Atomic Types
+#### 原子类型
 
-Flink treats primitives (`Integer`, `Double`, `String`) or generic types (types that cannot be analyzed and decomposed) as atomic types. A `DataStream` or `DataSet` of an atomic type is converted into a `Table` with a single attribute. The type of the attribute is inferred from the atomic type and the name of the attribute can be specified.
+Flink 将原始数据类型（`Integer`、`Double`、`String`）或者通用数据类型（不可再拆分的数据类型）视为原子类型。原子类型的`流数据集`或者`批数据集`会被转换成只有一条属性的`表`。属性的数据类型可以由原子类型推断出，还可以重新命名属性。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
 DataStream<Long> stream = ...
 
-// convert DataStream into Table with default field name "f0"
+// 将流数据集转换成有默认字段 “f0” 的表
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with field name "myLong"
-Table table = tableEnv.fromDataStream(stream, "myLong");
+// 将流数据集转换成有字段 “myLong” 的表
+Table table = tableEnv.fromDataStream(stream, “myLong”);
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[Long] = ...
 
-// convert DataStream into Table with default field name "f0"
+// 将流数据集转换成有默认字段 “f0” 的表
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with field name "myLong"
+// 将流数据集转换成有字段 “myLong” 的表
 val table: Table = tableEnv.fromDataStream(stream, 'myLong)
 {% endhighlight %}
 </div>
 </div>
 
-#### Tuples (Scala and Java) and Case Classes (Scala only)
+#### Tuple类型（Scala 和 Java）和 Case Class类型（仅 Scala）
 
-Flink supports Scala's built-in tuples and provides its own tuple classes for Java. DataStreams and DataSets of both kinds of tuples can be converted into tables. Fields can be renamed by providing names for all fields (mapping based on position). If no field names are specified, the default field names are used. If the original field names (`f0`, `f1`, ... for Flink Tuples and `_1`, `_2`, ... for Scala Tuples) are referenced, the API assumes that the mapping is name-based instead of position-based. Name-based mapping allows for reordering fields and projection with alias (`as`).
+Flink 支持 Scala 的内置 tuple 类型并给 Java 提供自己的 tuple 类型。两种 tuple 的流数据集和批数据集都能被转换成表。可以通过提供所有字段名称来重命名字段（基于位置映射）。如果没有指明任何字段名称，则会使用默认的字段名称。如果引用了原始字段名称（对于 Flink tuple 为`f0`、`f1` ... ...，对于 Scala tuple 为`_1`、`_2` ... ...），则 API 会假定映射是基于名称的而不是基于位置的。基于名称的映射可以通过 `as` 对字段和投影进行重新排序。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
 DataStream<Tuple2<Long, String>> stream = ...
 
-// convert DataStream into Table with default field names "f0", "f1"
+// 将流数据集转换成有默认字段字段“f0”、“f1”的表
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with renamed field names "myLong", "myString" (position-based)
+// 将流数据集转换成表并重命名字段为“myLong”、“myString”（基于位置）
 Table table = tableEnv.fromDataStream(stream, "myLong, myString");
 
-// convert DataStream into Table with reordered fields "f1", "f0" (name-based)
+// 将流数据集转换成表并将字段重新排序为“f1”、“f1”（基于名称）
 Table table = tableEnv.fromDataStream(stream, "f1, f0");
 
-// convert DataStream into Table with projected field "f1" (name-based)
-Table table = tableEnv.fromDataStream(stream, "f1");
+// 将流数据集转换成有投影字段“f1”的表（基于名称）
+Table table = tableEnv.fromDataStream(stream, “f1”);
 
-// convert DataStream into Table with reordered and aliased fields "myString", "myLong" (name-based)
+// 将流数据集转换成表并重新命名排序为“myString”、“myLong”（基于名称）
 Table table = tableEnv.fromDataStream(stream, "f1 as 'myString', f0 as 'myLong'");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
 val stream: DataStream[(Long, String)] = ...
 
-// convert DataStream into Table with renamed default field names '_1, '_2
+// 将流数据集转换成有重命名默认字段 '_1、'_2 的表
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with field names "myLong", "myString" (position-based)
+// 将流数据集转换成有字段“myLong”、“myString”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'myLong, 'myString)
 
-// convert DataStream into Table with reordered fields "_2", "_1" (name-based)
+// 将流数据集转换成有重排序字段“_2”、“_1”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, '_2, '_1)
 
-// convert DataStream into Table with projected field "_2" (name-based)
+// 将流数据集转换成有映射字段“_2”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, '_2)
 
-// convert DataStream into Table with reordered and aliased fields "myString", "myLong" (name-based)
+// 将流数据集转换成表并重新命名排序为“myString”、“myLong”（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, '_2 as 'myString, '_1 as 'myLong)
 
-// define case class
+// 定义 case class 类型
 case class Person(name: String, age: Int)
 val streamCC: DataStream[Person] = ...
 
-// convert DataStream into Table with default field names 'name, 'age
+// 将流数据集转换成有默认字段 'name、'age 的表
 val table = tableEnv.fromDataStream(streamCC)
 
-// convert DataStream into Table with field names 'myName, 'myAge (position-based)
+// 将流数据集转换成有字段 'myName、'myAge 的表（基于位置）
 val table = tableEnv.fromDataStream(streamCC, 'myName, 'myAge)
 
-// convert DataStream into Table with reordered and aliased fields "myAge", "myName" (name-based)
+// 将流数据集转换成有重排序字段“myAge”、“myName”的表（基于名称））
 val table: Table = tableEnv.fromDataStream(stream, 'age as 'myAge, 'name as 'myName)
 
 {% endhighlight %}
 </div>
 </div>
 
-#### POJO (Java and Scala)
+#### POJO 类型 （Java 和 Scala）
 
-Flink supports POJOs as composite types. The rules for what determines a POJO are documented [here]({{ site.baseurl }}/dev/api_concepts.html#pojos).
+Flink 支持 POJO 类型作为复合类型。确定 POJO 类型的规则记录在[这里]({{ site.baseurl }}/zh/dev/api_concepts.html#pojos).
 
-When converting a POJO `DataStream` or `DataSet` into a `Table` without specifying field names, the names of the original POJO fields are used. The name mapping requires the original names and cannot be done by positions. Fields can be renamed using an alias (with the `as` keyword), reordered, and projected.
+在不指定字段名称的情况下将 POJO 类型的`流数据集`或`批数据集`转换成`表`时，将使用原始 POJO 类型字段的名称。名称映射需要原始名称，并且不能按位置进行。字段可以使用别名（带有 `as` 关键字）来重命名，重新排序和投影。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// Person is a POJO with fields "name" and "age"
+// Person类时带有“name”和“age”字段的POJO类型
 DataStream<Person> stream = ...
 
-// convert DataStream into Table with default field names "age", "name" (fields are ordered by name!)
+// 将流数据集转换成带有默认字段“age”和“name”的表（字段根据名称排序）
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with renamed fields "myAge", "myName" (name-based)
+// 将流数据集转换成带有重命名字段“myAge”和“myName”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "age as myAge, name as myName");
 
-// convert DataStream into Table with projected field "name" (name-based)
+// 将流数据集转换成带有映射字段“name”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "name");
 
-// convert DataStream into Table with projected and renamed field "myName" (name-based)
+// 将流数据集转换成带有重命名的映射字段“myName”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "name as myName");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
-// Person is a POJO with field names "name" and "age"
+// Person类是拥有字段“name”和“age”的 POJO 类型
 val stream: DataStream[Person] = ...
 
-// convert DataStream into Table with default field names "age", "name" (fields are ordered by name!)
+// 将流数据集转换成带有默认字段“age”和“name”的表（字段根据名称排序）
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with renamed fields "myAge", "myName" (name-based)
+// 将流数据集转换成带有重命名字段“myAge”和“myName”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'age as 'myAge, 'name as 'myName)
 
-// convert DataStream into Table with projected field "name" (name-based)
+// 将流数据集转换成带有映射字段“name”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'name)
 
-// convert DataStream into Table with projected and renamed field "myName" (name-based)
+// 将流数据集转换成带有重命名的映射字段“myName”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'name as 'myName)
 {% endhighlight %}
 </div>
 </div>
 
-#### Row
+#### Row类型
 
-The `Row` data type supports an arbitrary number of fields and fields with `null` values. Field names can be specified via a `RowTypeInfo` or when converting a `Row` `DataStream` or `DataSet` into a `Table`. The row type supports mapping of fields by position and by name. Fields can be renamed by providing names for all fields (mapping based on position) or selected individually for projection/ordering/renaming (mapping based on name).
+`Row` 类型支持任意数量的字段以及具有 `null` 值的字段。字段名称可以通过 `RowTypeInfo` 指定，也可以在将 `Row` 的`流数据集`或`批数据集`转换为`表`时指定。Row 类型的字段映射支持基于名称和基于位置两种方式。字段可以通过提供所有字段的名称的方式重命名（基于位置映射）或者分别选择进行投影/排序/重命名（基于名称映射）。
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
 {% highlight java %}
-// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
-StreamTableEnvironment tableEnv = ...; // see "Create a TableEnvironment" section
+// 创建 StreamTableEnvironment，创建 BatchTableEnvironment 方法相同
+StreamTableEnvironment tableEnv = ...; // 参阅“创建 TableEnvironment ”章节
 
-// DataStream of Row with two fields "name" and "age" specified in `RowTypeInfo`
+// 包含在两个字段“name”和“age”的 Row 的数据流，在 `RowTypeInfo` 中指明字段名称
 DataStream<Row> stream = ...
 
-// convert DataStream into Table with default field names "name", "age"
+// 将流数据集转换成含有默认字段“name”，“age”的表
 Table table = tableEnv.fromDataStream(stream);
 
-// convert DataStream into Table with renamed field names "myName", "myAge" (position-based)
+// 将流数据集转换成含有重命名字段“myName”，“myAge”的表（基于位置）
 Table table = tableEnv.fromDataStream(stream, "myName, myAge");
 
-// convert DataStream into Table with renamed fields "myName", "myAge" (name-based)
+// 将流数据集转换成含有重命名字段“myName”，“myAge”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "name as myName, age as myAge");
 
-// convert DataStream into Table with projected field "name" (name-based)
+// 将流数据集转换成带有映射字段“name”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "name");
 
-// convert DataStream into Table with projected and renamed field "myName" (name-based)
+// 将流数据集转换成带有重命名的映射字段“myName”的表（基于名称）
 Table table = tableEnv.fromDataStream(stream, "name as myName");
 {% endhighlight %}
 </div>
 
 <div data-lang="scala" markdown="1">
 {% highlight scala %}
-// get a TableEnvironment
-val tableEnv: StreamTableEnvironment = ... // see "Create a TableEnvironment" section
+// 创建 TableEnvironment
+val tableEnv: StreamTableEnvironment = ... // 参阅“创建 TableEnvironment ”章节
 
-// DataStream of Row with two fields "name" and "age" specified in `RowTypeInfo`
+// 包含在两个字段“name”和“age”的 Row 的数据流，在 `RowTypeInfo` 中指明字段名称
 val stream: DataStream[Row] = ...
 
-// convert DataStream into Table with default field names "name", "age"
+// 将流数据集转换成含有默认字段“name”，“age”的表
 val table: Table = tableEnv.fromDataStream(stream)
 
-// convert DataStream into Table with renamed field names "myName", "myAge" (position-based)
+// 将流数据集转换成含有重命名字段“myName”，“myAge”的表（基于位置）
 val table: Table = tableEnv.fromDataStream(stream, 'myName, 'myAge)
 
-// convert DataStream into Table with renamed fields "myName", "myAge" (name-based)
+// 将流数据集转换成含有重命名字段“myName”，“myAge”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'name as 'myName, 'age as 'myAge)
 
-// convert DataStream into Table with projected field "name" (name-based)
+// 将流数据集转换成带有映射字段“name”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'name)
 
-// convert DataStream into Table with projected and renamed field "myName" (name-based)
+// 将流数据集转换成带有重命名的映射字段“myName”的表（基于名称）
 val table: Table = tableEnv.fromDataStream(stream, 'name as 'myName)
 {% endhighlight %}
 </div>
@@ -1406,54 +1365,54 @@ val table: Table = tableEnv.fromDataStream(stream, 'name as 'myName)
 {% top %}
 
 
-Query Optimization
+查询优化
 ------------------
 
 <div class="codetabs" markdown="1">
 <div data-lang="Old planner" markdown="1">
 
-Apache Flink leverages Apache Calcite to optimize and translate queries. The optimization currently performed include projection and filter push-down, subquery decorrelation, and other kinds of query rewriting. Old planner does not yet optimize the order of joins, but executes them in the same order as defined in the query (order of Tables in the `FROM` clause and/or order of join predicates in the `WHERE` clause).
+Apache Flink 利用 Apache Calcite 来优化和解析查询。当前执行的优化包括投影和过滤器下推，子查询去相关以及其他类型的查询重写。原版计划程序尚未优化 join 的顺序，而是按照查询中定义的顺序执行它们（FROM 子句中的表顺序和/或 WHERE 子句中的 join 谓词顺序）。
 
-It is possible to tweak the set of optimization rules which are applied in different phases by providing a `CalciteConfig` object. This can be created via a builder by calling `CalciteConfig.createBuilder())` and is provided to the TableEnvironment by calling `tableEnv.getConfig.setPlannerConfig(calciteConfig)`.
+通过提供一个 `CalciteConfig` 对象，可以调整在不同阶段应用的优化规则集合。这个对象可以通过调用构造器 `CalciteConfig.createBuilder()` 创建，并通过调用 `tableEnv.getConfig.setPlannerConfig(calciteConfig)` 提供给 TableEnvironment。
 
 </div>
 
 <div data-lang="Blink planner" markdown="1">
 
-Apache Flink leverages and extends Apache Calcite to perform sophisticated query optimization.
-This includes a series of rule and cost-based optimizations such as:
+Apache Flink 使用并扩展了 Apache Calcite 来执行复杂的查询优化。
+这包括一系列基于规则和成本的优化，例如：
 
-* Subquery decorrelation based on Apache Calcite
-* Project pruning
-* Partition pruning
-* Filter push-down
-* Sub-plan deduplication to avoid duplicate computation
-* Special subquery rewriting, including two parts:
-    * Converts IN and EXISTS into left semi-joins
-    * Converts NOT IN and NOT EXISTS into left anti-join
-* Optional join reordering
-    * Enabled via `table.optimizer.join-reorder-enabled`
+* 基于 Apache Calcite 的子查询解相关
+* 模型剪枝
+* 分区剪枝
+* 过滤器下推
+* 子计划消除重复数据以避免重复计算
+* 特殊子查询重写，包括两部分：
+    * 将 IN 和 EXISTS 转换为 left semi-joins
+    * 将 NOT IN 和 NOT EXISTS 转换为 left anti-join
+* 可选 join 重新排序
+    * 通过 `table.optimizer.join-reorder-enabled` 启用
 
-**Note:** IN/EXISTS/NOT IN/NOT EXISTS are currently only supported in conjunctive conditions in subquery rewriting.
+**注释：** 当前仅在子查询重写的结合条件下支持 IN / EXISTS / NOT IN / NOT EXISTS。
 
-The optimizer makes intelligent decisions, based not only on the plan but also rich statistics available from the data sources and fine-grain costs for each operator such as io, cpu, network, and memory.
+优化器不仅基于计划，而且还基于可从数据源获得的丰富统计信息以及每个算子（例如 io，cpu，网络和内存）的细粒度成本来做出明智的决策。
 
-Advanced users may provide custom optimizations via a `CalciteConfig` object that can be provided to the table environment by calling `TableEnvironment#getConfig#setPlannerConfig`.
+高级用户可以通过 `CalciteConfig` 对象提供自定义优化，可以通过调用  `TableEnvironment＃getConfig＃setPlannerConfig` 将其提供给 TableEnvironment。
 
 </div>
 </div>
 
 
-### Explaining a Table
+### 解释表
 
-The Table API provides a mechanism to explain the logical and optimized query plans to compute a `Table`. 
-This is done through the `TableEnvironment.explain(table)` method or `TableEnvironment.explain()` method. `explain(table)` returns the plan of a given `Table`. `explain()` returns the result of a multiple-sinks plan and is mainly used for the Blink planner. It returns a String describing three plans:
+Table API 提供了一种机制来解释计算`表`的逻辑和优化查询计划。
+这是通过 `TableEnvironment.explain(table)` 或者 `TableEnvironment.explain()` 完成的。`explain(table)` 返回给定`表`的计划。 `explain()` 返回多 sink 计划的结果并且主要用于 Blink 计划器。它返回一个描述三中计划的字符串：
 
-1. the Abstract Syntax Tree of the relational query, i.e., the unoptimized logical query plan,
-2. the optimized logical query plan, and
-3. the physical execution plan.
+1. 关系查询的抽象语法树（the Abstract Syntax Tree），即未优化的逻辑查询计划，
+2. 优化的逻辑查询计划，以及
+3. 物理执行计划。
 
-The following code shows an example and the corresponding output for given `Table` using `explain(table)`:
+以下代码展示了一个示例以及对给定`表`使用 `explain（table）` 的相应输出：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
@@ -1530,17 +1489,17 @@ Stage 1 : Data Source
 Stage 2 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 3 : Operator
-		content : from: (count, word)
-		ship_strategy : REBALANCE
+	Stage 3 ： Operator
+		content ： from： (count, word)
+		ship_strategy ： REBALANCE
 
-		Stage 4 : Operator
-			content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word)
-			ship_strategy : FORWARD
+		Stage 4 ： Operator
+			content ： where： (LIKE(word, _UTF-16LE'F%')), select： (count, word)
+			ship_strategy ： FORWARD
 
-			Stage 5 : Operator
-				content : from: (count, word)
-				ship_strategy : REBALANCE
+			Stage 5 ： Operator
+				content ： from： (count, word)
+				ship_strategy ： REBALANCE
 {% endhighlight %}
 </div>
 
@@ -1565,17 +1524,17 @@ Stage 1 : Data Source
 Stage 2 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 3 : Operator
-		content : from: (count, word)
-		ship_strategy : REBALANCE
+	Stage 3 ： Operator
+		content ： from： (count, word)
+		ship_strategy ： REBALANCE
 
-		Stage 4 : Operator
-			content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word)
-			ship_strategy : FORWARD
+		Stage 4 ： Operator
+			content ： where： (LIKE(word, _UTF-16LE'F%')), select： (count, word)
+			ship_strategy ： FORWARD
 
-			Stage 5 : Operator
-				content : from: (count, word)
-				ship_strategy : REBALANCE
+			Stage 5 ： Operator
+				content ： from： (count, word)
+				ship_strategy ： REBALANCE
 {% endhighlight %}
 </div>
 
@@ -1597,41 +1556,41 @@ DataStreamUnion(all=[true], union all=[count, word])
 Stage 1 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 2 : Operator
-		content : Flat Map
-		ship_strategy : FORWARD
+	Stage 2 ： Operator
+		content ： Flat Map
+		ship_strategy ： FORWARD
 
-		Stage 3 : Operator
-			content : Map
-			ship_strategy : FORWARD
+		Stage 3 ： Operator
+			content ： Map
+			ship_strategy ： FORWARD
 
 Stage 4 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 5 : Operator
-		content : Flat Map
-		ship_strategy : FORWARD
+	Stage 5 ： Operator
+		content ： Flat Map
+		ship_strategy ： FORWARD
 
-		Stage 6 : Operator
-			content : Map
-			ship_strategy : FORWARD
+		Stage 6 ： Operator
+			content ： Map
+			ship_strategy ： FORWARD
 
-			Stage 7 : Operator
-				content : Map
-				ship_strategy : FORWARD
+			Stage 7 ： Operator
+				content ： Map
+				ship_strategy ： FORWARD
 
-				Stage 8 : Operator
-					content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word)
-					ship_strategy : FORWARD
+				Stage 8 ： Operator
+					content ： where： (LIKE(word, _UTF-16LE'F%')), select： (count, word)
+					ship_strategy ： FORWARD
 
-					Stage 9 : Operator
-						content : Map
-						ship_strategy : FORWARD
+					Stage 9 ： Operator
+						content ： Map
+						ship_strategy ： FORWARD
 {% endhighlight %}
 </div>
 </div>
 
-The following code shows an example and the corresponding output for multiple-sinks plan using `explain()`:
+以下代码展示了一个示例以及使用 `explain（）` 的多 sink 计划的相应输出：
 
 <div class="codetabs" markdown="1">
 <div data-lang="java" markdown="1">
@@ -1640,31 +1599,17 @@ The following code shows an example and the corresponding output for multiple-si
 EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
 TableEnvironment tEnv = TableEnvironment.create(settings);
 
-final Schema schema = new Schema()
-    .field("count", DataTypes.INT())
-    .field("word", DataTypes.STRING());
-
-tEnv.connect(new FileSystem("/source/path1"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySource1");
-tEnv.connect(new FileSystem("/source/path2"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySource2");
-tEnv.connect(new FileSystem("/sink/path1"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySink1");
-tEnv.connect(new FileSystem("/sink/path2"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySink2");
-
-Table table1 = tEnv.from("MySource1").where("LIKE(word, 'F%')");
+String[] fieldNames = { "count", "word" };
+TypeInformation[] fieldTypes = { Types.INT, Types.STRING };
+tEnv.registerTableSource("MySource1", new CsvTableSource("/source/path1", fieldNames, fieldTypes));
+tEnv.registerTableSource("MySource2", new CsvTableSource("/source/path2", fieldNames, fieldTypes));
+tEnv.registerTableSink("MySink1", new CsvTableSink("/sink/path1").configure(fieldNames, fieldTypes));
+tEnv.registerTableSink("MySink2", new CsvTableSink("/sink/path2").configure(fieldNames, fieldTypes));
+
+Table table1 = tEnv.scan("MySource1").where("LIKE(word, 'F%')");
 table1.insertInto("MySink1");
 
-Table table2 = table1.unionAll(tEnv.from("MySource2"));
+Table table2 = table1.unionAll(tEnv.scan("MySource2"));
 table2.insertInto("MySink2");
 
 String explanation = tEnv.explain(false);
@@ -1678,31 +1623,17 @@ System.out.println(explanation);
 val settings = EnvironmentSettings.newInstance.useBlinkPlanner.inStreamingMode.build
 val tEnv = TableEnvironment.create(settings)
 
-val schema = new Schema()
-    .field("count", DataTypes.INT())
-    .field("word", DataTypes.STRING())
-
-tEnv.connect(new FileSystem("/source/path1"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySource1")
-tEnv.connect(new FileSystem("/source/path2"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySource2")
-tEnv.connect(new FileSystem("/sink/path1"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySink1")
-tEnv.connect(new FileSystem("/sink/path2"))
-    .withFormat(new Csv().deriveSchema())
-    .withSchema(schema)
-    .createTemporaryTable("MySink2")
-
-val table1 = tEnv.from("MySource1").where("LIKE(word, 'F%')")
+val fieldNames = Array("count", "word")
+val fieldTypes = Array[TypeInformation[_]](Types.INT, Types.STRING)
+tEnv.registerTableSource("MySource1", new CsvTableSource("/source/path1", fieldNames, fieldTypes))
+tEnv.registerTableSource("MySource2", new CsvTableSource("/source/path2",fieldNames, fieldTypes))
+tEnv.registerTableSink("MySink1", new CsvTableSink("/sink/path1").configure(fieldNames, fieldTypes))
+tEnv.registerTableSink("MySink2", new CsvTableSink("/sink/path2").configure(fieldNames, fieldTypes))
+
+val table1 = tEnv.scan("MySource1").where("LIKE(word, 'F%')")
 table1.insertInto("MySink1")
 
-val table2 = table1.unionAll(tEnv.from("MySource2"))
+val table2 = table1.unionAll(tEnv.scan("MySource2"))
 table2.insertInto("MySink2")
 
 val explanation = tEnv.explain(false)
@@ -1716,31 +1647,17 @@ println(explanation)
 settings = EnvironmentSettings.new_instance().use_blink_planner().in_streaming_mode().build()
 t_env = TableEnvironment.create(environment_settings=settings)
 
-schema = Schema()
-    .field("count", DataTypes.INT())
-    .field("word", DataTypes.STRING())
-
-t_env.connect(FileSystem().path("/source/path1")))
-    .with_format(Csv().deriveSchema())
-    .with_schema(schema)
-    .create_temporary_table("MySource1")
-t_env.connect(FileSystem().path("/source/path2")))
-    .with_format(Csv().deriveSchema())
-    .with_schema(schema)
-    .create_temporary_table("MySource2")
-t_env.connect(FileSystem().path("/sink/path1")))
-    .with_format(Csv().deriveSchema())
-    .with_schema(schema)
-    .create_temporary_table("MySink1")
-t_env.connect(FileSystem().path("/sink/path2")))
-    .with_format(Csv().deriveSchema())
-    .with_schema(schema)
-    .create_temporary_table("MySink2")
-
-table1 = t_env.from_path("MySource1").where("LIKE(word, 'F%')")
+field_names = ["count", "word"]
+field_types = [DataTypes.INT(), DataTypes.STRING()]
+t_env.register_table_source("MySource1", CsvTableSource("/source/path1", field_names, field_types))
+t_env.register_table_source("MySource2", CsvTableSource("/source/path2", field_names, field_types))
+t_env.register_table_sink("MySink1", CsvTableSink("/sink/path1", field_names, field_types))
+t_env.register_table_sink("MySink2", CsvTableSink("/sink/path2", field_names, field_types))
+
+table1 = t_env.scan("MySource1").where("LIKE(word, 'F%')")
 table1.insert_into("MySink1")
 
-table2 = table1.union_all(t_env.from_path("MySource2"))
+table2 = table1.union_all(t_env.scan("MySource2"))
 table2.insert_into("MySink2")
 
 explanation = t_env.explain()
@@ -1749,7 +1666,7 @@ print(explanation)
 </div>
 </div>
 
-the result of multiple-sinks plan is
+多 sink 计划的结果是：
 <div>
 {% highlight text %}
 
@@ -1780,56 +1697,54 @@ Sink(name=[MySink2], fields=[count, word])
 Stage 1 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 2 : Operator
-		content : CsvTableSource(read fields: count, word)
-		ship_strategy : REBALANCE
+	Stage 2 ： Operator
+		content ： CsvTableSource(read fields： count, word)
+		ship_strategy ： REBALANCE
 
-		Stage 3 : Operator
-			content : SourceConversion(table:Buffer(default_catalog, default_database, MySource1, source: [CsvTableSource(read fields: count, word)]), fields:(count, word))
-			ship_strategy : FORWARD
+		Stage 3 ： Operator
+			content ： SourceConversion(table：Buffer(default_catalog, default_database, MySource1, source： [CsvTableSource(read fields： count, word)]), fields：(count, word))
+			ship_strategy ： FORWARD
 
-			Stage 4 : Operator
-				content : Calc(where: (word LIKE _UTF-16LE'F%'), select: (count, word))
-				ship_strategy : FORWARD
+			Stage 4 ： Operator
+				content ： Calc(where： (word LIKE _UTF-16LE'F%'), select： (count, word))
+				ship_strategy ： FORWARD
 
-				Stage 5 : Operator
-					content : SinkConversionToRow
-					ship_strategy : FORWARD
+				Stage 5 ： Operator
+					content ： SinkConversionToRow
+					ship_strategy ： FORWARD
 
-					Stage 6 : Operator
-						content : Map
-						ship_strategy : FORWARD
+					Stage 6 ： Operator
+						content ： Map
+						ship_strategy ： FORWARD
 
 Stage 8 : Data Source
 	content : collect elements with CollectionInputFormat
 
-	Stage 9 : Operator
-		content : CsvTableSource(read fields: count, word)
-		ship_strategy : REBALANCE
+	Stage 9 ： Operator
+		content ： CsvTableSource(read fields： count, word)
+		ship_strategy ： REBALANCE
 
-		Stage 10 : Operator
-			content : SourceConversion(table:Buffer(default_catalog, default_database, MySource2, source: [CsvTableSource(read fields: count, word)]), fields:(count, word))
-			ship_strategy : FORWARD
+		Stage 10 ： Operator
+			content ： SourceConversion(table：Buffer(default_catalog, default_database, MySource2, source： [CsvTableSource(read fields： count, word)]), fields：(count, word))
+			ship_strategy ： FORWARD
 
-			Stage 12 : Operator
-				content : SinkConversionToRow
-				ship_strategy : FORWARD
+			Stage 12 ： Operator
+				content ： SinkConversionToRow
+				ship_strategy ： FORWARD
 
-				Stage 13 : Operator
-					content : Map
-					ship_strategy : FORWARD
+				Stage 13 ： Operator
+					content ： Map
+					ship_strategy ： FORWARD
 
-					Stage 7 : Data Sink
-						content : Sink: CsvTableSink(count, word)
-						ship_strategy : FORWARD
+					Stage 7 ： Data Sink
+						content ： Sink： CsvTableSink(count, word)
+						ship_strategy ： FORWARD
 
-						Stage 14 : Data Sink
-							content : Sink: CsvTableSink(count, word)
-							ship_strategy : FORWARD
+						Stage 14 ： Data Sink
+							content ： Sink： CsvTableSink(count, word)
+							ship_strategy ： FORWARD
 
 {% endhighlight %}
 </div>
 
 {% top %}
-
-
