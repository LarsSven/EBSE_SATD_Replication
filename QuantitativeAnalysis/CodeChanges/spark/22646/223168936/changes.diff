diff --git a/docs/sql-programming-guide.md b/docs/sql-programming-guide.md
index a1d7b1108bf73..0fa1cbc56844a 100644
--- a/docs/sql-programming-guide.md
+++ b/docs/sql-programming-guide.md
@@ -304,9 +304,9 @@ registered as a table. Tables can be used in subsequent SQL statements.
 
 Spark SQL supports automatically converting an RDD of
 [JavaBeans](http://stackoverflow.com/questions/3295496/what-is-a-javabean-exactly) into a DataFrame.
-The `BeanInfo`, obtained using reflection, defines the schema of the table. Currently, Spark SQL
-does not support JavaBeans that contain `Map` field(s). Nested JavaBeans and `List` or `Array`
-fields are supported though. You can create a JavaBean by creating a class that implements
+The `BeanInfo`, obtained using reflection, defines the schema of the table. Spark SQL supports
+fields that contain `List`, `Array`, `Map` or a nested JavaBean. JavaBeans are also supported as collection elements.
+You can create a JavaBean by creating a class that implements
 Serializable and has getters and setters for all of its fields.
 
 {% include_example schema_inferring java/org/apache/spark/examples/sql/JavaSparkSQLExample.java %}
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala b/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
index 7d770bad686cd..f36e66e1d4a31 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala
@@ -17,8 +17,10 @@
 
 package org.apache.spark.sql
 
+import java.lang.reflect.{Array => JavaArray, ParameterizedType, Type}
 import java.util.Properties
 
+import scala.collection.JavaConverters._
 import scala.collection.immutable
 import scala.reflect.runtime.universe.TypeTag
 
@@ -1099,13 +1101,14 @@ object SQLContext {
       data: Iterator[_],
       beanClass: Class[_],
       attrs: Seq[AttributeReference]): Iterator[InternalRow] = {
-    import scala.collection.JavaConverters._
-    import java.lang.reflect.{Type, ParameterizedType, Array => JavaArray}
-    def interfaceParameters(t: Type, interface: Class[_]): Array[Type] = t match {
-      case parType: ParameterizedType if parType.getRawType == interface =>
-        parType.getActualTypeArguments
-      case _ => throw new UnsupportedOperationException(s"$t is not an $interface")
-    }
+    def interfaceParameters(t: Type, interface: Class[_], dataType: DataType): Array[Type] =
+      t match {
+        case parType: ParameterizedType if parType.getRawType == interface =>
+          parType.getActualTypeArguments
+        case _ => throw new UnsupportedOperationException(
+          s"Type ${t.getTypeName} is not supported for data type ${dataType.simpleString}. " +
+            s"Expected ${interface.getName}")
+      }
     def createStructConverter(cls: Class[_], fieldTypes: Seq[DataType]): Any => InternalRow = {
       val methodConverters =
         JavaTypeInference.getJavaBeanReadableProperties(cls).zip(fieldTypes)
@@ -1125,21 +1128,25 @@ object SQLContext {
     }
     def createConverter(t: Type, dataType: DataType): Any => Any = (t, dataType) match {
       case (cls: Class[_], struct: StructType) =>
+        // bean type
         createStructConverter(cls, struct.map(_.dataType))
-      case (arrayType: Class[_], array: ArrayType) =>
+      case (arrayType: Class[_], array: ArrayType) if arrayType.isArray =>
+        // array type
         val converter = createConverter(arrayType.getComponentType, array.elementType)
         value => new GenericArrayData(
           (0 until JavaArray.getLength(value)).map(i =>
             converter(JavaArray.get(value, i))).toArray)
       case (_, array: ArrayType) =>
+        // java.util.List type
         val cls = classOf[java.util.List[_]]
-        val params = interfaceParameters(t, cls)
+        val params = interfaceParameters(t, cls, dataType)
         val converter = createConverter(params(0), array.elementType)
         value => new GenericArrayData(
           value.asInstanceOf[java.util.List[_]].asScala.map(converter).toArray)
       case (_, map: MapType) =>
+        // java.util.Map type
         val cls = classOf[java.util.Map[_, _]]
-        val params = interfaceParameters(t, cls)
+        val params = interfaceParameters(t, cls, dataType)
         val keyConverter = createConverter(params(0), map.keyType)
         val valueConverter = createConverter(params(1), map.valueType)
         value => {
@@ -1148,7 +1155,9 @@ object SQLContext {
             new GenericArrayData(keys.map(keyConverter).toArray),
             new GenericArrayData(values.map(valueConverter).toArray))
         }
-      case _ => CatalystTypeConverters.createToCatalystConverter(dataType)
+      case _ =>
+        // other types
+        CatalystTypeConverters.createToCatalystConverter(dataType)
     }
     val dataConverter = createStructConverter(beanClass, attrs.map(_.dataType))
     data.map(dataConverter)
