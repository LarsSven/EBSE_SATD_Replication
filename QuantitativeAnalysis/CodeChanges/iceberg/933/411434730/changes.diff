diff --git a/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java b/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java
new file mode 100644
index 00000000000..8ddb543f449
--- /dev/null
+++ b/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.FileScanTask;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.avro.Avro;
+import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.orc.GenericOrcReader;
+import org.apache.iceberg.data.parquet.GenericParquetReaders;
+import org.apache.iceberg.expressions.Evaluator;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.expressions.Expressions;
+import org.apache.iceberg.hadoop.HadoopInputFile;
+import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.orc.ORC;
+import org.apache.iceberg.parquet.Parquet;
+
+public class IcebergRecordReader {
+
+  private boolean applyResidual;
+  private boolean caseSensitive;
+  private boolean reuseContainers;
+
+  private void initialize(Configuration conf) {
+    this.applyResidual = !conf.getBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, false);
+    this.caseSensitive = conf.getBoolean(InputFormatConfig.CASE_SENSITIVE, true);
+    this.reuseContainers = conf.getBoolean(InputFormatConfig.REUSE_CONTAINERS, false);
+  }
+
+  public CloseableIterable createReader(Configuration config, FileScanTask currentTask, Schema readSchema) {
+    initialize(config);
+    DataFile file = currentTask.file();
+    // TODO we should make use of FileIO to create inputFile
+    InputFile inputFile = HadoopInputFile.fromLocation(file.path(), config);
+    switch (file.format()) {
+      case AVRO:
+        return newAvroIterable(inputFile, currentTask, readSchema);
+      case ORC:
+        return newOrcIterable(inputFile, currentTask, readSchema);
+      case PARQUET:
+        return newParquetIterable(inputFile, currentTask, readSchema);
+      default:
+        throw new UnsupportedOperationException(
+            String.format("Cannot read %s file: %s", file.format().name(), file.path()));
+    }
+  }
+
+  private CloseableIterable newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile).project(readSchema).split(task.start(), task.length());
+    if (reuseContainers) {
+      avroReadBuilder.reuseContainers();
+    }
+    avroReadBuilder.createReaderFunc(DataReader::create);
+    return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    Parquet.ReadBuilder parquetReadBuilder = Parquet
+        .read(inputFile)
+        .project(readSchema)
+        .filter(task.residual())
+        .caseSensitive(caseSensitive)
+        .split(task.start(), task.length());
+    if (reuseContainers) {
+      parquetReadBuilder.reuseContainers();
+    }
+
+    parquetReadBuilder.createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));
+
+    return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    ORC.ReadBuilder orcReadBuilder = ORC
+        .read(inputFile)
+        .project(readSchema)
+        .caseSensitive(caseSensitive)
+        .split(task.start(), task.length());
+    // ORC does not support reuse containers yet
+    orcReadBuilder.createReaderFunc(fileSchema -> GenericOrcReader.buildReader(readSchema, fileSchema));
+    return applyResidualFiltering(orcReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable applyResidualFiltering(CloseableIterable iter, Expression residual, Schema readSchema) {
+    if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
+      Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
+      return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
+    } else {
+      return iter;
+    }
+  }
+
+}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java b/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
new file mode 100644
index 00000000000..bbfecd0e6b2
--- /dev/null
+++ b/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import com.google.common.base.Preconditions;
+import java.util.function.Function;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.SchemaParser;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.catalog.Catalog;
+import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.common.DynConstructors;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.hadoop.HadoopTables;
+
+public class InputFormatConfig {
+
+  private InputFormatConfig() {}
+
+  public static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
+  public static final String CASE_SENSITIVE = "iceberg.mr.case.sensitive";
+  public static final String SKIP_RESIDUAL_FILTERING = "skip.residual.filtering";
+  public static final String AS_OF_TIMESTAMP = "iceberg.mr.as.of.time";
+  public static final String FILTER_EXPRESSION = "iceberg.mr.filter.expression";
+  public static final String IN_MEMORY_DATA_MODEL = "iceberg.mr.in.memory.data.model";
+  public static final String READ_SCHEMA = "iceberg.mr.read.schema";
+  public static final String SNAPSHOT_ID = "iceberg.mr.snapshot.id";
+  public static final String SPLIT_SIZE = "iceberg.mr.split.size";
+  public static final String TABLE_PATH = "iceberg.mr.table.path";
+  public static final String TABLE_SCHEMA = "iceberg.mr.table.schema";
+  public static final String LOCALITY = "iceberg.mr.locality";
+  public static final String CATALOG = "iceberg.mr.catalog";
+
+  public static class ConfigBuilder {
+    private final Configuration conf;
+
+    public ConfigBuilder(Configuration conf) {
+      this.conf = conf;
+      // defaults
+      conf.setBoolean(SKIP_RESIDUAL_FILTERING, false);
+      conf.setBoolean(CASE_SENSITIVE, true);
+      conf.setBoolean(REUSE_CONTAINERS, false);
+      conf.setBoolean(LOCALITY, false);
+    }
+
+    public ConfigBuilder readFrom(String path) {
+      conf.set(TABLE_PATH, path);
+      Table table = findTable(conf);
+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));
+      return this;
+    }
+
+    public ConfigBuilder filter(Expression expression) {
+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));
+      return this;
+    }
+
+    public ConfigBuilder project(Schema schema) {
+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));
+      return this;
+    }
+
+    public ConfigBuilder reuseContainers(boolean reuse) {
+      conf.setBoolean(InputFormatConfig.REUSE_CONTAINERS, reuse);
+      return this;
+    }
+
+    public ConfigBuilder caseSensitive(boolean caseSensitive) {
+      conf.setBoolean(InputFormatConfig.CASE_SENSITIVE, caseSensitive);
+      return this;
+    }
+
+    public ConfigBuilder snapshotId(long snapshotId) {
+      conf.setLong(SNAPSHOT_ID, snapshotId);
+      return this;
+    }
+
+    public ConfigBuilder asOfTime(long asOfTime) {
+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);
+      return this;
+    }
+
+    public ConfigBuilder splitSize(long splitSize) {
+      conf.setLong(SPLIT_SIZE, splitSize);
+      return this;
+    }
+
+    /**
+     * If this API is called. The input splits
+     * constructed will have host location information
+     */
+    public ConfigBuilder preferLocality() {
+      conf.setBoolean(LOCALITY, true);
+      return this;
+    }
+
+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {
+      conf.setClass(CATALOG, catalogFuncClass, Function.class);
+      return this;
+    }
+
+    /**
+     * Compute platforms pass down filters to data sources. If the data source cannot apply some filters, or only
+     * partially applies the filter, it will return the residual filter back. If the platform can correctly apply
+     * the residual filters, then it should call this api. Otherwise the current api will throw an exception if the
+     * passed in filter is not completely satisfied.
+     */
+    public ConfigBuilder skipResidualFiltering() {
+      conf.setBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, true);
+      return this;
+    }
+  }
+
+  public static Table findTable(Configuration conf) {
+    String path = conf.get(InputFormatConfig.TABLE_PATH);
+    Preconditions.checkArgument(path != null, "Table path should not be null");
+    if (path.contains("/")) {
+      HadoopTables tables = new HadoopTables(conf);
+      return tables.load(path);
+    }
+
+    String catalogFuncClass = conf.get(InputFormatConfig.CATALOG);
+    if (catalogFuncClass != null) {
+      Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)
+          DynConstructors.builder(Function.class)
+                         .impl(catalogFuncClass)
+                         .build()
+                         .newInstance();
+      Catalog catalog = catalogFunc.apply(conf);
+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);
+      return catalog.loadTable(tableIdentifier);
+    } else {
+      throw new IllegalArgumentException("No custom catalog specified to load table " + path);
+    }
+  }
+
+}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
index 1b4428128b4..32900b3888c 100644
--- a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
+++ b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
@@ -38,15 +38,12 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.iceberg.CombinedScanTask;
-import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileScanTask;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.hadoop.HadoopInputFile;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.mr.SerializationUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -60,7 +57,6 @@
   private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);
 
   static final String TABLE_LOCATION = "location";
-  static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
 
   private Table table;
 
@@ -116,12 +112,10 @@ public class IcebergRecordReader implements RecordReader<Void, IcebergWritable>
     private CloseableIterable<Record> reader;
     private Iterator<Record> recordIterator;
     private Record currentRecord;
-    private boolean reuseContainers;
 
     public IcebergRecordReader(InputSplit split, JobConf conf) throws IOException {
       this.split = (IcebergSplit) split;
       this.conf = conf;
-      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);
       initialise();
     }
 
@@ -132,11 +126,9 @@ private void initialise() {
 
     private void nextTask() {
       FileScanTask currentTask = tasks.next();
-      DataFile file = currentTask.file();
-      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), conf);
       Schema tableSchema = table.schema();
-      IcebergReaderFactory<Record> readerFactory = new IcebergReaderFactory<Record>();
-      reader = readerFactory.createReader(file, currentTask, inputFile, tableSchema, reuseContainers);
+      org.apache.iceberg.mr.IcebergRecordReader wrappedReader = new org.apache.iceberg.mr.IcebergRecordReader();
+      reader = wrappedReader.createReader(conf, currentTask, tableSchema);
       recordIterator = reader.iterator();
     }
 
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java
deleted file mode 100644
index 1f3f8e6489a..00000000000
--- a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.mr.mapred;
-
-import org.apache.iceberg.DataFile;
-import org.apache.iceberg.FileScanTask;
-import org.apache.iceberg.Schema;
-import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.data.avro.DataReader;
-import org.apache.iceberg.data.orc.GenericOrcReader;
-import org.apache.iceberg.data.parquet.GenericParquetReaders;
-import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
-import org.apache.iceberg.orc.ORC;
-import org.apache.iceberg.parquet.Parquet;
-
-class IcebergReaderFactory<T> {
-
-  public CloseableIterable<T> createReader(DataFile file, FileScanTask currentTask, InputFile inputFile,
-      Schema tableSchema, boolean reuseContainers) {
-    switch (file.format()) {
-      case AVRO:
-        return buildAvroReader(currentTask, inputFile, tableSchema, reuseContainers);
-      case ORC:
-        return buildOrcReader(currentTask, inputFile, tableSchema, reuseContainers);
-      case PARQUET:
-        return buildParquetReader(currentTask, inputFile, tableSchema, reuseContainers);
-
-      default:
-        throw new UnsupportedOperationException(String.format("Cannot read %s file: %s", file.format().name(),
-            file.path()));
-    }
-  }
-
-  private CloseableIterable<T> buildAvroReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    Avro.ReadBuilder builder = Avro.read(inputFile)
-        .createReaderFunc(DataReader::create)
-        .project(schema)
-        .split(task.start(), task.length());
-
-    if (reuseContainers) {
-      builder.reuseContainers();
-    }
-
-    return builder.build();
-  }
-
-  private CloseableIterable<T> buildOrcReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    ORC.ReadBuilder builder = ORC.read(inputFile)
-        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))
-        .project(schema)
-        .split(task.start(), task.length());
-
-    return builder.build();
-  }
-
-  private CloseableIterable<T> buildParquetReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    Parquet.ReadBuilder builder = Parquet.read(inputFile)
-        .createReaderFunc(fileSchema  -> GenericParquetReaders.buildReader(schema, fileSchema))
-        .project(schema)
-        .split(task.start(), task.length());
-
-    if (reuseContainers) {
-      builder.reuseContainers();
-    }
-
-    return builder.build();
-  }
-}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
index 0d35644ab89..9371ec70e65 100644
--- a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
+++ b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.mr.mapreduce;
 
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -32,7 +31,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.function.Function;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.InputFormat;
@@ -52,27 +50,15 @@
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.TableScan;
-import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.catalog.Catalog;
-import org.apache.iceberg.catalog.TableIdentifier;
-import org.apache.iceberg.common.DynConstructors;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
-import org.apache.iceberg.data.orc.GenericOrcReader;
-import org.apache.iceberg.data.parquet.GenericParquetReaders;
 import org.apache.iceberg.exceptions.RuntimeIOException;
-import org.apache.iceberg.expressions.Evaluator;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.Expressions;
-import org.apache.iceberg.hadoop.HadoopInputFile;
-import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.hadoop.Util;
 import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.SerializationUtil;
-import org.apache.iceberg.orc.ORC;
-import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
 import org.slf4j.Logger;
@@ -85,20 +71,6 @@
 public class IcebergInputFormat<T> extends InputFormat<Void, T> {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);
 
-  static final String AS_OF_TIMESTAMP = "iceberg.mr.as.of.time";
-  static final String CASE_SENSITIVE = "iceberg.mr.case.sensitive";
-  static final String FILTER_EXPRESSION = "iceberg.mr.filter.expression";
-  static final String IN_MEMORY_DATA_MODEL = "iceberg.mr.in.memory.data.model";
-  static final String READ_SCHEMA = "iceberg.mr.read.schema";
-  static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
-  static final String SNAPSHOT_ID = "iceberg.mr.snapshot.id";
-  static final String SPLIT_SIZE = "iceberg.mr.split.size";
-  static final String TABLE_PATH = "iceberg.mr.table.path";
-  static final String TABLE_SCHEMA = "iceberg.mr.table.schema";
-  static final String LOCALITY = "iceberg.mr.locality";
-  static final String CATALOG = "iceberg.mr.catalog";
-  static final String SKIP_RESIDUAL_FILTERING = "skip.residual.filtering";
-
   private transient List<InputSplit> splits;
 
   private enum InMemoryDataModel {
@@ -113,100 +85,9 @@ private enum InMemoryDataModel {
    *
    * @param job the {@code Job} to configure
    */
-  public static ConfigBuilder configure(Job job) {
+  public static InputFormatConfig.ConfigBuilder configure(Job job) {
     job.setInputFormatClass(IcebergInputFormat.class);
-    return new ConfigBuilder(job.getConfiguration());
-  }
-
-  public static class ConfigBuilder {
-    private final Configuration conf;
-
-    public ConfigBuilder(Configuration conf) {
-      this.conf = conf;
-      // defaults
-      conf.setEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
-      conf.setBoolean(SKIP_RESIDUAL_FILTERING, false);
-      conf.setBoolean(CASE_SENSITIVE, true);
-      conf.setBoolean(REUSE_CONTAINERS, false);
-      conf.setBoolean(LOCALITY, false);
-    }
-
-    public ConfigBuilder readFrom(String path) {
-      conf.set(TABLE_PATH, path);
-      Table table = findTable(conf);
-      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));
-      return this;
-    }
-
-    public ConfigBuilder filter(Expression expression) {
-      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));
-      return this;
-    }
-
-    public ConfigBuilder project(Schema schema) {
-      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));
-      return this;
-    }
-
-    public ConfigBuilder reuseContainers(boolean reuse) {
-      conf.setBoolean(REUSE_CONTAINERS, reuse);
-      return this;
-    }
-
-    public ConfigBuilder caseSensitive(boolean caseSensitive) {
-      conf.setBoolean(CASE_SENSITIVE, caseSensitive);
-      return this;
-    }
-
-    public ConfigBuilder snapshotId(long snapshotId) {
-      conf.setLong(SNAPSHOT_ID, snapshotId);
-      return this;
-    }
-
-    public ConfigBuilder asOfTime(long asOfTime) {
-      conf.setLong(AS_OF_TIMESTAMP, asOfTime);
-      return this;
-    }
-
-    public ConfigBuilder splitSize(long splitSize) {
-      conf.setLong(SPLIT_SIZE, splitSize);
-      return this;
-    }
-
-    /**
-     * If this API is called. The input splits
-     * constructed will have host location information
-     */
-    public ConfigBuilder preferLocality() {
-      conf.setBoolean(LOCALITY, true);
-      return this;
-    }
-
-    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {
-      conf.setClass(CATALOG, catalogFuncClass, Function.class);
-      return this;
-    }
-
-    public ConfigBuilder useHiveRows() {
-      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());
-      return this;
-    }
-
-    public ConfigBuilder usePigTuples() {
-      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());
-      return this;
-    }
-
-    /**
-     * Compute platforms pass down filters to data sources. If the data source cannot apply some filters, or only
-     * partially applies the filter, it will return the residual filter back. If the platform can correctly apply
-     * the residual filters, then it should call this api. Otherwise the current api will throw an exception if the
-     * passed in filter is not completely satisfied.
-     */
-    public ConfigBuilder skipResidualFiltering() {
-      conf.setBoolean(SKIP_RESIDUAL_FILTERING, true);
-      return this;
-    }
+    return new InputFormatConfig.ConfigBuilder(job.getConfiguration());
   }
 
   @Override
@@ -217,35 +98,35 @@ public List<InputSplit> getSplits(JobContext context) {
     }
 
     Configuration conf = context.getConfiguration();
-    Table table = findTable(conf);
+    Table table = InputFormatConfig.findTable(conf);
     TableScan scan = table.newScan()
-                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));
-    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);
+                          .caseSensitive(conf.getBoolean(InputFormatConfig.CASE_SENSITIVE, true));
+    long snapshotId = conf.getLong(InputFormatConfig.SNAPSHOT_ID, -1);
     if (snapshotId != -1) {
       scan = scan.useSnapshot(snapshotId);
     }
-    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);
+    long asOfTime = conf.getLong(InputFormatConfig.AS_OF_TIMESTAMP, -1);
     if (asOfTime != -1) {
       scan = scan.asOfTime(asOfTime);
     }
-    long splitSize = conf.getLong(SPLIT_SIZE, 0);
+    long splitSize = conf.getLong(InputFormatConfig.SPLIT_SIZE, 0);
     if (splitSize > 0) {
       scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));
     }
-    String schemaStr = conf.get(READ_SCHEMA);
+    String schemaStr = conf.get(InputFormatConfig.READ_SCHEMA);
     if (schemaStr != null) {
       scan.project(SchemaParser.fromJson(schemaStr));
     }
 
     // TODO add a filter parser to get rid of Serialization
-    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));
+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(InputFormatConfig.FILTER_EXPRESSION));
     if (filter != null) {
       scan = scan.filter(filter);
     }
 
     splits = Lists.newArrayList();
-    boolean applyResidual = !conf.getBoolean(SKIP_RESIDUAL_FILTERING, false);
-    InMemoryDataModel model = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
+    boolean applyResidual = !conf.getBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, false);
+    InMemoryDataModel model = conf.getEnum(InputFormatConfig.IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
     try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {
       tasksIterable.forEach(task -> {
         if (applyResidual && (model == InMemoryDataModel.HIVE || model == InMemoryDataModel.PIG)) {
@@ -282,8 +163,6 @@ public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptCon
     private TaskAttemptContext context;
     private Schema tableSchema;
     private Schema expectedSchema;
-    private boolean reuseContainers;
-    private boolean caseSensitive;
     private InMemoryDataModel inMemoryDataModel;
     private Map<String, Integer> namesToPos;
     private Iterator<FileScanTask> tasks;
@@ -298,13 +177,11 @@ public void initialize(InputSplit split, TaskAttemptContext newContext) {
       CombinedScanTask task = ((IcebergSplit) split).task;
       this.context = newContext;
       this.tasks = task.files().iterator();
-      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));
-      String readSchemaStr = conf.get(READ_SCHEMA);
+      this.tableSchema = SchemaParser.fromJson(conf.get(InputFormatConfig.TABLE_SCHEMA));
+      String readSchemaStr = conf.get(InputFormatConfig.READ_SCHEMA);
       this.expectedSchema = readSchemaStr != null ? SchemaParser.fromJson(readSchemaStr) : tableSchema;
       this.namesToPos = buildNameToPos(expectedSchema);
-      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);
-      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);
-      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
+      this.inMemoryDataModel = conf.getEnum(InputFormatConfig.IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
       this.currentIterator = open(tasks.next());
     }
 
@@ -378,24 +255,8 @@ private Iterator<T> open(FileScanTask currentTask) {
     }
 
     private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {
-      DataFile file = currentTask.file();
-      // TODO we should make use of FileIO to create inputFile
-      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());
-      CloseableIterable<T> iterable;
-      switch (file.format()) {
-        case AVRO:
-          iterable = newAvroIterable(inputFile, currentTask, readSchema);
-          break;
-        case ORC:
-          iterable = newOrcIterable(inputFile, currentTask, readSchema);
-          break;
-        case PARQUET:
-          iterable = newParquetIterable(inputFile, currentTask, readSchema);
-          break;
-        default:
-          throw new UnsupportedOperationException(
-              String.format("Cannot read %s file: %s", file.format().name(), file.path()));
-      }
+      org.apache.iceberg.mr.IcebergRecordReader wrappedReader = new org.apache.iceberg.mr.IcebergRecordReader();
+      CloseableIterable<T> iterable = wrappedReader.createReader(context.getConfiguration(), currentTask, readSchema);
       currentCloseable = iterable;
       return iterable.iterator();
     }
@@ -443,99 +304,6 @@ private Record withIdentityPartitionColumns(
       return row;
     }
 
-    private CloseableIterable<T> applyResidualFiltering(CloseableIterable<T> iter, Expression residual,
-                                                        Schema readSchema) {
-      boolean applyResidual = !context.getConfiguration().getBoolean(SKIP_RESIDUAL_FILTERING, false);
-
-      if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
-        Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
-        return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
-      } else {
-        return iter;
-      }
-    }
-
-    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)
-          .project(readSchema)
-          .split(task.start(), task.length());
-      if (reuseContainers) {
-        avroReadBuilder.reuseContainers();
-      }
-
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("Avro support not yet supported for Pig and Hive");
-        case GENERIC:
-          avroReadBuilder.createReaderFunc(DataReader::create);
-      }
-      return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);
-    }
-
-    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)
-          .project(readSchema)
-          .filter(task.residual())
-          .caseSensitive(caseSensitive)
-          .split(task.start(), task.length());
-      if (reuseContainers) {
-        parquetReadBuilder.reuseContainers();
-      }
-
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("Parquet support not yet supported for Pig and Hive");
-        case GENERIC:
-          parquetReadBuilder.createReaderFunc(
-              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));
-      }
-      return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);
-    }
-
-    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)
-          .project(readSchema)
-          .caseSensitive(caseSensitive)
-          .split(task.start(), task.length());
-      // ORC does not support reuse containers yet
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO: implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("ORC support not yet supported for Pig and Hive");
-        case GENERIC:
-          orcReadBuilder.createReaderFunc(fileSchema -> GenericOrcReader.buildReader(readSchema, fileSchema));
-      }
-
-      return applyResidualFiltering(orcReadBuilder.build(), task.residual(), readSchema);
-    }
-  }
-
-  private static Table findTable(Configuration conf) {
-    String path = conf.get(TABLE_PATH);
-    Preconditions.checkArgument(path != null, "Table path should not be null");
-    if (path.contains("/")) {
-      HadoopTables tables = new HadoopTables(conf);
-      return tables.load(path);
-    }
-
-    String catalogFuncClass = conf.get(CATALOG);
-    if (catalogFuncClass != null) {
-      Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)
-          DynConstructors.builder(Function.class)
-                         .impl(catalogFuncClass)
-                         .build()
-                         .newInstance();
-      Catalog catalog = catalogFunc.apply(conf);
-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);
-      return catalog.loadTable(tableIdentifier);
-    } else {
-      throw new IllegalArgumentException("No custom catalog specified to load table " + path);
-    }
   }
 
   static class IcebergSplit extends InputSplit implements Writable {
@@ -556,7 +324,7 @@ public long getLength() {
 
     @Override
     public String[] getLocations() {
-      boolean localityPreferred = conf.getBoolean(LOCALITY, false);
+      boolean localityPreferred = conf.getBoolean(InputFormatConfig.LOCALITY, false);
       if (!localityPreferred) {
         return ANYWHERE;
       }
diff --git a/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java b/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
index 957f165ea7f..7b8258db8ee 100644
--- a/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
+++ b/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
@@ -40,7 +40,6 @@
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.iceberg.AppendFiles;
-import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.PartitionSpec;
@@ -54,6 +53,7 @@
 import org.apache.iceberg.expressions.Expressions;
 import org.apache.iceberg.hadoop.HadoopCatalog;
 import org.apache.iceberg.mr.BaseInputFormatTest;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.TestHelpers.Row;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
@@ -74,7 +74,7 @@ public TestIcebergInputFormat(String format) {
   @Override
   protected void runAndValidate(File tableLocation, List<Record> expectedRecords) throws IOException {
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(tableLocation.toString());
     validate(job, expectedRecords);
   }
@@ -99,7 +99,7 @@ public void testFilterExp() throws Exception {
          .appendFile(dataFile2)
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString())
                  .filter(Expressions.equal("date", "2020-03-20"));
     validate(job, expectedRecords);
@@ -129,7 +129,7 @@ public void testResiduals() throws Exception {
          .appendFile(dataFile2)
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString())
         .filter(Expressions.and(
             Expressions.equal("date", "2020-03-20"),
@@ -146,45 +146,6 @@ public void testResiduals() throws Exception {
     validate(job, writeRecords);
   }
 
-  @Test
-  public void testFailedResidualFiltering() throws Exception {
-    File location = temp.newFolder(fileFormat.name());
-    Assert.assertTrue(location.delete());
-    Table table = tables.create(SCHEMA, SPEC,
-        ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
-        location.toString());
-    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 2, 0L);
-    expectedRecords.get(0).set(2, "2020-03-20");
-    expectedRecords.get(1).set(2, "2020-03-20");
-
-    DataFile dataFile1 = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, expectedRecords);
-    table.newAppend()
-        .appendFile(dataFile1)
-        .commit();
-
-    Job jobShouldFail1 = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(jobShouldFail1);
-    configBuilder.useHiveRows().readFrom(location.toString())
-        .filter(Expressions.and(
-            Expressions.equal("date", "2020-03-20"),
-            Expressions.equal("id", 0)));
-    AssertHelpers.assertThrows(
-        "Residuals are not evaluated today for Iceberg Generics In memory model of HIVE",
-        UnsupportedOperationException.class, "Filter expression ref(name=\"id\") == 0 is not completely satisfied.",
-        () -> validate(jobShouldFail1, expectedRecords));
-
-    Job jobShouldFail2 = Job.getInstance(conf);
-    configBuilder = IcebergInputFormat.configure(jobShouldFail2);
-    configBuilder.usePigTuples().readFrom(location.toString())
-        .filter(Expressions.and(
-            Expressions.equal("date", "2020-03-20"),
-            Expressions.equal("id", 0)));
-    AssertHelpers.assertThrows(
-        "Residuals are not evaluated today for Iceberg Generics In memory model of PIG",
-        UnsupportedOperationException.class, "Filter expression ref(name=\"id\") == 0 is not completely satisfied.",
-        () -> validate(jobShouldFail2, expectedRecords));
-  }
-
   @Test
   public void testProjection() throws Exception {
     File location = temp.newFolder(fileFormat.name());
@@ -200,7 +161,7 @@ public void testProjection() throws Exception {
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(location.toString())
         .project(projectedSchema);
@@ -275,7 +236,7 @@ private static Schema withColumns(String... names) {
   private void validateIdentityPartitionProjections(
       String tablePath, Schema projectedSchema, List<Record> inputRecords) throws Exception {
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(tablePath)
         .project(projectedSchema);
@@ -311,7 +272,7 @@ public void testSnapshotReads() throws Exception {
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(location.toString())
         .snapshotId(snapshotId);
@@ -331,7 +292,7 @@ public void testLocality() throws Exception {
          .appendFile(writeFile(temp.newFile(), table, null, fileFormat, expectedRecords))
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString());
 
     for (InputSplit split : splits(job.getConfiguration())) {
@@ -368,7 +329,7 @@ public void testCustomCatalog() throws Exception {
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .catalogFunc(HadoopCatalogFunc.class)
         .readFrom(tableIdentifier.toString());
