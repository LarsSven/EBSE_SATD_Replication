diff --git a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java b/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java
index c89c7404db1..c83add3720f 100644
--- a/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java
+++ b/extensions-core/kafka-indexing-service/src/main/java/io/druid/indexing/kafka/KafkaIndexTask.java
@@ -1427,7 +1427,7 @@ public Response setEndOffsets(
       final boolean finish // this field is only for internal purposes, shouldn't be usually set by users
   ) throws InterruptedException
   {
-    // for backwards compatibility, should be remove from versions greater than 0.11.1
+    // for backwards compatibility, should be removed from versions greater than 0.11.1
     if (useLegacy) {
       return setEndOffsetsLegacy(offsets, resume);
     }
diff --git a/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java b/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java
index 962b354fc47..7e1acce1a99 100644
--- a/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java
+++ b/extensions-core/kafka-indexing-service/src/test/java/io/druid/indexing/kafka/KafkaIndexTaskTest.java
@@ -33,6 +33,7 @@
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
 import com.google.common.io.Files;
 import com.google.common.util.concurrent.ListenableFuture;
 import com.google.common.util.concurrent.ListeningExecutorService;
@@ -64,6 +65,7 @@
 import io.druid.indexing.common.task.Task;
 import io.druid.indexing.kafka.supervisor.KafkaSupervisor;
 import io.druid.indexing.kafka.test.TestBroker;
+import io.druid.indexing.overlord.DataSourceMetadata;
 import io.druid.indexing.overlord.IndexerMetadataStorageCoordinator;
 import io.druid.indexing.overlord.MetadataTaskStorage;
 import io.druid.indexing.overlord.TaskLockbox;
@@ -146,6 +148,7 @@
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 
+import javax.annotation.Nullable;
 import java.io.File;
 import java.io.IOException;
 import java.lang.reflect.InvocationTargetException;
@@ -153,6 +156,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.Callable;
 import java.util.concurrent.Executor;
@@ -177,7 +181,9 @@
 
   private long handoffConditionTimeout = 0;
   private boolean reportParseExceptions = false;
+  private boolean resetOffsetAutomatically = false;
   private boolean doHandoff = true;
+  private Integer maxRowsPerSegment = null;
 
   private TaskToolboxFactory toolboxFactory;
   private IndexerMetadataStorageCoordinator metadataStorageCoordinator;
@@ -187,7 +193,10 @@
   private String topic;
   private List<ProducerRecord<byte[], byte[]>> records;
   private final boolean isIncrementalHandoffSupported;
+  private final Set<Integer> checkpointRequestsHash = Sets.newHashSet();
 
+  // This should be removed in versions greater that 0.11.1
+  // isIncrementalHandoffSupported should always be set to true in those later versions
   @Parameterized.Parameters(name = "isIncrementalHandoffSupported = {0}")
   public static Iterable<Object[]> constructorFeeder()
   {
@@ -341,8 +350,7 @@ public void testRunAfterDataInserted() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -384,8 +392,7 @@ public void testRunBeforeDataInserted() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -424,6 +431,86 @@ public void testRunBeforeDataInserted() throws Exception
     Assert.assertEquals(ImmutableList.of("d", "e"), readSegmentColumn("dim1", desc2));
   }
 
+  @Test(timeout = 60_000L)
+  public void testIncrementalHandOff() throws Exception
+  {
+    if (!isIncrementalHandoffSupported) {
+      return;
+    }
+    final String baseSequenceName = "sequence0";
+    maxRowsPerSegment = 1;
+
+    // Insert data
+    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {
+      for (ProducerRecord<byte[], byte[]> record : records) {
+        kafkaProducer.send(record).get();
+      }
+    }
+    Map<String, String> consumerProps = kafkaServer.consumerProperties();
+    consumerProps.put("max.poll.records", "1");
+
+    final KafkaPartitions startPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L));
+    final KafkaPartitions checkpointPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 0L));
+    final KafkaPartitions endPartitions = new KafkaPartitions(topic, ImmutableMap.of(0, 8L, 1, 2L));
+    final KafkaIndexTask task = createTask(
+        null,
+        new KafkaIOConfig(
+            baseSequenceName,
+            startPartitions,
+            endPartitions,
+            consumerProps,
+            true,
+            false,
+            null,
+            null,
+            false
+        )
+    );
+    final ListenableFuture<TaskStatus> future = runTask(task);
+    while (task.getStatus() != KafkaIndexTask.Status.PAUSED) {
+      Thread.sleep(10);
+    }
+    task.setEndOffsets(checkpointPartitions.getPartitionOffsetMap(), true, false);
+    Assert.assertEquals(TaskStatus.Status.SUCCESS, future.get().getStatusCode());
+
+    Assert.assertEquals(1, checkpointRequestsHash.size());
+    Assert.assertTrue(checkpointRequestsHash.contains(
+        Objects.hash(DATA_SCHEMA.getDataSource(),
+                     baseSequenceName,
+                     new KafkaDataSourceMetadata(startPartitions),
+                     new KafkaDataSourceMetadata(checkpointPartitions)
+        )
+    ));
+
+    // Check metrics
+    Assert.assertEquals(8, task.getFireDepartmentMetrics().processed());
+    Assert.assertEquals(1, task.getFireDepartmentMetrics().unparseable());
+    Assert.assertEquals(1, task.getFireDepartmentMetrics().thrownAway());
+
+    // Check published metadata
+    SegmentDescriptor desc1 = SD(task, "2008/P1D", 0);
+    SegmentDescriptor desc2 = SD(task, "2009/P1D", 0);
+    SegmentDescriptor desc3 = SD(task, "2010/P1D", 0);
+    SegmentDescriptor desc4 = SD(task, "2011/P1D", 0);
+    SegmentDescriptor desc5 = SD(task, "2011/P1D", 1);
+    SegmentDescriptor desc6 = SD(task, "2012/P1D", 0);
+    SegmentDescriptor desc7 = SD(task, "2013/P1D", 0);
+    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());
+    Assert.assertEquals(
+        new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 8L, 1, 2L))),
+        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())
+    );
+
+    // Check segments in deep storage
+    Assert.assertEquals(ImmutableList.of("a"), readSegmentColumn("dim1", desc1));
+    Assert.assertEquals(ImmutableList.of("b"), readSegmentColumn("dim1", desc2));
+    Assert.assertEquals(ImmutableList.of("c"), readSegmentColumn("dim1", desc3));
+    Assert.assertEquals(ImmutableList.of("d", "e"), readSegmentColumn("dim1", desc4));
+    Assert.assertEquals(ImmutableList.of("h"), readSegmentColumn("dim1", desc5));
+    Assert.assertEquals(ImmutableList.of("g"), readSegmentColumn("dim1", desc6));
+    Assert.assertEquals(ImmutableList.of("f"), readSegmentColumn("dim1", desc7));
+  }
+
   @Test(timeout = 60_000L)
   public void testRunWithMinimumMessageTime() throws Exception
   {
@@ -439,8 +526,7 @@ public void testRunWithMinimumMessageTime() throws Exception
             DateTimes.of("2010"),
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -494,8 +580,7 @@ public void testRunWithMaximumMessageTime() throws Exception
             null,
             DateTimes.of("2010"),
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -559,8 +644,7 @@ public void testRunWithTransformSpec() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -620,8 +704,7 @@ public void testRunOnNothing() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -662,8 +745,7 @@ public void testHandoffConditionTimeoutWhenHandoffOccurs() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -715,8 +797,7 @@ public void testHandoffConditionTimeoutWhenHandoffDoesNotOccur() throws Exceptio
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -770,8 +851,7 @@ public void testReportParseExceptions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -804,8 +884,7 @@ public void testRunReplicas() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
     final KafkaIndexTask task2 = createTask(
         null,
@@ -819,8 +898,7 @@ public void testRunReplicas() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future1 = runTask(task1);
@@ -874,8 +952,7 @@ public void testRunConflicting() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
     final KafkaIndexTask task2 = createTask(
         null,
@@ -889,8 +966,7 @@ public void testRunConflicting() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     // Insert data
@@ -945,8 +1021,7 @@ public void testRunConflictingWithoutTransactions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
     final KafkaIndexTask task2 = createTask(
         null,
@@ -960,8 +1035,7 @@ public void testRunConflictingWithoutTransactions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     // Insert data
@@ -1021,8 +1095,7 @@ public void testRunOneTaskTwoPartitions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -1046,9 +1119,12 @@ public void testRunOneTaskTwoPartitions() throws Exception
     // Check published segments & metadata
     SegmentDescriptor desc1 = SD(task, "2010/P1D", 0);
     SegmentDescriptor desc2 = SD(task, "2011/P1D", 0);
-    SegmentDescriptor desc3 = SD(task, "2011/P1D", isIncrementalHandoffSupported ? 0 : 1);
+    // desc3 will not be created in KafkaIndexTask (0.11.1) as it does not create per Kafka partition Druid segments
+    SegmentDescriptor desc3 = SD(task, "2011/P1D", 1);
     SegmentDescriptor desc4 = SD(task, "2012/P1D", 0);
-    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4), publishedDescriptors());
+    Assert.assertEquals(isIncrementalHandoffSupported
+                        ? ImmutableSet.of(desc1, desc2, desc4)
+                        : ImmutableSet.of(desc1, desc2, desc3, desc4), publishedDescriptors());
     Assert.assertEquals(
         new KafkaDataSourceMetadata(new KafkaPartitions(topic, ImmutableMap.of(0, 5L, 1, 2L))),
         metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())
@@ -1063,7 +1139,9 @@ public void testRunOneTaskTwoPartitions() throws Exception
         isIncrementalHandoffSupported
         ? ImmutableSet.of(ImmutableList.of("d", "e", "h"))
         : ImmutableSet.of(ImmutableList.of("d", "e"), ImmutableList.of("h")),
-        ImmutableSet.of(readSegmentColumn("dim1", desc2), readSegmentColumn("dim1", desc3))
+        isIncrementalHandoffSupported
+        ? ImmutableSet.of(readSegmentColumn("dim1", desc2))
+        : ImmutableSet.of(readSegmentColumn("dim1", desc2), readSegmentColumn("dim1", desc3))
     );
   }
 
@@ -1082,8 +1160,7 @@ public void testRunTwoTasksTwoPartitions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
     final KafkaIndexTask task2 = createTask(
         null,
@@ -1097,8 +1174,7 @@ public void testRunTwoTasksTwoPartitions() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future1 = runTask(task1);
@@ -1154,8 +1230,7 @@ public void testRestore() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future1 = runTask(task1);
@@ -1192,8 +1267,7 @@ public void testRestore() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future2 = runTask(task2);
@@ -1245,8 +1319,7 @@ public void testRunWithPauseAndResume() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -1330,8 +1403,7 @@ public void testRunAndPauseAfterReadWithModifiedEndOffsets() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     final ListenableFuture<TaskStatus> future = runTask(task);
@@ -1418,8 +1490,7 @@ public void testRunWithOffsetOutOfRangeExceptionAndPause() throws Exception
             null,
             null,
             false
-        ),
-        null
+        )
     );
 
     runTask(task);
@@ -1438,6 +1509,7 @@ public void testRunWithOffsetOutOfRangeExceptionAndPause() throws Exception
   @Test(timeout = 30_000L)
   public void testRunWithOffsetOutOfRangeExceptionAndNextOffsetGreaterThanLeastAvailable() throws Exception
   {
+    resetOffsetAutomatically = true;
     // Insert data
     try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {
       for (ProducerRecord<byte[], byte[]> record : records) {
@@ -1457,8 +1529,7 @@ public void testRunWithOffsetOutOfRangeExceptionAndNextOffsetGreaterThanLeastAva
             null,
             null,
             false
-        ),
-        true
+        )
     );
 
     runTask(task);
@@ -1526,23 +1597,21 @@ public boolean apply(TaskLock lock)
 
   private KafkaIndexTask createTask(
       final String taskId,
-      final KafkaIOConfig ioConfig,
-      final Boolean resetOffsetAutomatically
+      final KafkaIOConfig ioConfig
   )
   {
-    return createTask(taskId, DATA_SCHEMA, ioConfig, resetOffsetAutomatically);
+    return createTask(taskId, DATA_SCHEMA, ioConfig);
   }
 
   private KafkaIndexTask createTask(
       final String taskId,
       final DataSchema dataSchema,
-      final KafkaIOConfig ioConfig,
-      final Boolean resetOffsetAutomatically
+      final KafkaIOConfig ioConfig
   )
   {
     final KafkaTuningConfig tuningConfig = new KafkaTuningConfig(
         1000,
-        null,
+        maxRowsPerSegment,
         new Period("P1Y"),
         null,
         null,
@@ -1662,6 +1731,24 @@ private void makeToolboxFactory() throws IOException
         metadataStorageCoordinator,
         emitter,
         new SupervisorManager(null)
+        {
+          @Override
+          public boolean checkPointDataSourceMetadata(
+              String supervisorId,
+              @Nullable String sequenceName,
+              @Nullable DataSourceMetadata previousDataSourceMetadata,
+              @Nullable DataSourceMetadata currentDataSourceMetadata
+          )
+          {
+            checkpointRequestsHash.add(Objects.hash(
+                supervisorId,
+                sequenceName,
+                previousDataSourceMetadata,
+                currentDataSourceMetadata
+            ));
+            return true;
+          }
+        }
     );
     final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(
         taskStorage,
diff --git a/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java b/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java
index 24e9c459845..bba14966865 100644
--- a/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java
+++ b/server/src/main/java/io/druid/segment/realtime/appenderator/AppenderatorImpl.java
@@ -42,7 +42,6 @@
 import io.druid.client.cache.Cache;
 import io.druid.client.cache.CacheConfig;
 import io.druid.common.guava.ThreadRenamingCallable;
-import io.druid.java.util.common.concurrent.Execs;
 import io.druid.data.input.Committer;
 import io.druid.data.input.InputRow;
 import io.druid.java.util.common.DateTimes;
@@ -51,6 +50,7 @@
 import io.druid.java.util.common.Pair;
 import io.druid.java.util.common.RetryUtils;
 import io.druid.java.util.common.StringUtils;
+import io.druid.java.util.common.concurrent.Execs;
 import io.druid.java.util.common.io.Closer;
 import io.druid.query.Query;
 import io.druid.query.QueryRunner;
