diff --git a/example/quantization/README.md b/example/quantization/README.md
index 7f68dd795401..0e63cd84ede7 100644
--- a/example/quantization/README.md
+++ b/example/quantization/README.md
@@ -320,4 +320,6 @@ the console to run model quantization for a specific configuration.
 - `launch_inference.sh` This is a shell script that calculate the accuracies of all the quantized models generated
 by invoking `launch_quantize.sh`.
 
-**NOTE**: This example has only been tested on Linux systems.
\ No newline at end of file
+**NOTE**: 
+- This example has only been tested on Linux systems.
+- Performance is expected to decrease with GPU, however the memory footprint of a quantized model is smaller. The purpose of the quantization implementation is to minimize accuracy loss when converting FP32 models to INT8. MXNet community is working on improving the performance. 
\ No newline at end of file
diff --git a/example/quantization/imagenet_inference.py b/example/quantization/imagenet_inference.py
index 7d380d3e1b46..3fdb52f40cb2 100644
--- a/example/quantization/imagenet_inference.py
+++ b/example/quantization/imagenet_inference.py
@@ -93,6 +93,7 @@ def score(sym, arg_params, aux_params, data, devs, label_name, max_num_examples,
     if logger is not None:
         logger.info('Finished inference with %d images' % num)
         logger.info('Finished with %f images per second', speed)
+        logger.warn('Note: GPU performance is expected to be slower than CPU. Please refer quantization/README.md for details')
         for m in metrics:
             logger.info(m.get())
 
