diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java
index 69ef940ade29..73fcf2f67584 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java
@@ -412,8 +412,10 @@ public void deleteFilesystem() throws AzureBlobFileSystemException {
     }
   }
 
-  public OutputStream createFile(final Path path, final FileSystem.Statistics statistics, final boolean overwrite, final FsPermission permission,
-                                 final FsPermission umask) throws AzureBlobFileSystemException {
+  public OutputStream createFile(final Path path,
+      final FileSystem.Statistics statistics,
+      final boolean overwrite, final FsPermission permission,
+      final FsPermission umask) throws AzureBlobFileSystemException {
     try (AbfsPerfInfo perfInfo = startTracking("createFile", "createPath")) {
       boolean isNamespaceEnabled = getIsNamespaceEnabled();
       LOG.debug("createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}",
diff --git a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java
index 77c92ca338e3..ba662e96b947 100644
--- a/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java
+++ b/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java
@@ -256,7 +256,7 @@ int readRemote(long position, byte[] b, int offset, int length) throws IOExcepti
   }
 
   /**
-   * Increment Read Operations
+   * Increment Read Operations.
    */
   public void incrementReadOps() {
     statistics.incrementReadOps(1);
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java
index ea27e8856505..8ddaca1440b0 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/AbstractAbfsTestWithTimeout.java
@@ -17,8 +17,8 @@
  */
 package org.apache.hadoop.fs.azurebfs;
 
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.Path;
+import java.io.IOException;
+
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -28,8 +28,8 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.IOException;
-import java.util.Arrays;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.Path;
 
 import static org.apache.hadoop.fs.azurebfs.constants.TestConfigurationKeys.TEST_TIMEOUT;
 
@@ -91,10 +91,10 @@ protected void describe(String text, Object... args) {
   }
 
   /**
-   * Validate Contents written on a file in Abfs
+   * Validate Contents written on a file in Abfs.
    *
-   * @param fs AzureBlobFileSystem
-   * @param path Path of the file
+   * @param fs                AzureBlobFileSystem
+   * @param path              Path of the file
    * @param originalByteArray original byte array
    * @return
    * @throws IOException
@@ -103,16 +103,20 @@ protected boolean validateContent(AzureBlobFileSystem fs, Path path,
       byte[] originalByteArray)
       throws IOException {
     FSDataInputStream in = fs.open(path);
-    byte[] contentByteArray = new byte[originalByteArray.length];
-    int seekPos = 0;
-    while (in.read() != -1) {
-      in.seek(seekPos);
-      contentByteArray[seekPos] = (byte) (in.read());
-      seekPos++;
-    }
 
-    return Arrays.equals(contentByteArray, originalByteArray);
+    int pos = 0;
+    int lenOfOriginalByteArray = originalByteArray.length;
+    byte valueOfContentAtPos = (byte) in.read();
 
+    while (valueOfContentAtPos != -1 && pos < lenOfOriginalByteArray) {
+      if (originalByteArray[pos] != valueOfContentAtPos)
+        return false;
+      valueOfContentAtPos = (byte) in.read();
+      pos++;
+    }
+    if (valueOfContentAtPos != -1)
+      return false;
+    return true;
   }
 
 }
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java
deleted file mode 100644
index d0cf3ef863c6..000000000000
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStream.java
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.fs.azurebfs;
-
-import org.junit.Assert;
-import org.junit.Test;
-
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;
-
-/**
- * Test Abfs Input Stream.
- */
-
-public class ITestAbfsInputStream extends AbstractAbfsIntegrationTest {
-  public ITestAbfsInputStream() throws Exception {
-  }
-
-  /***
-   * {@link AbfsInputStream#incrementReadOps()}
-   *
-   * @throws Exception
-   */
-  @Test
-  public void testAbfsInputStreamReadOps() throws Exception {
-    describe("Test to see correct population of Read operations in Abfs");
-
-    final AzureBlobFileSystem fs = getFileSystem();
-    Path smallFile = new Path("testOneReadCall");
-    Path largeFile = new Path("testLargeReadCalls");
-    FileSystem.Statistics statistics = fs.getFsStatistics();
-    String testReadOps = "test this";
-    statistics.reset();
-
-    //Test for zero read operation
-    Assert.assertEquals(0, statistics.getReadOps());
-
-    FSDataOutputStream outForOneCall = fs.create(smallFile);
-    statistics.reset();
-    outForOneCall.write(testReadOps.getBytes());
-    FSDataInputStream inForOneCall = fs.open(smallFile);
-    inForOneCall.read(testReadOps.getBytes(), 0, testReadOps.getBytes().length);
-
-    //Test for one read operation
-    Assert.assertEquals(1, statistics.getReadOps());
-
-    FSDataOutputStream outForLargeCalls = fs.create(largeFile);
-    statistics.reset();
-    outForLargeCalls.write(testReadOps.getBytes());
-    FSDataInputStream inForLargeCalls = fs.open(largeFile);
-
-    for (int i = 0; i < 1000; i++)
-      inForLargeCalls
-          .read(testReadOps.getBytes(), 0, testReadOps.getBytes().length);
-
-    //Test for thousand read operations
-    Assert.assertEquals(1000, statistics.getReadOps());
-    statistics.reset();
-
-  }
-
-}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java
new file mode 100644
index 000000000000..a4370f042056
--- /dev/null
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsStreamStatistics.java
@@ -0,0 +1,109 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.fs.azurebfs;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.azurebfs.services.AbfsInputStream;
+
+/**
+ * Test Abfs Stream.
+ */
+
+public class ITestAbfsStreamStatistics extends AbstractAbfsIntegrationTest {
+  public ITestAbfsStreamStatistics() throws Exception {
+  }
+
+  /***
+   * {@link AbfsInputStream#incrementReadOps()}.
+   *
+   * @throws Exception
+   */
+  @Test
+  public void testAbfsStreamOps() throws Exception {
+    describe("Test to see correct population of read and write operations in "
+        + "Abfs");
+
+    final AzureBlobFileSystem fs = getFileSystem();
+    Path smallOperaionsFile = new Path("testOneReadWriteOps");
+    Path largeOperationsFile = new Path("testLargeReadWriteOps");
+    FileSystem.Statistics statistics = fs.getFsStatistics();
+    String testReadWriteOps = "test this";
+    statistics.reset();
+
+    //Test for zero read and write operation
+    Assert.assertEquals("Zero read operations", 0, statistics.getReadOps());
+    Assert.assertEquals("Zero write operations", 0, statistics.getWriteOps());
+
+    FSDataOutputStream outForOneOperation = fs.create(smallOperaionsFile);
+    statistics.reset();
+    outForOneOperation.write(testReadWriteOps.getBytes());
+    FSDataInputStream inForOneCall = fs.open(smallOperaionsFile);
+    inForOneCall.read(testReadWriteOps.getBytes(), 0,
+        testReadWriteOps.getBytes().length);
+
+    //Test for one read and write operation
+    Assert.assertEquals("one read operation is performed", 1,
+        statistics.getReadOps());
+    Assert.assertEquals("one write operation is performed", 1,
+        statistics.getWriteOps());
+
+    outForOneOperation.close();
+    //validating Content of file
+    Assert.assertEquals("one operation Content validation", true,
+        validateContent(fs, smallOperaionsFile,
+            testReadWriteOps.getBytes()));
+
+    FSDataOutputStream outForLargeOperations = fs.create(largeOperationsFile);
+    statistics.reset();
+
+    StringBuilder largeOperationsValidationString = new StringBuilder();
+    for (int i = 0; i < 1000000; i++) {
+      outForLargeOperations.write(testReadWriteOps.getBytes());
+
+      //Creating the String for content Validation
+      largeOperationsValidationString.append(testReadWriteOps);
+    }
+
+    FSDataInputStream inForLargeCalls = fs.open(largeOperationsFile);
+
+    for (int i = 0; i < 1000000; i++)
+      inForLargeCalls
+          .read(testReadWriteOps.getBytes(), 0,
+              testReadWriteOps.getBytes().length);
+
+    //Test for one million read and write operations
+    Assert.assertEquals("Large read operations", 1000000,
+        statistics.getReadOps());
+    Assert.assertEquals("Large write operations", 1000000,
+        statistics.getWriteOps());
+
+    outForLargeOperations.close();
+    //Validating if actually "test" is being written million times in largeOperationsFile
+    Assert.assertEquals("Large File content validation", true,
+        validateContent(fs, largeOperationsFile,
+            largeOperationsValidationString.toString().getBytes()));
+
+  }
+}
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java
index cf99c3023d06..94368a4f3695 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemCreate.java
@@ -23,9 +23,6 @@
 import java.io.IOException;
 import java.util.EnumSet;
 
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream;
-import org.junit.Assert;
 import org.junit.Test;
 
 import org.apache.hadoop.fs.CreateFlag;
@@ -62,59 +59,6 @@ public void testEnsureFileCreatedImmediately() throws Exception {
     assertIsFile(fs, TEST_FILE_PATH);
   }
 
-  /**
-   * {@link AbfsOutputStream#incrementWriteOps()}
-   *
-   * @throws Exception
-   */
-  @Test
-  public void testWriteOpsMetric() throws Exception {
-    describe("Test to see correct population of write operations in Abfs");
-    final AzureBlobFileSystem fs = getFileSystem();
-    Path smallFile = new Path("testOneCall");
-    Path largeFile = new Path("testLargeCalls");
-    String testWriteOps = "test";
-    FileSystem.Statistics statistics = fs.getFsStatistics();
-    statistics.reset();
-
-    //Test for zero write operation
-    Assert.assertEquals(0, statistics.getWriteOps());
-
-    FSDataOutputStream outForOneCall = fs.create(smallFile);
-    statistics.reset();
-    //Writing "test" 1 time
-    outForOneCall
-        .write(testWriteOps.getBytes(), 0, testWriteOps.getBytes().length);
-
-    //Test for one write operation
-    Assert.assertEquals(1, statistics.getWriteOps());
-
-    outForOneCall.close();
-    //validating Content of file
-    Assert.assertEquals(true, validateContent(fs, smallFile,
-        testWriteOps.getBytes()));
-
-    String largeFileValidationString = "";
-    FSDataOutputStream outForLargeCalls = fs.create(largeFile);
-    statistics.reset();
-    //Writing "test" 1000 times
-    for (int i = 0; i < 1000; i++) {
-      outForLargeCalls.write(testWriteOps.getBytes(), 0,
-          testWriteOps.getBytes().length);
-
-      //Creating Validation string of "test" 1000 times
-      largeFileValidationString += testWriteOps;
-    }
-
-    //Test for thousand write operations
-    Assert.assertEquals(1000, statistics.getWriteOps());
-
-    outForLargeCalls.close();
-    //Validating if actually "test" is being written thousand times in largeFile
-    Assert.assertEquals(true, validateContent(fs, largeFile,
-        largeFileValidationString.getBytes()));
-  }
-
   @Test
   @SuppressWarnings("deprecation")
   public void testCreateNonRecursive() throws Exception {
diff --git a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java
index 39a75958e784..5016609676d7 100644
--- a/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java
+++ b/hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemOauth.java
@@ -143,7 +143,7 @@ public void testBlobDataReader() throws Exception {
 
     // TEST WRITE FILE
     try {
-      abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(),true);
+      abfsStore.openFileForWrite(EXISTED_FILE_PATH, fs.getFsStatistics(), true);
     } catch (AbfsRestOperationException e) {
       assertEquals(AzureServiceErrorCode.AUTHORIZATION_PERMISSION_MISS_MATCH, e.getErrorCode());
     }
