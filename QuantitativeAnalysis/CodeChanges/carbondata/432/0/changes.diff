diff --git a/core/pom.xml b/core/pom.xml
index 09c895eb920..5bed8b5fbe7 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -72,6 +72,24 @@
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-hdfs</artifactId>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-core</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>scala-library</artifactId>
+    </dependency>
+    <dependency>
+      <groupId>net.jpountz.lz4</groupId>
+      <artifactId>lz4</artifactId>
+      <version>${lz4.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.commons</groupId>
+      <artifactId>commons-lang3</artifactId>
+      <version>${commonslang3.version}</version>
+    </dependency>
     <dependency>
       <groupId>org.xerial.snappy</groupId>
       <artifactId>snappy-java</artifactId>
@@ -85,10 +103,6 @@
       <groupId>junit</groupId>
       <artifactId>junit</artifactId>
     </dependency>
-    <dependency>
-      <groupId>org.apache.spark</groupId>
-      <artifactId>spark-sql_${scala.binary.version}</artifactId>
-    </dependency>
     <dependency>
       <groupId>org.apache.zookeeper</groupId>
       <artifactId>zookeeper</artifactId>
diff --git a/core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java b/core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
index ab9b3d48456..697112640cd 100644
--- a/core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
+++ b/core/src/main/java/org/apache/carbondata/core/memory/MemoryBlock.java
@@ -19,8 +19,6 @@
 
 import javax.annotation.Nullable;
 
-import org.apache.spark.unsafe.Platform;
-
 /**
  * Code ported from Apache Spark {org.apache.spark.unsafe.memory} package
  * A consecutive block of memory, starting at a {@link MemoryLocation} with a fixed size.
@@ -29,13 +27,6 @@ public class MemoryBlock extends MemoryLocation {
 
   private final long length;
 
-  /**
-   * Optional page number; used when this MemoryBlock represents a page allocated by a
-   * TaskMemoryManager. This field is public so that it can be modified by the TaskMemoryManager,
-   * which lives in a different package.
-   */
-  public int pageNumber = -1;
-
   public MemoryBlock(@Nullable Object obj, long offset, long length) {
     super(obj, offset);
     this.length = length;
@@ -48,10 +39,4 @@ public long size() {
     return length;
   }
 
-  /**
-   * Creates a memory block pointing to the memory used by the long array.
-   */
-  public static MemoryBlock fromLongArray(final long[] array) {
-    return new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8);
-  }
 }
diff --git a/core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java b/core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
index e61d1d2064e..f1e216e8ed0 100644
--- a/core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
+++ b/core/src/main/java/org/apache/carbondata/core/util/CarbonUtil.java
@@ -556,7 +556,7 @@ public static void writeLevelCardinalityFile(String loadFolderLoc, String tableN
   /**
    * From beeline if a delimeter is passed as \001, in code we get it as
    * escaped string as \\001. So this method will unescape the slash again and
-   * convert it back t0 \001
+   * convert it back to \001
    *
    * @param parseStr
    * @return
@@ -565,6 +565,7 @@ public static String unescapeChar(String parseStr) {
     return scala.StringContext.treatEscapes(parseStr);
   }
 
+
   /**
    * special char delimiter Converter
    *
diff --git a/core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java b/core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
index c9ed23428b5..39296db9db5 100644
--- a/core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
+++ b/core/src/main/java/org/apache/carbondata/core/util/DataTypeUtil.java
@@ -36,8 +36,6 @@
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonMeasure;
 import org.apache.carbondata.core.constants.CarbonCommonConstants;
 
-import org.apache.spark.unsafe.types.UTF8String;
-
 public final class DataTypeUtil {
 
   /**
@@ -270,16 +268,14 @@ public static Object getDataBasedOnDataType(String data, DataType actualDataType
           if (data.isEmpty()) {
             return null;
           }
-          java.math.BigDecimal javaDecVal = new java.math.BigDecimal(data);
-          return org.apache.spark.sql.types.Decimal.apply(javaDecVal);
+          return new java.math.BigDecimal(data);
         default:
-          return UTF8String.fromString(data);
+          return data;
       }
     } catch (NumberFormatException ex) {
       LOGGER.error("Problem while converting data type" + data);
       return null;
     }
-
   }
 
   public static Object getMeasureDataBasedOnDataType(Object data, DataType dataType) {
@@ -294,7 +290,7 @@ public static Object getMeasureDataBasedOnDataType(Object data, DataType dataTyp
         case LONG:
           return data;
         case DECIMAL:
-          return org.apache.spark.sql.types.Decimal.apply((java.math.BigDecimal) data);
+          return data;
         default:
           return data;
       }
diff --git a/core/src/main/java/org/apache/carbondata/scan/collector/impl/AbstractScannedResultCollector.java b/core/src/main/java/org/apache/carbondata/scan/collector/impl/AbstractScannedResultCollector.java
index e48d469aff1..db74cea0f50 100644
--- a/core/src/main/java/org/apache/carbondata/scan/collector/impl/AbstractScannedResultCollector.java
+++ b/core/src/main/java/org/apache/carbondata/scan/collector/impl/AbstractScannedResultCollector.java
@@ -106,8 +106,7 @@ private Object getMeasureData(MeasureColumnDataChunk dataChunk, int index, DataT
         case LONG:
           return dataChunk.getMeasureDataHolder().getReadableLongValueByIndex(index);
         case DECIMAL:
-          return org.apache.spark.sql.types.Decimal
-              .apply(dataChunk.getMeasureDataHolder().getReadableBigDecimalValueByIndex(index));
+          return dataChunk.getMeasureDataHolder().getReadableBigDecimalValueByIndex(index);
         default:
           return dataChunk.getMeasureDataHolder().getReadableDoubleValueByIndex(index);
       }
diff --git a/core/src/main/java/org/apache/carbondata/scan/complextypes/ArrayQueryType.java b/core/src/main/java/org/apache/carbondata/scan/complextypes/ArrayQueryType.java
index 4d6d30ca16f..b8091bdd752 100644
--- a/core/src/main/java/org/apache/carbondata/scan/complextypes/ArrayQueryType.java
+++ b/core/src/main/java/org/apache/carbondata/scan/complextypes/ArrayQueryType.java
@@ -27,9 +27,6 @@
 import org.apache.carbondata.scan.filter.GenericQueryType;
 import org.apache.carbondata.scan.processor.BlocksChunkHolder;
 
-import org.apache.spark.sql.catalyst.util.*;
-import org.apache.spark.sql.types.*;
-
 public class ArrayQueryType extends ComplexQueryType implements GenericQueryType {
 
   private GenericQueryType children;
@@ -85,10 +82,6 @@ public void parseBlocksAndReturnComplexColumnByteArray(
     return children.getColsCount() + 1;
   }
 
-  @Override public DataType getSchemaType() {
-    return new ArrayType(null, true);
-  }
-
   @Override public void fillRequiredBlockData(BlocksChunkHolder blockChunkHolder)
       throws IOException {
     readBlockDataChunk(blockChunkHolder);
@@ -104,7 +97,7 @@ public void parseBlocksAndReturnComplexColumnByteArray(
     for (int i = 0; i < dataLength; i++) {
       data[i] = children.getDataBasedOnDataTypeFromSurrogates(surrogateData);
     }
-    return new GenericArrayData(data);
+    return data;
   }
 
 }
diff --git a/core/src/main/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryType.java b/core/src/main/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryType.java
index ac717a9ed95..d2df3fbb896 100644
--- a/core/src/main/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryType.java
+++ b/core/src/main/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryType.java
@@ -32,8 +32,6 @@
 import org.apache.carbondata.scan.filter.GenericQueryType;
 import org.apache.carbondata.scan.processor.BlocksChunkHolder;
 
-import org.apache.spark.sql.types.*;
-
 public class PrimitiveQueryType extends ComplexQueryType implements GenericQueryType {
 
   private String name;
@@ -96,25 +94,6 @@ public PrimitiveQueryType(String name, String parentname, int blockIndex,
     dataOutputStream.write(currentVal);
   }
 
-  @Override public DataType getSchemaType() {
-    switch (dataType) {
-      case INT:
-        return IntegerType$.MODULE$;
-      case DOUBLE:
-        return DoubleType$.MODULE$;
-      case LONG:
-        return LongType$.MODULE$;
-      case BOOLEAN:
-        return BooleanType$.MODULE$;
-      case TIMESTAMP:
-        return TimestampType$.MODULE$;
-      case DATE:
-        return DateType$.MODULE$;
-      default:
-        return IntegerType$.MODULE$;
-    }
-  }
-
   @Override public void fillRequiredBlockData(BlocksChunkHolder blockChunkHolder)
       throws IOException {
     readBlockDataChunk(blockChunkHolder);
diff --git a/core/src/main/java/org/apache/carbondata/scan/complextypes/StructQueryType.java b/core/src/main/java/org/apache/carbondata/scan/complextypes/StructQueryType.java
index 36d483b951b..58e6f181e51 100644
--- a/core/src/main/java/org/apache/carbondata/scan/complextypes/StructQueryType.java
+++ b/core/src/main/java/org/apache/carbondata/scan/complextypes/StructQueryType.java
@@ -29,12 +29,6 @@
 import org.apache.carbondata.scan.filter.GenericQueryType;
 import org.apache.carbondata.scan.processor.BlocksChunkHolder;
 
-import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
-import org.apache.spark.sql.types.DataType;
-import org.apache.spark.sql.types.Metadata;
-import org.apache.spark.sql.types.StructField;
-import org.apache.spark.sql.types.StructType;
-
 public class StructQueryType extends ComplexQueryType implements GenericQueryType {
 
   private List<GenericQueryType> children = new ArrayList<GenericQueryType>();
@@ -100,15 +94,6 @@ public StructQueryType(String name, String parentname, int blockIndex) {
     }
   }
 
-  @Override public DataType getSchemaType() {
-    StructField[] fields = new StructField[children.size()];
-    for (int i = 0; i < children.size(); i++) {
-      fields[i] = new StructField(children.get(i).getName(), null, true,
-          Metadata.empty());
-    }
-    return new StructType(fields);
-  }
-
   @Override public void fillRequiredBlockData(BlocksChunkHolder blockChunkHolder)
       throws IOException {
     readBlockDataChunk(blockChunkHolder);
@@ -124,7 +109,6 @@ public StructQueryType(String name, String parentname, int blockIndex) {
     for (int i = 0; i < childLength; i++) {
       fields[i] =  children.get(i).getDataBasedOnDataTypeFromSurrogates(surrogateData);
     }
-
-    return new GenericInternalRow(fields);
+    return fields;
   }
 }
diff --git a/core/src/main/java/org/apache/carbondata/scan/filter/GenericQueryType.java b/core/src/main/java/org/apache/carbondata/scan/filter/GenericQueryType.java
index bcf0dc315b2..b0d0352d5ae 100644
--- a/core/src/main/java/org/apache/carbondata/scan/filter/GenericQueryType.java
+++ b/core/src/main/java/org/apache/carbondata/scan/filter/GenericQueryType.java
@@ -26,8 +26,6 @@
 import org.apache.carbondata.core.carbon.datastore.chunk.DimensionColumnDataChunk;
 import org.apache.carbondata.scan.processor.BlocksChunkHolder;
 
-import org.apache.spark.sql.types.DataType;
-
 public interface GenericQueryType {
 
   String getName();
@@ -45,8 +43,6 @@ public interface GenericQueryType {
   void parseBlocksAndReturnComplexColumnByteArray(DimensionColumnDataChunk[] dimensionDataChunks,
       int rowNumber, DataOutputStream dataOutputStream) throws IOException;
 
-  DataType getSchemaType();
-
   void fillRequiredBlockData(BlocksChunkHolder blockChunkHolder) throws IOException;
 
   Object getDataBasedOnDataTypeFromSurrogates(ByteBuffer surrogateData);
diff --git a/core/src/test/java/org/apache/carbondata/core/util/DataTypeUtilTest.java b/core/src/test/java/org/apache/carbondata/core/util/DataTypeUtilTest.java
index 6c68a73737d..c8c8d6d9c3b 100644
--- a/core/src/test/java/org/apache/carbondata/core/util/DataTypeUtilTest.java
+++ b/core/src/test/java/org/apache/carbondata/core/util/DataTypeUtilTest.java
@@ -19,29 +19,25 @@
 
 package org.apache.carbondata.core.util;
 
-import mockit.Mock;
-import mockit.MockUp;
+import java.math.BigDecimal;
+import java.math.BigInteger;
 
 import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
-import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonDimension;
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonMeasure;
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.ColumnSchema;
-
-import org.apache.spark.unsafe.types.UTF8String;
-import org.junit.Rule;
 import org.junit.Test;
-import org.junit.rules.ExpectedException;
-
-import java.math.BigDecimal;
-import java.math.BigInteger;
-import java.text.SimpleDateFormat;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.Map;
 
-import static org.apache.carbondata.core.util.DataTypeUtil.*;
-import static junit.framework.TestCase.*;
+import static junit.framework.TestCase.assertEquals;
+import static junit.framework.TestCase.assertTrue;
+import static org.apache.carbondata.core.util.DataTypeUtil.bigDecimalToByte;
+import static org.apache.carbondata.core.util.DataTypeUtil.byteToBigDecimal;
+import static org.apache.carbondata.core.util.DataTypeUtil.getAggType;
+import static org.apache.carbondata.core.util.DataTypeUtil.getColumnDataTypeDisplayName;
 import static org.apache.carbondata.core.util.DataTypeUtil.getDataBasedOnDataType;
+import static org.apache.carbondata.core.util.DataTypeUtil.getDataType;
+import static org.apache.carbondata.core.util.DataTypeUtil.getMeasureDataBasedOnDataType;
+import static org.apache.carbondata.core.util.DataTypeUtil.getMeasureValueBasedOnDataType;
+import static org.apache.carbondata.core.util.DataTypeUtil.normalizeIntAndLongValues;
 
 public class DataTypeUtilTest {
 
@@ -101,12 +97,8 @@ public class DataTypeUtilTest {
     assertEquals(getDataBasedOnDataType("0", DataType.DOUBLE), 0.0d);
     assertEquals(getDataBasedOnDataType("0", DataType.LONG), 0L);
     java.math.BigDecimal javaDecVal = new java.math.BigDecimal(1);
-    scala.math.BigDecimal scalaDecVal = new scala.math.BigDecimal(javaDecVal);
-    org.apache.spark.sql.types.Decimal expected =
-        new org.apache.spark.sql.types.Decimal().set(scalaDecVal);
-    assertEquals(getDataBasedOnDataType("1", DataType.DECIMAL), expected);
-    assertEquals(getDataBasedOnDataType("default", DataType.NULL),
-        UTF8String.fromString("default"));
+    assertEquals(getDataBasedOnDataType("1", DataType.DECIMAL), javaDecVal);
+    assertEquals(getDataBasedOnDataType("default", DataType.NULL), "default");
     assertEquals(getDataBasedOnDataType(null, DataType.NULL), null);
   }
 
@@ -115,14 +107,11 @@ public class DataTypeUtilTest {
     assertEquals(getMeasureDataBasedOnDataType(new Double("1"), DataType.DOUBLE),
         Double.parseDouble("1"));
     java.math.BigDecimal javaDecVal = new java.math.BigDecimal(1);
-    scala.math.BigDecimal scalaDecVal = new scala.math.BigDecimal(javaDecVal);
-    org.apache.spark.sql.types.Decimal expected =
-        new org.apache.spark.sql.types.Decimal().set(scalaDecVal);
     assertEquals(
             getMeasureDataBasedOnDataType(
                     new java.math.BigDecimal(1),
                     DataType.DECIMAL),
-            expected);
+            javaDecVal);
     assertEquals(getMeasureDataBasedOnDataType("1", DataType.STRING), "1");
   }
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryTypeTest.java b/core/src/test/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryTypeTest.java
index 3d8736d8f61..11fa3595f6a 100644
--- a/core/src/test/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryTypeTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/complextypes/PrimitiveQueryTypeTest.java
@@ -31,11 +31,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.BooleanType$;
-import org.apache.spark.sql.types.DoubleType$;
-import org.apache.spark.sql.types.IntegerType$;
-import org.apache.spark.sql.types.LongType$;
-import org.apache.spark.sql.types.TimestampType$;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
@@ -80,30 +75,6 @@ public class PrimitiveQueryTypeTest {
 
   }
 
-  @Test public void testGetDataTypeForDefault() {
-    assertEquals(IntegerType$.MODULE$, primitiveQueryType.getSchemaType());
-  }
-
-  @Test public void testGetDataTypeForInt() {
-    assertEquals(IntegerType$.MODULE$, primitiveQueryTypeForInt.getSchemaType());
-  }
-
-  @Test public void testGetDataTypeForDouble() {
-    assertEquals(DoubleType$.MODULE$, primitiveQueryTypeForDouble.getSchemaType());
-  }
-
-  @Test public void testGetDataTypeForBoolean() {
-    assertEquals(BooleanType$.MODULE$, primitiveQueryTypeForBoolean.getSchemaType());
-  }
-
-  @Test public void testGetDataTypeForTimeStamp() {
-    assertEquals(TimestampType$.MODULE$, primitiveQueryTypeForTimeStamp.getSchemaType());
-  }
-
-  @Test public void testGetDataTypeForLong() {
-    assertEquals(LongType$.MODULE$, primitiveQueryTypeForLong.getSchemaType());
-  }
-
   @Test public void testGetDataBasedOnDataTypeFromSurrogates() {
     ByteBuffer surrogateData = ByteBuffer.allocate(10);
     surrogateData.put(3, (byte) 1);
diff --git a/core/src/test/java/org/apache/carbondata/scan/complextypes/StructQueryTypeTest.java b/core/src/test/java/org/apache/carbondata/scan/complextypes/StructQueryTypeTest.java
index c0294950508..554af7e21ab 100644
--- a/core/src/test/java/org/apache/carbondata/scan/complextypes/StructQueryTypeTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/complextypes/StructQueryTypeTest.java
@@ -65,10 +65,4 @@ public class StructQueryTypeTest {
     int expectedValue = 2;
     assertEquals(expectedValue, actualValue);
   }
-
-  @Test public void testGetSchemaType() {
-    List children = new ArrayList();
-    children.add(null);
-    assertNotNull(structQueryType.getSchemaType());
-  }
 }
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/EqualToExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/EqualToExpressionUnitTest.java
index 8cc344e0f13..65e79dadfd6 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/EqualToExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/EqualToExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -299,7 +298,7 @@ public class EqualToExpressionUnitTest {
     right.setColIndex(0);
     equalToExpression = new EqualToExpression(right, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(12345.0) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(12345.0) };
     Object objectRow[] = { row };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanEqualToExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanEqualToExpressionUnitTest.java
index c0762204829..ec791e8d31e 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanEqualToExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanEqualToExpressionUnitTest.java
@@ -32,10 +32,8 @@
 import org.apache.carbondata.scan.expression.exception.FilterUnsupportedException;
 import org.apache.carbondata.scan.filter.intf.RowImpl;
 
-import junit.framework.Assert;
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -213,7 +211,7 @@ public class GreaterThanEqualToExpressionUnitTest {
     left.setColIndex(1);
     greaterThanEqualToExpression = new GreaterThanEqualToExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(12345.0) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(12345.0) };
     Object objectRow[] = { row, row };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanExpressionUnitTest.java
index f8ecce33ecf..7f889b69bb5 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/GreaterThanExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -258,8 +257,8 @@ public class GreaterThanExpressionUnitTest {
     left.setColIndex(1);
     greaterThanExpression = new GreaterThanExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(12345.0) };
-    Decimal[] row1 = new Decimal[] { Decimal.apply(123451245.0) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(12345.0) };
+    BigDecimal[] row1 = new BigDecimal[] { new BigDecimal(123451245.0) };
     Object objectRow[] = { row1, row };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/InExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/InExpressionUnitTest.java
index 407f6a6bfda..b3850f46ecb 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/InExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/InExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static org.junit.Assert.assertEquals;
@@ -210,8 +209,8 @@ public class InExpressionUnitTest {
     right.setColIndex(1);
     inExpression = new InExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal row = Decimal.apply(123452154.0);
-    Decimal row1 = Decimal.apply(123452154.0);
+    BigDecimal row = new BigDecimal(123452154.0);
+    BigDecimal row1 = new BigDecimal(123452154.0);
     Object objectRow[] = { row, row1 };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanEqualToExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanEqualToExpressionUnitTest.java
index 1a77ec29ed8..db6430faa16 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanEqualToExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanEqualToExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -261,8 +260,8 @@ public class LessThanEqualToExpressionUnitTest {
     left.setColIndex(1);
     lessThanEqualToExpression = new LessThanEqualToExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(46851.2) };
-    Decimal[] row1 = new Decimal[] { Decimal.apply(45821.02) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(46851.2) };
+    BigDecimal[] row1 = new BigDecimal[] { new BigDecimal(45821.02) };
     Object objectRow[] = { row1, row };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanExpressionUnitTest.java
index 88beb39c2bc..63b9d824223 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/LessThanExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -258,8 +257,8 @@ public class LessThanExpressionUnitTest {
     left.setColIndex(1);
     lessThanExpression = new LessThanExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(256324.0) };
-    Decimal[] row1 = new Decimal[] { Decimal.apply(123451245.0) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(256324.0) };
+    BigDecimal[] row1 = new BigDecimal[] { new BigDecimal(123451245.0) };
     Object objectRow[] = { row1, row };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotEqualsExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotEqualsExpressionUnitTest.java
index e284153a9d7..9773c3f5ba2 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotEqualsExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotEqualsExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static junit.framework.Assert.assertEquals;
@@ -268,8 +267,8 @@ public class NotEqualsExpressionUnitTest {
     left.setColIndex(0);
     notEqualsExpression = new NotEqualsExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal[] row = new Decimal[] { Decimal.apply(12345.0) };
-    Decimal[] row1 = new Decimal[] { Decimal.apply(1235445.0) };
+    BigDecimal[] row = new BigDecimal[] { new BigDecimal(12345.0) };
+    BigDecimal[] row1 = new BigDecimal[] { new BigDecimal(1235445.0) };
     Object objectRow[] = { row, row1 };
     value.setValues(objectRow);
 
diff --git a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotInExpressionUnitTest.java b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotInExpressionUnitTest.java
index ef23b9b7f77..6e186550cf4 100644
--- a/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotInExpressionUnitTest.java
+++ b/core/src/test/java/org/apache/carbondata/scan/expression/conditional/NotInExpressionUnitTest.java
@@ -34,7 +34,6 @@
 
 import mockit.Mock;
 import mockit.MockUp;
-import org.apache.spark.sql.types.Decimal;
 import org.junit.Test;
 
 import static org.junit.Assert.assertEquals;
@@ -209,8 +208,8 @@ public class NotInExpressionUnitTest {
     right.setColIndex(1);
     notInExpression = new NotInExpression(left, right);
     RowImpl value = new RowImpl();
-    Decimal row = Decimal.apply(123452154.0);
-    Decimal row1 = Decimal.apply(1234521215454.0);
+    BigDecimal row = new BigDecimal(123452154.0);
+    BigDecimal row1 = new BigDecimal(1234521215454.0);
     Object objectRow[] = { row, row1 };
     value.setValues(objectRow);
 
diff --git a/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodedReadSupportImpl.java b/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodedReadSupportImpl.java
index d81dd7fcc69..df734d263d8 100644
--- a/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodedReadSupportImpl.java
+++ b/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/DictionaryDecodedReadSupportImpl.java
@@ -18,6 +18,14 @@
  */
 package org.apache.carbondata.hadoop.readsupport.impl;
 
+import java.math.BigDecimal;
+
+import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
+
+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
+import org.apache.spark.sql.types.Decimal;
+import org.apache.spark.sql.types.GenericArrayData;
+
 /**
  * It decodes the dictionary values to actual values.
  */
@@ -29,6 +37,12 @@
     for (int i = 0; i < dictionaries.length; i++) {
       if (dictionaries[i] != null) {
         data[i] = dictionaries[i].getDictionaryValueForKey((int) data[i]);
+      }  else if (carbonColumns[i].getDataType().equals(DataType.DECIMAL)) {
+        data[i] = Decimal.apply((BigDecimal) data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.ARRAY)) {
+        data[i] = new GenericArrayData((Object[]) data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.STRUCT)) {
+        data[i] = new GenericInternalRow((Object[]) data[i]);
       }
     }
     return data;
diff --git a/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/RawDataReadSupport.java b/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/RawDataReadSupport.java
index c381bb7c74a..3a16992986d 100644
--- a/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/RawDataReadSupport.java
+++ b/hadoop/src/main/java/org/apache/carbondata/hadoop/readsupport/impl/RawDataReadSupport.java
@@ -26,10 +26,12 @@
 import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
 
 public class RawDataReadSupport implements CarbonReadSupport<InternalRow> {
+  private CarbonColumn[] carbonColumns;
 
   @Override
   public void initialize(CarbonColumn[] carbonColumns,
       AbsoluteTableIdentifier absoluteTableIdentifier) {
+    this.carbonColumns = carbonColumns;
   }
 
   /**
diff --git a/integration/spark-common/src/main/java/org/apache/carbondata/spark/util/DataTypeUtil.java b/integration/spark-common/src/main/java/org/apache/carbondata/spark/util/DataTypeUtil.java
new file mode 100644
index 00000000000..02f3b84a17b
--- /dev/null
+++ b/integration/spark-common/src/main/java/org/apache/carbondata/spark/util/DataTypeUtil.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.carbondata.spark.util;
+
+import java.text.DateFormat;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.util.Date;
+
+import org.apache.carbondata.common.logging.LogService;
+import org.apache.carbondata.common.logging.LogServiceFactory;
+import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
+import org.apache.carbondata.core.constants.CarbonCommonConstants;
+import org.apache.carbondata.core.util.CarbonProperties;
+
+import org.apache.spark.sql.types.Decimal;
+import org.apache.spark.unsafe.types.UTF8String;
+
+public class DataTypeUtil {
+  private static final LogService LOGGER =
+      LogServiceFactory.getLogService(DataTypeUtil.class.getCanonicalName());
+
+  private static final ThreadLocal<DateFormat> formatter = new ThreadLocal<DateFormat>() {
+    @Override
+    protected DateFormat initialValue() {
+      return new SimpleDateFormat(
+          CarbonProperties.getInstance().getProperty(CarbonCommonConstants.CARBON_TIMESTAMP_FORMAT,
+              CarbonCommonConstants.CARBON_TIMESTAMP_DEFAULT_FORMAT));
+    }
+  };
+
+  public static Object getDataBasedOnDataType(String data, DataType actualDataType) {
+    if (null == data || CarbonCommonConstants.MEMBER_DEFAULT_VAL.equals(data)) {
+      return null;
+    }
+    try {
+      switch (actualDataType) {
+        case INT:
+          if (data.isEmpty()) {
+            return null;
+          }
+          return Integer.parseInt(data);
+        case SHORT:
+          if (data.isEmpty()) {
+            return null;
+          }
+          return Short.parseShort(data);
+        case DOUBLE:
+          if (data.isEmpty()) {
+            return null;
+          }
+          return Double.parseDouble(data);
+        case LONG:
+          if (data.isEmpty()) {
+            return null;
+          }
+          return Long.parseLong(data);
+        case TIMESTAMP:
+          if (data.isEmpty()) {
+            return null;
+          }
+          try {
+            Date dateToStr = formatter.get().parse(data);
+            return dateToStr.getTime() * 1000;
+          } catch (ParseException e) {
+            LOGGER.error("Cannot convert" + data + " to Time/Long type value" + e.getMessage());
+            return null;
+          }
+        case DECIMAL:
+          if (data.isEmpty()) {
+            return null;
+          }
+          return Decimal.apply(data);
+        default:
+          return UTF8String.fromString(data);
+      }
+    } catch (NumberFormatException ex) {
+      LOGGER.error("Problem while converting data type" + data);
+      return null;
+    }
+  }
+}
diff --git a/integration/spark/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java b/integration/spark/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
index 27e61926fb6..40924ab5393 100644
--- a/integration/spark/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
+++ b/integration/spark/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
@@ -18,8 +18,12 @@
  */
 package org.apache.carbondata.spark.readsupport;
 
+<<<<<<< HEAD
+import java.math.BigDecimal;
+=======
 import java.io.IOException;
 import java.sql.Date;
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
 import java.sql.Timestamp;
 
 import org.apache.carbondata.core.carbon.AbsoluteTableIdentifier;
@@ -30,7 +34,10 @@
 import org.apache.carbondata.hadoop.readsupport.impl.AbstractDictionaryDecodedReadSupport;
 
 import org.apache.spark.sql.Row;
+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
 import org.apache.spark.sql.catalyst.expressions.GenericRow;
+import org.apache.spark.sql.types.Decimal;
+import org.apache.spark.sql.types.GenericArrayData;
 import org.apache.spark.unsafe.types.UTF8String;
 
 public class SparkRowReadSupportImpl extends AbstractDictionaryDecodedReadSupport<Row> {
@@ -68,14 +75,19 @@ public class SparkRowReadSupportImpl extends AbstractDictionaryDecodedReadSuppor
             break;
           default:
         }
-      }
-      else if (carbonColumns[i].hasEncoding(Encoding.DIRECT_DICTIONARY)) {
+      } else if (carbonColumns[i].hasEncoding(Encoding.DIRECT_DICTIONARY)) {
         //convert the long to timestamp in case of direct dictionary column
         if (DataType.TIMESTAMP == carbonColumns[i].getDataType()) {
           data[i] = new Timestamp((long) data[i] / 1000L);
         } else if (DataType.DATE == carbonColumns[i].getDataType()) {
           data[i] = new Date((long) data[i]);
         }
+      } else if (carbonColumns[i].getDataType().equals(DataType.DECIMAL)) {
+        data[i] = Decimal.apply((BigDecimal) data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.ARRAY)) {
+        data[i] = new GenericArrayData((Object[]) data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.STRUCT)) {
+        data[i] = new GenericInternalRow((Object[]) data[i]);
       }
     }
     return new GenericRow(data);
diff --git a/integration/spark/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala b/integration/spark/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
index 11c17264e3a..d1aa07a3a55 100644
--- a/integration/spark/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
+++ b/integration/spark/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
@@ -34,8 +34,9 @@ import org.apache.carbondata.core.carbon.metadata.datatype.DataType
 import org.apache.carbondata.core.carbon.metadata.encoder.Encoding
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonDimension
 import org.apache.carbondata.core.carbon.querystatistics._
-import org.apache.carbondata.core.util.{CarbonTimeStatisticsFactory, DataTypeUtil}
+import org.apache.carbondata.core.util.CarbonTimeStatisticsFactory
 import org.apache.carbondata.spark.CarbonAliasDecoderRelation
+import org.apache.carbondata.spark.util.DataTypeUtil
 
 /**
  * It decodes the dictionary key to value
diff --git a/integration/spark/src/main/scala/org/apache/spark/sql/CarbonScan.scala b/integration/spark/src/main/scala/org/apache/spark/sql/CarbonScan.scala
index 40a85a1a75b..9d21e24b00b 100644
--- a/integration/spark/src/main/scala/org/apache/spark/sql/CarbonScan.scala
+++ b/integration/spark/src/main/scala/org/apache/spark/sql/CarbonScan.scala
@@ -25,11 +25,13 @@ import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.execution.LeafNode
 import org.apache.spark.sql.hive.CarbonMetastore
+import org.apache.spark.unsafe.types.UTF8String
 
 import org.apache.carbondata.hadoop.CarbonProjection
 import org.apache.carbondata.scan.model._
 import org.apache.carbondata.spark.CarbonFilters
 import org.apache.carbondata.spark.rdd.CarbonScanRDD
+import org.apache.carbondata.spark.util.DataTypeUtil
 
 case class CarbonScan(
     var columnProjection: Seq[Attribute],
@@ -143,8 +145,19 @@ case class CarbonScan(
 
         override def next(): InternalRow = {
           val value = iter.next
+          val converted = value.map {
+            case str: String =>
+              UTF8String.fromString(str)
+            case data =>
+              data
+          }
+
           if (outUnsafeRows) {
+<<<<<<< HEAD
+            unsafeProjection(new GenericMutableRow(converted))
+=======
             unsafeProjection(value)
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
           } else {
             value
           }
diff --git a/integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java b/integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
index a87a26c6c43..f4a26464f36 100644
--- a/integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
+++ b/integration/spark2/src/main/java/org/apache/carbondata/spark/readsupport/SparkRowReadSupportImpl.java
@@ -18,15 +18,28 @@
  */
 package org.apache.carbondata.spark.readsupport;
 
+<<<<<<< HEAD
+import java.math.BigDecimal;
+import java.sql.Timestamp;
+=======
 import java.io.IOException;
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
 
 import org.apache.carbondata.core.carbon.AbsoluteTableIdentifier;
 import org.apache.carbondata.core.carbon.metadata.datatype.DataType;
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonColumn;
 import org.apache.carbondata.hadoop.readsupport.impl.AbstractDictionaryDecodedReadSupport;
 
+<<<<<<< HEAD
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
+import org.apache.spark.sql.catalyst.expressions.GenericRow;
+import org.apache.spark.sql.catalyst.util.GenericArrayData;
+import org.apache.spark.sql.types.Decimal;
+=======
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
 
 public class SparkRowReadSupportImpl extends AbstractDictionaryDecodedReadSupport<InternalRow> {
 
@@ -47,6 +60,12 @@ public class SparkRowReadSupportImpl extends AbstractDictionaryDecodedReadSuppor
         } else if(dataTypes[i].equals(DataType.SHORT)) {
           data[i] = ((Long)(data[i])).shortValue();
         }
+      } else if (carbonColumns[i].getDataType().equals(DataType.DECIMAL)) {
+        data[i] = Decimal.apply((BigDecimal) data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.ARRAY)) {
+        data[i] = new GenericArrayData(data[i]);
+      } else if (carbonColumns[i].getDataType().equals(DataType.STRUCT)) {
+        data[i] = new GenericInternalRow((Object[]) data[i]);
       }
     }
     return new GenericInternalRow(data);
diff --git a/integration/spark2/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala b/integration/spark2/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
index fbcfbc85ca8..a91c08a6d3e 100644
--- a/integration/spark2/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
+++ b/integration/spark2/src/main/scala/org/apache/spark/sql/CarbonDictionaryDecoder.scala
@@ -34,7 +34,7 @@ import org.apache.carbondata.core.carbon.{AbsoluteTableIdentifier, ColumnIdentif
 import org.apache.carbondata.core.carbon.metadata.datatype.DataType
 import org.apache.carbondata.core.carbon.metadata.encoder.Encoding
 import org.apache.carbondata.core.carbon.metadata.schema.table.column.CarbonDimension
-import org.apache.carbondata.core.util.DataTypeUtil
+import org.apache.carbondata.spark.util.DataTypeUtil
 import org.apache.carbondata.spark.CarbonAliasDecoderRelation
 
 /**
diff --git a/integration/spark2/src/test/scala/org/apache/spark/carbondata/CarbonDataSourceSuite.scala b/integration/spark2/src/test/scala/org/apache/spark/carbondata/CarbonDataSourceSuite.scala
index 97a180b0ee4..083743aa83e 100644
--- a/integration/spark2/src/test/scala/org/apache/spark/carbondata/CarbonDataSourceSuite.scala
+++ b/integration/spark2/src/test/scala/org/apache/spark/carbondata/CarbonDataSourceSuite.scala
@@ -23,9 +23,14 @@ import org.scalatest.BeforeAndAfterAll
 class CarbonDataSourceSuite extends QueryTest with BeforeAndAfterAll {
   override def beforeAll(): Unit = {
     // Drop table
+<<<<<<< HEAD
+    spark.sql("DROP TABLE IF EXISTS carbon_testtable")
+    spark.sql("DROP TABLE IF EXISTS csv_table")
+=======
     sql("DROP TABLE IF EXISTS carbon_testtable")
     sql("DROP TABLE IF EXISTS csv_table")
 
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
     // Create table
     sql(
       s"""
@@ -55,8 +60,15 @@ class CarbonDataSourceSuite extends QueryTest with BeforeAndAfterAll {
   }
 
   override def afterAll(): Unit = {
+<<<<<<< HEAD
+    spark.sql("drop table csv_table")
+    spark.sql("drop table carbon_testtable")
+    spark.sparkContext.stop()
+    spark = null
+=======
     sql("drop table carbon_testtable")
     sql("DROP TABLE IF EXISTS csv_table")
+>>>>>>> bc5a061e9fac489f997cfd68238622e348512d6f
   }
 
   test("project") {
diff --git a/pom.xml b/pom.xml
index f68b24b9ff0..20720282dc2 100644
--- a/pom.xml
+++ b/pom.xml
@@ -106,6 +106,8 @@
     <snappy.version>1.1.2.6</snappy.version>
     <hadoop.version>2.2.0</hadoop.version>
     <kettle.version>4.4.0-stable</kettle.version>
+    <lz4.version>1.3.0</lz4.version>
+    <commonslang3.version>3.5</commonslang3.version>
     <use.kettle>true</use.kettle>
     <hadoop.deps.scope>compile</hadoop.deps.scope>
     <spark.deps.scope>compile</spark.deps.scope>
