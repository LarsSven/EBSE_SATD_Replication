diff --git a/flink-python/pyflink/table/__init__.py b/flink-python/pyflink/table/__init__.py
index 1e367f311df06..010434ce5fb94 100644
--- a/flink-python/pyflink/table/__init__.py
+++ b/flink-python/pyflink/table/__init__.py
@@ -55,6 +55,8 @@
       user-defined function is executed, such as the metric group, and global job parameters, etc.
     - :class:`pyflink.table.ScalarFunction`
       Base interface for user-defined scalar function.
+    - :class:`pyflink.table.StatementSet`
+      Base interface accepts DML statements or Tables.
 """
 from __future__ import absolute_import
 
@@ -71,6 +73,7 @@
 from pyflink.table.table_schema import TableSchema
 from pyflink.table.udf import FunctionContext, ScalarFunction
 from pyflink.table.explain_detail import ExplainDetail
+from pyflink.table.statement_set import StatementSet
 
 __all__ = [
     'TableEnvironment',
@@ -95,5 +98,6 @@
     'FunctionContext',
     'ScalarFunction',
     'SqlDialect',
-    'ExplainDetail'
+    'ExplainDetail',
+    'StatementSet'
 ]
diff --git a/flink-python/pyflink/table/statement_set.py b/flink-python/pyflink/table/statement_set.py
new file mode 100644
index 0000000000000..9bf5dc6aa443f
--- /dev/null
+++ b/flink-python/pyflink/table/statement_set.py
@@ -0,0 +1,93 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+from pyflink.util.utils import to_j_explain_detail_arr
+
+__all__ = ['StatementSet']
+
+
+class StatementSet(object):
+    """
+    A StatementSet accepts DML statements or Tables,
+    the planner can optimize all added statements and Tables together
+    and then submit as one job.
+
+    .. note::
+
+        The added statements and Tables will be cleared
+        when calling the `execute` method.
+
+    """
+
+    def __init__(self, _j_statement_set):
+        self._j_statement_set = _j_statement_set
+
+    def add_insert_sql(self, stmt):
+        """
+        add insert statement to the set.
+
+        :param stmt: The statement to be added.
+        :type stmt: str
+        :return: current StatementSet instance.
+        :rtype: pyflink.table.StatementSet
+        """
+        self._j_statement_set.addInsertSql(stmt)
+        return self
+
+    def add_insert(self, target_path, table, overwrite=False):
+        """
+        add Table with the given sink table name to the set.
+
+        :param target_path: The path of the registered :class:`~pyflink.table.TableSink` to which
+                            the :class:`~pyflink.table.Table` is written.
+        :type target_path: str
+        :param table: The Table to add.
+        :type table: pyflink.table.Table
+        :param overwrite: The flag that indicates whether the insert
+                          should overwrite existing data or not.
+        :type overwrite: bool
+        :return: current StatementSet instance.
+        :rtype: pyflink.table.StatementSet
+        """
+        self._j_statement_set.addInsert(target_path, table._j_table, overwrite)
+        return self
+
+    def explain(self, *extra_details):
+        """
+        returns the AST and the execution plan of all statements and Tables.
+
+        :param extra_details: The extra explain details which the explain result should include,
+                              e.g. estimated cost, changelog mode for streaming
+        :type extra_details: tuple[ExplainDetail] (variable-length arguments of ExplainDetail)
+        :return: All statements and Tables for which the AST and execution plan will be returned.
+        :rtype: str
+        """
+        j_extra_details = to_j_explain_detail_arr(extra_details)
+        return self._j_statement_set.explain(j_extra_details)
+
+    def execute(self):
+        """
+        execute all statements and Tables as a batch.
+
+        .. note::
+            The added statements and Tables will be cleared when executing this method.
+
+        :return: execution result.
+        """
+        # TODO convert java TableResult to python TableResult once FLINK-17303 is finished
+        return self._j_statement_set.execute()
diff --git a/flink-python/pyflink/table/table_environment.py b/flink-python/pyflink/table/table_environment.py
index 91073d8f8a36a..4962a7178beea 100644
--- a/flink-python/pyflink/table/table_environment.py
+++ b/flink-python/pyflink/table/table_environment.py
@@ -28,6 +28,7 @@
 from pyflink.serializers import BatchedSerializer, PickleSerializer
 from pyflink.table.catalog import Catalog
 from pyflink.table.serializers import ArrowSerializer
+from pyflink.table.statement_set import StatementSet
 from pyflink.table.table_config import TableConfig
 from pyflink.table.descriptors import StreamTableDescriptor, BatchTableDescriptor
 
@@ -524,6 +525,18 @@ def execute_sql(self, stmt):
         """
         return self._j_tenv.executeSql(stmt)
 
+    def create_statement_set(self):
+        """
+        Create a StatementSet instance which accepts DML statements or Tables,
+        the planner can optimize all added statements and Tables together
+        and then submit as one job.
+
+        :return statement_set instance
+        :rtype: pyflink.table.StatementSet
+        """
+        _j_statement_set = self._j_tenv.createStatementSet()
+        return StatementSet(_j_statement_set)
+
     def sql_update(self, stmt):
         """
         Evaluates a SQL statement such as INSERT, UPDATE or DELETE or a DDL statement
diff --git a/flink-python/pyflink/table/tests/test_table_environment_api.py b/flink-python/pyflink/table/tests/test_table_environment_api.py
index 96987de6310b1..5219efbd1715e 100644
--- a/flink-python/pyflink/table/tests/test_table_environment_api.py
+++ b/flink-python/pyflink/table/tests/test_table_environment_api.py
@@ -224,6 +224,26 @@ def test_insert_into(self):
         expected = ['1,Hi,Hello']
         self.assert_equals(actual, expected)
 
+    def test_statement_set(self):
+        t_env = self.t_env
+        source = t_env.from_elements([(1, "Hi", "Hello"), (2, "Hello", "Hello")], ["a", "b", "c"])
+        field_names = ["a", "b", "c"]
+        field_types = [DataTypes.BIGINT(), DataTypes.STRING(), DataTypes.STRING()]
+        t_env.register_table_sink(
+            "sink1",
+            source_sink_utils.TestAppendSink(field_names, field_types))
+        t_env.register_table_sink(
+            "sink2",
+            source_sink_utils.TestAppendSink(field_names, field_types))
+
+        stmt_set = t_env.create_statement_set()
+
+        stmt_set.add_insert_sql("insert into sink1 select * from %s where a > 100" % source)\
+            .add_insert("sink2", source.filter("a < 100"), False)
+
+        actual = stmt_set.explain(ExplainDetail.CHANGELOG_MODE)
+        assert isinstance(actual, str)
+
     def test_explain_with_multi_sinks(self):
         t_env = self.t_env
         source = t_env.from_elements([(1, "Hi", "Hello"), (2, "Hello", "Hello")], ["a", "b", "c"])
@@ -431,6 +451,26 @@ def test_explain_with_multi_sinks(self):
 
         assert isinstance(actual, str)
 
+    def test_statement_set(self):
+        t_env = self.t_env
+        source = t_env.from_elements([(1, "Hi", "Hello"), (2, "Hello", "Hello")], ["a", "b", "c"])
+        field_names = ["a", "b", "c"]
+        field_types = [DataTypes.BIGINT(), DataTypes.STRING(), DataTypes.STRING()]
+        t_env.register_table_sink(
+            "sink1",
+            CsvTableSink(field_names, field_types, "path1"))
+        t_env.register_table_sink(
+            "sink2",
+            CsvTableSink(field_names, field_types, "path2"))
+
+        stmt_set = t_env.create_statement_set()
+
+        stmt_set.add_insert_sql("insert into sink1 select * from %s where a > 100" % source)\
+            .add_insert("sink2", source.filter("a < 100"))
+
+        actual = stmt_set.explain()
+        assert isinstance(actual, str)
+
     def test_create_table_environment(self):
         table_config = TableConfig()
         table_config.set_max_generated_code_length(32000)
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/StatementSet.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/StatementSet.java
new file mode 100644
index 0000000000000..a465fce2f7658
--- /dev/null
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/StatementSet.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.api;
+
+import org.apache.flink.annotation.PublicEvolving;
+
+/**
+ * A {@link StatementSet} accepts DML statements or {@link Table}s,
+ * the planner can optimize all added statements and Tables together
+ * and then submit as one job.
+ *
+ * <p>The added statements and Tables will be cleared
+ * when calling the `execute` method.
+ */
+@PublicEvolving
+public interface StatementSet {
+
+	/**
+	 * add insert statement to the set.
+	 */
+	StatementSet addInsertSql(String statement);
+
+	/**
+	 * add Table with the given sink table name to the set.
+	 */
+	StatementSet addInsert(String targetPath, Table table);
+
+	/**
+	 * add {@link Table} with the given sink table name to the set.
+	 */
+	StatementSet addInsert(String targetPath, Table table, boolean overwrite);
+
+	/**
+	 * returns the AST and the execution plan to compute the result of the
+	 * all statements and Tables.
+	 *
+	 * @param extraDetails The extra explain details which the explain result should include,
+	 *                     e.g. estimated cost, changelog mode for streaming
+	 * @return AST and the execution plan.
+	 */
+	String explain(ExplainDetail... extraDetails);
+
+	/**
+	 * execute all statements and Tables as a batch.
+	 *
+	 * <p>The added statements and Tables will be cleared when executing this method.
+	 */
+	TableResult execute();
+}
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
index 12d21ec4fde32..689898854ba06 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/TableEnvironment.java
@@ -1132,4 +1132,11 @@ default Table fromValues(DataType rowType, Object... values) {
 	 * @throws Exception which occurs during job execution.
 	 */
 	JobExecutionResult execute(String jobName) throws Exception;
+
+	/**
+	 * Create a {@link StatementSet} instance which accepts DML statements or Tables,
+	 * the planner can optimize all added statements and Tables together
+	 * and then submit as one job.
+	 */
+	StatementSet createStatementSet();
 }
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/StatementSetImpl.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/StatementSetImpl.java
new file mode 100644
index 0000000000000..7852ca41f152c
--- /dev/null
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/StatementSetImpl.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.api.internal;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.table.api.ExplainDetail;
+import org.apache.flink.table.api.StatementSet;
+import org.apache.flink.table.api.Table;
+import org.apache.flink.table.api.TableException;
+import org.apache.flink.table.api.TableResult;
+import org.apache.flink.table.catalog.ObjectIdentifier;
+import org.apache.flink.table.catalog.UnresolvedIdentifier;
+import org.apache.flink.table.operations.CatalogSinkModifyOperation;
+import org.apache.flink.table.operations.ModifyOperation;
+import org.apache.flink.table.operations.Operation;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+
+/**
+ * Implementation for {@link StatementSet}.
+ */
+@Internal
+class StatementSetImpl implements StatementSet {
+	private final TableEnvironmentInternal tableEnvironment;
+	private List<ModifyOperation> operations = new ArrayList<>();
+
+	protected StatementSetImpl(TableEnvironmentInternal tableEnvironment) {
+		this.tableEnvironment = tableEnvironment;
+	}
+
+	@Override
+	public StatementSet addInsertSql(String statement) {
+		List<Operation> operations = tableEnvironment.getParser().parse(statement);
+
+		if (operations.size() != 1) {
+			throw new TableException("Only single statement is supported.");
+		}
+
+		Operation operation = operations.get(0);
+		if (operation instanceof ModifyOperation) {
+			this.operations.add((ModifyOperation) operation);
+		} else {
+			throw new TableException("Only insert statement is supported now.");
+		}
+		return this;
+	}
+
+	@Override
+	public StatementSet addInsert(String targetPath, Table table) {
+		return addInsert(targetPath, table, false);
+	}
+
+	@Override
+	public StatementSet addInsert(String targetPath, Table table, boolean overwrite) {
+		UnresolvedIdentifier unresolvedIdentifier = tableEnvironment.getParser().parseIdentifier(targetPath);
+		ObjectIdentifier objectIdentifier = tableEnvironment.getCatalogManager()
+				.qualifyIdentifier(unresolvedIdentifier);
+
+		operations.add(new CatalogSinkModifyOperation(
+				objectIdentifier,
+				table.getQueryOperation(),
+				Collections.emptyMap(),
+				overwrite,
+				Collections.emptyMap()));
+
+		return this;
+	}
+
+	@Override
+	public String explain(ExplainDetail... extraDetails) {
+		List<Operation> operationList = operations.stream().map(o -> (Operation) o).collect(Collectors.toList());
+		return tableEnvironment.explainInternal(operationList, extraDetails);
+	}
+
+	@Override
+	public TableResult execute() {
+		try {
+			return tableEnvironment.executeInternal(operations);
+		} finally {
+			operations.clear();
+		}
+	}
+}
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
index 490e4166bf97d..8e1ba418e74e7 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/api/internal/TableEnvironmentImpl.java
@@ -30,6 +30,7 @@
 import org.apache.flink.table.api.ExplainDetail;
 import org.apache.flink.table.api.ResultKind;
 import org.apache.flink.table.api.SqlParserException;
+import org.apache.flink.table.api.StatementSet;
 import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.TableConfig;
 import org.apache.flink.table.api.TableEnvironment;
@@ -654,13 +655,36 @@ public TableResult executeSql(String statement) {
 		return executeOperation(operations.get(0));
 	}
 
+	@Override
+	public StatementSet createStatementSet() {
+		return new StatementSetImpl(this);
+	}
+
 	@Override
 	public TableResult executeInternal(List<ModifyOperation> operations) {
-		if (operations.size() != 1) {
-			throw new TableException("Only one ModifyOperation is supported now.");
-		}
+		List<Transformation<?>> transformations = translate(operations);
+		List<String> sinkIdentifierNames = extractSinkIdentifierNames(operations);
+		String jobName = "insert_into_" + String.join(",", sinkIdentifierNames);
+		Pipeline pipeline = execEnv.createPipeline(transformations, tableConfig, jobName);
+		try {
+			JobClient jobClient = execEnv.executeAsync(pipeline);
+			TableSchema.Builder builder = TableSchema.builder();
+			Object[] affectedRowCounts = new Long[operations.size()];
+			for (int i = 0; i < operations.size(); ++i) {
+				// use sink identifier name as field name
+				builder.field(sinkIdentifierNames.get(i), DataTypes.BIGINT());
+				affectedRowCounts[i] = -1L;
+			}
 
-		return executeOperation(operations.get(0));
+			return TableResultImpl.builder()
+					.jobClient(jobClient)
+					.resultKind(ResultKind.SUCCESS_WITH_CONTENT)
+					.tableSchema(builder.build())
+					.data(Collections.singletonList(Row.of(affectedRowCounts)))
+					.build();
+		} catch (Exception e) {
+			throw new TableException("Failed to execute sql", e);
+		}
 	}
 
 	@Override
@@ -698,20 +722,7 @@ public void sqlUpdate(String stmt) {
 
 	private TableResult executeOperation(Operation operation) {
 		if (operation instanceof ModifyOperation) {
-			List<Transformation<?>> transformations = translate(Collections.singletonList((ModifyOperation) operation));
-			String jobName = extractJobName(operation);
-			Pipeline pipeline = execEnv.createPipeline(transformations, tableConfig, jobName);
-			try {
-				JobClient jobClient = execEnv.executeAsync(pipeline);
-				return TableResultImpl.builder()
-						.jobClient(jobClient)
-						.resultKind(ResultKind.SUCCESS_WITH_CONTENT)
-						.tableSchema(TableSchema.builder().field("affected_rowcount", DataTypes.BIGINT()).build())
-						.data(Collections.singletonList(Row.of(-1L)))
-						.build();
-			} catch (Exception e) {
-				throw new TableException("Failed to execute sql", e);
-			}
+			return executeInternal(Collections.singletonList((ModifyOperation) operation));
 		} else if (operation instanceof CreateTableOperation) {
 			CreateTableOperation createTableOperation = (CreateTableOperation) operation;
 			if (createTableOperation.isTemporary()) {
@@ -749,7 +760,7 @@ private TableResult executeOperation(Operation operation) {
 							alterTableRenameOp.getTableIdentifier().toObjectPath(),
 							alterTableRenameOp.getNewTableIdentifier().getObjectName(),
 							false);
-				} else if (alterTableOperation instanceof AlterTablePropertiesOperation){
+				} else if (alterTableOperation instanceof AlterTablePropertiesOperation) {
 					AlterTablePropertiesOperation alterTablePropertiesOp = (AlterTablePropertiesOperation) operation;
 					catalog.alterTable(
 							alterTablePropertiesOp.getTableIdentifier().toObjectPath(),
@@ -894,14 +905,20 @@ private TableResult buildShowResult(String[] objects) {
 				.build();
 	}
 
-	private String extractJobName(Operation operation) {
-		String tableName;
-		if (operation instanceof CatalogSinkModifyOperation) {
-			tableName = ((CatalogSinkModifyOperation) operation).getTableIdentifier().toString();
-		} else {
-			throw new UnsupportedOperationException("Unsupported operation: " + operation);
+	/**
+	 * extract sink identifier names from {@link ModifyOperation}s.
+	 */
+	private List<String> extractSinkIdentifierNames(List<ModifyOperation> operations) {
+		List<String> tableNames = new ArrayList<>(operations.size());
+		for (ModifyOperation operation : operations) {
+			if (operation instanceof CatalogSinkModifyOperation) {
+				ObjectIdentifier identifier = ((CatalogSinkModifyOperation) operation).getTableIdentifier();
+				tableNames.add(identifier.asSummaryString());
+			} else {
+				throw new UnsupportedOperationException("Unsupported operation: " + operation);
+			}
 		}
-		return "insert_into_" + tableName;
+		return tableNames;
 	}
 
 	/** Get catalog from catalogName or throw a ValidationException if the catalog not exists. */
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/explain/testStatementSet.out b/flink-table/flink-table-planner-blink/src/test/resources/explain/testStatementSet.out
new file mode 100644
index 0000000000000..bcf62f78f4818
--- /dev/null
+++ b/flink-table/flink-table-planner-blink/src/test/resources/explain/testStatementSet.out
@@ -0,0 +1,63 @@
+== Abstract Syntax Tree ==
+LogicalSink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[first])
++- LogicalProject(first=[$0])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first, id, score, last)]]])
+
+LogicalSink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
++- LogicalProject(last=[$3])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first, id, score, last)]]])
+
+== Optimized Logical Plan ==
+Sink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[first])
++- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: first)]]], fields=[first])
+
+Sink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[last])
++- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [CsvTableSource(read fields: last)]]], fields=[last])
+
+== Physical Execution Plan ==
+ : Data Source
+	content : Source: Custom File source
+
+	 : Operator
+		content : CsvTableSource(read fields: first)
+		ship_strategy : REBALANCE
+
+		 : Operator
+			content : SourceConversion(table=[default_catalog.default_database.MyTable, source: [CsvTableSource(read fields: first)]], fields=[first])
+			ship_strategy : FORWARD
+
+			 : Operator
+				content : SinkConversionToRow
+				ship_strategy : FORWARD
+
+				 : Operator
+					content : Map
+					ship_strategy : REBALANCE
+
+ : Data Source
+	content : Source: Custom File source
+
+	 : Operator
+		content : CsvTableSource(read fields: last)
+		ship_strategy : REBALANCE
+
+		 : Operator
+			content : SourceConversion(table=[default_catalog.default_database.MyTable, source: [CsvTableSource(read fields: last)]], fields=[last])
+			ship_strategy : FORWARD
+
+			 : Operator
+				content : SinkConversionToRow
+				ship_strategy : FORWARD
+
+				 : Operator
+					content : Map
+					ship_strategy : REBALANCE
+
+					 : Data Sink
+						content : Sink: CsvTableSink(first)
+						ship_strategy : FORWARD
+
+						 : Data Sink
+							content : Sink: CsvTableSink(last)
+							ship_strategy : FORWARD
+
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
index 21540d4cd14ba..5e1451e2b6ccc 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.api.scala.{StreamTableEnvironment => ScalaStreamTa
 import org.apache.flink.table.planner.factories.utils.TestCollectionTableFactory
 import org.apache.flink.table.planner.runtime.utils.TestingAppendSink
 import org.apache.flink.table.planner.utils.TableTestUtil.{readFromResource, replaceStageId}
-import org.apache.flink.table.planner.utils.TestTableSourceSinks
+import org.apache.flink.table.planner.utils.{TableTestUtil, TestTableSourceSinks}
 import org.apache.flink.types.Row
 import org.apache.flink.util.{FileUtils, TestLogger}
 
@@ -268,7 +268,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
       tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
     checkEmptyFile(sinkPath)
     val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -297,7 +297,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     )
 
     val tableResult1 = tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -305,7 +305,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     assertFirstValues(sinkPath)
 
     val tableResult2 =  tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -323,7 +323,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     checkEmptyFile(sink2Path)
 
     val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -361,7 +361,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     resultSet.addSink(sink)
 
     val tableResult = streamTableEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -386,7 +386,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     checkEmptyFile(sinkPath)
     val table = tEnv.sqlQuery("select first from MyTable")
     val tableResult = table.executeInsert("MySink")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -401,7 +401,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
       return
     }
     val sinkPath = _tempFolder.newFolder().toString
-    tEnv.sqlUpdate(
+    tEnv.executeSql(
       s"""
          |create table MySink (
          |  first string
@@ -413,7 +413,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
        """.stripMargin
     )
     val tableResult1 = tEnv.sqlQuery("select first from MyTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -421,7 +421,7 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     assertFirstValues(sinkPath)
 
     val tableResult2 = tEnv.sqlQuery("select first from MyTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -429,6 +429,103 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     assertFirstValues(sinkPath)
   }
 
+  @Test
+  def testStatementSet(): Unit = {
+    val sink1Path = TestTableSourceSinks.createCsvTemporarySinkTable(
+      tEnv, new TableSchema(Array("first"), Array(STRING)), "MySink1")
+
+    val sink2Path = TestTableSourceSinks.createCsvTemporarySinkTable(
+      tEnv, new TableSchema(Array("last"), Array(STRING)), "MySink2")
+
+    val stmtSet = tEnv.createStatementSet()
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"))
+      .addInsertSql("insert into MySink2 select last from MyTable")
+
+    val actual = stmtSet.explain()
+    val expected = TableTestUtil.readFromResource("/explain/testStatementSet.out")
+    assertEquals(replaceStageId(expected), replaceStageId(actual))
+
+    val tableResult = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+  }
+
+  @Test
+  def testStatementSetWithOverwrite(): Unit = {
+    if(isStreaming) {
+      // Streaming mode not support overwrite for FileSystemTableSink.
+      return
+    }
+    val sink1Path = _tempFolder.newFolder().toString
+    tEnv.executeSql(
+      s"""
+         |create table MySink1 (
+         |  first string
+         |) with (
+         |  'connector' = 'filesystem',
+         |  'path' = '$sink1Path',
+         |  'format' = 'testcsv'
+         |)
+       """.stripMargin
+    )
+
+    val sink2Path = _tempFolder.newFolder().toString
+    tEnv.executeSql(
+      s"""
+         |create table MySink2 (
+         |  last string
+         |) with (
+         |  'connector' = 'filesystem',
+         |  'path' = '$sink2Path',
+         |  'format' = 'testcsv'
+         |)
+       """.stripMargin
+    )
+
+    val stmtSet = tEnv.createStatementSet()
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"), true)
+    stmtSet.addInsertSql("insert overwrite MySink2 select last from MyTable")
+
+    val tableResult1 = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult1,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult1.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+
+    // execute again using same StatementSet instance
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"), true)
+      .addInsertSql("insert overwrite MySink2 select last from MyTable")
+
+    val tableResult2 = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult2,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult2.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+  }
+
   @Test
   def testClearOperation(): Unit = {
     TestCollectionTableFactory.reset()
@@ -492,12 +589,16 @@ class TableEnvironmentITCase(tableEnvName: String, isStreaming: Boolean) extends
     assertFalse(new File(path).exists())
   }
 
-  private def checkInsertTableResult(tableResult: TableResult): Unit = {
+  private def checkInsertTableResult(tableResult: TableResult, fieldNames: String*): Unit = {
     assertTrue(tableResult.getJobClient.isPresent)
     assertEquals(ResultKind.SUCCESS_WITH_CONTENT, tableResult.getResultKind)
+    assertEquals(
+      util.Arrays.asList(fieldNames: _*),
+      util.Arrays.asList(tableResult.getTableSchema.getFieldNames: _*))
     val it = tableResult.collect()
     assertTrue(it.hasNext)
-    assertEquals(Row.of(JLong.valueOf(-1L)), it.next())
+    val affectedRowCounts = fieldNames.map(_ => JLong.valueOf(-1L))
+    assertEquals(Row.of(affectedRowCounts: _*), it.next())
     assertFalse(it.hasNext)
   }
 
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/api/internal/TableEnvImpl.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/api/internal/TableEnvImpl.scala
index 4c6cbd4214998..2f4047c15763c 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/api/internal/TableEnvImpl.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/api/internal/TableEnvImpl.scala
@@ -575,11 +575,42 @@ abstract class TableEnvImpl(
     executeOperation(operations.get(0))
   }
 
+  override def createStatementSet = new StatementSetImpl(this)
+
   override def executeInternal(operations: JList[ModifyOperation]): TableResult = {
-    if (operations.size() != 1) {
-      throw new TableException("Only one ModifyOperation is supported now.");
+    val dataSinks = operations.map {
+      case catalogSinkModifyOperation: CatalogSinkModifyOperation =>
+        writeToSinkAndTranslate(
+          catalogSinkModifyOperation.getChild,
+          InsertOptions(
+            catalogSinkModifyOperation.getDynamicOptions,
+            catalogSinkModifyOperation.isOverwrite),
+          catalogSinkModifyOperation.getTableIdentifier)
+      case o =>
+        throw new TableException("Unsupported operation: " + o)
+    }
+
+    val sinkIdentifierNames = extractSinkIdentifierNames(operations)
+    val jobName = "insert_into_" + String.join(",", sinkIdentifierNames)
+    try {
+      val jobClient = execute(dataSinks, jobName)
+      val builder = TableSchema.builder()
+      val affectedRowCounts = new Array[JLong](operations.size())
+      operations.indices.foreach { idx =>
+        // use sink identifier name as field name
+        builder.field(sinkIdentifierNames(idx), DataTypes.BIGINT())
+        affectedRowCounts(idx) = -1L
+      }
+      TableResultImpl.builder()
+        .jobClient(jobClient)
+        .resultKind(ResultKind.SUCCESS_WITH_CONTENT)
+        .tableSchema(builder.build())
+        .data(JCollections.singletonList(Row.of(affectedRowCounts: _*)))
+        .build()
+    } catch {
+      case e: Exception =>
+        throw new TableException("Failed to execute sql", e);
     }
-    executeOperation(operations.get(0))
   }
 
   override def sqlUpdate(stmt: String): Unit = {
@@ -610,20 +641,7 @@ abstract class TableEnvImpl(
   private def executeOperation(operation: Operation): TableResult = {
     operation match {
       case catalogSinkModifyOperation: CatalogSinkModifyOperation =>
-        val dataSink = writeToSinkAndTranslate(
-          catalogSinkModifyOperation.getChild,
-          InsertOptions(
-            catalogSinkModifyOperation.getDynamicOptions,
-            catalogSinkModifyOperation.isOverwrite),
-          catalogSinkModifyOperation.getTableIdentifier)
-        val jobName = extractJobName(catalogSinkModifyOperation)
-        val jobClient = execute(JCollections.singletonList(dataSink), jobName)
-        TableResultImpl.builder()
-          .jobClient(jobClient)
-          .resultKind(ResultKind.SUCCESS_WITH_CONTENT)
-          .tableSchema(TableSchema.builder().field("affected_rowcount", DataTypes.BIGINT()).build())
-          .data(JCollections.singletonList(Row.of(JLong.valueOf(-1L))))
-          .build()
+        executeInternal(JCollections.singletonList(catalogSinkModifyOperation))
       case createTableOperation: CreateTableOperation =>
         if (createTableOperation.isTemporary) {
           catalogManager.createTemporaryTable(
@@ -807,14 +825,16 @@ abstract class TableEnvImpl(
       }))
   }
 
-  private def extractJobName(operation: Operation): String = {
-    val tableName = operation match {
+  /**
+    * extract sink identifier names from [[ModifyOperation]]s.
+    */
+  private def extractSinkIdentifierNames(operations: JList[ModifyOperation]): JList[String] = {
+    operations.map {
       case catalogSinkModifyOperation: CatalogSinkModifyOperation =>
-        catalogSinkModifyOperation.getTableIdentifier.toString
-      case _ =>
-        throw new UnsupportedOperationException("Unsupported operation: " + operation)
+        catalogSinkModifyOperation.getTableIdentifier.asSummaryString()
+      case o =>
+        throw new UnsupportedOperationException("Unsupported operation: " + o)
     }
-    "insert_into_" + tableName
   }
 
   /**
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
index dd83a3ed33e76..9a740dc7a993c 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/api/TableEnvironmentITCase.scala
@@ -24,10 +24,10 @@ import org.apache.flink.api.scala._
 import org.apache.flink.core.fs.FileSystem.WriteMode
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment
 import org.apache.flink.streaming.api.scala.{StreamExecutionEnvironment => ScalaStreamExecutionEnvironment}
-import org.apache.flink.table.api.TableEnvironmentITCase.{getPersonCsvTableSource, getPersonData}
+import org.apache.flink.table.api.TableEnvironmentITCase.getPersonCsvTableSource
 import org.apache.flink.table.api.internal.TableEnvironmentImpl
 import org.apache.flink.table.api.java.StreamTableEnvironment
-import org.apache.flink.table.api.scala.{StreamTableEnvironment => ScalaStreamTableEnvironment, _}
+import org.apache.flink.table.api.scala.{StreamTableEnvironment => ScalaStreamTableEnvironment}
 import org.apache.flink.table.runtime.utils.StreamITCase
 import org.apache.flink.table.sinks.CsvTableSink
 import org.apache.flink.table.sources.CsvTableSource
@@ -254,7 +254,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     val sinkPath = registerCsvTableSink(tEnv, Array("first"), Array(STRING), "MySink1")
     checkEmptyFile(sinkPath)
     val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -272,7 +272,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
 
     checkEmptyFile(sinkPath)
     val tableResult1 = tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -280,7 +280,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     assertFirstValues(sinkPath)
 
     val tableResult2 = tEnv.executeSql("insert overwrite MySink select first from MyTable")
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -296,7 +296,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     checkEmptyFile(sink2Path)
 
     val tableResult = tEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -332,7 +332,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     resultSet.addSink(new StreamITCase.StringSink[Row])
 
     val tableResult = streamTableEnv.executeSql("insert into MySink1 select first from MyTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink1")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -356,7 +356,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     checkEmptyFile(sinkPath)
     val table = tEnv.sqlQuery("select first from MyTable")
     val tableResult = table.executeInsert("MySink")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -374,7 +374,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
 
     checkEmptyFile(sinkPath)
     val tableResult1 = tEnv.sqlQuery("select first from MyTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -382,7 +382,7 @@ class TableEnvironmentITCase(tableEnvName: String) {
     assertFirstValues(sinkPath)
 
     val tableResult2 = tEnv.sqlQuery("select first from MyTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -390,6 +390,82 @@ class TableEnvironmentITCase(tableEnvName: String) {
     assertFirstValues(sinkPath)
   }
 
+  @Test
+  def testStatementSet(): Unit = {
+    val sink1Path = registerCsvTableSink(tEnv, Array("first"), Array(STRING), "MySink1")
+    val sink2Path = registerCsvTableSink(tEnv, Array("last"), Array(STRING), "MySink2")
+
+    val stmtSet = tEnv.createStatementSet()
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"))
+    stmtSet.addInsertSql("insert into MySink2 select last from MyTable")
+
+    val actual = stmtSet.explain()
+    val expected = readFromResource("testStatementSet0.out")
+    assertEquals(replaceStageId(expected), replaceStageId(actual))
+
+    val tableResult = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+  }
+
+  @Test
+  def testStatementSetWithOverwrite(): Unit = {
+    val sink1Path = _tempFolder.newFile().getAbsolutePath
+    val configuredSink1 = new TestingOverwritableTableSink(sink1Path)
+      .configure(Array("first"), Array(STRING))
+    tEnv.registerTableSink("MySink1", configuredSink1)
+    checkEmptyFile(sink1Path)
+
+    val sink2Path = _tempFolder.newFile().getAbsolutePath
+    val configuredSink2 = new TestingOverwritableTableSink(sink2Path)
+      .configure(Array("last"), Array(STRING))
+    tEnv.registerTableSink("MySink2", configuredSink2)
+    checkEmptyFile(sink2Path)
+
+    val stmtSet = tEnv.createStatementSet()
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"), true)
+      .addInsertSql("insert overwrite MySink2 select last from MyTable")
+
+    val tableResult1 = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult1,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult1.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+
+    // execute again using same StatementSet instance
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select first from MyTable"), true)
+      .addInsertSql("insert overwrite MySink2 select last from MyTable")
+
+    val tableResult2 = stmtSet.execute()
+    checkInsertTableResult(
+      tableResult2,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    // wait job finished
+    tableResult2.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+
+    assertFirstValues(sink1Path)
+    assertLastValues(sink2Path)
+  }
+
   private def registerCsvTableSink(
       tEnv: TableEnvironment,
       fieldNames: Array[String],
@@ -433,12 +509,16 @@ class TableEnvironmentITCase(tableEnvName: String) {
     assertFalse(new File(path).exists())
   }
 
-  private def checkInsertTableResult(tableResult: TableResult): Unit = {
+  private def checkInsertTableResult(tableResult: TableResult, fieldNames: String*): Unit = {
     assertTrue(tableResult.getJobClient.isPresent)
     assertEquals(ResultKind.SUCCESS_WITH_CONTENT, tableResult.getResultKind)
+    assertEquals(
+      util.Arrays.asList(fieldNames: _*),
+      util.Arrays.asList(tableResult.getTableSchema.getFieldNames: _*))
     val it = tableResult.collect()
     assertTrue(it.hasNext)
-    assertEquals(Row.of(JLong.valueOf(-1L)), it.next())
+    val affectedRowCounts = fieldNames.map(_ => JLong.valueOf(-1L))
+    assertEquals(Row.of(affectedRowCounts: _*), it.next())
     assertFalse(it.hasNext)
   }
 }
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/TableEnvironmentITCase.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/TableEnvironmentITCase.scala
index dde6ecf5ec60d..65c53a3ab0047 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/TableEnvironmentITCase.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/runtime/batch/sql/TableEnvironmentITCase.scala
@@ -19,7 +19,7 @@
 package org.apache.flink.table.runtime.batch.sql
 
 import org.apache.flink.api.common.typeinfo.TypeInformation
-import org.apache.flink.api.common.typeinfo.Types.STRING
+import org.apache.flink.api.common.typeinfo.Types.{INT, LONG, STRING}
 import org.apache.flink.api.scala._
 import org.apache.flink.api.scala.util.CollectionDataSets
 import org.apache.flink.core.fs.FileSystem
@@ -29,6 +29,7 @@ import org.apache.flink.table.api.{ResultKind, TableEnvironment, TableEnvironmen
 import org.apache.flink.table.runtime.utils.TableProgramsCollectionTestBase
 import org.apache.flink.table.runtime.utils.TableProgramsTestBase.TableConfigMode
 import org.apache.flink.table.sinks.CsvTableSink
+import org.apache.flink.table.utils.TableTestUtil.{readFromResource, replaceStageId}
 import org.apache.flink.table.utils.{MemoryTableSourceSinkUtil, TestingOverwritableTableSink}
 import org.apache.flink.test.util.TestBaseUtils
 import org.apache.flink.types.Row
@@ -42,6 +43,7 @@ import org.junit.runners.Parameterized
 
 import java.io.File
 import java.lang.{Long => JLong}
+import java.util
 
 import scala.collection.JavaConverters._
 import scala.io.Source
@@ -287,7 +289,7 @@ class TableEnvironmentITCase(
     tEnv.registerTableSink("targetTable", sink1.configure(fieldNames, fieldTypes))
 
     val tableResult = tEnv.executeSql("INSERT INTO targetTable SELECT a, b, c FROM sourceTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.targetTable")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -311,7 +313,7 @@ class TableEnvironmentITCase(
     tEnv.registerTableSink("MySink", configuredSink)
 
     val tableResult1 = tEnv.executeSql("INSERT overwrite MySink SELECT c FROM sourceTable")
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -321,7 +323,7 @@ class TableEnvironmentITCase(
     assertEquals(expected1.sorted, actual1.sorted)
 
     val tableResult2 = tEnv.executeSql("INSERT overwrite MySink SELECT c FROM sourceTable")
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -351,7 +353,7 @@ class TableEnvironmentITCase(
     tEnv.sqlUpdate("INSERT INTO MySink1 SELECT * FROM sourceTable where a > 2")
 
     val tableResult = tEnv.executeSql("INSERT INTO targetTable SELECT a, b, c FROM sourceTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.targetTable")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -388,7 +390,7 @@ class TableEnvironmentITCase(
       .writeAsCsv(resultFile, writeMode=FileSystem.WriteMode.OVERWRITE)
 
     val tableResult = tEnv.executeSql("INSERT INTO targetTable SELECT a, b, c FROM sourceTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.targetTable")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -422,7 +424,7 @@ class TableEnvironmentITCase(
 
     val table = tEnv.sqlQuery("SELECT a, b, c FROM sourceTable")
     val tableResult = table.executeInsert("targetTable")
-    checkInsertTableResult(tableResult)
+    checkInsertTableResult(tableResult, "default_catalog.default_database.targetTable")
     // wait job finished
     tableResult.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -446,7 +448,7 @@ class TableEnvironmentITCase(
     tEnv.registerTableSink("MySink", configuredSink)
 
     val tableResult1 = tEnv.sqlQuery("SELECT c FROM sourceTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult1)
+    checkInsertTableResult(tableResult1, "default_catalog.default_database.MySink")
     // wait job finished
     tableResult1.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -456,7 +458,7 @@ class TableEnvironmentITCase(
     assertEquals(expected1.sorted, actual1.sorted)
 
     val tableResult2 = tEnv.sqlQuery("SELECT c FROM sourceTable").executeInsert("MySink", true)
-    checkInsertTableResult(tableResult2)
+    checkInsertTableResult(tableResult2,  "default_catalog.default_database.MySink")
     // wait job finished
     tableResult2.getJobClient.get()
       .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
@@ -466,6 +468,55 @@ class TableEnvironmentITCase(
     assertEquals(expected2.sorted, actual2.sorted)
   }
 
+  @Test
+  def testStatementSet(): Unit = {
+    val env = ExecutionEnvironment.getExecutionEnvironment
+    val tEnv = BatchTableEnvironment.create(env)
+    MemoryTableSourceSinkUtil.clear()
+
+    val t = CollectionDataSets.getSmall3TupleDataSet(env).toTable(tEnv).as('a, 'b, 'c)
+    tEnv.registerTable("MyTable", t)
+
+    val sink1Path = _tempFolder.newFile().getAbsolutePath
+    val configuredSink1 = new TestingOverwritableTableSink(sink1Path)
+      .configure(Array("d", "e", "f"),  Array(INT, LONG, STRING))
+    tEnv.registerTableSink("MySink1", configuredSink1)
+    assertTrue(FileUtils.readFileUtf8(new File(sink1Path)).isEmpty)
+
+    val sink2Path = _tempFolder.newFile().getAbsolutePath
+    val configuredSink2 = new TestingOverwritableTableSink(sink2Path)
+      .configure(Array("i", "j", "k"),  Array(INT, LONG, STRING))
+    tEnv.registerTableSink("MySink2", configuredSink2)
+    assertTrue(FileUtils.readFileUtf8(new File(sink2Path)).isEmpty)
+
+    val stmtSet = tEnv.createStatementSet()
+    stmtSet.addInsert("MySink1", tEnv.sqlQuery("select * from MyTable where a > 2"), true)
+      .addInsertSql("INSERT OVERWRITE MySink2 SELECT a, b, c FROM MyTable where a <= 2")
+
+    val actual = stmtSet.explain()
+    val expected = readFromResource("testStatementSet1.out")
+    assertEquals(replaceStageId(expected), replaceTempVariables(replaceStageId(actual)))
+
+    val tableResult = stmtSet.execute()
+    // wait job finished
+    tableResult.getJobClient.get()
+      .getJobExecutionResult(Thread.currentThread().getContextClassLoader)
+      .get()
+    checkInsertTableResult(
+      tableResult,
+      "default_catalog.default_database.MySink1",
+      "default_catalog.default_database.MySink2")
+    val expected1 = List("3,2,Hello world")
+    assertEquals(
+      expected1.sorted,
+      FileUtils.readFileUtf8(new File(sink1Path)).split("\n").toList.sorted)
+
+    val expected2 = List("1,1,Hi", "2,2,Hello")
+    assertEquals(
+      expected2.sorted,
+      FileUtils.readFileUtf8(new File(sink2Path)).split("\n").toList.sorted)
+  }
+
   private def registerCsvTableSink(
       tEnv: TableEnvironment,
       fieldNames: Array[String],
@@ -481,12 +532,21 @@ class TableEnvironmentITCase(
     path
   }
 
-  private def checkInsertTableResult(tableResult: TableResult): Unit = {
+  private def checkInsertTableResult(tableResult: TableResult, fieldNames: String*): Unit = {
     assertTrue(tableResult.getJobClient.isPresent)
     assertEquals(ResultKind.SUCCESS_WITH_CONTENT, tableResult.getResultKind)
+    assertEquals(
+      util.Arrays.asList(fieldNames: _*),
+      util.Arrays.asList(tableResult.getTableSchema.getFieldNames: _*))
     val it = tableResult.collect()
     assertTrue(it.hasNext)
-    assertEquals(Row.of(JLong.valueOf(-1L)), it.next())
+    val affectedRowCounts = fieldNames.map(_ => JLong.valueOf(-1L))
+    assertEquals(Row.of(affectedRowCounts: _*), it.next())
     assertFalse(it.hasNext)
   }
+
+  private def replaceTempVariables(s: String): String = {
+    s.replaceAll("content : TextOutputFormat \\(.*\\)", "content : TextOutputFormat ()")
+      .replaceAll("DataSetScan\\(ref=\\[\\d+\\]", "DataSetScan(ref=[]")
+  }
 }
diff --git a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/utils/MockTableEnvironment.scala b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/utils/MockTableEnvironment.scala
index 312d980759b4e..928559d0c895a 100644
--- a/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/utils/MockTableEnvironment.scala
+++ b/flink-table/flink-table-planner/src/test/scala/org/apache/flink/table/utils/MockTableEnvironment.scala
@@ -20,7 +20,7 @@ package org.apache.flink.table.utils
 
 import org.apache.flink.api.common.JobExecutionResult
 import org.apache.flink.api.common.typeinfo.TypeInformation
-import org.apache.flink.table.api.{ExplainDetail, Table, TableConfig, TableEnvironment, TableResult}
+import org.apache.flink.table.api.{ExplainDetail, StatementSet, Table, TableConfig, TableEnvironment, TableResult}
 import org.apache.flink.table.catalog.Catalog
 import org.apache.flink.table.descriptors.{ConnectTableDescriptor, ConnectorDescriptor}
 import org.apache.flink.table.expressions.Expression
@@ -82,6 +82,8 @@ class MockTableEnvironment extends TableEnvironment {
 
   override def executeSql(statement: String): TableResult = ???
 
+  override def createStatementSet(): StatementSet = ???
+
   override def sqlUpdate(stmt: String): Unit = ???
 
   override def getConfig: TableConfig = ???
diff --git a/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet0.out b/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet0.out
new file mode 100644
index 0000000000000..dff130e6fb58e
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet0.out
@@ -0,0 +1,63 @@
+== Abstract Syntax Tree ==
+LogicalSink(name=[default_catalog.default_database.MySink1], fields=[first])
+  LogicalProject(first=[$0])
+    LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+
+LogicalSink(name=[default_catalog.default_database.MySink2], fields=[last])
+  LogicalProject(last=[$3])
+    LogicalTableScan(table=[[default_catalog, default_database, MyTable]])
+
+== Optimized Logical Plan ==
+DataStreamSink(name=[default_catalog.default_database.MySink1], fields=[first])
+  StreamTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[first], source=[CsvTableSource(read fields: first)])
+
+DataStreamSink(name=[default_catalog.default_database.MySink2], fields=[last])
+  StreamTableSourceScan(table=[[default_catalog, default_database, MyTable]], fields=[last], source=[CsvTableSource(read fields: last)])
+
+== Physical Execution Plan ==
+ : Data Source
+	content : collect elements with CollectionInputFormat
+
+	 : Operator
+		content : CsvTableSource(read fields: first)
+		ship_strategy : REBALANCE
+
+		 : Operator
+			content : Map
+			ship_strategy : FORWARD
+
+			 : Operator
+				content : to: Row
+				ship_strategy : FORWARD
+
+				 : Operator
+					content : Map
+					ship_strategy : REBALANCE
+
+ : Data Source
+	content : collect elements with CollectionInputFormat
+
+	 : Operator
+		content : CsvTableSource(read fields: last)
+		ship_strategy : REBALANCE
+
+		 : Operator
+			content : Map
+			ship_strategy : FORWARD
+
+			 : Operator
+				content : to: Row
+				ship_strategy : FORWARD
+
+				 : Operator
+					content : Map
+					ship_strategy : REBALANCE
+
+					 : Data Sink
+						content : Sink: CsvTableSink(first)
+						ship_strategy : FORWARD
+
+						 : Data Sink
+							content : Sink: CsvTableSink(last)
+							ship_strategy : FORWARD
+
diff --git a/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet1.out b/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet1.out
new file mode 100644
index 0000000000000..7c7bd52c52ddc
--- /dev/null
+++ b/flink-table/flink-table-planner/src/test/scala/resources/testStatementSet1.out
@@ -0,0 +1,81 @@
+== Abstract Syntax Tree ==
+LogicalSink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[d, e, f])
+  LogicalProject(a=[$0], b=[$1], c=[$2])
+    LogicalFilter(condition=[>($0, 2)])
+      LogicalProject(a=[AS($0, _UTF-16LE'a')], b=[AS($1, _UTF-16LE'b')], c=[AS($2, _UTF-16LE'c')])
+        FlinkLogicalDataSetScan(ref=[], fields=[_1, _2, _3])
+
+LogicalSink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[i, j, k])
+  LogicalProject(a=[$0], b=[$1], c=[$2])
+    LogicalFilter(condition=[<=($0, 2)])
+      LogicalProject(a=[AS($0, _UTF-16LE'a')], b=[AS($1, _UTF-16LE'b')], c=[AS($2, _UTF-16LE'c')])
+        FlinkLogicalDataSetScan(ref=[], fields=[_1, _2, _3])
+
+== Optimized Logical Plan ==
+DataSetSink(name=[`default_catalog`.`default_database`.`MySink1`], fields=[d, e, f])
+  DataSetCalc(select=[_1 AS a, _2 AS b, _3 AS c], where=[>(_1, 2)])
+    DataSetScan(ref=[], fields=[_1, _2, _3])
+
+DataSetSink(name=[`default_catalog`.`default_database`.`MySink2`], fields=[i, j, k])
+  DataSetCalc(select=[_1 AS a, _2 AS b, _3 AS c], where=[<=(_1, 2)])
+    DataSetScan(ref=[], fields=[_1, _2, _3])
+
+== Physical Execution Plan ==
+ : Data Source
+	content : collect elements with CollectionInputFormat
+	Partitioning : RANDOM_PARTITIONED
+
+	 : Map
+		content : from: (_1, _2, _3)
+		ship_strategy : Forward
+		exchange_mode : PIPELINED
+		driver_strategy : Map
+		Partitioning : RANDOM_PARTITIONED
+
+		 : FlatMap
+			content : where: (>(_1, 2)), select: (_1 AS a, _2 AS b, _3 AS c)
+			ship_strategy : Forward
+			exchange_mode : PIPELINED
+			driver_strategy : FlatMap
+			Partitioning : RANDOM_PARTITIONED
+
+			 : Map
+				content : to: Row
+				ship_strategy : Forward
+				exchange_mode : PIPELINED
+				driver_strategy : Map
+				Partitioning : RANDOM_PARTITIONED
+
+				 : Data Sink
+					content : TextOutputFormat () - UTF-8
+					ship_strategy : Forward
+					exchange_mode : PIPELINED
+					Partitioning : RANDOM_PARTITIONED
+
+					 : Map
+						content : from: (_1, _2, _3)
+						ship_strategy : Forward
+						exchange_mode : PIPELINED
+						driver_strategy : Map
+						Partitioning : RANDOM_PARTITIONED
+
+						 : FlatMap
+							content : where: (<=(_1, 2)), select: (_1 AS a, _2 AS b, _3 AS c)
+							ship_strategy : Forward
+							exchange_mode : PIPELINED
+							driver_strategy : FlatMap
+							Partitioning : RANDOM_PARTITIONED
+
+							 : Map
+								content : to: Row
+								ship_strategy : Forward
+								exchange_mode : PIPELINED
+								driver_strategy : Map
+								Partitioning : RANDOM_PARTITIONED
+
+								 : Data Sink
+									content : TextOutputFormat () - UTF-8
+									ship_strategy : Forward
+									exchange_mode : PIPELINED
+									Partitioning : RANDOM_PARTITIONED
+
