diff --git a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java
index b2badf9765843..a56088b17d5fe 100644
--- a/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java
+++ b/flink-connectors/flink-connector-hive/src/test/java/org/apache/flink/connectors/hive/HiveTableSourceITCase.java
@@ -53,7 +53,7 @@
 import org.apache.flink.table.planner.runtime.utils.TestingAppendSink;
 import org.apache.flink.table.planner.utils.JavaScalaConversionUtil;
 import org.apache.flink.table.planner.utils.TableTestUtil;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.test.util.TestBaseUtils;
 import org.apache.flink.types.Row;
 
@@ -495,7 +495,7 @@ public void testStreamPartitionRead() throws Exception {
 
 		Table src = tEnv.from("hive.source_db.stream_test");
 
-		TestingAppendRowDataSink sink = new TestingAppendRowDataSink(new RowDataTypeInfo(
+		TestingAppendRowDataSink sink = new TestingAppendRowDataSink(InternalTypeInfo.ofFields(
 				DataTypes.INT().getLogicalType(),
 				DataTypes.STRING().getLogicalType(),
 				DataTypes.STRING().getLogicalType()));
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
index 24cf8c606910e..3f93b233bfc1e 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
@@ -25,7 +25,7 @@
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
@@ -76,7 +76,7 @@ public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
 			.map(DataType::getLogicalType)
 			.toArray(LogicalType[]::new),
 		fieldNames);
-	private static RowDataTypeInfo rowDataTypeInfo = RowDataTypeInfo.of(rowType);
+	private static InternalTypeInfo<RowData> rowDataTypeInfo = InternalTypeInfo.of(rowType);
 
 	@After
 	public void tearDown() throws Exception {
diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroFormatFactoryTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroFormatFactoryTest.java
index 2ad4d7d526bbc..e91c81ba9c085 100644
--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroFormatFactoryTest.java
+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroFormatFactoryTest.java
@@ -31,7 +31,7 @@
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.factories.TestDynamicTableFactory;
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.TestLogger;
 
@@ -58,7 +58,7 @@ public class AvroFormatFactoryTest extends TestLogger {
 	@Test
 	public void testSeDeSchema() {
 		final AvroRowDataDeserializationSchema expectedDeser =
-				new AvroRowDataDeserializationSchema(ROW_TYPE, WrapperTypeInfo.of(ROW_TYPE));
+				new AvroRowDataDeserializationSchema(ROW_TYPE, InternalTypeInfo.of(ROW_TYPE));
 
 		final Map<String, String> options = getAllOptions();
 
diff --git a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
index ae84768981dd1..89fcceb8f581f 100644
--- a/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
+++ b/flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/AvroRowDataDeSerializationSchemaTest.java
@@ -23,7 +23,7 @@
 import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
 
@@ -96,7 +96,7 @@ public void testSerializeDeserialize() throws Exception {
 			FIELD("map2map", MAP(STRING(), MAP(STRING(), INT()))),
 			FIELD("map2array", MAP(STRING(), ARRAY(INT()))));
 		final RowType rowType = (RowType) dataType.getLogicalType();
-		final TypeInformation<RowData> typeInfo = WrapperTypeInfo.of(rowType);
+		final TypeInformation<RowData> typeInfo = InternalTypeInfo.of(rowType);
 
 		final Schema schema = AvroSchemaConverter.convertToSchema(rowType);
 		final GenericRecord record = new GenericData.Record(schema);
@@ -180,7 +180,7 @@ public void testSpecificType() throws Exception {
 				FIELD("type_date", DATE()),
 				FIELD("type_time_millis", TIME(3)));
 		final RowType rowType = (RowType) dataType.getLogicalType();
-		final TypeInformation<RowData> typeInfo = WrapperTypeInfo.of(rowType);
+		final TypeInformation<RowData> typeInfo = InternalTypeInfo.of(rowType);
 		AvroRowDataSerializationSchema serializationSchema = new AvroRowDataSerializationSchema(rowType);
 		serializationSchema.open(null);
 		AvroRowDataDeserializationSchema deserializationSchema =
diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFormatFactoryTest.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFormatFactoryTest.java
index 4b5820e156dc7..f3708252a18eb 100644
--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFormatFactoryTest.java
+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvFormatFactoryTest.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.factories.TestDynamicTableFactory;
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.TestLogger;
 
@@ -65,7 +65,7 @@ public class CsvFormatFactoryTest extends TestLogger {
 	@Test
 	public void testSeDeSchema() {
 		final CsvRowDataDeserializationSchema expectedDeser =
-				new CsvRowDataDeserializationSchema.Builder(ROW_TYPE, WrapperTypeInfo.of(ROW_TYPE))
+				new CsvRowDataDeserializationSchema.Builder(ROW_TYPE, InternalTypeInfo.of(ROW_TYPE))
 						.setFieldDelimiter(';')
 						.setQuoteCharacter('\'')
 						.setAllowComments(true)
diff --git a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataSerDeSchemaTest.java b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataSerDeSchemaTest.java
index 9de707361180b..0522be658ac45 100644
--- a/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataSerDeSchemaTest.java
+++ b/flink-formats/flink-csv/src/test/java/org/apache/flink/formats/csv/CsvRowDataSerDeSchemaTest.java
@@ -22,7 +22,7 @@
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
@@ -276,7 +276,7 @@ private void testField(
 
 		// deserialization
 		CsvRowDataDeserializationSchema.Builder deserSchemaBuilder =
-			new CsvRowDataDeserializationSchema.Builder(rowType, WrapperTypeInfo.of(rowType));
+			new CsvRowDataDeserializationSchema.Builder(rowType, InternalTypeInfo.of(rowType));
 		deserializationConfig.accept(deserSchemaBuilder);
 		RowData deserializedRow = deserialize(deserSchemaBuilder, expectedCsv);
 
@@ -304,7 +304,7 @@ private void testField(
 
 		// deserialization
 		CsvRowDataDeserializationSchema.Builder deserSchemaBuilder =
-			new CsvRowDataDeserializationSchema.Builder(rowType, WrapperTypeInfo.of(rowType));
+			new CsvRowDataDeserializationSchema.Builder(rowType, InternalTypeInfo.of(rowType));
 		deserializationConfig.accept(deserSchemaBuilder);
 		RowData deserializedRow = deserialize(deserSchemaBuilder, csv);
 		Row actualRow = (Row) DataFormatConverters.getConverterForDataType(dataType)
@@ -323,7 +323,7 @@ private Row testDeserialization(
 			FIELD("f2", STRING()));
 		RowType rowType = (RowType) dataType.getLogicalType();
 		CsvRowDataDeserializationSchema.Builder deserSchemaBuilder =
-			new CsvRowDataDeserializationSchema.Builder(rowType, WrapperTypeInfo.of(rowType))
+			new CsvRowDataDeserializationSchema.Builder(rowType, InternalTypeInfo.of(rowType))
 				.setIgnoreParseErrors(allowParsingErrors)
 				.setAllowComments(allowComments);
 		RowData deserializedRow = deserialize(deserSchemaBuilder, string);
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFormatFactoryTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFormatFactoryTest.java
index d1c26be52cb84..5c07479334c7c 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFormatFactoryTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonFormatFactoryTest.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.factories.TestDynamicTableFactory;
 import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext;
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.TestLogger;
 
@@ -118,7 +118,7 @@ private void testSchemaDeserializationSchema(Map<String, String> options) {
 		final JsonRowDataDeserializationSchema expectedDeser =
 				new JsonRowDataDeserializationSchema(
 						ROW_TYPE,
-						WrapperTypeInfo.of(ROW_TYPE),
+						InternalTypeInfo.of(ROW_TYPE),
 						false,
 						true,
 						TimestampFormat.ISO_8601);
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonRowDataSerDeSchemaTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonRowDataSerDeSchemaTest.java
index 7b561aaf0c426..58dc120b02845 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonRowDataSerDeSchemaTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/JsonRowDataSerDeSchemaTest.java
@@ -21,7 +21,7 @@
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
@@ -136,7 +136,7 @@ public void testSerDe() throws Exception {
 			FIELD("map", MAP(STRING(), BIGINT())),
 			FIELD("map2map", MAP(STRING(), MAP(STRING(), INT()))));
 		RowType schema = (RowType) dataType.getLogicalType();
-		TypeInformation<RowData> resultTypeInfo = WrapperTypeInfo.of(schema);
+		TypeInformation<RowData> resultTypeInfo = InternalTypeInfo.of(schema);
 
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
 			schema, resultTypeInfo, false, false, TimestampFormat.ISO_8601);
@@ -207,7 +207,7 @@ public void testSlowDeserialization() throws Exception {
 		RowType rowType = (RowType) dataType.getLogicalType();
 
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
-			rowType, WrapperTypeInfo.of(rowType), false, false,  TimestampFormat.ISO_8601);
+			rowType, InternalTypeInfo.of(rowType), false, false,  TimestampFormat.ISO_8601);
 
 		Row expected = new Row(7);
 		expected.setField(0, bool);
@@ -232,7 +232,7 @@ public void testSerDeMultiRows() throws Exception {
 		).getLogicalType();
 
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
-			rowType, WrapperTypeInfo.of(rowType), false, false,  TimestampFormat.ISO_8601);
+			rowType, InternalTypeInfo.of(rowType), false, false,  TimestampFormat.ISO_8601);
 		JsonRowDataSerializationSchema serializationSchema = new JsonRowDataSerializationSchema(rowType, TimestampFormat.ISO_8601);
 
 		ObjectMapper objectMapper = new ObjectMapper();
@@ -286,7 +286,7 @@ public void testSerDeMultiRowsWithNullValues() throws Exception {
 		).getLogicalType();
 
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
-			rowType, WrapperTypeInfo.of(rowType), false, true, TimestampFormat.ISO_8601);
+			rowType, InternalTypeInfo.of(rowType), false, true, TimestampFormat.ISO_8601);
 		JsonRowDataSerializationSchema serializationSchema = new JsonRowDataSerializationSchema(rowType, TimestampFormat.ISO_8601);
 
 		for (int i = 0; i < jsons.length; i++) {
@@ -311,7 +311,7 @@ public void testDeserializationMissingNode() throws Exception {
 
 		// pass on missing field
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
-			schema, WrapperTypeInfo.of(schema), false, false, TimestampFormat.ISO_8601);
+			schema, InternalTypeInfo.of(schema), false, false, TimestampFormat.ISO_8601);
 
 		Row expected = new Row(1);
 		Row actual = convertToExternal(deserializationSchema.deserialize(serializedJson), dataType);
@@ -319,7 +319,7 @@ public void testDeserializationMissingNode() throws Exception {
 
 		// fail on missing field
 		deserializationSchema = deserializationSchema = new JsonRowDataDeserializationSchema(
-			schema, WrapperTypeInfo.of(schema), true, false, TimestampFormat.ISO_8601);
+			schema, InternalTypeInfo.of(schema), true, false, TimestampFormat.ISO_8601);
 
 		String errorMessage = "Failed to deserialize JSON '{\"id\":123123123}'.";
 		try {
@@ -331,7 +331,7 @@ public void testDeserializationMissingNode() throws Exception {
 
 		// ignore on parse error
 		deserializationSchema = new JsonRowDataDeserializationSchema(
-			schema, WrapperTypeInfo.of(schema), false, true, TimestampFormat.ISO_8601);
+			schema, InternalTypeInfo.of(schema), false, true, TimestampFormat.ISO_8601);
 		actual = convertToExternal(deserializationSchema.deserialize(serializedJson), dataType);
 		assertEquals(expected, actual);
 
@@ -339,7 +339,7 @@ public void testDeserializationMissingNode() throws Exception {
 		try {
 			// failOnMissingField and ignoreParseErrors both enabled
 			new JsonRowDataDeserializationSchema(
-				schema, WrapperTypeInfo.of(schema), true, true, TimestampFormat.ISO_8601);
+				schema, InternalTypeInfo.of(schema), true, true, TimestampFormat.ISO_8601);
 			Assert.fail("expecting exception message: " + errorMessage);
 		} catch (Throwable t) {
 			assertEquals(errorMessage, t.getMessage());
@@ -354,7 +354,7 @@ public void testSerDeSQLTimestampFormat() throws Exception{
 		).getLogicalType();
 
 		JsonRowDataDeserializationSchema deserializationSchema = new JsonRowDataDeserializationSchema(
-			rowType, WrapperTypeInfo.of(rowType), false, false, TimestampFormat.SQL);
+			rowType, InternalTypeInfo.of(rowType), false, false, TimestampFormat.SQL);
 		JsonRowDataSerializationSchema serializationSchema = new JsonRowDataSerializationSchema(rowType, TimestampFormat.SQL);
 
 		ObjectMapper objectMapper = new ObjectMapper();
@@ -381,7 +381,7 @@ public void testJsonParse() throws Exception {
 	private void testIgnoreParseErrors(TestSpec spec) throws Exception {
 		// the parsing field should be null and no exception is thrown
 		JsonRowDataDeserializationSchema ignoreErrorsSchema = new JsonRowDataDeserializationSchema(
-			spec.rowType, WrapperTypeInfo.of(spec.rowType), false, true,
+			spec.rowType, InternalTypeInfo.of(spec.rowType), false, true,
 			spec.timestampFormat);
 		Row expected;
 		if (spec.expected != null) {
@@ -399,7 +399,7 @@ private void testIgnoreParseErrors(TestSpec spec) throws Exception {
 	private void testParseErrors(TestSpec spec) throws Exception {
 		// expect exception if parse error is not ignored
 		JsonRowDataDeserializationSchema failingSchema = new JsonRowDataDeserializationSchema(
-			spec.rowType, WrapperTypeInfo.of(spec.rowType), false, false,
+			spec.rowType, InternalTypeInfo.of(spec.rowType), false, false,
 			spec.timestampFormat);
 
 		try {
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchemaTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchemaTest.java
index 1a4d222592d3d..b7135097c0d1e 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchemaTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonDeserializationSchemaTest.java
@@ -20,7 +20,7 @@
 
 import org.apache.flink.formats.json.TimestampFormat;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Collector;
 
@@ -65,7 +65,7 @@ public void testDeserialization() throws Exception {
 		List<String> lines = readLines("canal-data.txt");
 		CanalJsonDeserializationSchema deserializationSchema = new CanalJsonDeserializationSchema(
 			SCHEMA,
-			WrapperTypeInfo.of(SCHEMA),
+			InternalTypeInfo.of(SCHEMA),
 			false,
 			TimestampFormat.ISO_8601);
 
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonFormatFactoryTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonFormatFactoryTest.java
index 3b97114f72779..2d4761600aa94 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonFormatFactoryTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/canal/CanalJsonFormatFactoryTest.java
@@ -31,7 +31,7 @@
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.factories.TestDynamicTableFactory;
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.TestLogger;
 
@@ -65,7 +65,7 @@ public class CanalJsonFormatFactoryTest extends TestLogger {
 	public void testSeDeSchema() {
 		final CanalJsonDeserializationSchema expectedDeser = new CanalJsonDeserializationSchema(
 			ROW_TYPE,
-			WrapperTypeInfo.of(ROW_TYPE),
+			InternalTypeInfo.of(ROW_TYPE),
 			true,
 			TimestampFormat.ISO_8601);
 
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonDeserializationSchemaTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonDeserializationSchemaTest.java
index 3bcb16c9d06fe..fa448888b7800 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonDeserializationSchemaTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonDeserializationSchemaTest.java
@@ -20,7 +20,7 @@
 
 import org.apache.flink.formats.json.TimestampFormat;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Collector;
 
@@ -74,7 +74,7 @@ private void testDeserialization(String resourceFile, boolean schemaInclude) thr
 		List<String> lines = readLines(resourceFile);
 		DebeziumJsonDeserializationSchema deserializationSchema = new DebeziumJsonDeserializationSchema(
 			SCHEMA,
-			WrapperTypeInfo.of(SCHEMA),
+			InternalTypeInfo.of(SCHEMA),
 			schemaInclude,
 			false,
 			TimestampFormat.ISO_8601);
diff --git a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFormatFactoryTest.java b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFormatFactoryTest.java
index 134b7fb13f7ea..71cb13d678050 100644
--- a/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFormatFactoryTest.java
+++ b/flink-formats/flink-json/src/test/java/org/apache/flink/formats/json/debezium/DebeziumJsonFormatFactoryTest.java
@@ -31,7 +31,7 @@
 import org.apache.flink.table.factories.FactoryUtil;
 import org.apache.flink.table.factories.TestDynamicTableFactory;
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.TestLogger;
 
@@ -65,7 +65,7 @@ public class DebeziumJsonFormatFactoryTest extends TestLogger {
 	public void testSeDeSchema() {
 		final DebeziumJsonDeserializationSchema expectedDeser = new DebeziumJsonDeserializationSchema(
 			ROW_TYPE,
-			WrapperTypeInfo.of(ROW_TYPE),
+			InternalTypeInfo.of(ROW_TYPE),
 			true,
 			true,
 			TimestampFormat.ISO_8601);
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperator.java
index 8faa623a20792..b169f194b2ea7 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperator.java
@@ -96,7 +96,7 @@ public void open() throws Exception {
 		reuseJoinedRow = new JoinedRowData();
 
 		udtfInputProjection = createUdtfInputProjection();
-		forwardedInputSerializer = new RowDataSerializer(this.getExecutionConfig(), inputType);
+		forwardedInputSerializer = new RowDataSerializer(inputType);
 		udtfInputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionInputType);
 		udtfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionOutputType);
 	}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/ArrayDataSerializer.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/ArrayDataSerializer.java
index ffa2adc7c44f4..d93fa975c3be8 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/ArrayDataSerializer.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/ArrayDataSerializer.java
@@ -52,7 +52,7 @@ public class ArrayDataSerializer extends org.apache.flink.table.runtime.typeutil
 	private final int elementSize;
 
 	public ArrayDataSerializer(LogicalType eleType, TypeSerializer elementTypeSerializer) {
-		super(eleType, null);
+		super(eleType);
 		this.elementType = eleType;
 		this.elementTypeSerializer = elementTypeSerializer;
 		this.elementSize = BinaryArrayData.calculateFixLengthPartSize(this.elementType);
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/MapDataSerializer.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/MapDataSerializer.java
index ef6f176d06e81..b1290ee30d7d3 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/MapDataSerializer.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/MapDataSerializer.java
@@ -62,7 +62,7 @@ public class MapDataSerializer extends org.apache.flink.table.runtime.typeutils.
 
 	public MapDataSerializer(LogicalType keyType, LogicalType valueType
 		, TypeSerializer keyTypeSerializer, TypeSerializer valueTypeSerializer) {
-		super(keyType, valueType, null);
+		super(keyType, valueType);
 		this.keyType = keyType;
 		this.valueType = valueType;
 		this.keyTypeSerializer = keyTypeSerializer;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowDataArrowReaderWriterTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowDataArrowReaderWriterTest.java
index d3cf692613a73..5cc72c6b29d5a 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowDataArrowReaderWriterTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/RowDataArrowReaderWriterTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.arrow;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.table.data.DecimalData;
 import org.apache.flink.table.data.GenericArrayData;
@@ -77,8 +76,7 @@ public RowDataArrowReaderWriterTest() {
 			.withCustomCheck(
 				(o1, o2) -> o1 instanceof RowData && o2 instanceof RowData,
 				(o1, o2, checker) -> {
-					RowDataSerializer serializer = new RowDataSerializer(
-						new ExecutionConfig(), fieldTypes.toArray(new LogicalType[0]));
+					RowDataSerializer serializer = new RowDataSerializer(fieldTypes.toArray(new LogicalType[0]));
 					return deepEqualsRowData(
 						(RowData) o1,
 						(RowData) o2,
@@ -169,8 +167,8 @@ public RowData[] getTestData() {
 		BinaryRowData row2 = StreamRecordUtils.binaryrow((byte) 1, (short) 2, 3, 4L, false, 1.0f, 1.0, "中文", "中文".getBytes(), DecimalData.fromUnscaledLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,
 			Tuple2.of(TimestampData.fromEpochMillis(3600000), 0), Tuple2.of(TimestampData.fromEpochMillis(3600000), 2), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 4), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 8),
 			Tuple2.of(TimestampData.fromEpochMillis(3600000), 0), Tuple2.of(TimestampData.fromEpochMillis(3600000), 2), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 4), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 8),
-			Tuple2.of(new GenericArrayData(new String[] {null, null, null}), new ArrayDataSerializer(new VarCharType(), null)),
-			Tuple2.of(GenericRowData.of(1, null, new GenericArrayData(new StringData[] {StringData.fromString("hello")}), null, GenericRowData.of(1, StringData.fromString("hello"))), new RowDataSerializer(new ExecutionConfig(), rowFieldType)));
+			Tuple2.of(new GenericArrayData(new String[] {null, null, null}), new ArrayDataSerializer(new VarCharType())),
+			Tuple2.of(GenericRowData.of(1, null, new GenericArrayData(new StringData[] {StringData.fromString("hello")}), null, GenericRowData.of(1, StringData.fromString("hello"))), new RowDataSerializer(rowFieldType)));
 		RowData row3 = StreamRecordUtils.row(null, (short) 2, 3, 4L, false, 1.0f, 1.0, "中文", "中文".getBytes(), DecimalData.fromUnscaledLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,
 			TimestampData.fromEpochMillis(3600000), TimestampData.fromEpochMillis(3600000), TimestampData.fromEpochMillis(3600000, 100000), TimestampData.fromEpochMillis(3600000, 100000),
 			TimestampData.fromEpochMillis(3600000), TimestampData.fromEpochMillis(3600000), TimestampData.fromEpochMillis(3600000, 100000), TimestampData.fromEpochMillis(3600000, 100000),
@@ -179,8 +177,8 @@ public RowData[] getTestData() {
 		BinaryRowData row4 = StreamRecordUtils.binaryrow((byte) 1, null, 3, 4L, true, 1.0f, 1.0, "hello", "hello".getBytes(), DecimalData.fromUnscaledLong(1, 10, 3), 100, 3600000, 3600000, 3600000, 3600000,
 			Tuple2.of(TimestampData.fromEpochMillis(3600000), 0), Tuple2.of(TimestampData.fromEpochMillis(3600000), 2), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 4), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 8),
 			Tuple2.of(TimestampData.fromEpochMillis(3600000), 0), Tuple2.of(TimestampData.fromEpochMillis(3600000), 2), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 4), Tuple2.of(TimestampData.fromEpochMillis(3600000, 100000), 8),
-			Tuple2.of(new GenericArrayData(new StringData[] {StringData.fromString("hello"), StringData.fromString("中文"), null}), new ArrayDataSerializer(new VarCharType(), null)),
-			Tuple2.of(GenericRowData.of(1, null, new GenericArrayData(new StringData[] {StringData.fromString("hello")}), null, null), new RowDataSerializer(new ExecutionConfig(), rowFieldType)));
+			Tuple2.of(new GenericArrayData(new StringData[] {StringData.fromString("hello"), StringData.fromString("中文"), null}), new ArrayDataSerializer(new VarCharType())),
+			Tuple2.of(GenericRowData.of(1, null, new GenericArrayData(new StringData[] {StringData.fromString("hello")}), null, null), new RowDataSerializer(rowFieldType)));
 		RowData row5 = StreamRecordUtils.row(new Object[fieldTypes.size()]);
 		BinaryRowData row6 = StreamRecordUtils.binaryrow(new Object[fieldTypes.size()]);
 		return new RowData[]{row1, row2, row3, row4, row5, row6};
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTest.java
index 83dd070b6a58b..81f29f5e75024 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/arrow/sources/ArrowSourceFunctionTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.arrow.sources;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
@@ -90,8 +89,7 @@ public static void init() {
 		}
 		rowType = new RowType(rowFields);
 		dataType = TypeConversions.fromLogicalToDataType(rowType);
-		serializer = new RowDataSerializer(
-			new ExecutionConfig(), fieldTypes.toArray(new LogicalType[0]));
+		serializer = new RowDataSerializer(fieldTypes.toArray(new LogicalType[0]));
 		allocator = ArrowUtils.getRootAllocator().newChildAllocator("stdout", 0, Long.MAX_VALUE);
 	}
 
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
index f67abd6efc515..9853a9694e1b7 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/RowDataPythonScalarFunctionOperatorTest.java
@@ -18,13 +18,11 @@
 
 package org.apache.flink.table.runtime.operators.python.scalar;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.python.PythonFunctionRunner;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.EnvironmentSettings;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.data.RowData;
@@ -33,6 +31,7 @@
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.utils.PythonTestUtils;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 
@@ -48,10 +47,10 @@
 public class RowDataPythonScalarFunctionOperatorTest
 		extends PythonScalarFunctionOperatorTestBase<RowData, RowData, RowData> {
 
-	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new TypeInformation[]{
-		Types.STRING,
-		Types.STRING,
-		Types.LONG
+	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new LogicalType[]{
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.BIGINT().getLogicalType()
 	});
 
 	@Override
@@ -98,7 +97,7 @@ public StreamTableEnvironment createTableEnvironment(StreamExecutionEnvironment
 	@Override
 	public TypeSerializer<RowData> getOutputTypeSerializer(RowType rowType) {
 		// If not specified, PojoSerializer will be used which doesn't work well with the Arrow data structure.
-		return new RowDataSerializer(new ExecutionConfig(), rowType);
+		return new RowDataSerializer(rowType);
 	}
 
 	private static class PassThroughPythonScalarFunctionOperator extends RowDataPythonScalarFunctionOperator {
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
index 05d30ffaff173..76a673564a1e4 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/arrow/RowDataArrowPythonScalarFunctionOperatorTest.java
@@ -18,13 +18,11 @@
 
 package org.apache.flink.table.runtime.operators.python.scalar.arrow;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.python.PythonFunctionRunner;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.EnvironmentSettings;
 import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
 import org.apache.flink.table.data.RowData;
@@ -35,6 +33,7 @@
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.runtime.utils.PassThroughPythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.utils.PythonTestUtils;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 
@@ -50,10 +49,10 @@
 public class RowDataArrowPythonScalarFunctionOperatorTest
 	extends PythonScalarFunctionOperatorTestBase<RowData, RowData, RowData> {
 
-	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new TypeInformation[]{
-		Types.STRING,
-		Types.STRING,
-		Types.LONG
+	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new LogicalType[]{
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.BIGINT().getLogicalType()
 	});
 
 	@Override
@@ -93,7 +92,7 @@ public StreamTableEnvironment createTableEnvironment(StreamExecutionEnvironment
 
 	@Override
 	public TypeSerializer<RowData> getOutputTypeSerializer(RowType rowType) {
-		return new RowDataSerializer(new ExecutionConfig(), rowType);
+		return new RowDataSerializer(rowType);
 	}
 
 	private static class PassThroughRowDataArrowPythonScalarFunctionOperator extends RowDataArrowPythonScalarFunctionOperator {
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperatorTest.java
index 5e1ec6d89c604..1f690ff8c400a 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/RowDataPythonTableFunctionOperatorTest.java
@@ -18,21 +18,20 @@
 
 package org.apache.flink.table.runtime.operators.python.table;
 
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.python.PythonFunctionRunner;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.runtime.utils.PassThroughPythonTableFunctionRunner;
 import org.apache.flink.table.runtime.utils.PythonTestUtils;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
 
 import org.apache.calcite.rel.core.JoinRelType;
 
-import java.io.IOException;
 import java.util.Collection;
 import java.util.HashMap;
 
@@ -44,11 +43,11 @@
 public class RowDataPythonTableFunctionOperatorTest
 	extends PythonTableFunctionOperatorTestBase<RowData, RowData, RowData> {
 
-	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new TypeInformation[]{
-		Types.STRING,
-		Types.STRING,
-		Types.LONG,
-		Types.LONG
+	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new LogicalType[]{
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.BIGINT().getLogicalType(),
+		DataTypes.BIGINT().getLogicalType()
 	});
 
 	@Override
@@ -92,7 +91,7 @@ private static class RowDataPassThroughPythonTableFunctionOperator extends RowDa
 		}
 
 		@Override
-		public PythonFunctionRunner createPythonFunctionRunner() throws IOException {
+		public PythonFunctionRunner createPythonFunctionRunner() {
 			return new PassThroughPythonTableFunctionRunner(
 				getRuntimeContext().getTaskName(),
 				PythonTestUtils.createTestEnvironmentManager(),
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/RowDataSerializerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/RowDataSerializerTest.java
index 4a33d6efcb5ce..3da985dccc6a4 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/RowDataSerializerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/RowDataSerializerTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.typeutils.serializers.python;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.SerializerTestBase;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.base.LongSerializer;
@@ -46,7 +45,7 @@ public RowDataSerializerTest() {
 							new BigIntType(),
 							new BigIntType()
 						};
-						RowDataSerializer serializer = new RowDataSerializer(new ExecutionConfig(), fieldTypes);
+						RowDataSerializer serializer = new RowDataSerializer(fieldTypes);
 						return deepEqualsRowData(
 							(RowData) o1,
 							(RowData) o2,
diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/typeutils/FieldInfoUtils.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/typeutils/FieldInfoUtils.java
index 85be1339c56fc..dae0e163c1d27 100644
--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/typeutils/FieldInfoUtils.java
+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/typeutils/FieldInfoUtils.java
@@ -24,6 +24,7 @@
 import org.apache.flink.api.java.typeutils.GenericTypeInfo;
 import org.apache.flink.api.java.typeutils.PojoTypeInfo;
 import org.apache.flink.api.java.typeutils.TupleTypeInfoBase;
+import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.Types;
@@ -37,9 +38,13 @@
 import org.apache.flink.table.functions.BuiltInFunctionDefinitions;
 import org.apache.flink.table.types.AtomicDataType;
 import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.DataTypeQueryable;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.TimestampKind;
 import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.utils.LogicalTypeChecks;
+import org.apache.flink.table.types.utils.TypeConversions;
 import org.apache.flink.types.Row;
 
 import javax.annotation.Nullable;
@@ -54,11 +59,14 @@
 import java.util.Set;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
+import java.util.stream.Stream;
 
 import static java.lang.String.format;
 import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.hasRoot;
+import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.isCompositeType;
 import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.isProctimeAttribute;
 import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.isRowtimeAttribute;
+import static org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo;
 import static org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;
 
 /**
@@ -71,7 +79,7 @@ public class FieldInfoUtils {
 	/**
 	 * Describes fields' names, indices and {@link DataType}s extracted from a {@link TypeInformation} and possibly
 	 * transformed via {@link Expression} application. It is in fact a mapping between {@link TypeInformation} of an
-	 * input and {@link TableSchema} of a {@link org.apache.flink.table.api.Table} that can be created out of it.
+	 * input and {@link TableSchema} of a {@link Table} that can be created out of it.
 	 *
 	 * @see FieldInfoUtils#getFieldsInfo(TypeInformation)
 	 * @see FieldInfoUtils#getFieldsInfo(TypeInformation, Expression[])
@@ -155,12 +163,12 @@ public TableSchema toTableSchema() {
 	 * used if the input type has a defined field order (tuple, case class, Row) and no of fields
 	 * references a field of the input type.
 	 */
-	private static boolean isReferenceByPosition(CompositeType<?> ct, Expression[] fields) {
-		if (!(ct instanceof TupleTypeInfoBase)) {
+	private static boolean isReferenceByPosition(TypeInformation<?> inputType, Expression[] fields) {
+		if (!isIndexedComposite(inputType)) {
 			return false;
 		}
 
-		List<String> inputNames = Arrays.asList(ct.getFieldNames());
+		List<String> inputNames = Arrays.asList(getFieldNames(inputType));
 
 		// Use the by-position mode if no of the fields exists in the input.
 		// This prevents confusing cases like ('f2, 'f0, 'myName) for a Tuple3 where fields are renamed
@@ -260,16 +268,31 @@ private static <A> List<FieldInfo> extractFieldInformation(
 			throw new ValidationException(
 				"An input of GenericTypeInfo<Row> cannot be converted to Table. " +
 					"Please specify the type of the input with a RowTypeInfo.");
-		} else if (inputType instanceof TupleTypeInfoBase) {
-			fieldInfos = extractFieldInfosFromTupleType((TupleTypeInfoBase<?>) inputType, exprs);
-		} else if (inputType instanceof PojoTypeInfo) {
-			fieldInfos = extractFieldInfosByNameReference((CompositeType<?>) inputType, exprs);
+		} else if (isIndexedComposite(inputType)) {
+			fieldInfos = extractFieldInfosFromIndexedCompositeType(inputType, exprs);
+		} else if (isNonIndexedComposite(inputType)) {
+			fieldInfos = extractFieldInfosByNameReference(inputType, exprs);
 		} else {
 			fieldInfos = extractFieldInfoFromAtomicType(inputType, exprs);
 		}
 		return fieldInfos;
 	}
 
+	private static boolean isIndexedComposite(TypeInformation<?> inputType) {
+		// type originated from Table API
+		if (inputType instanceof DataTypeQueryable) {
+			final DataType dataType = ((DataTypeQueryable) inputType).getDataType();
+			final LogicalType type = dataType.getLogicalType();
+			return isCompositeType(type); // every composite in Table API is indexed
+		}
+		// type originated from other API
+		return inputType instanceof TupleTypeInfoBase;
+	}
+
+	private static boolean isNonIndexedComposite(TypeInformation<?> inputType) {
+		return inputType instanceof PojoTypeInfo;
+	}
+
 	private static void validateAtMostOneProctimeAttribute(List<FieldInfo> fieldInfos) {
 		List<FieldInfo> proctimeAttributes = fieldInfos.stream()
 			.filter(FieldInfoUtils::isProctimeField)
@@ -318,23 +341,35 @@ public static <A> String[] getFieldNames(TypeInformation<A> inputType) {
 	public static <A> String[] getFieldNames(TypeInformation<A> inputType, List<String> existingNames) {
 		validateInputTypeInfo(inputType);
 
-		String[] fieldNames;
-		if (inputType instanceof CompositeType) {
-			fieldNames = ((CompositeType<A>) inputType).getFieldNames();
-		} else {
+		List<String> fieldNames = null;
+		// type originated from Table API
+		if (inputType instanceof DataTypeQueryable) {
+			final DataType dataType = ((DataTypeQueryable) inputType).getDataType();
+			final LogicalType type = dataType.getLogicalType();
+			if (isCompositeType(type)) {
+				fieldNames = LogicalTypeChecks.getFieldNames(type);
+			}
+		}
+		// type originated from other API
+		else if (inputType instanceof CompositeType) {
+			fieldNames = Arrays.asList(((CompositeType<A>) inputType).getFieldNames());
+		}
+
+		// atomic in any case
+		if (fieldNames == null) {
 			int i = 0;
 			String fieldName = ATOMIC_FIELD_NAME;
 			while ((null != existingNames) && existingNames.contains(fieldName)) {
 				fieldName = ATOMIC_FIELD_NAME + "_" + i++;
 			}
-			fieldNames = new String[]{fieldName};
+			fieldNames = Collections.singletonList(fieldName);
 		}
 
-		if (Arrays.asList(fieldNames).contains("*")) {
+		if (fieldNames.contains("*")) {
 			throw new TableException("Field name can not be '*'.");
 		}
 
-		return fieldNames;
+		return fieldNames.toArray(new String[0]);
 	}
 
 	/**
@@ -387,6 +422,23 @@ public static TypeInformation<?>[] getFieldTypes(TypeInformation<?> inputType) {
 
 	/* Utility methods */
 
+	private static DataType[] getFieldDataTypes(TypeInformation<?> inputType) {
+		validateInputTypeInfo(inputType);
+
+		// type originated from Table API
+		if (inputType instanceof DataTypeQueryable) {
+			final DataType dataType = ((DataTypeQueryable) inputType).getDataType();
+			return dataType.getChildren().toArray(new DataType[0]);
+		}
+		// type originated from other API
+		else {
+			final TypeInformation<?>[] fieldTypes = getFieldTypes(inputType);
+			return Stream.of(fieldTypes)
+				.map(TypeConversions::fromLegacyInfoToDataType)
+				.toArray(DataType[]::new);
+		}
+	}
+
 	private static List<FieldInfo> extractFieldInfoFromAtomicType(TypeInformation<?> atomicType, Expression[] exprs) {
 		List<FieldInfo> fields = new ArrayList<>(exprs.length);
 		boolean alreadyReferenced = false;
@@ -413,7 +465,9 @@ private static List<FieldInfo> extractFieldInfoFromAtomicType(TypeInformation<?>
 		return fields;
 	}
 
-	private static List<FieldInfo> extractFieldInfosFromTupleType(TupleTypeInfoBase<?> inputType, Expression[] exprs) {
+	private static List<FieldInfo> extractFieldInfosFromIndexedCompositeType(
+			TypeInformation<?> inputType,
+			Expression[] exprs) {
 		boolean isRefByPos = isReferenceByPosition(inputType, exprs);
 
 		if (isRefByPos) {
@@ -425,7 +479,9 @@ private static List<FieldInfo> extractFieldInfosFromTupleType(TupleTypeInfoBase<
 		}
 	}
 
-	private static List<FieldInfo> extractFieldInfosByNameReference(CompositeType<?> inputType, Expression[] exprs) {
+	private static List<FieldInfo> extractFieldInfosByNameReference(
+			TypeInformation<?> inputType,
+			Expression[] exprs) {
 		ExprToFieldInfo exprToFieldInfo = new ExprToFieldInfo(inputType);
 		return Arrays.stream(exprs)
 			.map(expr -> expr.accept(exprToFieldInfo))
@@ -458,18 +514,20 @@ public DataType getType() {
 
 	private static class IndexedExprToFieldInfo extends ApiExpressionDefaultVisitor<FieldInfo> {
 
-		private final CompositeType<?> inputType;
+		private final String[] fieldNames;
+		private final DataType[] fieldDataTypes;
 		private final int index;
 
-		private IndexedExprToFieldInfo(CompositeType<?> inputType, int index) {
-			this.inputType = inputType;
+		private IndexedExprToFieldInfo(TypeInformation<?> inputType, int index) {
+			this.fieldNames = getFieldNames(inputType);
+			this.fieldDataTypes = getFieldDataTypes(inputType);
 			this.index = index;
 		}
 
 		@Override
 		public FieldInfo visit(UnresolvedReferenceExpression unresolvedReference) {
 			String fieldName = unresolvedReference.getName();
-			return new FieldInfo(fieldName, index, fromLegacyInfoToDataType(getTypeAt(unresolvedReference)));
+			return new FieldInfo(fieldName, index, getTypeAt(unresolvedReference));
 		}
 
 		@Override
@@ -502,29 +560,29 @@ private FieldInfo visitAlias(UnresolvedCallExpression unresolvedCall) {
 		}
 
 		private void validateRowtimeReplacesCompatibleType(UnresolvedCallExpression unresolvedCall) {
-			if (index < inputType.getArity()) {
-				checkRowtimeType(getTypeAt(unresolvedCall));
+			if (index < fieldDataTypes.length) {
+				checkRowtimeType(fromDataTypeToLegacyInfo(getTypeAt(unresolvedCall)));
 			}
 		}
 
 		private void validateProcTimeAttributeAppended(UnresolvedCallExpression unresolvedCall) {
-			if (index < inputType.getArity()) {
+			if (index < fieldDataTypes.length) {
 				throw new ValidationException(String.format("The proctime attribute can only be appended to the" +
 					" table schema and not replace an existing field. Please move '%s' to the end of the" +
 					" schema.", unresolvedCall));
 			}
 		}
 
-		private TypeInformation<Object> getTypeAt(Expression expr) {
-			if (index >= inputType.getArity()) {
+		private DataType getTypeAt(Expression expr) {
+			if (index >= fieldDataTypes.length) {
 				throw new ValidationException(String.format(
 					"Number of expressions does not match number of input fields.\n" +
 						"Available fields: %s\n" +
 						"Could not map: %s",
-					Arrays.toString(inputType.getFieldNames()),
+					Arrays.toString(fieldNames),
 					expr));
 			}
-			return inputType.getTypeAt(index);
+			return fieldDataTypes[index];
 		}
 
 		@Override
@@ -535,18 +593,20 @@ protected FieldInfo defaultMethod(Expression expression) {
 
 	private static class ExprToFieldInfo extends ApiExpressionDefaultVisitor<FieldInfo> {
 
-		private final CompositeType ct;
+		private final TypeInformation<?> inputType;
+		private final DataType[] fieldDataTypes;
 
-		private ExprToFieldInfo(CompositeType ct) {
-			this.ct = ct;
+		private ExprToFieldInfo(TypeInformation<?> inputType) {
+			this.inputType = inputType;
+			this.fieldDataTypes = getFieldDataTypes(inputType);
 		}
 
 		private ValidationException fieldNotFound(String name) {
 			return new ValidationException(format(
 				"%s is not a field of type %s. Expected: %s}",
 				name,
-				ct,
-				String.join(", ", ct.getFieldNames())));
+				inputType,
+				String.join(", ", getFieldNames(inputType))));
 		}
 
 		@Override
@@ -585,11 +645,11 @@ private FieldInfo visitAlias(UnresolvedCallExpression unresolvedCall) {
 
 		private FieldInfo createFieldInfo(UnresolvedReferenceExpression unresolvedReference, @Nullable String alias) {
 			String fieldName = unresolvedReference.getName();
-			return referenceByName(fieldName, ct)
+			return referenceByName(fieldName, inputType)
 				.map(idx -> new FieldInfo(
 					alias != null ? alias : fieldName,
 					idx,
-					fromLegacyInfoToDataType(ct.getTypeAt(idx))))
+					fieldDataTypes[idx]))
 				.orElseThrow(() -> fieldNotFound(fieldName));
 		}
 
@@ -602,7 +662,7 @@ private FieldInfo createProctimeFieldInfo(Expression expression, @Nullable Strin
 		}
 
 		private void validateProctimeDoesNotReplaceField(String originalName) {
-			if (referenceByName(originalName, ct).isPresent()) {
+			if (referenceByName(originalName, inputType).isPresent()) {
 				throw new ValidationException(String.format(
 					"The proctime attribute '%s' must not replace an existing field.",
 					originalName));
@@ -618,9 +678,9 @@ private FieldInfo createRowtimeFieldInfo(Expression expression, @Nullable String
 		}
 
 		private void verifyReferencesValidField(String origName, @Nullable String alias) {
-			Optional<Integer> refId = referenceByName(origName, ct);
+			Optional<Integer> refId = referenceByName(origName, inputType);
 			if (refId.isPresent()) {
-				checkRowtimeType(ct.getTypeAt(refId.get()));
+				checkRowtimeType(fromDataTypeToLegacyInfo(fieldDataTypes[refId.get()]));
 			} else if (alias != null) {
 				throw new ValidationException(String.format("Alias '%s' must reference an existing field.", alias));
 			}
@@ -667,8 +727,9 @@ private static boolean isProcTimeExpression(Expression origExpr) {
 			((UnresolvedCallExpression) origExpr).getFunctionDefinition() == BuiltInFunctionDefinitions.PROCTIME;
 	}
 
-	private static Optional<Integer> referenceByName(String name, CompositeType<?> ct) {
-		int inputIdx = ct.getFieldIndex(name);
+	private static Optional<Integer> referenceByName(String name, TypeInformation<?> inputType) {
+		final String[] fieldNames = getFieldNames(inputType);
+		int inputIdx = Arrays.asList(fieldNames).indexOf(name);
 		if (inputIdx < 0) {
 			return Optional.empty();
 		} else {
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/DataTypeQueryable.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/DataTypeQueryable.java
new file mode 100644
index 0000000000000..effc08ca90cb8
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/DataTypeQueryable.java
@@ -0,0 +1,30 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.types;
+
+import org.apache.flink.annotation.Internal;
+
+/**
+ * Indicates that a {@link DataType} can be retrieved from a class that implements this interfaces.
+ */
+@Internal
+public interface DataTypeQueryable {
+
+	DataType getDataType();
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/LegacyTypeInfoDataTypeConverter.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/LegacyTypeInfoDataTypeConverter.java
index f78a25675f1c8..8487ecb25e29a 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/LegacyTypeInfoDataTypeConverter.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/LegacyTypeInfoDataTypeConverter.java
@@ -30,9 +30,11 @@
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.api.TableException;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.AtomicDataType;
 import org.apache.flink.table.types.CollectionDataType;
 import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.DataTypeQueryable;
 import org.apache.flink.table.types.FieldsDataType;
 import org.apache.flink.table.types.KeyValueDataType;
 import org.apache.flink.table.types.logical.LegacyTypeInformationType;
@@ -176,7 +178,7 @@ else if (typeInfo instanceof MapTypeInfo) {
 			return convertToMapType((MapTypeInfo) typeInfo);
 		}
 
-		else if (typeInfo instanceof CompositeType) {
+		else if (typeInfo instanceof CompositeType || isRowData(typeInfo)) {
 			return createLegacyType(LogicalTypeRoot.STRUCTURED_TYPE, typeInfo);
 		}
 
@@ -396,6 +398,18 @@ private static TypeInformation<?> convertToRawTypeInfo(DataType dataType) {
 		return ((TypeInformationRawType) dataType.getLogicalType()).getTypeInformation();
 	}
 
+	/**
+	 * Temporary solution to enable tests with type information and internal data structures until we
+	 * drop all legacy types.
+	 */
+	private static boolean isRowData(TypeInformation<?> typeInfo) {
+		if (!(typeInfo instanceof DataTypeQueryable)) {
+			return false;
+		}
+		final DataType dataType = ((DataTypeQueryable) typeInfo).getDataType();
+		return dataType.getConversionClass() == RowData.class;
+	}
+
 	private LegacyTypeInfoDataTypeConverter() {
 		// no instantiation
 	}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/InternalTypeInfo.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/InternalTypeInfo.java
deleted file mode 100644
index a08e6d3da2f0b..0000000000000
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/typeutils/InternalTypeInfo.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.typeutils;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.AtomicType;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeutils.TypeComparator;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-
-import java.util.Objects;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * Type information for internal types of the Table API that are for translation purposes only
- * and should not be contained in final plan.
- */
-@Internal
-public abstract class InternalTypeInfo<T> extends TypeInformation<T> implements AtomicType<T> {
-
-	private static final long serialVersionUID = -13064574364925255L;
-
-	public final Class<T> clazz;
-
-	public InternalTypeInfo(Class<T> clazz) {
-		this.clazz = checkNotNull(clazz);
-	}
-
-	@Override
-	public boolean isBasicType() {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public boolean isTupleType() {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public int getArity() {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public int getTotalFields() {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public Class<T> getTypeClass() {
-		return clazz;
-	}
-
-	@Override
-	public boolean isKeyType() {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public TypeSerializer<T> createSerializer(ExecutionConfig config) {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	@Override
-	public TypeComparator<T> createComparator(
-			boolean sortOrderAscending,
-			ExecutionConfig executionConfig) {
-		throw new UnsupportedOperationException("This type is for internal use only.");
-	}
-
-	// ----------------------------------------------------------------------------------------------
-
-	@Override
-	public int hashCode() {
-		return Objects.hash(clazz);
-	}
-
-	public abstract boolean canEqual(Object obj);
-
-	@Override
-	public boolean equals(Object obj) {
-		if (obj instanceof InternalTypeInfo) {
-			InternalTypeInfo other = (InternalTypeInfo) obj;
-			return other.canEqual(this) && this.clazz.equals(other.clazz);
-		} else {
-			return false;
-		}
-	}
-
-	@Override
-	public String toString() {
-		return getClass().getSimpleName();
-	}
-}
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueAggFunction.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueAggFunction.java
index 496dd1331bf62..b311e984c3feb 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueAggFunction.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueAggFunction.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.data.binary.BinaryStringData;
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -90,7 +90,7 @@ public TypeInformation<GenericRowData> getAccumulatorType() {
 				"time"
 		};
 
-		return (TypeInformation) new RowDataTypeInfo(fieldTypes, fieldNames);
+		return (TypeInformation) InternalTypeInfo.ofFields(fieldTypes, fieldNames);
 	}
 
 	/**
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueWithRetractAggFunction.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueWithRetractAggFunction.java
index a6fd94e98e183..ee8547b496dfc 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueWithRetractAggFunction.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/FirstValueWithRetractAggFunction.java
@@ -42,7 +42,7 @@
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.runtime.typeutils.DecimalDataSerializer;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataSerializer;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.types.logical.BigIntType;
@@ -201,7 +201,7 @@ public TypeInformation<GenericRowData> getAccumulatorType() {
 				"orderToValueMapView"
 		};
 
-		return (TypeInformation) new RowDataTypeInfo(fieldTypes, fieldNames);
+		return (TypeInformation) InternalTypeInfo.ofFields(fieldTypes, fieldNames);
 	}
 
 	@SuppressWarnings("unchecked")
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunction.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunction.java
index 3bfe5e85bd01d..95004016b23b9 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunction.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueAggFunction.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.data.binary.BinaryStringData;
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.LogicalType;
@@ -89,7 +89,7 @@ public TypeInformation<GenericRowData> getAccumulatorType() {
 				"time"
 		};
 
-		return (TypeInformation) new RowDataTypeInfo(fieldTypes, fieldNames);
+		return (TypeInformation) InternalTypeInfo.ofFields(fieldTypes, fieldNames);
 	}
 
 	/**
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueWithRetractAggFunction.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueWithRetractAggFunction.java
index 48bd199415bd0..cdaabde53c23f 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueWithRetractAggFunction.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/aggfunctions/LastValueWithRetractAggFunction.java
@@ -42,7 +42,7 @@
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.runtime.typeutils.DecimalDataSerializer;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataSerializer;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.types.logical.BigIntType;
@@ -202,7 +202,7 @@ public TypeInformation<GenericRowData> getAccumulatorType() {
 				"orderToValueMapView"
 		};
 
-		return (TypeInformation) new RowDataTypeInfo(fieldTypes, fieldNames);
+		return (TypeInformation) InternalTypeInfo.ofFields(fieldTypes, fieldNames);
 	}
 
 	@SuppressWarnings("unchecked")
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/utils/KeySelectorUtil.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/utils/KeySelectorUtil.java
index 509d6769089d4..ac4d1458364d6 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/utils/KeySelectorUtil.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/utils/KeySelectorUtil.java
@@ -19,13 +19,14 @@
 package org.apache.flink.table.planner.plan.utils;
 
 import org.apache.flink.table.api.TableConfig;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.planner.codegen.CodeGeneratorContext;
 import org.apache.flink.table.planner.codegen.ProjectionCodeGenerator;
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.keyselector.BinaryRowDataKeySelector;
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 
@@ -35,15 +36,15 @@
 public class KeySelectorUtil {
 
 	/**
-	 * Create a RowDataKeySelector to extract keys from DataStream which type is RowDataTypeInfo.
+	 * Create a RowDataKeySelector to extract keys from DataStream which type is {@link InternalTypeInfo} of {@link RowData}.
 	 *
 	 * @param keyFields key fields
 	 * @param rowType type of DataStream to extract keys
-	 * @return the RowDataKeySelector to extract keys from DataStream which type is RowDataTypeInfo.
+	 * @return the RowDataKeySelector to extract keys from DataStream which type is {@link InternalTypeInfo} of {@link RowData}.
 	 */
-	public static RowDataKeySelector getRowDataSelector(int[] keyFields, RowDataTypeInfo rowType) {
+	public static RowDataKeySelector getRowDataSelector(int[] keyFields, InternalTypeInfo<RowData> rowType) {
 		if (keyFields.length > 0) {
-			LogicalType[] inputFieldTypes = rowType.getLogicalTypes();
+			LogicalType[] inputFieldTypes = rowType.toRowFieldTypes();
 			LogicalType[] keyFieldTypes = new LogicalType[keyFields.length];
 			for (int i = 0; i < keyFields.length; ++i) {
 				keyFieldTypes[i] = inputFieldTypes[keyFields[i]];
@@ -58,7 +59,7 @@ public static RowDataKeySelector getRowDataSelector(int[] keyFields, RowDataType
 				inputType,
 				returnType,
 				keyFields);
-			RowDataTypeInfo keyRowType = RowDataTypeInfo.of(returnType);
+			InternalTypeInfo<RowData> keyRowType = InternalTypeInfo.of(returnType);
 			return new BinaryRowDataKeySelector(keyRowType, generatedProjection);
 		} else {
 			return EmptyRowDataKeySelector.INSTANCE;
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java
index d7020cd0cb524..8ca36cad1b85c 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/BatchSelectTableSink.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.planner.sinks;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.data.RowData;
@@ -32,12 +31,12 @@
 public class BatchSelectTableSink extends SelectTableSinkBase<RowData> implements StreamTableSink<RowData> {
 
 	public BatchSelectTableSink(TableSchema tableSchema) {
-		super(tableSchema, createRowDataTypeInfo(tableSchema).createSerializer(new ExecutionConfig()));
+		super(tableSchema, createTypeInfo(tableSchema).toRowSerializer());
 	}
 
 	@Override
 	public TypeInformation<RowData> getOutputType() {
-		return createRowDataTypeInfo(getTableSchema());
+		return createTypeInfo(getTableSchema());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/SelectTableSinkBase.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/SelectTableSinkBase.java
index aa3b51210de12..af33a25a028e8 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/SelectTableSinkBase.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/SelectTableSinkBase.java
@@ -31,16 +31,13 @@
 import org.apache.flink.table.api.internal.SelectResultProvider;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.DataFormatConverters;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.sinks.StreamTableSink;
 import org.apache.flink.table.sinks.TableSink;
-import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.Row;
 import org.apache.flink.util.CloseableIterator;
 
 import java.util.UUID;
-import java.util.stream.Stream;
 
 /**
  * Basic implementation of {@link StreamTableSink} for select job to collect the result to local.
@@ -124,10 +121,9 @@ public void close() throws Exception {
 	protected abstract Row convertToRow(T element);
 
 	/**
-	 * Create RowDataTypeInfo based on given table schema.
+	 * Create {@link InternalTypeInfo} of {@link RowData} based on given table schema.
 	 */
-	protected static RowDataTypeInfo createRowDataTypeInfo(TableSchema tableSchema) {
-		return new RowDataTypeInfo(
-				Stream.of(tableSchema.getFieldDataTypes()).map(DataType::getLogicalType).toArray(LogicalType[]::new));
+	protected static InternalTypeInfo<RowData> createTypeInfo(TableSchema tableSchema) {
+		return InternalTypeInfo.of(tableSchema.toRowDataType().getLogicalType());
 	}
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/StreamSelectTableSink.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/StreamSelectTableSink.java
index be2b09d30727a..7345feef2a526 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/StreamSelectTableSink.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/sinks/StreamSelectTableSink.java
@@ -43,12 +43,12 @@
 	public StreamSelectTableSink(TableSchema tableSchema) {
 		super(tableSchema, new TupleTypeInfo<Tuple2<Boolean, RowData>>(
 				Types.BOOLEAN,
-				createRowDataTypeInfo(tableSchema)).createSerializer(new ExecutionConfig()));
+				createTypeInfo(tableSchema)).createSerializer(new ExecutionConfig()));
 	}
 
 	@Override
 	public TypeInformation<RowData> getRecordType() {
-		return createRowDataTypeInfo(getTableSchema());
+		return createTypeInfo(getTableSchema());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CalcCodeGenerator.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CalcCodeGenerator.scala
index 2d6b18dda0ca6..87ec9b77ad8a4 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CalcCodeGenerator.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CalcCodeGenerator.scala
@@ -23,7 +23,7 @@ import org.apache.flink.table.api.{TableConfig, TableException}
 import org.apache.flink.table.data.{BoxedWrapperRowData, RowData}
 import org.apache.flink.table.runtime.generated.GeneratedFunction
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.{RowDataTypeInfo, WrapperTypeInfo}
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 import org.apache.calcite.plan.RelOptCluster
 import org.apache.calcite.rex._
@@ -42,10 +42,9 @@ object CalcCodeGenerator {
       condition: Option[RexNode],
       retainHeader: Boolean = false,
       opName: String): CodeGenOperatorFactory[RowData] = {
-    val inputType = inputTransform.getOutputType match {
-      case rowDataTypeInfo: RowDataTypeInfo => rowDataTypeInfo.toRowType
-      case wrapperTypeInfo: WrapperTypeInfo[_] => wrapperTypeInfo.toRowType
-    }
+    val inputType = inputTransform.getOutputType
+      .asInstanceOf[InternalTypeInfo[RowData]]
+      .toRowType
     // filter out time attributes
     val inputTerm = CodeGenUtils.DEFAULT_INPUT1_TERM
     val processCode = generateProcessCode(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CodeGeneratorContext.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CodeGeneratorContext.scala
index b9bc6fc1882c0..67c048d0cb5bf 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CodeGeneratorContext.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CodeGeneratorContext.scala
@@ -22,19 +22,20 @@ import org.apache.flink.api.common.functions.{Function, RuntimeContext}
 import org.apache.flink.api.common.typeutils.TypeSerializer
 import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.data.GenericRowData
+import org.apache.flink.table.data.conversion.DataStructureConverter
 import org.apache.flink.table.functions.{FunctionContext, UserDefinedFunction}
 import org.apache.flink.table.planner.codegen.CodeGenUtils._
 import org.apache.flink.table.planner.codegen.GenerateUtils.generateRecordStatement
 import org.apache.flink.table.runtime.operators.TableStreamOperator
-import org.apache.flink.table.runtime.types.InternalSerializers
+import org.apache.flink.table.runtime.typeutils.InternalSerializers
 import org.apache.flink.table.runtime.util.collections._
 import org.apache.flink.table.types.logical.LogicalTypeRoot._
 import org.apache.flink.table.types.logical._
 import org.apache.flink.util.InstantiationUtil
+
 import org.apache.calcite.avatica.util.DateTimeUtils
-import java.util.TimeZone
 
-import org.apache.flink.table.data.conversion.DataStructureConverter
+import java.util.TimeZone
 
 import scala.collection.mutable
 
@@ -383,7 +384,7 @@ class CodeGeneratorContext(val tableConfig: TableConfig) {
   }
 
   /**
-    * Adds a reusable null [[org.apache.flink.table.dataformat.GenericRowData]] to the member area.
+    * Adds a reusable null [[GenericRowData]] to the member area.
     */
   def addReusableNullRow(rowTerm: String, arity: Int): Unit = {
     addReusableOutputRecord(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CorrelateCodeGenerator.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CorrelateCodeGenerator.scala
index dfab5c55811be..c09606f95aa89 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CorrelateCodeGenerator.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/CorrelateCodeGenerator.scala
@@ -31,7 +31,7 @@ import org.apache.flink.table.planner.functions.utils.TableSqlFunction
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
 import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.runtime.util.StreamRecordCollector
 import org.apache.flink.table.types.logical.RowType
 
@@ -112,7 +112,7 @@ object CorrelateCodeGenerator {
       inputTransformation,
       transformationName,
       substituteStreamOperator,
-      RowDataTypeInfo.of(returnType),
+      InternalTypeInfo.of(returnType),
       parallelism)
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ValuesCodeGenerator.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ValuesCodeGenerator.scala
index aff63e433ac0a..e6428ac2cf57f 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ValuesCodeGenerator.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ValuesCodeGenerator.scala
@@ -22,7 +22,7 @@ import org.apache.flink.table.api.TableConfig
 import org.apache.flink.table.data.{GenericRowData, RowData}
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.runtime.operators.values.ValuesInputFormat
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import com.google.common.collect.ImmutableList
 import org.apache.calcite.rel.`type`.RelDataType
@@ -54,7 +54,7 @@ object ValuesCodeGenerator {
       generatedRecords.map(_.code),
       outputType)
 
-    new ValuesInputFormat(generatedFunction, RowDataTypeInfo.of(outputType))
+    new ValuesInputFormat(generatedFunction, InternalTypeInfo.of(outputType))
   }
 
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/agg/batch/AggCodeGenHelper.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/agg/batch/AggCodeGenHelper.scala
index 1fbacc07878b7..e7fa871123575 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/agg/batch/AggCodeGenHelper.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/agg/batch/AggCodeGenHelper.scala
@@ -34,11 +34,12 @@ import org.apache.flink.table.planner.functions.aggfunctions.DeclarativeAggregat
 import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils.{getAccumulatorTypeOfAggregateFunction, getAggUserDefinedInputTypes}
 import org.apache.flink.table.runtime.context.ExecutionContextImpl
 import org.apache.flink.table.runtime.generated.{GeneratedAggsHandleFunction, GeneratedOperator}
-import org.apache.flink.table.runtime.types.InternalSerializers
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.{fromDataTypeToLogicalType, fromLogicalTypeToDataType}
+import org.apache.flink.table.runtime.typeutils.InternalSerializers
 import org.apache.flink.table.types.DataType
 import org.apache.flink.table.types.logical.LogicalTypeRoot._
 import org.apache.flink.table.types.logical.{DistinctType, LogicalType, RowType}
+
 import org.apache.calcite.rel.core.AggregateCall
 import org.apache.calcite.rex.RexNode
 import org.apache.calcite.tools.RelBuilder
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/dataview/DataViewUtils.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/dataview/DataViewUtils.scala
index 74f32690bb8f9..6cb96e35ed24c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/dataview/DataViewUtils.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/dataview/DataViewUtils.scala
@@ -22,12 +22,12 @@ import org.apache.flink.api.common.typeutils.CompositeType
 import org.apache.flink.api.java.typeutils.{PojoField, PojoTypeInfo}
 import org.apache.flink.table.api.TableException
 import org.apache.flink.table.api.dataview._
-import org.apache.flink.table.data.GenericRowData
 import org.apache.flink.table.data.binary.BinaryRawValueData
+import org.apache.flink.table.data.{GenericRowData, RowData}
 import org.apache.flink.table.dataview.{ListViewTypeInfo, MapViewTypeInfo}
 import org.apache.flink.table.functions.UserDefinedAggregateFunction
-import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.{fromLogicalTypeToTypeInfo, fromTypeInfoToLogicalType}
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.DataType
 import org.apache.flink.table.types.logical.LegacyTypeInformationType
 import org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType
@@ -88,15 +88,18 @@ object DataViewUtils {
             (fromLegacyInfoToDataType(pojoTypeInfo), accumulatorSpecs.toArray)
 
           // so we add another check => acc.isInstanceOf[GenericRowData]
-          case t: RowDataTypeInfo if acc.isInstanceOf[GenericRowData] =>
+          case t: InternalTypeInfo[RowData] if acc.isInstanceOf[GenericRowData] =>
             val accInstance = acc.asInstanceOf[GenericRowData]
-            val (arity, fieldNames, fieldTypes) = (t.getArity, t.getFieldNames, t.getFieldTypes)
+            val (arity, fieldNames, fieldTypes) = (
+              t.toRowSize,
+              t.toRowFieldNames,
+              t.toRowFieldTypes)
             val newFieldTypes = for (i <- 0 until arity) yield {
               val fieldName = fieldNames(i)
               val fieldInstance = accInstance.getField(i)
               val (newTypeInfo: TypeInformation[_], spec: Option[DataViewSpec]) =
                 decorateDataViewTypeInfo(
-                  fieldTypes(i),
+                  fromLogicalTypeToTypeInfo(fieldTypes(i)),
                   fieldInstance,
                   isStateBackedDataViews,
                   index,
@@ -108,7 +111,7 @@ object DataViewUtils {
               fromTypeInfoToLogicalType(newTypeInfo)
             }
 
-            val newType = new RowDataTypeInfo(newFieldTypes.toArray, fieldNames)
+            val newType = InternalTypeInfo.ofFields(newFieldTypes.toArray, fieldNames)
             (fromLegacyInfoToDataType(newType), accumulatorSpecs.toArray)
 
           case ct: CompositeType[_] if includesDataView(ct) =>
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonLookupJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonLookupJoin.scala
index f974d26178b9b..be5432765e6d1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonLookupJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonLookupJoin.scala
@@ -48,7 +48,7 @@ import org.apache.flink.table.runtime.types.ClassLogicalTypeConverter
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.{fromDataTypeToLogicalType, fromLogicalTypeToDataType}
 import org.apache.flink.table.runtime.types.PlannerTypeUtils.isInteroperable
 import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sources.LookupableTableSource
 import org.apache.flink.table.types.DataType
 import org.apache.flink.table.types.logical.utils.LogicalTypeUtils.toInternalConversionClass
@@ -335,7 +335,7 @@ abstract class CommonLookupJoin(
           generatedCalc,
           generatedResultFuture,
           producedTypeInfo,
-          RowDataTypeInfo.of(rightRowType),
+          InternalTypeInfo.of(rightRowType),
           leftOuterJoin,
           asyncBufferCapacity)
       } else {
@@ -351,7 +351,7 @@ abstract class CommonLookupJoin(
           generatedFetcher,
           generatedResultFuture,
           producedTypeInfo,
-          RowDataTypeInfo.of(rightRowType),
+          InternalTypeInfo.of(rightRowType),
           leftOuterJoin,
           asyncBufferCapacity)
       }
@@ -434,7 +434,7 @@ abstract class CommonLookupJoin(
       inputTransformation,
       getRelDetailedDescription,
       operatorFactory,
-      RowDataTypeInfo.of(resultRowType),
+      InternalTypeInfo.of(resultRowType),
       inputTransformation.getParallelism)
   }
 
@@ -742,7 +742,7 @@ abstract class CommonLookupJoin(
           s"implement LookupableTableSource interface if it is used in temporal table join.")
       }
       val tableSourceProducedType = fromDataTypeToTypeInfo(tableSource.getProducedDataType)
-      if (!tableSourceProducedType.isInstanceOf[RowDataTypeInfo] &&
+      if (!tableSourceProducedType.isInstanceOf[InternalTypeInfo[RowData]] &&
         !tableSourceProducedType.isInstanceOf[RowTypeInfo]) {
         throw new TableException(
           "Temporal table join only support Row or RowData type as return type of temporal table." +
@@ -754,7 +754,7 @@ abstract class CommonLookupJoin(
       udtfReturnTypeInfo: TypeInformation[_],
       extractedUdtfReturnTypeInfo: TypeInformation[_]): Unit = {
     if (udtfReturnTypeInfo != null) {
-      if (!udtfReturnTypeInfo.isInstanceOf[RowDataTypeInfo] &&
+      if (!udtfReturnTypeInfo.isInstanceOf[InternalTypeInfo[RowData]] &&
         !udtfReturnTypeInfo.isInstanceOf[RowTypeInfo]) {
         throw new TableException(
           s"Result type of the async lookup TableFunction of $tableSourceDescription " +
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalSink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalSink.scala
index 35771030042ba..9329fb495b921 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalSink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalSink.scala
@@ -37,7 +37,7 @@ import org.apache.flink.table.planner.plan.nodes.physical.FlinkPhysicalRel
 import org.apache.flink.table.planner.sinks.TableSinkUtils
 import org.apache.flink.table.runtime.connector.sink.SinkRuntimeProviderContext
 import org.apache.flink.table.runtime.operators.sink.SinkOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import scala.collection.JavaConversions._
@@ -64,7 +64,7 @@ class CommonPhysicalSink (
       tableConfig: TableConfig,
       rowtimeFieldIndex: Int,
       isBounded: Boolean): Transformation[Any] = {
-    val inputTypeInfo = new RowDataTypeInfo(FlinkTypeFactory.toLogicalRowType(getInput.getRowType))
+    val inputTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getInput.getRowType))
     val runtimeProvider = tableSink.getSinkRuntimeProvider(
       new SinkRuntimeProviderContext(isBounded))
     val sinkFunction = runtimeProvider match {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalTableSourceScan.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalTableSourceScan.scala
index f5a9b1848eb28..decc512bcd18d 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalTableSourceScan.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPhysicalTableSourceScan.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.data.RowData
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.plan.schema.TableSourceTable
 import org.apache.flink.table.runtime.connector.source.ScanRuntimeProviderContext
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelWriter
@@ -68,7 +68,7 @@ abstract class CommonPhysicalTableSourceScan(
       name: String): Transformation[RowData] = {
     val runtimeProvider = tableSource.getScanRuntimeProvider(ScanRuntimeProviderContext.INSTANCE)
     val outRowType = FlinkTypeFactory.toLogicalRowType(tableSourceTable.getRowType)
-    val outTypeInfo = new RowDataTypeInfo(outRowType)
+    val outTypeInfo = InternalTypeInfo.of(outRowType)
 
     runtimeProvider match {
       case provider: SourceFunctionProvider =>
@@ -90,5 +90,5 @@ abstract class CommonPhysicalTableSourceScan(
       env: StreamExecutionEnvironment,
       inputFormat: InputFormat[RowData, _],
       name: String,
-      outTypeInfo: RowDataTypeInfo): Transformation[RowData]
+      outTypeInfo: InternalTypeInfo[RowData]): Transformation[RowData]
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
index 90ba511350522..23ca17d1dd76f 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.ARROW_PYTHON_SCALAR_FUNCTION_OPERATOR_NAME
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCalc.PYTHON_SCALAR_FUNCTION_OPERATOR_NAME
 import org.apache.flink.table.planner.plan.utils.PythonUtil.containsPythonCall
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import scala.collection.JavaConversions._
@@ -55,8 +55,8 @@ trait CommonPythonCalc extends CommonPythonBase {
 
   private def getPythonScalarFunctionOperator(
       config: Configuration,
-      inputRowTypeInfo: RowDataTypeInfo,
-      outputRowTypeInfo: RowDataTypeInfo,
+      inputRowTypeInfo: InternalTypeInfo[RowData],
+      outputRowTypeInfo: InternalTypeInfo[RowData],
       udfInputOffsets: Array[Int],
       pythonFunctionInfos: Array[PythonFunctionInfo],
       forwardedFields: Array[Int],
@@ -102,9 +102,10 @@ trait CommonPythonCalc extends CommonPythonBase {
       extractPythonScalarFunctionInfos(pythonRexCalls)
 
     val inputLogicalTypes =
-      inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo].getLogicalTypes
-    val pythonOperatorInputTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val pythonOperatorResultTyeInfo = new RowDataTypeInfo(
+      inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowFieldTypes
+    val pythonOperatorInputTypeInfo = inputTransform.getOutputType
+      .asInstanceOf[InternalTypeInfo[RowData]]
+    val pythonOperatorResultTyeInfo = InternalTypeInfo.ofFields(
       forwardedFields.map(inputLogicalTypes(_)) ++
         pythonRexCalls.map(node => FlinkTypeFactory.toLogicalType(node.getType)): _*)
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCorrelate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCorrelate.scala
index 4cdcf39b07f73..253479361bca0 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCorrelate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCorrelate.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.functions.python.PythonFunctionInfo
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.plan.nodes.common.CommonPythonCorrelate.PYTHON_TABLE_FUNCTION_OPERATOR_NAME
 import org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalTableFunctionScan
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import scala.collection.mutable
@@ -38,8 +38,8 @@ import scala.collection.mutable
 trait CommonPythonCorrelate extends CommonPythonBase {
   private def getPythonTableFunctionOperator(
       config: Configuration,
-      inputRowType: RowDataTypeInfo,
-      outputRowType: RowDataTypeInfo,
+      inputRowType: InternalTypeInfo[RowData],
+      outputRowType: InternalTypeInfo[RowData],
       pythonFunctionInfo: PythonFunctionInfo,
       udtfInputOffsets: Array[Int],
       joinType: JoinRelType): OneInputStreamOperator[RowData, RowData] = {
@@ -81,8 +81,9 @@ trait CommonPythonCorrelate extends CommonPythonBase {
     val pythonTableFuncRexCall = scan.getCall.asInstanceOf[RexCall]
     val (pythonUdtfInputOffsets, pythonFunctionInfo) =
       extractPythonTableFunctionInfo(pythonTableFuncRexCall)
-    val pythonOperatorInputRowType = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val pythonOperatorOutputRowType = RowDataTypeInfo.of(
+    val pythonOperatorInputRowType = inputTransform.getOutputType
+      .asInstanceOf[InternalTypeInfo[RowData]]
+    val pythonOperatorOutputRowType = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalType(outputRowType).asInstanceOf[RowType])
     val pythonOperator = getPythonTableFunctionOperator(
       config,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalc.scala
index 5c771b5e62922..06267cf83b6f1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecCalc.scala
@@ -24,7 +24,7 @@ import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.codegen.{CalcCodeGenerator, CodeGeneratorContext}
 import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan._
 import org.apache.calcite.rel._
@@ -74,7 +74,7 @@ class BatchExecCalc(
       inputTransform,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       inputTransform.getParallelism)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExchange.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExchange.scala
index 0f0836226075c..dfc30f7e1e06e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExchange.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExchange.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalExchange
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil
 import org.apache.flink.table.runtime.partitioner.BinaryHashPartitioner
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
@@ -134,8 +134,8 @@ class BatchExecExchange(
         input
     }
 
-    val inputType = input.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val outputRowType = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+    val inputType = input.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val outputRowType = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
 
     val conf = planner.getTableConfig
     val shuffleMode = getShuffleMode(conf.getConfiguration)
@@ -183,7 +183,7 @@ class BatchExecExchange(
         val partitioner = new BinaryHashPartitioner(
           HashCodeGenerator.generateRowHash(
             CodeGeneratorContext(planner.getTableConfig),
-            RowType.of(inputType.getLogicalTypes: _*),
+            RowType.of(inputType.toRowFieldTypes: _*),
             "HashPartitioner",
             keys.map(_.intValue()).toArray),
           keys.map(getInput.getRowType.getFieldNames.get(_)).toArray
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExpand.scala
index 4b7c95049bd3c..ecf2950f8f161 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecExpand.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.calcite.Expand
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.RelExplainUtil
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
@@ -86,7 +86,7 @@ class BatchExecExpand(
     val config = planner.getTableConfig
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val inputType = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
+    val inputType = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
     val outputType = FlinkTypeFactory.toLogicalRowType(getRowType)
 
     val ctx = CodeGeneratorContext(config)
@@ -102,7 +102,7 @@ class BatchExecExpand(
       inputTransform,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       inputTransform.getParallelism)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashAggregateBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashAggregateBase.scala
index 9b84cf417b643..738466259f779 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashAggregateBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashAggregateBase.scala
@@ -32,7 +32,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToBatchAggregateInfoList
 import org.apache.flink.table.planner.plan.utils.FlinkRelMdUtil
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -144,7 +144,7 @@ abstract class BatchExecHashAggregateBase(
       input,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism,
       managedMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashJoin.scala
index 35b517f083048..6e1cf7f43de6a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashJoin.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.planner.plan.nodes.FlinkConventions
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
 import org.apache.flink.table.planner.plan.utils.{FlinkRelMdUtil, JoinUtil}
 import org.apache.flink.table.runtime.operators.join.{HashJoinOperator, HashJoinType}
-import org.apache.flink.table.runtime.typeutils.{RowDataTypeInfo, BinaryRowDataSerializer}
+import org.apache.flink.table.runtime.typeutils.{InternalTypeInfo, BinaryRowDataSerializer}
 import org.apache.flink.table.types.logical.RowType
 
 import org.apache.calcite.plan._
@@ -196,8 +196,8 @@ class BatchExecHashJoin(
         .asInstanceOf[Transformation[RowData]]
 
     // get type
-    val lType = lInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
-    val rType = rInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
+    val lType = lInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
+    val rType = rInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
 
     val keyType = RowType.of(leftKeys.map(lType.getTypeAt): _*)
 
@@ -259,7 +259,7 @@ class BatchExecHashJoin(
       probe,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType)),
+      InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType)),
       probe.getParallelism,
       managedMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashWindowAggregateBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashWindowAggregateBase.scala
index 3bd7596219b11..1c944134c09b1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashWindowAggregateBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecHashWindowAggregateBase.scala
@@ -34,7 +34,7 @@ import org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToBatchA
 import org.apache.flink.table.planner.plan.utils.FlinkRelMdUtil
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
 import org.apache.flink.table.runtime.operators.aggregate.BytesHashMap
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -146,7 +146,7 @@ abstract class BatchExecHashWindowAggregateBase(
       input,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism,
       managedMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecLegacySink.scala
index e985c8cb12b0f..40864d778248f 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecLegacySink.scala
@@ -31,7 +31,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.UpdatingPlanChecker
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
 import org.apache.flink.table.runtime.types.ClassLogicalTypeConverter
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sinks.{RetractStreamTableSink, StreamTableSink, TableSink, UpsertStreamTableSink}
 import org.apache.flink.table.types.DataType
 
@@ -135,7 +135,7 @@ class BatchExecLegacySink[T](
           val (converterOperator, outputTypeInfo) = generateRowConverterOperator[T](
             CodeGeneratorContext(config),
             config,
-            plan.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType,
+            plan.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType,
             sink,
             withChangeFlag,
             "SinkConversion"
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala
index c432fb7754815..7f2696b7622d0 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecNestedLoopJoin.scala
@@ -27,7 +27,7 @@ import org.apache.flink.table.planner.codegen.{CodeGeneratorContext, NestedLoopJ
 import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.cost.{FlinkCost, FlinkCostFactory}
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
-import org.apache.flink.table.runtime.typeutils.{RowDataTypeInfo, BinaryRowDataSerializer}
+import org.apache.flink.table.runtime.typeutils.{InternalTypeInfo, BinaryRowDataSerializer}
 
 import org.apache.calcite.plan._
 import org.apache.calcite.rel.core._
@@ -135,8 +135,8 @@ class BatchExecNestedLoopJoin(
         .asInstanceOf[Transformation[RowData]]
 
     // get type
-    val lType = lInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
-    val rType = rInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
+    val lType = lInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
+    val rType = rInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
     val outputType = FlinkTypeFactory.toLogicalRowType(getRowType)
 
     val op = new NestedLoopJoinCodeGenerator(
@@ -160,7 +160,7 @@ class BatchExecNestedLoopJoin(
       rInput,
       getRelDetailedDescription,
       op,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       parallelism,
       manageMem)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecOverAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecOverAggregate.scala
index b98bde659cf1d..1cbdbdd919267 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecOverAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecOverAggregate.scala
@@ -46,7 +46,7 @@ import org.apache.flink.table.runtime.generated.GeneratedRecordComparator
 import org.apache.flink.table.runtime.operators.over.frame.OffsetOverFrame.CalcOffsetFunc
 import org.apache.flink.table.runtime.operators.over.frame._
 import org.apache.flink.table.runtime.operators.over.{BufferDataOverWindowOperator, NonBufferOverWindowOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.LogicalTypeRoot.{BIGINT, INTEGER, SMALLINT}
 
 import org.apache.calcite.plan._
@@ -422,7 +422,7 @@ class BatchExecOverAggregate(
       input,
       getRelDetailedDescription,
       SimpleOperatorFactory.of(operator),
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism,
       managedMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecRank.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecRank.scala
index 85943b2ef6723..d049f2eba739c 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecRank.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecRank.scala
@@ -34,7 +34,7 @@ import org.apache.flink.table.planner.plan.rules.physical.batch.BatchExecJoinRul
 import org.apache.flink.table.planner.plan.utils.{FlinkRelOptUtil, RelExplainUtil}
 import org.apache.flink.table.runtime.operators.rank.{ConstantRankRange, RankRange, RankType}
 import org.apache.flink.table.runtime.operators.sort.RankOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan._
 import org.apache.calcite.rel.RelDistribution.Type
@@ -290,7 +290,7 @@ class BatchExecRank(
       input,
       getRelDetailedDescription,
       SimpleOperatorFactory.of(operator),
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSort.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSort.scala
index eef39215f37ab..35095863fe476 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSort.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSort.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.cost.{FlinkCost, FlinkCostFactory}
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.{FlinkRelMdUtil, RelExplainUtil, SortUtil}
 import org.apache.flink.table.runtime.operators.sort.SortOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.core.Sort
@@ -125,7 +125,7 @@ class BatchExecSort(
       input,
       getRelDetailedDescription,
       SimpleOperatorFactory.of(operator.asInstanceOf[OneInputStreamOperator[RowData, RowData]]),
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism,
       sortMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortAggregateBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortAggregateBase.scala
index 512fb8c9402c7..a85ad890e5ca9 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortAggregateBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortAggregateBase.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.plan.cost.{FlinkCost, FlinkCostFactory}
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToBatchAggregateInfoList
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -121,7 +121,7 @@ abstract class BatchExecSortAggregateBase(
       input,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortLimit.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortLimit.scala
index 77c8210e336cf..24b8cbfab0d3e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortLimit.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortLimit.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.plan.cost.{FlinkCost, FlinkCostFactory}
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.{RelExplainUtil, SortUtil}
 import org.apache.flink.table.runtime.operators.sort.SortLimitOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.core.Sort
@@ -131,8 +131,8 @@ class BatchExecSortLimit(
 
     val input = getInputNodes.get(0).translateToPlan(planner)
         .asInstanceOf[Transformation[RowData]]
-    val inputType = input.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val types = inputType.getLogicalTypes
+    val inputType = input.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val types = inputType.toRowFieldTypes
 
     // generate comparator
     val genComparator = ComparatorCodeGenerator.gen(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortMergeJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortMergeJoin.scala
index 86c9a21a91102..bb2b435a246c3 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortMergeJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortMergeJoin.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.planner.plan.cost.{FlinkCost, FlinkCostFactory}
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
 import org.apache.flink.table.planner.plan.utils.{FlinkRelMdUtil, FlinkRelOptUtil, JoinUtil, SortUtil}
 import org.apache.flink.table.runtime.operators.join.{FlinkJoinType, SortMergeJoinOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import org.apache.calcite.plan._
@@ -206,8 +206,8 @@ class BatchExecSortMergeJoin(
     val rightInput = getInputNodes.get(1).translateToPlan(planner)
         .asInstanceOf[Transformation[RowData]]
 
-    val leftType = leftInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
-    val rightType = rightInput.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
+    val leftType = leftInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
+    val rightType = rightInput.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
 
     val keyType = RowType.of(leftAllKey.map(leftType.getChildren.get(_)): _*)
 
@@ -260,7 +260,7 @@ class BatchExecSortMergeJoin(
       rightInput,
       getRelDetailedDescription,
       SimpleOperatorFactory.of(operator),
-      RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType)),
+      InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType)),
       rightInput.getParallelism,
       managedMemory)
   }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortWindowAggregateBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortWindowAggregateBase.scala
index 2942824608088..6b35841ebfc2e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortWindowAggregateBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecSortWindowAggregateBase.scala
@@ -32,7 +32,7 @@ import org.apache.flink.table.planner.plan.logical.LogicalWindow
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToBatchAggregateInfoList
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -136,7 +136,7 @@ abstract class BatchExecSortWindowAggregateBase(
       input,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       input.getParallelism)
   }
 }
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecTableSourceScan.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecTableSourceScan.scala
index 7b50316303038..5d8955f9c785f 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecTableSourceScan.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/batch/BatchExecTableSourceScan.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.delegation.BatchPlanner
 import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan
 import org.apache.flink.table.planner.plan.nodes.exec.{BatchExecNode, ExecNode}
 import org.apache.flink.table.planner.plan.schema.TableSourceTable
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan._
 import org.apache.calcite.rel.RelNode
@@ -81,7 +81,7 @@ class BatchExecTableSourceScan(
       env: StreamExecutionEnvironment,
       inputFormat: InputFormat[RowData, _],
       name: String,
-      outTypeInfo: RowDataTypeInfo): Transformation[RowData] = {
+      outTypeInfo: InternalTypeInfo[RowData]): Transformation[RowData] = {
     // env.createInput will use ContinuousFileReaderOperator, but it do not support multiple
     // paths. If read partitioned source, after partition pruning, we need let InputFormat
     // to read multiple partitions which are multiple paths.
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecCalc.scala
index 1cccba7d58863..3aacfc353e8f6 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecCalc.scala
@@ -25,7 +25,7 @@ import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.planner.codegen.{CalcCodeGenerator, CodeGeneratorContext}
 import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.runtime.operators.AbstractProcessStreamOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -89,7 +89,7 @@ class StreamExecCalc(
       inputTransform,
       getRelDetailedDescription,
       substituteStreamOperator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       inputTransform.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecDeduplicate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecDeduplicate.scala
index eb5dda2527644..997e962cfb153 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecDeduplicate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecDeduplicate.scala
@@ -32,7 +32,7 @@ import org.apache.flink.table.planner.plan.nodes.physical.stream.StreamExecDedup
 import org.apache.flink.table.planner.plan.utils.{AggregateUtil, ChangelogPlanUtils, KeySelectorUtil}
 import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator
 import org.apache.flink.table.runtime.operators.deduplicate.{DeduplicateKeepFirstRowFunction, DeduplicateKeepLastRowFunction, MiniBatchDeduplicateKeepFirstRowFunction, MiniBatchDeduplicateKeepLastRowFunction}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}
@@ -101,7 +101,7 @@ class StreamExecDeduplicate(
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
 
-    val rowTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val rowTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(this)
     val tableConfig = planner.getTableConfig
     val generateInsert = tableConfig.getConfiguration
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExchange.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExchange.scala
index 12108f1ea7d18..a091b48bfdc04 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExchange.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExchange.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalExchange
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.KeySelectorUtil
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.{RelDistribution, RelNode}
@@ -75,8 +75,8 @@ class StreamExecExchange(
       planner: StreamPlanner): Transformation[RowData] = {
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val inputTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val outputTypeInfo = RowDataTypeInfo.of(
+    val inputTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val outputTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getRowType))
     relDistribution.getType match {
       case RelDistribution.Type.SINGLETON =>
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExpand.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExpand.scala
index c331c06cd1b36..ac771b034a126 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExpand.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecExpand.scala
@@ -25,7 +25,7 @@ import org.apache.flink.table.planner.codegen.{CodeGeneratorContext, ExpandCodeG
 import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.nodes.calcite.Expand
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelNode
@@ -73,7 +73,7 @@ class StreamExecExpand(
     val config = planner.getTableConfig
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val inputType = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo].toRowType
+    val inputType = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]].toRowType
     val outputType = FlinkTypeFactory.toLogicalRowType(getRowType)
 
     val ctx = CodeGeneratorContext(config)
@@ -90,7 +90,7 @@ class StreamExecExpand(
       inputTransform,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outputType),
+      InternalTypeInfo.of(outputType),
       inputTransform.getParallelism)
     if (inputsContainSingleton()) {
       transform.setParallelism(1)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGlobalGroupAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGlobalGroupAggregate.scala
index 350b453d06cca..7caa79a11e414 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGlobalGroupAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGlobalGroupAggregate.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction
 import org.apache.flink.table.runtime.operators.aggregate.MiniBatchGlobalGroupAggFunction
 import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.DataType
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
@@ -171,7 +171,7 @@ class StreamExecGlobalGroupAggregate(
       throw new TableException("Local-Global optimization is only worked in miniBatch mode")
     }
 
-    val inputTypeInfo = inputTransformation.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val inputTypeInfo = inputTransformation.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val selector = KeySelectorUtil.getRowDataSelector(grouping, inputTypeInfo)
 
     // partitioned aggregation
@@ -179,7 +179,7 @@ class StreamExecGlobalGroupAggregate(
       inputTransformation,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outRowType),
+      InternalTypeInfo.of(outRowType),
       inputTransformation.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupAggregate.scala
index 18c2609470930..c3a44b851dccf 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupAggregate.scala
@@ -32,7 +32,7 @@ import org.apache.flink.table.planner.plan.utils.{AggregateInfoList, AggregateUt
 import org.apache.flink.table.runtime.operators.aggregate.{GroupAggFunction, MiniBatchGroupAggFunction}
 import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
 import org.apache.calcite.rel.core.AggregateCall
@@ -178,14 +178,14 @@ class StreamExecGroupAggregate(
 
     val selector = KeySelectorUtil.getRowDataSelector(
       grouping,
-      RowDataTypeInfo.of(inputRowType))
+      InternalTypeInfo.of(inputRowType))
 
     // partitioned aggregation
     val ret = new OneInputTransformation(
       inputTransformation,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outRowType),
+      InternalTypeInfo.of(outRowType),
       inputTransformation.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupTableAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupTableAggregate.scala
index f1f39030d860f..525d328ca8d66 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupTableAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupTableAggregate.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils._
 import org.apache.flink.table.runtime.operators.aggregate.GroupTableAggFunction
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
@@ -148,14 +148,14 @@ class StreamExecGroupTableAggregate(
 
     val selector = KeySelectorUtil.getRowDataSelector(
       grouping,
-      RowDataTypeInfo.of(inputRowType))
+      InternalTypeInfo.of(inputRowType))
 
     // partitioned aggregation
     val ret = new OneInputTransformation(
       inputTransformation,
       "GroupTableAggregate",
       operator,
-      RowDataTypeInfo.of(outRowType),
+      InternalTypeInfo.of(outRowType),
       inputTransformation.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupWindowAggregateBase.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupWindowAggregateBase.scala
index ba440130b4bf3..c3172d1e6b16d 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupWindowAggregateBase.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecGroupWindowAggregateBase.scala
@@ -34,7 +34,7 @@ import org.apache.flink.table.planner.plan.utils._
 import org.apache.flink.table.runtime.generated.{GeneratedClass, GeneratedNamespaceAggsHandleFunction, GeneratedNamespaceTableAggsHandleFunction, GeneratedRecordEqualiser}
 import org.apache.flink.table.runtime.operators.window.{CountWindow, TimeWindow, WindowOperator, WindowOperatorBuilder}
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.LogicalType
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
@@ -117,8 +117,8 @@ abstract class StreamExecGroupWindowAggregateBase(
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
 
-    val inputRowTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val outRowType = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(outputRowType))
+    val inputRowTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val outRowType = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(outputRowType))
 
     val isCountWindow = window match {
       case TumblingGroupWindow(_, _, size) if hasRowIntervalType(size) => true
@@ -158,7 +158,7 @@ abstract class StreamExecGroupWindowAggregateBase(
       aggInfoList,
       config,
       planner.getRelBuilder,
-      inputRowTypeInfo.getLogicalTypes,
+      inputRowTypeInfo.toRowFieldTypes,
       needRetraction)
 
     val aggResultTypes = aggInfoList.getActualValueTypes.map(fromDataTypeToLogicalType)
@@ -175,7 +175,7 @@ abstract class StreamExecGroupWindowAggregateBase(
       accTypes,
       windowPropertyTypes,
       aggValueTypes,
-      inputRowTypeInfo.getLogicalTypes,
+      inputRowTypeInfo.toRowFieldTypes,
       timeIdx)
 
     val transformation = new OneInputTransformation(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIncrementalGroupAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIncrementalGroupAggregate.scala
index 5710231b39cd0..dd8527d1ca724 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIncrementalGroupAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIncrementalGroupAggregate.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.utils.{KeySelectorUtil, _}
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction
 import org.apache.flink.table.runtime.operators.aggregate.MiniBatchIncrementalGroupAggFunction
 import org.apache.flink.table.runtime.operators.bundle.KeyedMapBundleOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.DataType
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
@@ -158,7 +158,7 @@ class StreamExecIncrementalGroupAggregate(
 
     val partialKeySelector = KeySelectorUtil.getRowDataSelector(
       partialAggGrouping,
-      RowDataTypeInfo.of(inRowType))
+      InternalTypeInfo.of(inRowType))
     val finalKeySelector = KeySelectorUtil.getRowDataSelector(
       finalAggGrouping,
       partialKeySelector.getProducedType)
@@ -177,7 +177,7 @@ class StreamExecIncrementalGroupAggregate(
       inputTransformation,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outRowType),
+      InternalTypeInfo.of(outRowType),
       inputTransformation.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIntervalJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIntervalJoin.scala
index b49b148b2112e..57183dfef2378 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIntervalJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecIntervalJoin.scala
@@ -35,7 +35,7 @@ import org.apache.flink.table.planner.plan.utils.{IntervalJoinUtil, JoinTypeUtil
 import org.apache.flink.table.runtime.generated.GeneratedFunction
 import org.apache.flink.table.runtime.operators.join.interval.{ProcTimeIntervalJoin, RowTimeIntervalJoin}
 import org.apache.flink.table.runtime.operators.join.{FlinkJoinType, KeyedCoProcessOperatorWithWatermarkDelay, OuterJoinPaddingUtil}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.util.Collector
 
 import org.apache.calcite.plan._
@@ -136,7 +136,7 @@ class StreamExecIntervalJoin(
            FlinkJoinType.FULL =>
         val leftRowType = FlinkTypeFactory.toLogicalRowType(getLeft.getRowType)
         val rightRowType = FlinkTypeFactory.toLogicalRowType(getRight.getRowType)
-        val returnType = RowDataTypeInfo.of(
+        val returnType = InternalTypeInfo.of(
           FlinkTypeFactory.toLogicalRowType(getRowType))
 
         val relativeWindowSize = leftUpperBound - leftLowerBound
@@ -199,12 +199,12 @@ class StreamExecIntervalJoin(
       rightPlan: Transformation[RowData],
       leftArity: Int,
       rightArity: Int,
-      returnTypeInfo: RowDataTypeInfo): Transformation[RowData] = {
+      returnTypeInfo: InternalTypeInfo[RowData]): Transformation[RowData] = {
     // We filter all records instead of adding an empty source to preserve the watermarks.
     val allFilter = new FlatMapFunction[RowData, RowData] with ResultTypeQueryable[RowData] {
       override def flatMap(value: RowData, out: Collector[RowData]): Unit = {}
 
-      override def getProducedType: RowDataTypeInfo = returnTypeInfo
+      override def getProducedType: InternalTypeInfo[RowData] = returnTypeInfo
     }
 
     val leftPadder = new MapFunction[RowData, RowData] with ResultTypeQueryable[RowData] {
@@ -212,7 +212,7 @@ class StreamExecIntervalJoin(
 
       override def map(value: RowData): RowData = paddingUtil.padLeft(value)
 
-      override def getProducedType: RowDataTypeInfo = returnTypeInfo
+      override def getProducedType: InternalTypeInfo[RowData] = returnTypeInfo
     }
 
     val rightPadder = new MapFunction[RowData, RowData] with ResultTypeQueryable[RowData] {
@@ -220,7 +220,7 @@ class StreamExecIntervalJoin(
 
       override def map(value: RowData): RowData = paddingUtil.padRight(value)
 
-      override def getProducedType: RowDataTypeInfo = returnTypeInfo
+      override def getProducedType: InternalTypeInfo[RowData] = returnTypeInfo
     }
 
     val leftParallelism = leftPlan.getParallelism
@@ -270,12 +270,12 @@ class StreamExecIntervalJoin(
   private def createProcTimeJoin(
       leftPlan: Transformation[RowData],
       rightPlan: Transformation[RowData],
-      returnTypeInfo: RowDataTypeInfo,
+      returnTypeInfo: InternalTypeInfo[RowData],
       joinFunction: GeneratedFunction[FlatJoinFunction[RowData, RowData, RowData]],
       leftKeys: Array[Int],
       rightKeys: Array[Int]): Transformation[RowData] = {
-    val leftTypeInfo = leftPlan.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val rightTypeInfo = rightPlan.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val leftTypeInfo = leftPlan.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val rightTypeInfo = rightPlan.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val procJoinFunc = new ProcTimeIntervalJoin(
       flinkJoinType,
       leftLowerBound,
@@ -310,13 +310,13 @@ class StreamExecIntervalJoin(
   private def createRowTimeJoin(
       leftPlan: Transformation[RowData],
       rightPlan: Transformation[RowData],
-      returnTypeInfo: RowDataTypeInfo,
+      returnTypeInfo: InternalTypeInfo[RowData],
       joinFunction: GeneratedFunction[FlatJoinFunction[RowData, RowData, RowData]],
       leftKeys: Array[Int],
       rightKeys: Array[Int]
   ): Transformation[RowData] = {
-    val leftTypeInfo = leftPlan.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val rightTypeInfo = rightPlan.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val leftTypeInfo = leftPlan.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val rightTypeInfo = rightPlan.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val rowJoinFunc = new RowTimeIntervalJoin(
       flinkJoinType,
       leftLowerBound,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecJoin.scala
index f3307d285672b..403c8bda81e1a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecJoin.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{JoinUtil, KeySelectorUtil}
 import org.apache.flink.table.runtime.operators.join.stream.state.JoinInputSideSpec
 import org.apache.flink.table.runtime.operators.join.stream.{StreamingJoinOperator, StreamingSemiAntiJoinOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan._
 import org.apache.calcite.rel.core.{Join, JoinRelType}
@@ -116,15 +116,15 @@ class StreamExecJoin(
       planner: StreamPlanner): Transformation[RowData] = {
 
     val tableConfig = planner.getTableConfig
-    val returnType = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+    val returnType = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
 
     val leftTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
     val rightTransform = getInputNodes.get(1).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
 
-    val leftType = leftTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
-    val rightType = rightTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val leftType = leftTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+    val rightType = rightTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
 
     val (leftJoinKey, rightJoinKey) =
       JoinUtil.checkAndGetJoinKeys(keyPairs, getLeft, getRight, allowEmptyKey = true)
@@ -193,7 +193,7 @@ class StreamExecJoin(
     if (uniqueKeys == null || uniqueKeys.isEmpty) {
       JoinInputSideSpec.withoutUniqueKey()
     } else {
-      val inRowType = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(input.getRowType))
+      val inRowType = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(input.getRowType))
       val joinKeys = if (input == left) {
         keyPairs.map(_.source).toArray
       } else {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLegacySink.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLegacySink.scala
index f8d5006a52a09..2dec3431eaab1 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLegacySink.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLegacySink.scala
@@ -31,7 +31,7 @@ import org.apache.flink.table.planner.plan.nodes.calcite.LegacySink
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{ChangelogPlanUtils, UpdatingPlanChecker}
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
-import org.apache.flink.table.runtime.typeutils.{RowDataTypeInfo, TypeCheckUtils}
+import org.apache.flink.table.runtime.typeutils.{InternalTypeInfo, TypeCheckUtils}
 import org.apache.flink.table.sinks._
 import org.apache.flink.table.types.logical.TimestampType
 
@@ -171,15 +171,15 @@ class StreamExecLegacySink[T](
           s"Please select the rowtime field that should be used as event-time timestamp for the " +
           s"DataStream by casting all other fields to TIMESTAMP.")
     } else if (rowtimeFields.size == 1) {
-      val origRowType = parTransformation.getOutputType.asInstanceOf[RowDataTypeInfo]
-      val convFieldTypes = origRowType.getLogicalTypes.map { t =>
+      val origRowType = parTransformation.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
+      val convFieldTypes = origRowType.toRowFieldTypes.map { t =>
         if (TypeCheckUtils.isRowTime(t)) {
           new TimestampType(3)
         } else {
           t
         }
       }
-      new RowDataTypeInfo(convFieldTypes, origRowType.getFieldNames)
+      InternalTypeInfo.ofFields(convFieldTypes, origRowType.toRowFieldNames)
     } else {
       parTransformation.getOutputType
     }
@@ -191,7 +191,7 @@ class StreamExecLegacySink[T](
       val (converterOperator, outputTypeInfo) = generateRowConverterOperator[T](
         CodeGeneratorContext(config),
         config,
-        convType.asInstanceOf[RowDataTypeInfo].toRowType,
+        convType.asInstanceOf[InternalTypeInfo[RowData]].toRowType,
         sink,
         withChangeFlag,
         "SinkConversion"
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLimit.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLimit.scala
index 523aca3fe114b..3d4c26d3d13ed 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLimit.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLimit.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{ChangelogPlanUtils, RelExplainUtil, SortUtil}
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector
 import org.apache.flink.table.runtime.operators.rank.{AppendOnlyTopNFunction, ConstantRankRange, RankType, RetractableTopNFunction}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel._
@@ -100,7 +100,7 @@ class StreamExecLimit(
       throw new TableException(
         "FETCH is missed, which on streaming table is not supported currently.")
     }
-    val inputRowTypeInfo = RowDataTypeInfo.of(
+    val inputRowTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getInput.getRowType))
     val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(this)
     val tableConfig = planner.getTableConfig
@@ -131,7 +131,7 @@ class StreamExecLimit(
         outputRankNumber,
         cacheSize)
     } else {
-      val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.getLogicalTypes)
+      val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.toRowFieldTypes)
       val generatedEqualiser = equaliserCodeGen.generateRecordEqualiser("LimitValueEqualiser")
       new RetractableTopNFunction(
         minIdleStateRetentionTime,
@@ -151,7 +151,7 @@ class StreamExecLimit(
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
 
-    val outputRowTypeInfo = RowDataTypeInfo.of(
+    val outputRowTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getRowType))
 
     // as input node is singleton exchange, its parallelism is 1.
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLocalGroupAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLocalGroupAggregate.scala
index a54ab5673b680..8ad558046decb 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLocalGroupAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecLocalGroupAggregate.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{KeySelectorUtil, _}
 import org.apache.flink.table.runtime.operators.aggregate.MiniBatchLocalGroupAggFunction
 import org.apache.flink.table.runtime.operators.bundle.MapBundleOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.`type`.RelDataType
@@ -127,7 +127,7 @@ class StreamExecLocalGroupAggregate(
     val aggsHandler = generator.generateAggsHandler("GroupAggsHandler", aggInfoList)
     val aggFunction = new MiniBatchLocalGroupAggFunction(aggsHandler)
 
-    val inputTypeInfo = inputTransformation.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val inputTypeInfo = inputTransformation.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val selector = KeySelectorUtil.getRowDataSelector(grouping, inputTypeInfo)
 
     val operator = new MapBundleOperator(
@@ -139,7 +139,7 @@ class StreamExecLocalGroupAggregate(
       inputTransformation,
       getRelDetailedDescription,
       operator,
-      RowDataTypeInfo.of(outRowType),
+      InternalTypeInfo.of(outRowType),
       inputTransformation.getParallelism)
 
     if (inputsContainSingleton()) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMatch.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMatch.scala
index e6c51ce950879..2256cb61bd24a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMatch.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMatch.scala
@@ -42,7 +42,7 @@ import org.apache.flink.table.planner.plan.utils.PythonUtil.containsPythonCall
 import org.apache.flink.table.planner.plan.utils.RelExplainUtil._
 import org.apache.flink.table.planner.plan.utils.{KeySelectorUtil, RexDefaultVisitor, SortUtil}
 import org.apache.flink.table.runtime.operators.`match`.{RowDataEventComparator, RowtimeProcessFunction}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 import org.apache.flink.util.MathUtils
 
@@ -195,7 +195,7 @@ class StreamExecMatch(
       val partitionKeys = logicalMatch.partitionKeys
       val timeOrderField = SortUtil.getFirstSortField(logicalMatch.orderKeys, getInput.getRowType)
       val isProctime = FlinkTypeFactory.isProctimeIndicatorType(timeOrderField.getType)
-      val inputTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
+      val inputTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
       val inputSerializer = inputTypeInfo.createSerializer(planner.getExecEnv.getConfig)
       val nfaFactory = NFACompiler.compileFactory(cepPattern, false)
       val generator = new MatchCodeGenerator(
@@ -217,7 +217,7 @@ class StreamExecMatch(
         patternProcessFunction,
         null
       )
-      val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+      val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
       val transformation = new OneInputTransformation[RowData, RowData](
         timestampedInput,
         getRelDetailedDescription,
@@ -299,7 +299,7 @@ class StreamExecMatch(
 
   private def setKeySelector(
       transform: OneInputTransformation[RowData, _],
-      inputTypeInfo: RowDataTypeInfo): Unit = {
+      inputTypeInfo: InternalTypeInfo[RowData]): Unit = {
     val selector = KeySelectorUtil.getRowDataSelector(
       logicalMatch.partitionKeys.toArray,
       inputTypeInfo)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMiniBatchAssigner.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMiniBatchAssigner.scala
index 3cd18ed3b09b4..0eaef44e08648 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMiniBatchAssigner.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecMiniBatchAssigner.scala
@@ -27,7 +27,7 @@ import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.`trait`.{MiniBatchIntervalTraitDef, MiniBatchMode}
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.runtime.operators.wmassigners.{ProcTimeMiniBatchAssignerOperator, RowTimeMiniBatchAssginerOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.{RelNode, RelWriter, SingleRel}
@@ -98,7 +98,7 @@ class StreamExecMiniBatchAssigner(
         s"mode, this is a bug, please file an issue.")
     }
 
-    val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+    val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
     val transformation = new OneInputTransformation[RowData, RowData](
       inputTransformation,
       getRelDetailedDescription,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecOverAggregate.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecOverAggregate.scala
index e76ef419331bf..4ff02e61e6d1e 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecOverAggregate.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecOverAggregate.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.planner.plan.utils.AggregateUtil.transformToStream
 import org.apache.flink.table.planner.plan.utils.{KeySelectorUtil, OverAggregateUtil, RelExplainUtil}
 import org.apache.flink.table.runtime.operators.over._
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelOptCost, RelOptPlanner, RelTraitSet}
 import org.apache.calcite.rel.RelFieldCollation.Direction.ASCENDING
@@ -253,12 +253,12 @@ class StreamExecOverAggregate(
     }
 
     val partitionKeys: Array[Int] = overWindow.keys.toArray
-    val inputTypeInfo = RowDataTypeInfo.of(inRowType)
+    val inputTypeInfo = InternalTypeInfo.of(inRowType)
 
     val selector = KeySelectorUtil.getRowDataSelector(partitionKeys, inputTypeInfo)
 
-    val returnTypeInfo = RowDataTypeInfo.of(outRowType)
-      .asInstanceOf[RowDataTypeInfo]
+    val returnTypeInfo = InternalTypeInfo.of(outRowType)
+      .asInstanceOf[InternalTypeInfo[RowData]]
     // partitioned aggregation
 
     val operator = new KeyedProcessOperator(overProcessFunction)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecRank.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecRank.scala
index e1fb2b2069369..51319d99004f3 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecRank.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecRank.scala
@@ -33,7 +33,7 @@ import org.apache.flink.table.planner.plan.nodes.calcite.Rank
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{KeySelectorUtil, _}
 import org.apache.flink.table.runtime.operators.rank._
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel._
@@ -139,14 +139,14 @@ class StreamExecRank(
         throw new TableException(s"Streaming tables do not support $k rank function.")
     }
 
-    val inputRowTypeInfo = RowDataTypeInfo.of(
+    val inputRowTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getInput.getRowType))
     val fieldCollations = orderKey.getFieldCollations
     val (sortFields, sortDirections, nullsIsLast) = SortUtil.getKeysAndOrders(fieldCollations)
     val sortKeySelector = KeySelectorUtil.getRowDataSelector(sortFields, inputRowTypeInfo)
     val sortKeyType = sortKeySelector.getProducedType
     val sortKeyComparator = ComparatorCodeGenerator.gen(tableConfig, "StreamExecSortComparator",
-      sortFields.indices.toArray, sortKeyType.getLogicalTypes, sortDirections, nullsIsLast)
+      sortFields.indices.toArray, sortKeyType.toRowFieldTypes, sortDirections, nullsIsLast)
     val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(this)
     val cacheSize = tableConfig.getConfiguration.getLong(StreamExecRank.TABLE_EXEC_TOPN_CACHE_SIZE)
     val minIdleStateRetentionTime = tableConfig.getMinIdleStateRetentionTime
@@ -183,7 +183,7 @@ class StreamExecRank(
 
       // TODO Use UnaryUpdateTopNFunction after SortedMapState is merged
       case RetractStrategy =>
-        val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.getLogicalTypes)
+        val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.toRowFieldTypes)
         val generatedEqualiser = equaliserCodeGen.generateRecordEqualiser("RankValueEqualiser")
 
         new RetractableTopNFunction(
@@ -202,7 +202,7 @@ class StreamExecRank(
     processFunction.setKeyContext(operator)
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val outputRowTypeInfo = RowDataTypeInfo.of(
+    val outputRowTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getRowType))
     val ret = new OneInputTransformation(
       inputTransform,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSort.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSort.scala
index c9b49aaccc21e..75ca2b619c52b 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSort.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSort.scala
@@ -31,7 +31,7 @@ import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{RelExplainUtil, SortUtil}
 import org.apache.flink.table.runtime.operators.sort.StreamSortOperator
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel._
@@ -112,10 +112,10 @@ class StreamExecSort(
     val keyTypes = keys.map(inputType.getTypeAt)
     val rowComparator = ComparatorCodeGenerator.gen(config, "StreamExecSortComparator",
       keys, keyTypes, orders, nullsIsLast)
-    val sortOperator = new StreamSortOperator(RowDataTypeInfo.of(inputType), rowComparator)
+    val sortOperator = new StreamSortOperator(InternalTypeInfo.of(inputType), rowComparator)
     val input = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+    val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
 
     // as input node is singleton exchange, its parallelism is 1.
     val ret = new OneInputTransformation(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSortLimit.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSortLimit.scala
index 2a5dda898296a..41fc340ea5a27 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSortLimit.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecSortLimit.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils._
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector
 import org.apache.flink.table.runtime.operators.rank._
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.core.Sort
@@ -120,7 +120,7 @@ class StreamExecSortLimit(
 
     val inputTransform = getInputNodes.get(0).translateToPlan(planner)
       .asInstanceOf[Transformation[RowData]]
-    val inputRowTypeInfo = inputTransform.getOutputType.asInstanceOf[RowDataTypeInfo]
+    val inputRowTypeInfo = inputTransform.getOutputType.asInstanceOf[InternalTypeInfo[RowData]]
     val fieldCollations = sortCollation.getFieldCollations
     val (sortFields, sortDirections, nullsIsLast) = SortUtil.getKeysAndOrders(fieldCollations)
     val sortKeySelector = KeySelectorUtil.getRowDataSelector(sortFields, inputRowTypeInfo)
@@ -130,7 +130,7 @@ class StreamExecSortLimit(
       tableConfig,
       "StreamExecSortComparator",
       sortFields.indices.toArray,
-      sortKeyType.getLogicalTypes,
+      sortKeyType.toRowFieldTypes,
       sortDirections,
       nullsIsLast)
     val generateUpdateBefore = ChangelogPlanUtils.generateUpdateBefore(this)
@@ -175,7 +175,7 @@ class StreamExecSortLimit(
 
       // TODO Use UnaryUpdateTopNFunction after SortedMapState is merged
       case RetractStrategy =>
-        val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.getLogicalTypes)
+        val equaliserCodeGen = new EqualiserCodeGenerator(inputRowTypeInfo.toRowFieldTypes)
         val generatedEqualiser = equaliserCodeGen.generateRecordEqualiser("RankValueEqualiser")
         new RetractableTopNFunction(
           minIdleStateRetentionTime,
@@ -192,7 +192,7 @@ class StreamExecSortLimit(
     val operator = new KeyedProcessOperator(processFunction)
     processFunction.setKeyContext(operator)
 
-    val outputRowTypeInfo = RowDataTypeInfo.of(
+    val outputRowTypeInfo = InternalTypeInfo.of(
       FlinkTypeFactory.toLogicalRowType(getRowType))
 
     val ret = new OneInputTransformation(
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTableSourceScan.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTableSourceScan.scala
index e5020cc8266b8..949e58b5b3e44 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTableSourceScan.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTableSourceScan.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.planner.delegation.StreamPlanner
 import org.apache.flink.table.planner.plan.nodes.common.CommonPhysicalTableSourceScan
 import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.schema.TableSourceTable
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sources.StreamTableSource
 
 import org.apache.calcite.plan._
@@ -77,7 +77,7 @@ class StreamExecTableSourceScan(
       env: StreamExecutionEnvironment,
       inputFormat: InputFormat[RowData, _],
       name: String,
-      outTypeInfo: RowDataTypeInfo): Transformation[RowData] = {
+      outTypeInfo: InternalTypeInfo[RowData]): Transformation[RowData] = {
     // It's better to use StreamExecutionEnvironment.createInput()
     // rather than addLegacySource() for streaming, because it take care of checkpoint.
     env
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalJoin.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalJoin.scala
index 8faa174df4722..09c2cd8c3bc88 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalJoin.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalJoin.scala
@@ -35,7 +35,7 @@ import org.apache.flink.table.planner.plan.utils.{InputRefVisitor, KeySelectorUt
 import org.apache.flink.table.runtime.generated.GeneratedJoinCondition
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector
 import org.apache.flink.table.runtime.operators.join.temporal.{TemporalProcessTimeJoinOperator, TemporalRowTimeJoinOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 import org.apache.flink.util.Preconditions.checkState
 
@@ -134,7 +134,7 @@ class StreamExecTemporalJoin(
       rightTransform,
       getRelDetailedDescription,
       joinOperator,
-      RowDataTypeInfo.of(returnType),
+      InternalTypeInfo.of(returnType),
       leftTransform.getParallelism)
 
     if (inputsContainSingleton()) {
@@ -191,14 +191,14 @@ class StreamExecTemporalJoinToCoProcessTranslator private (
   def getLeftKeySelector: RowDataKeySelector = {
     KeySelectorUtil.getRowDataSelector(
       joinInfo.leftKeys.toIntArray,
-      RowDataTypeInfo.of(leftInputType)
+      InternalTypeInfo.of(leftInputType)
     )
   }
 
   def getRightKeySelector: RowDataKeySelector = {
     KeySelectorUtil.getRowDataSelector(
       joinInfo.rightKeys.toIntArray,
-      RowDataTypeInfo.of(rightInputType)
+      InternalTypeInfo.of(rightInputType)
     )
   }
 
@@ -244,8 +244,8 @@ class StreamExecTemporalJoinToCoProcessTranslator private (
       case JoinRelType.INNER =>
         if (rightTimeAttributeInputReference.isDefined) {
           new TemporalRowTimeJoinOperator(
-            RowDataTypeInfo.of(leftInputType),
-            RowDataTypeInfo.of(rightInputType),
+            InternalTypeInfo.of(leftInputType),
+            InternalTypeInfo.of(rightInputType),
             generatedJoinCondition,
             leftTimeAttributeInputReference,
             rightTimeAttributeInputReference.get,
@@ -253,7 +253,7 @@ class StreamExecTemporalJoinToCoProcessTranslator private (
             maxRetentionTime)
         } else {
           new TemporalProcessTimeJoinOperator(
-            RowDataTypeInfo.of(rightInputType),
+            InternalTypeInfo.of(rightInputType),
             generatedJoinCondition,
             minRetentionTime,
             maxRetentionTime)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalSort.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalSort.scala
index 319c1b3d90e57..3a21ebb270caa 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalSort.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecTemporalSort.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.{RelExplainUtil, SortUtil}
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector
 import org.apache.flink.table.runtime.operators.sort.{ProcTimeSortOperator, RowTimeSortOperator}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.RelFieldCollation.Direction
@@ -133,8 +133,8 @@ class StreamExecTemporalSort(
       val keyTypes = keys.map(inputType.getTypeAt)
       val rowComparator = ComparatorCodeGenerator.gen(tableConfig, "ProcTimeSortComparator",
         keys, keyTypes, orders, nullsIsLast)
-      val sortOperator = new ProcTimeSortOperator(RowDataTypeInfo.of(inputType), rowComparator)
-      val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+      val sortOperator = new ProcTimeSortOperator(InternalTypeInfo.of(inputType), rowComparator)
+      val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
 
       // as input node is singleton exchange, its parallelism is 1.
       val ret = new OneInputTransformation(
@@ -174,8 +174,8 @@ class StreamExecTemporalSort(
       null
     }
     val sortOperator = new RowTimeSortOperator(
-      RowDataTypeInfo.of(inputType), rowTimeIdx, rowComparator)
-    val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+      InternalTypeInfo.of(inputType), rowTimeIdx, rowComparator)
+    val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
 
     val ret = new OneInputTransformation(
       input,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecWatermarkAssigner.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecWatermarkAssigner.scala
index 7f6561c47f1a8..862a8fe65f5fd 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecWatermarkAssigner.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/physical/stream/StreamExecWatermarkAssigner.scala
@@ -30,7 +30,7 @@ import org.apache.flink.table.planner.plan.nodes.exec.{ExecNode, StreamExecNode}
 import org.apache.flink.table.planner.plan.utils.RelExplainUtil.preferExpressionFormat
 import org.apache.flink.table.planner.utils.TableConfigUtils.getMillisecondFromConfigDuration
 import org.apache.flink.table.runtime.operators.wmassigners.WatermarkAssignerOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 
 import org.apache.calcite.plan.{RelOptCluster, RelTraitSet}
 import org.apache.calcite.rel.{RelNode, RelWriter}
@@ -109,7 +109,7 @@ class StreamExecWatermarkAssigner(
         idleTimeout,
         watermarkGenerator)
 
-    val outputRowTypeInfo = RowDataTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
+    val outputRowTypeInfo = InternalTypeInfo.of(FlinkTypeFactory.toLogicalRowType(getRowType))
     val transformation = new OneInputTransformation[RowData, RowData](
       inputTransformation,
       getRelDetailedDescription,
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
index ebc2f66e35c2f..9c397a8fbb208 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/PartitionPruner.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.data.{DecimalDataUtils, GenericRowData, StringData
 import org.apache.flink.table.planner.codegen.CodeGenUtils.DEFAULT_COLLECTOR_TERM
 import org.apache.flink.table.planner.codegen.{ConstantCodeGeneratorContext, ExprCodeGenerator, FunctionCodeGenerator}
 import org.apache.flink.table.runtime.functions.SqlDateTimeUtils
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.LogicalTypeRoot._
 import org.apache.flink.table.types.logical.{BooleanType, DecimalType, LogicalType}
 
@@ -83,7 +83,7 @@ object PartitionPruner {
       return allPartitions
     }
 
-    val inputType = new RowDataTypeInfo(partitionFieldTypes, partitionFieldNames).toRowType
+    val inputType = InternalTypeInfo.ofFields(partitionFieldTypes, partitionFieldNames).toRowType
     val returnType: LogicalType = new BooleanType(false)
 
     val ctx = new ConstantCodeGeneratorContext(config)
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ScanUtil.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ScanUtil.scala
index 0fa74a7a4058d..29ab073164df8 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ScanUtil.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/utils/ScanUtil.scala
@@ -28,7 +28,7 @@ import org.apache.flink.table.planner.codegen.{CodeGenUtils, CodeGeneratorContex
 import org.apache.flink.table.planner.plan.nodes.exec.ExecNode
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter.fromDataTypeToLogicalType
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sources.TableSource
 import org.apache.flink.table.types.DataType
 import org.apache.flink.table.types.logical.RowType
@@ -123,7 +123,7 @@ object ScanUtil {
       input.asInstanceOf[Transformation[RowData]],
       getOperatorName(qualifiedName, outRowType),
       substituteStreamOperator,
-      RowDataTypeInfo.of(outputRowType),
+      InternalTypeInfo.of(outputRowType),
       input.getParallelism)
   }
 
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
index 6ee6a8a37a472..618e8a6a03741 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/sinks/TableSinkUtils.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.data.RowData
 import org.apache.flink.table.operations.CatalogSinkModifyOperation
 import org.apache.flink.table.planner.calcite.FlinkTypeFactory
 import org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sinks._
 import org.apache.flink.table.types.DataType
 import org.apache.flink.table.types.inference.TypeTransformations.{legacyDecimalToDefaultDecimal, legacyRawToTypeInfoRaw, toNullable}
@@ -322,8 +322,8 @@ object TableSinkUtils {
         fromLogicalToDataType(queryLogicalType).bridgedTo(classOf[Row])
       case gt: GenericTypeInfo[RowData] if gt.getTypeClass == classOf[RowData] =>
         fromLogicalToDataType(queryLogicalType).bridgedTo(classOf[RowData])
-      case bt: RowDataTypeInfo =>
-        val fields = bt.getFieldNames.zip(bt.getLogicalTypes).map { case (n, t) =>
+      case bt: InternalTypeInfo[RowData] =>
+        val fields = bt.toRowFieldNames.zip(bt.toRowFieldTypes).map { case (n, t) =>
           DataTypes.FIELD(n, fromLogicalToDataType(t))
         }
         DataTypes.ROW(fields: _*).bridgedTo(classOf[RowData])
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/codegen/SortCodeGeneratorTest.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/codegen/SortCodeGeneratorTest.java
index 7b35c6b3ecff3..f5c44b6d8908c 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/codegen/SortCodeGeneratorTest.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/codegen/SortCodeGeneratorTest.java
@@ -49,9 +49,9 @@
 import org.apache.flink.table.runtime.generated.RecordComparator;
 import org.apache.flink.table.runtime.operators.sort.BinaryInMemorySortBuffer;
 import org.apache.flink.table.runtime.operators.sort.ListMemorySegmentPool;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.runtime.typeutils.AbstractRowDataSerializer;
 import org.apache.flink.table.runtime.typeutils.BinaryRowDataSerializer;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
 import org.apache.flink.table.runtime.typeutils.RawValueDataSerializer;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.ArrayType;
@@ -191,8 +191,12 @@ private BinaryRowData row(int i, Object[][] values) {
 			if (value == null) {
 				writer.setNullAt(j);
 			} else {
-				BinaryWriter.write(writer, j, value, types[fields[j]],
-						InternalSerializers.create(types[fields[j]], new ExecutionConfig()));
+				BinaryWriter.write(
+					writer,
+					j,
+					value,
+					types[fields[j]],
+					InternalSerializers.create(types[fields[j]]));
 			}
 		}
 
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/AggFunctionTestBase.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/AggFunctionTestBase.java
index f4e12f6ddba74..e3683b38bc624 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/AggFunctionTestBase.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/functions/aggfunctions/AggFunctionTestBase.java
@@ -25,12 +25,13 @@
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RawValueData;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.functions.AggregateFunction;
 import org.apache.flink.table.planner.functions.aggfunctions.MaxWithRetractAggFunction.MaxWithRetractAccumulator;
 import org.apache.flink.table.planner.functions.aggfunctions.MinWithRetractAggFunction.MinWithRetractAccumulator;
 import org.apache.flink.table.planner.functions.utils.UserDefinedFunctionUtils;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.RawValueDataSerializer;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
 import org.apache.flink.util.Preconditions;
 
 import org.junit.Test;
@@ -42,6 +43,7 @@
 import java.util.Collections;
 import java.util.List;
 
+import static org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromLogicalTypeToTypeInfo;
 import static org.apache.flink.table.utils.RawValueDataAsserter.equivalent;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertThat;
@@ -239,19 +241,19 @@ protected <E> void validateResult(E expected, E result, TypeInformation<?> typeI
 					(RawValueData) result,
 					equivalent((RawValueData<?>) expected, new RawValueDataSerializer<>(serializer)));
 		} else if (expected instanceof GenericRowData && result instanceof GenericRowData) {
-			validateGenericRow((GenericRowData) expected, (GenericRowData) result, (RowDataTypeInfo) typeInfo);
+			validateGenericRow((GenericRowData) expected, (GenericRowData) result, (InternalTypeInfo<RowData>) typeInfo);
 		} else {
 			assertEquals(expected, result);
 		}
 	}
 
-	private void validateGenericRow(GenericRowData expected, GenericRowData result, RowDataTypeInfo typeInfo) {
+	private void validateGenericRow(GenericRowData expected, GenericRowData result, InternalTypeInfo<RowData> typeInfo) {
 		assertEquals(expected.getArity(), result.getArity());
 
 		for (int i = 0; i < expected.getArity(); ++i) {
 			Object expectedObj = expected.getField(i);
 			Object resultObj = result.getField(i);
-			validateResult(expectedObj, resultObj, typeInfo.getTypeAt(i));
+			validateResult(expectedObj, resultObj, fromLogicalTypeToTypeInfo(typeInfo.toRowFieldTypes()[i]));
 		}
 	}
 
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/RowDataTestUtil.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/RowDataTestUtil.java
index 17dc15d532085..801476e39edc1 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/RowDataTestUtil.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/utils/RowDataTestUtil.java
@@ -18,12 +18,11 @@
 
 package org.apache.flink.table.planner.utils;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.writer.BinaryWriter;
-import org.apache.flink.table.runtime.types.InternalSerializers;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.StringUtils;
@@ -40,12 +39,12 @@
  */
 public class RowDataTestUtil {
 
-	public static String rowToString(RowData value, RowDataTypeInfo rowTypeInfo, TimeZone tz) {
+	public static String rowToString(RowData value, InternalTypeInfo<RowData> rowTypeInfo, TimeZone tz) {
 		return rowToString(value, rowTypeInfo, tz, true);
 	}
 
-	public static String rowToString(RowData value, RowDataTypeInfo rowTypeInfo, TimeZone tz, boolean withHeader) {
-		GenericRowData genericRow = toGenericRowDeeply(value, rowTypeInfo.getLogicalTypes());
+	public static String rowToString(RowData value, InternalTypeInfo<RowData> rowTypeInfo, TimeZone tz, boolean withHeader) {
+		GenericRowData genericRow = toGenericRowDeeply(value, rowTypeInfo.toRowFieldTypes());
 		return genericRowToString(genericRow, tz, withHeader);
 	}
 
@@ -102,8 +101,7 @@ public static GenericRowData toGenericRowDeeply(RowData rowData, List<LogicalTyp
 	}
 
 	public static void write(BinaryWriter writer, int pos, Object o, LogicalType type) {
-		BinaryWriter.write(writer, pos, o, type,
-				InternalSerializers.create(type, new ExecutionConfig()));
+		BinaryWriter.write(writer, pos, o, type, InternalSerializers.create(type));
 	}
 
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/agg/batch/BatchAggTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/agg/batch/BatchAggTestBase.scala
index c51035c91bead..64e0d4bed61dd 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/agg/batch/BatchAggTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/codegen/agg/batch/BatchAggTestBase.scala
@@ -26,7 +26,7 @@ import org.apache.flink.table.data.{GenericRowData, RowData, StringData}
 import org.apache.flink.table.planner.codegen.agg.AggTestBase
 import org.apache.flink.table.planner.utils.RowDataTestUtil
 import org.apache.flink.table.runtime.operators.CodeGenOperatorFactory
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical._
 import org.apache.flink.util.function.FunctionWithException
 import org.junit.Assert
@@ -67,7 +67,7 @@ abstract class BatchAggTestBase extends AggTestBase(isBatchMode = true) {
     val testHarness = new OneInputStreamTaskTestHarness[RowData, RowData](
       new FunctionWithException[Environment, OneInputStreamTask[RowData, RowData], Exception] {
         override def apply(t: Environment) = new OneInputStreamTask(t)
-      }, 1, 1, RowDataTypeInfo.of(args._2), RowDataTypeInfo.of(args._3))
+      }, 1, 1, InternalTypeInfo.of(args._2), InternalTypeInfo.of(args._3))
     testHarness.memorySize = 32 * 100 * 1024
 
     testHarness.setupOutputForSingletonOperatorChain()
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/UnionITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/UnionITCase.scala
index cd3303937c0fb..b7a430266fc02 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/UnionITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/batch/sql/UnionITCase.scala
@@ -23,7 +23,7 @@ import org.apache.flink.table.data.StringData.fromString
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase.{binaryRow, row}
 import org.apache.flink.table.planner.runtime.utils.TestData._
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.{BigIntType, IntType, VarCharType}
 
 import org.junit._
@@ -32,14 +32,14 @@ import scala.collection.Seq
 
 class UnionITCase extends BatchTestBase {
 
-  val type6 = new RowDataTypeInfo(
+  val type6 = InternalTypeInfo.ofFields(
     new IntType(), new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH))
 
   val data6 = Seq(
-    binaryRow(type6.getLogicalTypes, 1, 1L, fromString("Hi")),
-    binaryRow(type6.getLogicalTypes, 2, 2L, fromString("Hello")),
-    binaryRow(type6.getLogicalTypes, 3, 2L, fromString("Hello world")),
-    binaryRow(type6.getLogicalTypes, 4, 3L, fromString("Hello world, how are you?"))
+    binaryRow(type6.toRowFieldTypes, 1, 1L, fromString("Hi")),
+    binaryRow(type6.toRowFieldTypes, 2, 2L, fromString("Hello")),
+    binaryRow(type6.toRowFieldTypes, 3, 2L, fromString("Hello world")),
+    binaryRow(type6.toRowFieldTypes, 4, 3L, fromString("Hello world, how are you?"))
   )
 
   @Before
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/GroupAggregateHarnessTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/GroupAggregateHarnessTest.scala
index 4aa0715ff7b13..55bdae1ec7158 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/GroupAggregateHarnessTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/GroupAggregateHarnessTest.scala
@@ -23,7 +23,7 @@ import org.apache.flink.api.scala._
 import org.apache.flink.table.api._
 import org.apache.flink.table.api.bridge.scala._
 import org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl
-import org.apache.flink.table.api.{EnvironmentSettings, Types}
+import org.apache.flink.table.api.EnvironmentSettings
 import org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.StateBackendMode
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor
 import org.apache.flink.table.runtime.util.StreamRecordUtils.binaryRecord
@@ -67,7 +67,10 @@ class GroupAggregateHarnessTest(mode: StateBackendMode) extends HarnessTestBase(
 
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(2), Time.seconds(3))
     val testHarness = createHarnessTester(t1.toRetractStream[Row], "GroupAggregate")
-    val assertor = new RowDataHarnessAssertor(Array( Types.STRING, Types.LONG))
+    val assertor = new RowDataHarnessAssertor(
+      Array(
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/OverWindowHarnessTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/OverWindowHarnessTest.scala
index e05b1d246bce7..8ce2138881251 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/OverWindowHarnessTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/OverWindowHarnessTest.scala
@@ -71,7 +71,14 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(2), Time.seconds(4))
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
@@ -179,7 +186,13 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
 
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
     testHarness.open()
 
     testHarness.setProcessingTime(3)
@@ -289,7 +302,13 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(2), Time.seconds(4))
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
@@ -391,7 +410,12 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
 
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
@@ -507,7 +531,12 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(1), Time.seconds(2))
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
@@ -655,7 +684,12 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(1), Time.seconds(2))
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
@@ -793,7 +827,12 @@ class OverWindowHarnessTest(mode: StateBackendMode) extends HarnessTestBase(mode
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(1), Time.seconds(2))
     val testHarness = createHarnessTester(t1.toAppendStream[Row], "OverAggregate")
     val assertor = new RowDataHarnessAssertor(
-      Array(Types.LONG, Types.STRING, Types.LONG, Types.LONG, Types.LONG))
+      Array(
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.STRING().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType,
+        DataTypes.BIGINT().getLogicalType))
 
     testHarness.open()
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala
index ae050863f44cb..64f4b7acf2ce0 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/harness/TableAggregateHarnessTest.scala
@@ -25,7 +25,7 @@ import org.apache.flink.api.scala._
 import org.apache.flink.table.api._
 import org.apache.flink.table.api.bridge.scala._
 import org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl
-import org.apache.flink.table.api.{EnvironmentSettings, Types}
+import org.apache.flink.table.api.EnvironmentSettings
 import org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.StateBackendMode
 import org.apache.flink.table.planner.utils.{Top3WithMapView, Top3WithRetractInput}
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor
@@ -63,7 +63,11 @@ class TableAggregateHarnessTest(mode: StateBackendMode) extends HarnessTestBase(
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(2), Time.seconds(2))
     val testHarness = createHarnessTester(
       resultTable.toRetractStream[Row], "GroupTableAggregate")
-    val assertor = new RowDataHarnessAssertor(Array(Types.INT, Types.INT, Types.INT))
+    val assertor = new RowDataHarnessAssertor(
+      Array(
+        DataTypes.INT().getLogicalType,
+        DataTypes.INT().getLogicalType,
+        DataTypes.INT().getLogicalType))
 
     testHarness.open()
     val expectedOutput = new ConcurrentLinkedQueue[Object]()
@@ -124,7 +128,10 @@ class TableAggregateHarnessTest(mode: StateBackendMode) extends HarnessTestBase(
     tEnv.getConfig.setIdleStateRetentionTime(Time.seconds(2), Time.seconds(2))
     val testHarness = createHarnessTester(
       resultTable.toRetractStream[Row], "GroupTableAggregate")
-    val assertor = new RowDataHarnessAssertor(Array(Types.INT, Types.INT))
+    val assertor = new RowDataHarnessAssertor(
+      Array(
+        DataTypes.INT().getLogicalType,
+        DataTypes.INT().getLogicalType))
 
     testHarness.open()
     val expectedOutput = new ConcurrentLinkedQueue[Object]()
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/CalcITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/CalcITCase.scala
index 3771acc51de39..a9b20e92b4b05 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/CalcITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/CalcITCase.scala
@@ -29,7 +29,7 @@ import org.apache.flink.table.data.{GenericRowData, RowData}
 import org.apache.flink.table.planner.factories.TestValuesTableFactory
 import org.apache.flink.table.planner.runtime.utils.BatchTestBase.row
 import org.apache.flink.table.planner.runtime.utils._
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.{BigIntType, IntType, VarCharType}
 import org.apache.flink.types.Row
 
@@ -52,7 +52,7 @@ class CalcITCase extends StreamingTestBase {
     val data = List(rowData)
 
     implicit val tpe: TypeInformation[GenericRowData] =
-      new RowDataTypeInfo(
+      InternalTypeInfo.ofFields(
         new IntType(),
         new IntType(),
         new BigIntType()).asInstanceOf[TypeInformation[GenericRowData]]
@@ -62,7 +62,7 @@ class CalcITCase extends StreamingTestBase {
     val t = ds.toTable(tEnv, 'a, 'b, 'c)
     tEnv.registerTable("MyTableRow", t)
 
-    val outputType = new RowDataTypeInfo(
+    val outputType = InternalTypeInfo.ofFields(
       new IntType(),
       new IntType(),
       new BigIntType())
@@ -95,7 +95,7 @@ class CalcITCase extends StreamingTestBase {
     val t = ds.toTable(tEnv, 'a, 'b, 'c)
     tEnv.registerTable("MyTableRow", t)
 
-    val outputType = new RowDataTypeInfo(
+    val outputType = InternalTypeInfo.ofFields(
       new VarCharType(VarCharType.MAX_LENGTH),
       new VarCharType(VarCharType.MAX_LENGTH),
       new IntType())
@@ -121,7 +121,7 @@ class CalcITCase extends StreamingTestBase {
     val data = List(rowData)
 
     implicit val tpe: TypeInformation[GenericRowData] =
-      new RowDataTypeInfo(
+      InternalTypeInfo.ofFields(
         new IntType(),
         new IntType(),
         new BigIntType()).asInstanceOf[TypeInformation[GenericRowData]]
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ValuesITCase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ValuesITCase.scala
index 54fc36bba866d..e8bfe9c21b856 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ValuesITCase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/stream/sql/ValuesITCase.scala
@@ -22,7 +22,7 @@ import org.apache.flink.streaming.api.scala._
 import org.apache.flink.table.api.bridge.scala._
 import org.apache.flink.table.data.RowData
 import org.apache.flink.table.planner.runtime.utils.{StreamingTestBase, TestingAppendRowDataSink}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.{IntType, VarCharType}
 
 import org.junit.Assert._
@@ -35,7 +35,7 @@ class ValuesITCase extends StreamingTestBase {
 
     val sqlQuery = "SELECT * FROM (VALUES (1, 'Bob'), (1, 'Alice')) T(a, b)"
 
-    val outputType = new RowDataTypeInfo(
+    val outputType = InternalTypeInfo.ofFields(
       new IntType(),
       new VarCharType(5))
 
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/BatchTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/BatchTestBase.scala
index 403a24002a381..8b3be2c1c3c45 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/BatchTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/BatchTestBase.scala
@@ -37,7 +37,7 @@ import org.apache.flink.table.planner.plan.stats.FlinkStatistic
 import org.apache.flink.table.planner.plan.utils.FlinkRelOptUtil
 import org.apache.flink.table.planner.runtime.utils.BatchAbstractTestBase.DEFAULT_PARALLELISM
 import org.apache.flink.table.planner.utils.{RowDataTestUtil, TableTestUtil, TestingTableEnvironment}
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.{BigIntType, LogicalType}
 import org.apache.flink.types.Row
 
@@ -415,7 +415,7 @@ class BatchTestBase extends BatchAbstractTestBase {
   }
 
   def newRangeSource(start: Long, end: Long): DataStream[RowData] = {
-    val typeInfo: TypeInformation[RowData] = new RowDataTypeInfo(new BigIntType)
+    val typeInfo: TypeInformation[RowData] = InternalTypeInfo.ofFields(new BigIntType)
     val boundedStream = env.createInput(new RangeInputFormat(start, end), typeInfo)
     boundedStream.setParallelism(1)
     boundedStream
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
index 30af4d2981a0b..a981c5db16534 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamTestSink.scala
@@ -33,7 +33,8 @@ import org.apache.flink.table.api.Types
 import org.apache.flink.table.data.util.DataFormatConverters
 import org.apache.flink.table.data.{GenericRowData, RowData}
 import org.apache.flink.table.planner.utils.RowDataTestUtil
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.sinks._
 import org.apache.flink.table.types.utils.TypeConversions
 import org.apache.flink.types.Row
@@ -140,10 +141,10 @@ final class StringSink[T] extends AbstractExactlyOnceSink[T]() {
 }
 
 final class TestingAppendRowDataSink(
-    rowTypeInfo: RowDataTypeInfo, tz: TimeZone)
+    rowTypeInfo: InternalTypeInfo[RowData], tz: TimeZone)
   extends AbstractExactlyOnceSink[RowData] {
 
-  def this(rowTypeInfo: RowDataTypeInfo) {
+  def this(rowTypeInfo: InternalTypeInfo[RowData]) {
     this(rowTypeInfo, TimeZone.getTimeZone("UTC"))
   }
 
@@ -297,8 +298,8 @@ final class TestingUpsertTableSink(val keys: Array[Int], val tz: TimeZone)
   }
 
   override def getRecordType: TypeInformation[RowData] =
-    new RowDataTypeInfo(
-      fTypes.map(TypeConversions.fromLegacyInfoToDataType(_).getLogicalType),
+    InternalTypeInfo.ofFields(
+      fTypes.map(fromTypeInfoToLogicalType),
       fNames)
 
   override def getFieldNames: Array[String] = fNames
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamingWithStateTestBase.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamingWithStateTestBase.scala
index 3182baaf48da6..e4dbc39acf314 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamingWithStateTestBase.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/runtime/utils/StreamingWithStateTestBase.scala
@@ -34,7 +34,7 @@ import org.apache.flink.table.data.{RowData, StringData}
 import org.apache.flink.table.planner.runtime.utils.StreamingWithStateTestBase.{HEAP_BACKEND, ROCKSDB_BACKEND, StateBackendMode}
 import org.apache.flink.table.planner.utils.TableTestUtil
 import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.RowType
 
 import org.junit.runners.Parameterized
@@ -109,7 +109,7 @@ class StreamingWithStateTestBase(state: StateBackendMode) extends StreamingTestB
         result += reuse.copy()
       case _ => throw new UnsupportedOperationException
     }
-    val newTypeInfo = RowDataTypeInfo.of(
+    val newTypeInfo = InternalTypeInfo.of(
       TypeInfoLogicalTypeConverter.fromTypeInfoToLogicalType(typeInfo).asInstanceOf[RowType])
     failingDataSource(result)(newTypeInfo.asInstanceOf[TypeInformation[RowData]])
   }
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/UserDefinedTableAggFunctions.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/UserDefinedTableAggFunctions.scala
index 2ec194553d8ab..c8300954d51de 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/UserDefinedTableAggFunctions.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/utils/UserDefinedTableAggFunctions.scala
@@ -24,7 +24,7 @@ import org.apache.flink.table.api.Types
 import org.apache.flink.table.api.dataview.MapView
 import org.apache.flink.table.data.GenericRowData
 import org.apache.flink.table.functions.TableAggregateFunction
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo
 import org.apache.flink.table.types.logical.IntType
 import org.apache.flink.util.Collector
 
@@ -265,7 +265,7 @@ class TableAggSum extends TableAggregateFunction[JInt, GenericRowData] {
   }
 
   override def getAccumulatorType: TypeInformation[GenericRowData] = {
-    new RowDataTypeInfo(new IntType()).asInstanceOf[TypeInformation[GenericRowData]]
+    InternalTypeInfo.ofFields(new IntType()).asInstanceOf[TypeInformation[GenericRowData]]
   }
 }
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/util/DataFormatConverters.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/util/DataFormatConverters.java
index acef60d9bbc6d..037fa451610ce 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/util/DataFormatConverters.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/util/DataFormatConverters.java
@@ -45,10 +45,11 @@
 import org.apache.flink.table.data.writer.BinaryArrayWriter;
 import org.apache.flink.table.data.writer.BinaryWriter;
 import org.apache.flink.table.runtime.functions.SqlDateTimeUtils;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.runtime.types.LogicalTypeDataTypeConverter;
 import org.apache.flink.table.runtime.typeutils.BigDecimalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyInstantTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyLocalDateTimeTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyTimestampTypeInfo;
@@ -88,6 +89,7 @@
 import scala.Product;
 
 import static org.apache.flink.table.runtime.types.TypeInfoDataTypeConverter.fromDataTypeToTypeInfo;
+import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.getFieldCount;
 import static org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;
 
 /**
@@ -249,7 +251,15 @@ public static DataFormatConverter getConverterForDataType(DataType originDataTyp
 						DataTypes.INT().bridgedTo(Integer.class));
 			case ROW:
 			case STRUCTURED_TYPE:
-				CompositeType compositeType = (CompositeType) fromDataTypeToTypeInfo(dataType);
+				TypeInformation<?> asTypeInfo = fromDataTypeToTypeInfo(dataType);
+				if (asTypeInfo instanceof InternalTypeInfo && clazz == RowData.class) {
+					LogicalType realLogicalType = ((InternalTypeInfo<?>) asTypeInfo).toLogicalType();
+					return new RowDataConverter(getFieldCount(realLogicalType));
+				}
+
+				// legacy
+
+				CompositeType compositeType = (CompositeType) asTypeInfo;
 				DataType[] fieldTypes = Stream.iterate(0, x -> x + 1).limit(compositeType.getArity())
 						.map((Function<Integer, TypeInformation>) compositeType::getTypeAt)
 						.map(TypeConversions::fromLegacyInfoToDataType).toArray(DataType[]::new);
@@ -1111,7 +1121,7 @@ public ObjectArrayConverter(DataType elementType) {
 			this.elementType = LogicalTypeDataTypeConverter.fromDataTypeToLogicalType(elementType);
 			this.elementConverter = DataFormatConverters.getConverterForDataType(elementType);
 			this.elementSize = BinaryArrayData.calculateFixLengthPartSize(this.elementType);
-			this.eleSer = InternalSerializers.create(this.elementType, new ExecutionConfig());
+			this.eleSer = InternalSerializers.create(this.elementType);
 			this.isEleIndentity = elementConverter instanceof IdentityConverter;
 		}
 
@@ -1238,8 +1248,8 @@ public MapConverter(DataType keyTypeInfo, DataType valueTypeInfo) {
 			this.valueComponentClass = valueTypeInfo.getConversionClass();
 			this.isKeyValueIndentity = keyConverter instanceof IdentityConverter &&
 					valueConverter instanceof IdentityConverter;
-			this.keySer = InternalSerializers.create(this.keyType, new ExecutionConfig());
-			this.valueSer = InternalSerializers.create(this.valueType, new ExecutionConfig());
+			this.keySer = InternalSerializers.create(this.keyType);
+			this.valueSer = InternalSerializers.create(this.valueType);
 		}
 
 		@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/writer/BinaryWriter.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/writer/BinaryWriter.java
index cae12324895d4..d7c0428874ed9 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/writer/BinaryWriter.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/data/writer/BinaryWriter.java
@@ -26,8 +26,8 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.data.TimestampData;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.runtime.typeutils.ArrayDataSerializer;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
 import org.apache.flink.table.runtime.typeutils.MapDataSerializer;
 import org.apache.flink.table.runtime.typeutils.RawValueDataSerializer;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
@@ -210,21 +210,21 @@ static ValueSetter createValueSetter(LogicalType elementType) {
 			case TIMESTAMP_WITH_TIME_ZONE:
 				throw new UnsupportedOperationException();
 			case ARRAY:
-				final ArrayDataSerializer arraySerializer = (ArrayDataSerializer) InternalSerializers.create(elementType);
-				return (writer, pos, value) -> writer.writeArray(pos, (ArrayData) value, arraySerializer);
+				final TypeSerializer<ArrayData> arraySerializer = InternalSerializers.create(elementType);
+				return (writer, pos, value) -> writer.writeArray(pos, (ArrayData) value, (ArrayDataSerializer) arraySerializer);
 			case MULTISET:
 			case MAP:
-				final MapDataSerializer mapSerializer = (MapDataSerializer) InternalSerializers.create(elementType);
-				return (writer, pos, value) -> writer.writeMap(pos, (MapData) value, mapSerializer);
+				final TypeSerializer<MapData> mapSerializer = InternalSerializers.create(elementType);
+				return (writer, pos, value) -> writer.writeMap(pos, (MapData) value, (MapDataSerializer) mapSerializer);
 			case ROW:
 			case STRUCTURED_TYPE:
-				final RowDataSerializer rowSerializer = (RowDataSerializer) InternalSerializers.create(elementType);
-				return (writer, pos, value) -> writer.writeRow(pos, (RowData) value, rowSerializer);
+				final TypeSerializer<RowData> rowSerializer = InternalSerializers.create(elementType);
+				return (writer, pos, value) -> writer.writeRow(pos, (RowData) value, (RowDataSerializer) rowSerializer);
 			case DISTINCT_TYPE:
 				return createValueSetter(((DistinctType) elementType).getSourceType());
 			case RAW:
-				final RawValueDataSerializer<?> rawSerializer = (RawValueDataSerializer<?>) InternalSerializers.create(elementType);
-				return (writer, pos, value) -> writer.writeRawValue(pos, (RawValueData<?>) value, rawSerializer);
+				final TypeSerializer<?> rawSerializer = InternalSerializers.create(elementType);
+				return (writer, pos, value) -> writer.writeRawValue(pos, (RawValueData<?>) value, (RawValueDataSerializer<?>) rawSerializer);
 			case NULL:
 			case SYMBOL:
 			case UNRESOLVED:
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java
index d8b86336ee647..e8fc29a9914e2 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/filesystem/FileSystemLookupFunction.java
@@ -30,7 +30,7 @@
 import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;
 import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.functions.TableFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.Row;
@@ -105,7 +105,7 @@ public FileSystemLookupFunction(
 
 	@Override
 	public TypeInformation<RowData> getResultType() {
-		return new RowDataTypeInfo(
+		return InternalTypeInfo.ofFields(
 				Arrays.stream(producedTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new),
 				producedNames);
 	}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/sink/SinkRuntimeProviderContext.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/sink/SinkRuntimeProviderContext.java
index 3e4ed5ba11226..2ece47f72857c 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/sink/SinkRuntimeProviderContext.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/sink/SinkRuntimeProviderContext.java
@@ -22,7 +22,7 @@
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.table.connector.sink.DynamicTableSink;
 import org.apache.flink.table.data.conversion.DataStructureConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 
 import static org.apache.flink.table.types.utils.DataTypeUtils.validateOutputDataType;
@@ -47,7 +47,7 @@ public boolean isBounded() {
 	@Override
 	public TypeInformation<?> createTypeInformation(DataType consumedDataType) {
 		validateOutputDataType(consumedDataType);
-		return WrapperTypeInfo.of(consumedDataType.getLogicalType());
+		return InternalTypeInfo.of(consumedDataType.getLogicalType());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/LookupRuntimeProviderContext.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/LookupRuntimeProviderContext.java
index 55aabbc37bc9d..a6cd22babd116 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/LookupRuntimeProviderContext.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/LookupRuntimeProviderContext.java
@@ -23,7 +23,7 @@
 import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter;
 import org.apache.flink.table.connector.source.LookupTableSource;
 import org.apache.flink.table.data.conversion.DataStructureConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 
 import static org.apache.flink.table.types.utils.DataTypeUtils.validateInputDataType;
@@ -48,7 +48,7 @@ public int[][] getKeys() {
 	@Override
 	public TypeInformation<?> createTypeInformation(DataType producedDataType) {
 		validateInputDataType(producedDataType);
-		return WrapperTypeInfo.of(producedDataType.getLogicalType());
+		return InternalTypeInfo.of(producedDataType.getLogicalType());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/ScanRuntimeProviderContext.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/ScanRuntimeProviderContext.java
index 6cb092db75747..afc54425633dd 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/ScanRuntimeProviderContext.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/connector/source/ScanRuntimeProviderContext.java
@@ -23,7 +23,7 @@
 import org.apache.flink.table.connector.source.DynamicTableSource.DataStructureConverter;
 import org.apache.flink.table.connector.source.ScanTableSource;
 import org.apache.flink.table.data.conversion.DataStructureConverters;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.DataType;
 
 import static org.apache.flink.table.types.utils.DataTypeUtils.validateInputDataType;
@@ -39,7 +39,7 @@ public final class ScanRuntimeProviderContext implements ScanTableSource.ScanCon
 	@Override
 	public TypeInformation<?> createTypeInformation(DataType producedDataType) {
 		validateInputDataType(producedDataType);
-		return WrapperTypeInfo.of(producedDataType.getLogicalType());
+		return InternalTypeInfo.of(producedDataType.getLogicalType());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/BinaryRowDataKeySelector.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/BinaryRowDataKeySelector.java
index 3ed8066e3709f..e2d6224b364bf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/BinaryRowDataKeySelector.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/BinaryRowDataKeySelector.java
@@ -22,7 +22,7 @@
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.Projection;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * A KeySelector which will extract key from RowData. The key type is BinaryRowData.
@@ -31,11 +31,11 @@ public class BinaryRowDataKeySelector implements RowDataKeySelector {
 
 	private static final long serialVersionUID = 5375355285015381919L;
 
-	private final RowDataTypeInfo keyRowType;
+	private final InternalTypeInfo<RowData> keyRowType;
 	private final GeneratedProjection generatedProjection;
 	private transient Projection<RowData, BinaryRowData> projection;
 
-	public BinaryRowDataKeySelector(RowDataTypeInfo keyRowType, GeneratedProjection generatedProjection) {
+	public BinaryRowDataKeySelector(InternalTypeInfo<RowData> keyRowType, GeneratedProjection generatedProjection) {
 		this.keyRowType = keyRowType;
 		this.generatedProjection = generatedProjection;
 	}
@@ -51,7 +51,7 @@ public RowData getKey(RowData value) throws Exception {
 	}
 
 	@Override
-	public RowDataTypeInfo getProducedType() {
+	public InternalTypeInfo<RowData> getProducedType() {
 		return keyRowType;
 	}
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/EmptyRowDataKeySelector.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/EmptyRowDataKeySelector.java
index ebb6237344e8f..51bbbfc40128d 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/EmptyRowDataKeySelector.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/EmptyRowDataKeySelector.java
@@ -20,7 +20,7 @@
 
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryRowDataUtil;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * A utility class which key is always empty no matter what the input row is.
@@ -31,7 +31,7 @@ public class EmptyRowDataKeySelector implements RowDataKeySelector {
 
 	private static final long serialVersionUID = -2079386198687082032L;
 
-	private final RowDataTypeInfo returnType = new RowDataTypeInfo();
+	private final InternalTypeInfo<RowData> returnType = InternalTypeInfo.ofFields();
 
 	@Override
 	public RowData getKey(RowData value) throws Exception {
@@ -39,7 +39,7 @@ public RowData getKey(RowData value) throws Exception {
 	}
 
 	@Override
-	public RowDataTypeInfo getProducedType() {
+	public InternalTypeInfo<RowData> getProducedType() {
 		return returnType;
 	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/RowDataKeySelector.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/RowDataKeySelector.java
index 7b1aed7c4e129..3927bda0347f3 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/RowDataKeySelector.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/keyselector/RowDataKeySelector.java
@@ -21,13 +21,13 @@
 import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * RowDataKeySelector takes an RowData and extracts the deterministic key for the RowData.
  */
 public interface RowDataKeySelector extends KeySelector<RowData, RowData>, ResultTypeQueryable<RowData> {
 
-	RowDataTypeInfo getProducedType();
+	InternalTypeInfo<RowData> getProducedType();
 
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java
index 2e03357d0c90c..6ddd7c112e17b 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupAggFunction.java
@@ -29,7 +29,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser;
 import org.apache.flink.table.runtime.generated.RecordEqualiser;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.Collector;
@@ -121,7 +121,7 @@ public void open(Configuration parameters) throws Exception {
 		// instantiate equaliser
 		equaliser = genRecordEqualiser.newInstance(getRuntimeContext().getUserCodeClassLoader());
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accDesc = new ValueStateDescriptor<>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(accDesc);
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java
index 41eb6d46812fc..bfb61706de4c8 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/GroupTableAggFunction.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.GeneratedTableAggsHandleFunction;
 import org.apache.flink.table.runtime.generated.TableAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
@@ -98,7 +98,7 @@ public void open(Configuration parameters) throws Exception {
 		function = genAggsHandler.newInstance(getRuntimeContext().getUserCodeClassLoader());
 		function.open(new PerKeyStateDataViewStore(getRuntimeContext()));
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accDesc = new ValueStateDescriptor<>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(accDesc);
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGlobalGroupAggFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGlobalGroupAggFunction.java
index 5c90dae771d0b..d1aedd0787582 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGlobalGroupAggFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGlobalGroupAggFunction.java
@@ -29,7 +29,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser;
 import org.apache.flink.table.runtime.generated.RecordEqualiser;
 import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.Collector;
@@ -131,7 +131,7 @@ public void open(ExecutionContext ctx) throws Exception {
 
 		equaliser = genRecordEqualiser.newInstance(ctx.getRuntimeContext().getUserCodeClassLoader());
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accDesc = new ValueStateDescriptor<>("accState", accTypeInfo);
 		accState = ctx.getRuntimeContext().getState(accDesc);
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGroupAggFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGroupAggFunction.java
index be2111631acb1..f204db95fc025 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGroupAggFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/aggregate/MiniBatchGroupAggFunction.java
@@ -30,8 +30,8 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser;
 import org.apache.flink.table.runtime.generated.RecordEqualiser;
 import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;
-import org.apache.flink.table.runtime.types.InternalSerializers;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.RowKind;
@@ -137,13 +137,11 @@ public void open(ExecutionContext ctx) throws Exception {
 		// instantiate equaliser
 		equaliser = genRecordEqualiser.newInstance(ctx.getRuntimeContext().getUserCodeClassLoader());
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accDesc = new ValueStateDescriptor<>("accState", accTypeInfo);
 		accState = ctx.getRuntimeContext().getState(accDesc);
 
-		//noinspection unchecked
-		inputRowSerializer = (TypeSerializer) InternalSerializers.create(
-				inputType, ctx.getRuntimeContext().getExecutionConfig());
+		inputRowSerializer = InternalSerializers.create(inputType);
 
 		resultRow = new JoinedRowData();
 	}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateKeepLastRowFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateKeepLastRowFunction.java
index 01e315187575c..42cedc3d7a350 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateKeepLastRowFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateKeepLastRowFunction.java
@@ -24,7 +24,7 @@
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.functions.KeyedProcessFunction;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.Collector;
 
 import static org.apache.flink.table.runtime.operators.deduplicate.DeduplicateFunctionHelper.processLastRow;
@@ -37,7 +37,7 @@
 		extends KeyedProcessFunction<RowData, RowData, RowData> {
 
 	private static final long serialVersionUID = -291348892087180350L;
-	private final RowDataTypeInfo rowTypeInfo;
+	private final InternalTypeInfo<RowData> rowTypeInfo;
 	private final boolean generateUpdateBefore;
 	private final boolean generateInsert;
 
@@ -47,7 +47,7 @@
 
 	public DeduplicateKeepLastRowFunction(
 			long minRetentionTime,
-			RowDataTypeInfo rowTypeInfo,
+			InternalTypeInfo<RowData> rowTypeInfo,
 			boolean generateUpdateBefore,
 			boolean generateInsert) {
 		this.minRetentionTime = minRetentionTime;
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateKeepLastRowFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateKeepLastRowFunction.java
index a2fb39908fd08..5185480847c5f 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateKeepLastRowFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/deduplicate/MiniBatchDeduplicateKeepLastRowFunction.java
@@ -25,7 +25,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.context.ExecutionContext;
 import org.apache.flink.table.runtime.operators.bundle.MapBundleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.Collector;
 
 import javax.annotation.Nullable;
@@ -43,7 +43,7 @@
 
 	private static final long serialVersionUID = -8981813609115029119L;
 
-	private final RowDataTypeInfo rowTypeInfo;
+	private final InternalTypeInfo<RowData> rowTypeInfo;
 	private final boolean generateUpdateBefore;
 	private final boolean generateInsert;
 	private final TypeSerializer<RowData> typeSerializer;
@@ -52,7 +52,7 @@
 	private ValueState<RowData> state;
 
 	public MiniBatchDeduplicateKeepLastRowFunction(
-			RowDataTypeInfo rowTypeInfo,
+			InternalTypeInfo<RowData> rowTypeInfo,
 			boolean generateUpdateBefore,
 			boolean generateInsert,
 			TypeSerializer<RowData> typeSerializer,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoin.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoin.java
index 01d5895f3f842..9bb1eaa5f435d 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoin.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoin.java
@@ -22,7 +22,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * The function to execute processing time interval stream inner-join.
@@ -35,8 +35,8 @@ public ProcTimeIntervalJoin(
 			FlinkJoinType joinType,
 			long leftLowerBound,
 			long leftUpperBound,
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedFunction<FlatJoinFunction<RowData, RowData, RowData>> genJoinFunc) {
 		super(joinType, leftLowerBound, leftUpperBound, 0L, leftType, rightType, genJoinFunc);
 	}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoin.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoin.java
index c2b7a90f85e8c..6bf424a988b3c 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoin.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoin.java
@@ -22,7 +22,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * The function to execute row(event) time interval stream inner-join.
@@ -39,8 +39,8 @@ public RowTimeIntervalJoin(
 			long leftLowerBound,
 			long leftUpperBound,
 			long allowedLateness,
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedFunction<FlatJoinFunction<RowData, RowData, RowData>> genJoinFunc,
 			int leftTimeIdx,
 			int rightTimeIdx) {
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalJoin.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalJoin.java
index 270650e143e14..a516c02a1c9c1 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalJoin.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalJoin.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType;
 import org.apache.flink.table.runtime.operators.join.OuterJoinPaddingUtil;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.Collector;
 
 import org.slf4j.Logger;
@@ -60,8 +60,8 @@ abstract class TimeIntervalJoin extends KeyedCoProcessFunction<RowData, RowData,
 	// Minimum interval by which state is cleaned up
 	private final long minCleanUpInterval;
 	protected final long allowedLateness;
-	private final RowDataTypeInfo leftType;
-	private final RowDataTypeInfo rightType;
+	private final InternalTypeInfo<RowData> leftType;
+	private final InternalTypeInfo<RowData> rightType;
 	private GeneratedFunction<FlatJoinFunction<RowData, RowData, RowData>> genJoinFunc;
 	private transient OuterJoinPaddingUtil paddingUtil;
 
@@ -93,8 +93,8 @@ abstract class TimeIntervalJoin extends KeyedCoProcessFunction<RowData, RowData,
 			long leftLowerBound,
 			long leftUpperBound,
 			long allowedLateness,
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedFunction<FlatJoinFunction<RowData, RowData, RowData>> genJoinFunc) {
 		this.joinType = joinType;
 		this.leftRelativeSize = -leftLowerBound;
@@ -146,7 +146,7 @@ public void open(Configuration parameters) throws Exception {
 				Long.class);
 		rightTimerState = getRuntimeContext().getState(rightValueStateDescriptor);
 
-		paddingUtil = new OuterJoinPaddingUtil(leftType.getArity(), rightType.getArity());
+		paddingUtil = new OuterJoinPaddingUtil(leftType.toRowSize(), rightType.toRowSize());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinRunner.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinRunner.java
index dc123701ef73d..c6d0f06b30ddf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinRunner.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinRunner.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.runtime.collector.TableFunctionResultFuture;
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
 import org.apache.flink.table.runtime.generated.GeneratedResultFuture;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.types.Row;
 
 import javax.annotation.Nullable;
@@ -58,7 +58,7 @@ public class AsyncLookupJoinRunner extends RichAsyncFunction<RowData, RowData> {
 	private final boolean isLeftOuterJoin;
 	private final int asyncBufferCapacity;
 	private final TypeInformation<?> fetcherReturnType;
-	private final RowDataTypeInfo rightRowTypeInfo;
+	private final InternalTypeInfo<RowData> rightRowTypeInfo;
 
 	private transient AsyncFunction<RowData, Object> fetcher;
 
@@ -79,7 +79,7 @@ public AsyncLookupJoinRunner(
 			GeneratedFunction<AsyncFunction<RowData, Object>> generatedFetcher,
 			GeneratedResultFuture<TableFunctionResultFuture<RowData>> generatedResultFuture,
 			TypeInformation<?> fetcherReturnType,
-			RowDataTypeInfo rightRowTypeInfo,
+			InternalTypeInfo<RowData> rightRowTypeInfo,
 			boolean isLeftOuterJoin,
 			int asyncBufferCapacity) {
 		this.generatedFetcher = generatedFetcher;
@@ -105,11 +105,11 @@ public void open(Configuration parameters) throws Exception {
 		if (fetcherReturnType instanceof RowTypeInfo) {
 			rowConverter = (DataFormatConverters.RowConverter) DataFormatConverters.getConverterForDataType(
 					fromLegacyInfoToDataType(fetcherReturnType));
-		} else if (fetcherReturnType instanceof RowDataTypeInfo) {
+		} else if (fetcherReturnType instanceof InternalTypeInfo) {
 			rowConverter = null;
 		} else {
 			throw new IllegalStateException("This should never happen, " +
-				"currently fetcherReturnType can only be RowDataTypeInfo or RowTypeInfo");
+				"currently fetcherReturnType can only be InternalTypeInfo<RowData> or RowTypeInfo");
 		}
 
 		// asyncBufferCapacity + 1 as the queue size in order to avoid
@@ -122,7 +122,7 @@ public void open(Configuration parameters) throws Exception {
 				createFetcherResultFuture(parameters),
 				rowConverter,
 				isLeftOuterJoin,
-				rightRowTypeInfo.getArity());
+				rightRowTypeInfo.toRowSize());
 			// add will throw exception immediately if the queue is full which should never happen
 			resultFutureBuffer.add(rf);
 			allResultFutures.add(rf);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinWithCalcRunner.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinWithCalcRunner.java
index 3952880d49268..0129e382a1c25 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinWithCalcRunner.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/lookup/AsyncLookupJoinWithCalcRunner.java
@@ -29,7 +29,7 @@
 import org.apache.flink.table.runtime.collector.TableFunctionResultFuture;
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
 import org.apache.flink.table.runtime.generated.GeneratedResultFuture;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.Collector;
 
 import java.util.ArrayList;
@@ -43,7 +43,7 @@ public class AsyncLookupJoinWithCalcRunner extends AsyncLookupJoinRunner {
 	private static final long serialVersionUID = 8758670006385551407L;
 
 	private final GeneratedFunction<FlatMapFunction<RowData, RowData>> generatedCalc;
-	private final RowDataTypeInfo rightRowTypeInfo;
+	private final InternalTypeInfo<RowData> rightRowTypeInfo;
 	private transient TypeSerializer<RowData> rightSerializer;
 
 	public AsyncLookupJoinWithCalcRunner(
@@ -51,7 +51,7 @@ public AsyncLookupJoinWithCalcRunner(
 			GeneratedFunction<FlatMapFunction<RowData, RowData>> generatedCalc,
 			GeneratedResultFuture<TableFunctionResultFuture<RowData>> generatedResultFuture,
 			TypeInformation<?> fetcherReturnType,
-			RowDataTypeInfo rightRowTypeInfo,
+			InternalTypeInfo<RowData> rightRowTypeInfo,
 			boolean isLeftOuterJoin,
 			int asyncBufferCapacity) {
 		super(generatedFetcher, generatedResultFuture, fetcherReturnType,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/AbstractStreamingJoinOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/AbstractStreamingJoinOperator.java
index 3fb9f2a3a7f4a..b2717465f397e 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/AbstractStreamingJoinOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/AbstractStreamingJoinOperator.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.runtime.operators.join.stream.state.JoinInputSideSpec;
 import org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateView;
 import org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateView;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.IterableIterator;
 
 import java.util.ArrayList;
@@ -54,8 +54,8 @@ public abstract class AbstractStreamingJoinOperator extends AbstractStreamOperat
 	protected static final String RIGHT_RECORDS_STATE_NAME = "right-records";
 
 	private final GeneratedJoinCondition generatedJoinCondition;
-	protected final RowDataTypeInfo leftType;
-	protected final RowDataTypeInfo rightType;
+	protected final InternalTypeInfo<RowData> leftType;
+	protected final InternalTypeInfo<RowData> rightType;
 
 	protected final JoinInputSideSpec leftInputSideSpec;
 	protected final JoinInputSideSpec rightInputSideSpec;
@@ -81,8 +81,8 @@ public abstract class AbstractStreamingJoinOperator extends AbstractStreamOperat
 	protected transient TimestampedCollector<RowData> collector;
 
 	public AbstractStreamingJoinOperator(
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedJoinCondition generatedJoinCondition,
 			JoinInputSideSpec leftInputSideSpec,
 			JoinInputSideSpec rightInputSideSpec,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingJoinOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingJoinOperator.java
index 5c6ef6469c3ee..59cd83501c888 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingJoinOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingJoinOperator.java
@@ -29,7 +29,7 @@
 import org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateViews;
 import org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateView;
 import org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateViews;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.types.RowKind;
 
 /**
@@ -54,8 +54,8 @@ public class StreamingJoinOperator extends AbstractStreamingJoinOperator {
 	private transient JoinRecordStateView rightRecordStateView;
 
 	public StreamingJoinOperator(
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedJoinCondition generatedJoinCondition,
 			JoinInputSideSpec leftInputSideSpec,
 			JoinInputSideSpec rightInputSideSpec,
@@ -73,8 +73,8 @@ public void open() throws Exception {
 		super.open();
 
 		this.outRow = new JoinedRowData();
-		this.leftNullRow = new GenericRowData(leftType.getArity());
-		this.rightNullRow = new GenericRowData(rightType.getArity());
+		this.leftNullRow = new GenericRowData(leftType.toRowSize());
+		this.rightNullRow = new GenericRowData(rightType.toRowSize());
 
 		// initialize states
 		if (leftIsOuter) {
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingSemiAntiJoinOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingSemiAntiJoinOperator.java
index b84e1d5e69e04..69f56073cfe45 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingSemiAntiJoinOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/StreamingSemiAntiJoinOperator.java
@@ -27,7 +27,7 @@
 import org.apache.flink.table.runtime.operators.join.stream.state.JoinRecordStateViews;
 import org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateView;
 import org.apache.flink.table.runtime.operators.join.stream.state.OuterJoinRecordStateViews;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.types.RowKind;
 
 /**
@@ -47,8 +47,8 @@ public class StreamingSemiAntiJoinOperator extends AbstractStreamingJoinOperator
 
 	public StreamingSemiAntiJoinOperator(
 			boolean isAntiJoin,
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedJoinCondition generatedJoinCondition,
 			JoinInputSideSpec leftInputSideSpec,
 			JoinInputSideSpec rightInputSideSpec,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinInputSideSpec.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinInputSideSpec.java
index 0b6751b6105c5..4e9c5debde37f 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinInputSideSpec.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinInputSideSpec.java
@@ -21,7 +21,7 @@
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 import javax.annotation.Nullable;
 
@@ -38,12 +38,12 @@ public class JoinInputSideSpec implements Serializable {
 
 	private final boolean inputSideHasUniqueKey;
 	private final boolean joinKeyContainsUniqueKey;
-	@Nullable private final RowDataTypeInfo uniqueKeyType;
+	@Nullable private final InternalTypeInfo<RowData> uniqueKeyType;
 	@Nullable private final KeySelector<RowData, RowData> uniqueKeySelector;
 
 	private JoinInputSideSpec(
 			boolean joinKeyContainsUniqueKey,
-			@Nullable RowDataTypeInfo uniqueKeyType,
+			@Nullable InternalTypeInfo<RowData> uniqueKeyType,
 			@Nullable KeySelector<RowData, RowData> uniqueKeySelector) {
 		this.inputSideHasUniqueKey = uniqueKeyType != null && uniqueKeySelector != null;
 		this.joinKeyContainsUniqueKey = joinKeyContainsUniqueKey;
@@ -70,7 +70,7 @@ public boolean joinKeyContainsUniqueKey() {
 	 * Returns null if the input hasn't unique key.
 	 */
 	@Nullable
-	public RowDataTypeInfo getUniqueKeyType() {
+	public InternalTypeInfo<RowData> getUniqueKeyType() {
 		return uniqueKeyType;
 	}
 
@@ -88,7 +88,7 @@ public KeySelector<RowData, RowData> getUniqueKeySelector() {
 	 * @param uniqueKeyType type information of the unique key
 	 * @param uniqueKeySelector key selector to extract unique key from the input row
 	 */
-	public static JoinInputSideSpec withUniqueKey(RowDataTypeInfo uniqueKeyType, KeySelector<RowData, RowData> uniqueKeySelector) {
+	public static JoinInputSideSpec withUniqueKey(InternalTypeInfo<RowData> uniqueKeyType, KeySelector<RowData, RowData> uniqueKeySelector) {
 		checkNotNull(uniqueKeyType);
 		checkNotNull(uniqueKeySelector);
 		return new JoinInputSideSpec(false, uniqueKeyType, uniqueKeySelector);
@@ -100,7 +100,7 @@ public static JoinInputSideSpec withUniqueKey(RowDataTypeInfo uniqueKeyType, Key
 	 * @param uniqueKeyType type information of the unique key
 	 * @param uniqueKeySelector key selector to extract unique key from the input row
 	 */
-	public static JoinInputSideSpec withUniqueKeyContainedByJoinKey(RowDataTypeInfo uniqueKeyType, KeySelector<RowData, RowData> uniqueKeySelector) {
+	public static JoinInputSideSpec withUniqueKeyContainedByJoinKey(InternalTypeInfo<RowData> uniqueKeyType, KeySelector<RowData, RowData> uniqueKeySelector) {
 		checkNotNull(uniqueKeyType);
 		checkNotNull(uniqueKeySelector);
 		return new JoinInputSideSpec(true, uniqueKeyType, uniqueKeySelector);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinRecordStateViews.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinRecordStateViews.java
index 68173deb582bc..d16465578d18a 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinRecordStateViews.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/JoinRecordStateViews.java
@@ -27,7 +27,7 @@
 import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.java.functions.KeySelector;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.IterableIterator;
 
 import java.util.ArrayList;
@@ -50,7 +50,7 @@ public static JoinRecordStateView create(
 			RuntimeContext ctx,
 			String stateName,
 			JoinInputSideSpec inputSideSpec,
-			RowDataTypeInfo recordType,
+			InternalTypeInfo<RowData> recordType,
 			long retentionTime) {
 		StateTtlConfig ttlConfig = createTtlConfig(retentionTime);
 		if (inputSideSpec.hasUniqueKey()) {
@@ -80,7 +80,7 @@ private static final class JoinKeyContainsUniqueKey implements JoinRecordStateVi
 		private JoinKeyContainsUniqueKey(
 				RuntimeContext ctx,
 				String stateName,
-				RowDataTypeInfo recordType,
+				InternalTypeInfo<RowData> recordType,
 				StateTtlConfig ttlConfig) {
 			ValueStateDescriptor<RowData> recordStateDesc = new ValueStateDescriptor<>(
 				stateName,
@@ -123,8 +123,8 @@ private static final class InputSideHasUniqueKey implements JoinRecordStateView
 		private InputSideHasUniqueKey(
 				RuntimeContext ctx,
 				String stateName,
-				RowDataTypeInfo recordType,
-				RowDataTypeInfo uniqueKeyType,
+				InternalTypeInfo<RowData> recordType,
+				InternalTypeInfo<RowData> uniqueKeyType,
 				KeySelector<RowData, RowData> uniqueKeySelector,
 				StateTtlConfig ttlConfig) {
 			checkNotNull(uniqueKeyType);
@@ -165,7 +165,7 @@ private static final class InputSideHasNoUniqueKey implements JoinRecordStateVie
 		private InputSideHasNoUniqueKey(
 				RuntimeContext ctx,
 				String stateName,
-				RowDataTypeInfo recordType,
+				InternalTypeInfo<RowData> recordType,
 				StateTtlConfig ttlConfig) {
 			MapStateDescriptor<RowData, Integer> recordStateDesc = new MapStateDescriptor<>(
 				stateName,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/OuterJoinRecordStateViews.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/OuterJoinRecordStateViews.java
index 1c777bf342ad7..d429e679f78c1 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/OuterJoinRecordStateViews.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/stream/state/OuterJoinRecordStateViews.java
@@ -29,7 +29,7 @@
 import org.apache.flink.api.java.tuple.Tuple2;
 import org.apache.flink.api.java.typeutils.TupleTypeInfo;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.IterableIterator;
 
 import java.util.ArrayList;
@@ -52,7 +52,7 @@ public static OuterJoinRecordStateView create(
 			RuntimeContext ctx,
 			String stateName,
 			JoinInputSideSpec inputSideSpec,
-			RowDataTypeInfo recordType,
+			InternalTypeInfo<RowData> recordType,
 			long retentionTime) {
 		StateTtlConfig ttlConfig = createTtlConfig(retentionTime);
 		if (inputSideSpec.hasUniqueKey()) {
@@ -80,7 +80,7 @@ private static final class JoinKeyContainsUniqueKey implements OuterJoinRecordSt
 		private final List<RowData> reusedRecordList;
 		private final List<Tuple2<RowData, Integer>> reusedTupleList;
 
-		private JoinKeyContainsUniqueKey(RuntimeContext ctx, String stateName, RowDataTypeInfo recordType, StateTtlConfig ttlConfig) {
+		private JoinKeyContainsUniqueKey(RuntimeContext ctx, String stateName, InternalTypeInfo<RowData> recordType, StateTtlConfig ttlConfig) {
 			TupleTypeInfo<Tuple2<RowData, Integer>> valueTypeInfo = new TupleTypeInfo<>(recordType, Types.INT);
 			ValueStateDescriptor<Tuple2<RowData, Integer>> recordStateDesc = new ValueStateDescriptor<>(
 				stateName,
@@ -145,8 +145,8 @@ private static final class InputSideHasUniqueKey implements OuterJoinRecordState
 		private InputSideHasUniqueKey(
 				RuntimeContext ctx,
 				String stateName,
-				RowDataTypeInfo recordType,
-				RowDataTypeInfo uniqueKeyType,
+				InternalTypeInfo<RowData> recordType,
+				InternalTypeInfo<RowData> uniqueKeyType,
 				KeySelector<RowData, RowData> uniqueKeySelector,
 				StateTtlConfig ttlConfig) {
 			checkNotNull(uniqueKeyType);
@@ -205,7 +205,7 @@ private static final class InputSideHasNoUniqueKey implements OuterJoinRecordSta
 		private InputSideHasNoUniqueKey(
 				RuntimeContext ctx,
 				String stateName,
-				RowDataTypeInfo recordType,
+				InternalTypeInfo<RowData> recordType,
 				StateTtlConfig ttlConfig) {
 			TupleTypeInfo<Tuple2<Integer, Integer>> tupleTypeInfo = new TupleTypeInfo<>(Types.INT, Types.INT);
 			MapStateDescriptor<RowData, Tuple2<Integer, Integer>> recordStateDesc = new MapStateDescriptor<>(
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalProcessTimeJoinOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalProcessTimeJoinOperator.java
index 35a37528020be..d049ca4db1fce 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalProcessTimeJoinOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalProcessTimeJoinOperator.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.data.util.RowDataUtil;
 import org.apache.flink.table.runtime.generated.GeneratedJoinCondition;
 import org.apache.flink.table.runtime.generated.JoinCondition;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 /**
  * The operator to temporal join a stream on processing time.
@@ -42,7 +42,7 @@
 
 	private static final long serialVersionUID = -5182289624027523612L;
 
-	private final RowDataTypeInfo rightType;
+	private final InternalTypeInfo<RowData> rightType;
 	private final GeneratedJoinCondition generatedJoinCondition;
 
 	private transient ValueState<RowData> rightState;
@@ -52,7 +52,7 @@
 	private transient TimestampedCollector<RowData> collector;
 
 	public TemporalProcessTimeJoinOperator(
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedJoinCondition generatedJoinCondition,
 			long minRetentionTime,
 			long maxRetentionTime) {
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalRowTimeJoinOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalRowTimeJoinOperator.java
index 1846e7f33511f..e3e4a8cedb01f 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalRowTimeJoinOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/join/temporal/TemporalRowTimeJoinOperator.java
@@ -35,7 +35,7 @@
 import org.apache.flink.table.data.util.RowDataUtil;
 import org.apache.flink.table.runtime.generated.GeneratedJoinCondition;
 import org.apache.flink.table.runtime.generated.JoinCondition;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.types.RowKind;
 
 import java.io.IOException;
@@ -80,8 +80,8 @@
 	private static final String REGISTERED_TIMER_STATE_NAME = "timer";
 	private static final String TIMERS_STATE_NAME = "timers";
 
-	private final RowDataTypeInfo leftType;
-	private final RowDataTypeInfo rightType;
+	private final InternalTypeInfo<RowData> leftType;
+	private final InternalTypeInfo<RowData> rightType;
 	private final GeneratedJoinCondition generatedJoinCondition;
 	private final int leftTimeAttribute;
 	private final int rightTimeAttribute;
@@ -119,8 +119,8 @@
 	private transient JoinedRowData outRow;
 
 	public TemporalRowTimeJoinOperator(
-			RowDataTypeInfo leftType,
-			RowDataTypeInfo rightType,
+			InternalTypeInfo<RowData> leftType,
+			InternalTypeInfo<RowData> rightType,
 			GeneratedJoinCondition generatedJoinCondition,
 			int leftTimeAttribute,
 			int rightTimeAttribute,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/AbstractRowTimeUnboundedPrecedingOver.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/AbstractRowTimeUnboundedPrecedingOver.java
index fa550f6ca6cb0..b279a30a84434 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/AbstractRowTimeUnboundedPrecedingOver.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/AbstractRowTimeUnboundedPrecedingOver.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
@@ -92,13 +92,13 @@ public void open(Configuration parameters) throws Exception {
 		sortedTimestamps = new LinkedList<Long>();
 
 		// initialize accumulator state
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accStateDesc =
 			new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(accStateDesc);
 
 		// input element are all binary row as they are came from network
-		RowDataTypeInfo inputType = new RowDataTypeInfo(inputFieldTypes);
+		InternalTypeInfo<RowData> inputType = InternalTypeInfo.ofFields(inputFieldTypes);
 		ListTypeInfo<RowData> rowListTypeInfo = new ListTypeInfo<RowData>(inputType);
 		MapStateDescriptor<Long, List<RowData>> inputStateDesc = new MapStateDescriptor<Long, List<RowData>>(
 			"inputState",
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRangeBoundedPrecedingFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRangeBoundedPrecedingFunction.java
index de4deff57f25f..0d4dcf4f436a2 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRangeBoundedPrecedingFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRangeBoundedPrecedingFunction.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.runtime.dataview.PerKeyStateDataViewStore;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
@@ -95,14 +95,14 @@ public void open(Configuration parameters) throws Exception {
 		output = new JoinedRowData();
 
 		// input element are all binary row as they are came from network
-		RowDataTypeInfo inputType = new RowDataTypeInfo(inputFieldTypes);
+		InternalTypeInfo<RowData> inputType = InternalTypeInfo.ofFields(inputFieldTypes);
 		// we keep the elements received in a map state indexed based on their ingestion time
 		ListTypeInfo<RowData> rowListTypeInfo = new ListTypeInfo<>(inputType);
 		MapStateDescriptor<Long, List<RowData>> mapStateDescriptor = new MapStateDescriptor<>(
 			"inputState", BasicTypeInfo.LONG_TYPE_INFO, rowListTypeInfo);
 		inputState = getRuntimeContext().getMapState(mapStateDescriptor);
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> stateDescriptor =
 			new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(stateDescriptor);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRowsBoundedPrecedingFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRowsBoundedPrecedingFunction.java
index 4092b4ecf947f..38d6cd983758f 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRowsBoundedPrecedingFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeRowsBoundedPrecedingFunction.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 import org.apache.flink.util.Preconditions;
@@ -98,7 +98,7 @@ public void open(Configuration parameters) throws Exception {
 		output = new JoinedRowData();
 
 		// input element are all binary row as they are came from network
-		RowDataTypeInfo inputType = new RowDataTypeInfo(inputFieldTypes);
+		InternalTypeInfo<RowData> inputType = InternalTypeInfo.ofFields(inputFieldTypes);
 		// We keep the elements received in a Map state keyed
 		// by the ingestion time in the operator.
 		// we also keep counter of processed elements
@@ -108,7 +108,7 @@ public void open(Configuration parameters) throws Exception {
 			"inputState", BasicTypeInfo.LONG_TYPE_INFO, rowListTypeInfo);
 		inputState = getRuntimeContext().getMapState(mapStateDescriptor);
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> stateDescriptor =
 			new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(stateDescriptor);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeUnboundedPrecedingFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeUnboundedPrecedingFunction.java
index 3410e2584c3c1..1d1b1c0424071 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeUnboundedPrecedingFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/ProcTimeUnboundedPrecedingFunction.java
@@ -28,7 +28,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
@@ -70,7 +70,7 @@ public void open(Configuration parameters) throws Exception {
 
 		output = new JoinedRowData();
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> stateDescriptor =
 			new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(stateDescriptor);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRangeBoundedPrecedingFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRangeBoundedPrecedingFunction.java
index 51d11226071c3..8b68575954a64 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRangeBoundedPrecedingFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRangeBoundedPrecedingFunction.java
@@ -31,7 +31,7 @@
 import org.apache.flink.table.runtime.dataview.PerKeyStateDataViewStore;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 import org.apache.flink.util.Preconditions;
@@ -112,12 +112,12 @@ public void open(Configuration parameters) throws Exception {
 			Types.LONG);
 		lastTriggeringTsState = getRuntimeContext().getState(lastTriggeringTsDescriptor);
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accStateDesc = new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(accStateDesc);
 
 		// input element are all binary row as they are came from network
-		RowDataTypeInfo inputType = new RowDataTypeInfo(inputFieldTypes);
+		InternalTypeInfo<RowData> inputType = InternalTypeInfo.ofFields(inputFieldTypes);
 		ListTypeInfo<RowData> rowListTypeInfo = new ListTypeInfo<RowData>(inputType);
 		MapStateDescriptor<Long, List<RowData>> inputStateDesc = new MapStateDescriptor<Long, List<RowData>>(
 			"inputState",
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRowsBoundedPrecedingFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRowsBoundedPrecedingFunction.java
index a096969f96168..19f42779834fd 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRowsBoundedPrecedingFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/RowTimeRowsBoundedPrecedingFunction.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.AggsHandleFunction;
 import org.apache.flink.table.runtime.generated.GeneratedAggsHandleFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 import org.apache.flink.util.Preconditions;
@@ -121,12 +121,12 @@ public void open(Configuration parameters) throws Exception {
 			Types.LONG);
 		counterState = getRuntimeContext().getState(dataCountStateDescriptor);
 
-		RowDataTypeInfo accTypeInfo = new RowDataTypeInfo(accTypes);
+		InternalTypeInfo<RowData> accTypeInfo = InternalTypeInfo.ofFields(accTypes);
 		ValueStateDescriptor<RowData> accStateDesc = new ValueStateDescriptor<RowData>("accState", accTypeInfo);
 		accState = getRuntimeContext().getState(accStateDesc);
 
 		// input element are all binary row as they are came from network
-		RowDataTypeInfo inputType = new RowDataTypeInfo(inputFieldTypes);
+		InternalTypeInfo<RowData> inputType = InternalTypeInfo.ofFields(inputFieldTypes);
 		ListTypeInfo<RowData> rowListTypeInfo = new ListTypeInfo<RowData>(inputType);
 		MapStateDescriptor<Long, List<RowData>> inputStateDesc = new MapStateDescriptor<Long, List<RowData>>(
 			"inputState",
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/SlidingOverFrame.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/SlidingOverFrame.java
index 3a31323d357bc..6ce42b57279fa 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/SlidingOverFrame.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/SlidingOverFrame.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.operators.over.frame;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.runtime.context.ExecutionContext;
@@ -66,9 +65,8 @@ public SlidingOverFrame(
 
 	@Override
 	public void open(ExecutionContext ctx) throws Exception {
-		ExecutionConfig conf = ctx.getRuntimeContext().getExecutionConfig();
-		this.inputSer = new RowDataSerializer(conf, inputType);
-		this.valueSer = new RowDataSerializer(conf, valueType);
+		this.inputSer = new RowDataSerializer(inputType);
+		this.valueSer = new RowDataSerializer(valueType);
 
 		ClassLoader cl = ctx.getRuntimeContext().getUserCodeClassLoader();
 		processor = aggsHandleFunction.newInstance(cl);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedFollowingOverFrame.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedFollowingOverFrame.java
index 7b82afb330693..413a6ad427066 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedFollowingOverFrame.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedFollowingOverFrame.java
@@ -65,7 +65,7 @@ public void open(ExecutionContext ctx) throws Exception {
 		processor.open(new PerKeyStateDataViewStore(ctx.getRuntimeContext()));
 
 		this.aggsHandleFunction = null;
-		this.valueSer = new RowDataSerializer(ctx.getRuntimeContext().getExecutionConfig(), valueType);
+		this.valueSer = new RowDataSerializer(valueType);
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedOverWindowFrame.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedOverWindowFrame.java
index 2db08bd71dc82..5f3e9699e4edf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedOverWindowFrame.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/over/frame/UnboundedOverWindowFrame.java
@@ -56,9 +56,7 @@ public void open(ExecutionContext ctx) throws Exception {
 		processor = aggsHandleFunction.newInstance(cl);
 		processor.open(new PerKeyStateDataViewStore(ctx.getRuntimeContext()));
 		this.aggsHandleFunction = null;
-		this.valueSer = new RowDataSerializer(
-				ctx.getRuntimeContext().getExecutionConfig(),
-				valueType.getChildren().toArray(new LogicalType[0]));
+		this.valueSer = new RowDataSerializer(valueType.getChildren().toArray(new LogicalType[0]));
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AbstractTopNFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AbstractTopNFunction.java
index 6050767100e04..bd6ad6ea2ab82 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AbstractTopNFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AbstractTopNFunction.java
@@ -32,7 +32,7 @@
 import org.apache.flink.table.runtime.functions.KeyedProcessFunctionWithCleanupState;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.Collector;
@@ -67,7 +67,7 @@ public abstract class AbstractTopNFunction extends KeyedProcessFunctionWithClean
 
 	private final boolean generateUpdateBefore;
 	protected final boolean outputRankNumber;
-	protected final RowDataTypeInfo inputRowType;
+	protected final InternalTypeInfo<RowData> inputRowType;
 	protected final KeySelector<RowData, RowData> sortKeySelector;
 
 	protected KeyContext keyContext;
@@ -88,7 +88,7 @@ public abstract class AbstractTopNFunction extends KeyedProcessFunctionWithClean
 	AbstractTopNFunction(
 			long minRetentionTime,
 			long maxRetentionTime,
-			RowDataTypeInfo inputRowType,
+			InternalTypeInfo<RowData> inputRowType,
 			GeneratedRecordComparator generatedSortKeyComparator,
 			RowDataKeySelector sortKeySelector,
 			RankType rankType,
@@ -152,7 +152,7 @@ public void open(Configuration parameters) throws Exception {
 
 		// initialize rankEndFetcher
 		if (!isConstantRankEnd) {
-			LogicalType rankEndIdxType = inputRowType.getLogicalTypes()[rankEndIndex];
+			LogicalType rankEndIdxType = inputRowType.toRowFieldTypes()[rankEndIndex];
 			switch (rankEndIdxType.getTypeRoot()) {
 				case BIGINT:
 					rankEndFetcher = (RowData row) -> row.getLong(rankEndIndex);
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java
index 82e2fae4fead7..2b3938989376c 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/AppendOnlyTopNFunction.java
@@ -27,7 +27,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.LRUMap;
 import org.apache.flink.util.Collector;
 
@@ -51,7 +51,7 @@ public class AppendOnlyTopNFunction extends AbstractTopNFunction {
 
 	private static final Logger LOG = LoggerFactory.getLogger(AppendOnlyTopNFunction.class);
 
-	private final RowDataTypeInfo sortKeyType;
+	private final InternalTypeInfo<RowData> sortKeyType;
 	private final TypeSerializer<RowData> inputRowSer;
 	private final long cacheSize;
 
@@ -67,7 +67,7 @@ public class AppendOnlyTopNFunction extends AbstractTopNFunction {
 	public AppendOnlyTopNFunction(
 			long minRetentionTime,
 			long maxRetentionTime,
-			RowDataTypeInfo inputRowType,
+			InternalTypeInfo<RowData> inputRowType,
 			GeneratedRecordComparator sortKeyGeneratedRecordComparator,
 			RowDataKeySelector sortKeySelector,
 			RankType rankType,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/RetractableTopNFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/RetractableTopNFunction.java
index 7592e0449cf7e..2799b96bae21c 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/RetractableTopNFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/RetractableTopNFunction.java
@@ -31,7 +31,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordEqualiser;
 import org.apache.flink.table.runtime.generated.RecordEqualiser;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.SortedMapTypeInfo;
 import org.apache.flink.types.RowKind;
 import org.apache.flink.util.Collector;
@@ -64,7 +64,7 @@ public class RetractableTopNFunction extends AbstractTopNFunction {
 	private static final String STATE_CLEARED_WARN_MSG = "The state is cleared because of state ttl. " +
 			"This will result in incorrect result. You can increase the state ttl to avoid this.";
 
-	private final RowDataTypeInfo sortKeyType;
+	private final InternalTypeInfo<RowData> sortKeyType;
 
 	// flag to skip records with non-exist error instead to fail, true by default.
 	private final boolean lenient = true;
@@ -84,7 +84,7 @@ public class RetractableTopNFunction extends AbstractTopNFunction {
 	public RetractableTopNFunction(
 			long minRetentionTime,
 			long maxRetentionTime,
-			RowDataTypeInfo inputRowType,
+			InternalTypeInfo<RowData> inputRowType,
 			GeneratedRecordComparator generatedRecordComparator,
 			RowDataKeySelector sortKeySelector,
 			RankType rankType,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/UpdatableTopNFunction.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/UpdatableTopNFunction.java
index 8f07e508c5c32..3cdd64acf31bf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/UpdatableTopNFunction.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/rank/UpdatableTopNFunction.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.LRUMap;
 import org.apache.flink.util.Collector;
 
@@ -66,7 +66,7 @@ public class UpdatableTopNFunction extends AbstractTopNFunction implements Check
 
 	private static final Logger LOG = LoggerFactory.getLogger(UpdatableTopNFunction.class);
 
-	private final RowDataTypeInfo rowKeyType;
+	private final InternalTypeInfo<RowData> rowKeyType;
 	private final long cacheSize;
 
 	// a map state stores mapping from row key to record which is in topN
@@ -92,7 +92,7 @@ public class UpdatableTopNFunction extends AbstractTopNFunction implements Check
 	public UpdatableTopNFunction(
 			long minRetentionTime,
 			long maxRetentionTime,
-			RowDataTypeInfo inputRowType,
+			InternalTypeInfo<RowData> inputRowType,
 			RowDataKeySelector rowKeySelector,
 			GeneratedRecordComparator generatedRecordComparator,
 			RowDataKeySelector sortKeySelector,
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperator.java
index 85882228af180..8f33f4eded50b 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperator.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -43,7 +43,7 @@ public class ProcTimeSortOperator extends BaseTemporalSortOperator {
 
 	private static final Logger LOG = LoggerFactory.getLogger(ProcTimeSortOperator.class);
 
-	private final RowDataTypeInfo inputRowType;
+	private final InternalTypeInfo<RowData> inputRowType;
 
 	private GeneratedRecordComparator gComparator;
 	private transient RecordComparator comparator;
@@ -55,7 +55,7 @@ public class ProcTimeSortOperator extends BaseTemporalSortOperator {
 	 * @param inputRowType The data type of the input data.
 	 * @param gComparator generated comparator.
 	 */
-	public ProcTimeSortOperator(RowDataTypeInfo inputRowType, GeneratedRecordComparator gComparator) {
+	public ProcTimeSortOperator(InternalTypeInfo<RowData> inputRowType, GeneratedRecordComparator gComparator) {
 		this.inputRowType = inputRowType;
 		this.gComparator = gComparator;
 	}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperator.java
index d072816026206..f55a3708b2b10 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperator.java
@@ -30,7 +30,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.util.Preconditions;
 
 import org.slf4j.Logger;
@@ -48,7 +48,7 @@ public class RowTimeSortOperator extends BaseTemporalSortOperator {
 
 	private static final Logger LOG = LoggerFactory.getLogger(RowTimeSortOperator.class);
 
-	private final RowDataTypeInfo inputRowType;
+	private final InternalTypeInfo<RowData> inputRowType;
 	private final int rowTimeIdx;
 
 	private GeneratedRecordComparator gComparator;
@@ -64,9 +64,9 @@ public class RowTimeSortOperator extends BaseTemporalSortOperator {
 	 * @param rowTimeIdx The index of the rowTime field.
 	 * @param gComparator generated comparator, could be null if only sort on RowTime field
 	 */
-	public RowTimeSortOperator(RowDataTypeInfo inputRowType, int rowTimeIdx, GeneratedRecordComparator gComparator) {
+	public RowTimeSortOperator(InternalTypeInfo<RowData> inputRowType, int rowTimeIdx, GeneratedRecordComparator gComparator) {
 		this.inputRowType = inputRowType;
-		Preconditions.checkArgument(rowTimeIdx >= 0 && rowTimeIdx < inputRowType.getArity(),
+		Preconditions.checkArgument(rowTimeIdx >= 0 && rowTimeIdx < inputRowType.toRowSize(),
 				"RowTimeIdx must be 0 or positive number and smaller than input row arity!");
 		this.rowTimeIdx = rowTimeIdx;
 		this.gComparator = gComparator;
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperator.java
index 48c835b9442d0..728a1768c51dc 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperator.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.operators.sort;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.state.ListState;
 import org.apache.flink.api.common.state.ListStateDescriptor;
 import org.apache.flink.api.common.typeinfo.Types;
@@ -34,8 +33,8 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
 import org.apache.flink.table.runtime.operators.TableStreamOperator;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
 import org.apache.flink.table.runtime.util.StreamRecordCollector;
 import org.apache.flink.types.RowKind;
 
@@ -56,7 +55,7 @@ public class StreamSortOperator extends TableStreamOperator<RowData> implements
 
 	private static final Logger LOG = LoggerFactory.getLogger(StreamSortOperator.class);
 
-	private final RowDataTypeInfo inputRowType;
+	private final InternalTypeInfo<RowData> inputRowType;
 	private GeneratedRecordComparator gComparator;
 	private transient RecordComparator comparator;
 	private transient RowDataSerializer rowDataSerializer;
@@ -67,7 +66,7 @@ public class StreamSortOperator extends TableStreamOperator<RowData> implements
 	// inputBuffer buffers all input elements, key is RowData, value is appear times.
 	private transient HashMap<RowData, Long> inputBuffer;
 
-	public StreamSortOperator(RowDataTypeInfo inputRowType, GeneratedRecordComparator gComparator) {
+	public StreamSortOperator(InternalTypeInfo<RowData> inputRowType, GeneratedRecordComparator gComparator) {
 		this.inputRowType = inputRowType;
 		this.gComparator = gComparator;
 	}
@@ -77,8 +76,7 @@ public void open() throws Exception {
 		super.open();
 
 		LOG.info("Opening StreamSortOperator");
-		ExecutionConfig executionConfig = getExecutionConfig();
-		this.rowDataSerializer = inputRowType.createSerializer(executionConfig);
+		this.rowDataSerializer = inputRowType.toRowSerializer();
 
 		comparator = gComparator.newInstance(getContainingTask().getUserCodeClassLoader());
 		gComparator = null;
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/values/ValuesInputFormat.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/values/ValuesInputFormat.java
index 50e0219362075..727e68abfef69 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/values/ValuesInputFormat.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/values/ValuesInputFormat.java
@@ -24,7 +24,7 @@
 import org.apache.flink.core.io.GenericInputSplit;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedInput;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -42,10 +42,10 @@
 	private static final long serialVersionUID = 1L;
 
 	private GeneratedInput<GenericInputFormat<RowData>> generatedInput;
-	private final RowDataTypeInfo returnType;
+	private final InternalTypeInfo<RowData> returnType;
 	private GenericInputFormat<RowData> format;
 
-	public ValuesInputFormat(GeneratedInput<GenericInputFormat<RowData>> generatedInput, RowDataTypeInfo returnType) {
+	public ValuesInputFormat(GeneratedInput<GenericInputFormat<RowData>> generatedInput, InternalTypeInfo<RowData> returnType) {
 		this.generatedInput = generatedInput;
 		this.returnType = returnType;
 	}
@@ -71,7 +71,7 @@ public RowData nextRecord(RowData reuse) throws IOException {
 	}
 
 	@Override
-	public RowDataTypeInfo getProducedType() {
+	public InternalTypeInfo<RowData> getProducedType() {
 		return returnType;
 	}
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/window/WindowOperator.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/window/WindowOperator.java
index 120a13ee2851e..ce85c0c68f23b 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/window/WindowOperator.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/operators/window/WindowOperator.java
@@ -246,14 +246,14 @@ public void open() throws Exception {
 
 		StateDescriptor<ValueState<RowData>, RowData> windowStateDescriptor = new ValueStateDescriptor<>(
 				"window-aggs",
-				new RowDataSerializer(getExecutionConfig(), accumulatorTypes));
+				new RowDataSerializer(accumulatorTypes));
 		this.windowState = (InternalValueState<K, W, RowData>) getOrCreateKeyedState(windowSerializer, windowStateDescriptor);
 
 		if (produceUpdates) {
 			LogicalType[] valueTypes = ArrayUtils.addAll(aggResultTypes, windowPropertyTypes);
 			StateDescriptor<ValueState<RowData>, RowData> previousStateDescriptor = new ValueStateDescriptor<>(
 					"previous-aggs",
-					new RowDataSerializer(getExecutionConfig(), valueTypes));
+					new RowDataSerializer(valueTypes));
 			this.previousState = (InternalValueState<K, W, RowData>) getOrCreateKeyedState(windowSerializer, previousStateDescriptor);
 		}
 
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/PlannerTypeUtils.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/PlannerTypeUtils.java
index 6f433ebc5d787..75db55fa967df 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/PlannerTypeUtils.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/PlannerTypeUtils.java
@@ -26,6 +26,7 @@
 import org.apache.flink.table.data.DecimalDataUtils;
 import org.apache.flink.table.runtime.typeutils.BigDecimalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyInstantTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyLocalDateTimeTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyTimestampTypeInfo;
@@ -233,6 +234,8 @@ protected LogicalType defaultMethod(LogicalType logicalType) {
 				} else if (typeInfo instanceof LegacyInstantTypeInfo) {
 					LegacyInstantTypeInfo instantTypeInfo = (LegacyInstantTypeInfo) typeInfo;
 					return new LocalZonedTimestampType(instantTypeInfo.getPrecision());
+				} else if (typeInfo instanceof InternalTypeInfo) {
+					return ((InternalTypeInfo<?>) typeInfo).toLogicalType();
 				} else {
 					return new TypeInformationRawType<>(typeInfo);
 				}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/TypeInfoDataTypeConverter.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/TypeInfoDataTypeConverter.java
index 0582327aa4e09..9e38193f5d12d 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/TypeInfoDataTypeConverter.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/TypeInfoDataTypeConverter.java
@@ -35,13 +35,14 @@
 import org.apache.flink.table.functions.TableFunctionDefinition;
 import org.apache.flink.table.runtime.typeutils.BigDecimalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyInstantTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyLocalDateTimeTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyTimestampTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.runtime.typeutils.TimestampDataTypeInfo;
-import org.apache.flink.table.runtime.typeutils.WrapperTypeInfo;
 import org.apache.flink.table.types.CollectionDataType;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.KeyValueDataType;
@@ -80,7 +81,7 @@
  * 2.See {@link AggregateFunctionDefinition#getAccumulatorTypeInfo()}.
  * 3.See {@link MapViewTypeInfo#getKeyType()}.
  *
- * @deprecated Use {@link WrapperTypeInfo#of(LogicalType)} instead if {@link TypeInformation} is really
+ * @deprecated Use {@link InternalTypeInfo#of(LogicalType)} instead if {@link TypeInformation} is really
  *             required. In many cases, {@link InternalSerializers#create(LogicalType)} should be sufficient.
  */
 @Deprecated
@@ -170,7 +171,7 @@ public static TypeInformation<?> fromDataTypeToTypeInfo(DataType dataType) {
 						fromDataTypeToTypeInfo(((CollectionDataType) dataType).getElementDataType()));
 			case ROW:
 				if (RowData.class.isAssignableFrom(dataType.getConversionClass())) {
-					return RowDataTypeInfo.of((RowType) fromDataTypeToLogicalType(dataType));
+					return InternalTypeInfo.of((RowType) fromDataTypeToLogicalType(dataType));
 				} else if (Row.class == dataType.getConversionClass()) {
 					RowType logicalRowType = (RowType) logicalType;
 					return new RowTypeInfo(
@@ -184,16 +185,11 @@ public static TypeInformation<?> fromDataTypeToTypeInfo(DataType dataType) {
 				}
 			case RAW:
 				if (logicalType instanceof RawType) {
-					final RawType<?> rawType = (RawType<?>) logicalType;
-					return createWrapperTypeInfo(rawType);
+					return ExternalTypeInfo.of(dataType);
 				}
 				return TypeConversions.fromDataTypeToLegacyInfo(dataType);
 			default:
 				return TypeConversions.fromDataTypeToLegacyInfo(dataType);
 		}
 	}
-
-	private static <T> WrapperTypeInfo<T> createWrapperTypeInfo(RawType<T> rawType) {
-		return new WrapperTypeInfo<>(rawType, rawType.getOriginatingClass(), rawType.getTypeSerializer());
-	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializer.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializer.java
index 215bd88768d12..c0910d2d41539 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializer.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializer.java
@@ -20,7 +20,6 @@
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
@@ -36,7 +35,6 @@
 import org.apache.flink.table.data.binary.BinarySegmentUtils;
 import org.apache.flink.table.data.writer.BinaryArrayWriter;
 import org.apache.flink.table.data.writer.BinaryWriter;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
 import org.apache.flink.util.InstantiationUtil;
@@ -58,10 +56,9 @@ public class ArrayDataSerializer extends TypeSerializer<ArrayData> {
 	private transient BinaryArrayData reuseArray;
 	private transient BinaryArrayWriter reuseWriter;
 
-	@SuppressWarnings("unchecked")
-	public ArrayDataSerializer(LogicalType eleType, ExecutionConfig conf) {
+	public ArrayDataSerializer(LogicalType eleType) {
 		this.eleType = eleType;
-		this.eleSer = (TypeSerializer<Object>) InternalSerializers.create(eleType, conf);
+		this.eleSer = InternalSerializers.create(eleType);
 	}
 
 	private ArrayDataSerializer(LogicalType eleType, TypeSerializer<Object> eleSer) {
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfo.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfo.java
new file mode 100644
index 0000000000000..0cc358f18fab8
--- /dev/null
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfo.java
@@ -0,0 +1,158 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.DataTypeQueryable;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RawType;
+import org.apache.flink.util.Preconditions;
+
+import java.util.Objects;
+
+/**
+ * Type information that wraps a serializer that originated from a {@link DataType}.
+ *
+ * <p>{@link TypeInformation} is a legacy class for the sole purpose of creating a {@link TypeSerializer}.
+ * This class acts as an adapter when entering or leaving the table ecosystem to other APIs where type
+ * information is required.
+ *
+ * <p>The original {@link DataType} is stored in every instance (for access during pre-flight phase
+ * and planning) but has no effect on equality because only serialization matters during runtime.
+ *
+ * <p>Note: This class is incomplete yet and only supports RAW types. But will be updated to support
+ * all kinds of data types in the future.
+ *
+ * @param <T> external data structure
+ */
+@Internal
+public final class ExternalTypeInfo<T> extends TypeInformation<T> implements DataTypeQueryable {
+
+	private static final String FORMAT = "%s(%s, %s)";
+
+	private final DataType dataType;
+
+	private final TypeSerializer<T> typeSerializer;
+
+	private ExternalTypeInfo(DataType dataType, TypeSerializer<T> typeSerializer) {
+		this.dataType = Preconditions.checkNotNull(dataType);
+		this.typeSerializer = Preconditions.checkNotNull(typeSerializer);
+	}
+
+	/**
+	 * Creates type information for a {@link DataType} that is possibly represented by internal data
+	 * structures but serialized and deserialized into external data structures.
+	 */
+	public static <T> ExternalTypeInfo<T> of(DataType dataType) {
+		final TypeSerializer<T> serializer = createExternalTypeSerializer(dataType);
+		return new ExternalTypeInfo<>(dataType, serializer);
+	}
+
+	@SuppressWarnings("unchecked")
+	private static <T> TypeSerializer<T> createExternalTypeSerializer(DataType dataType) {
+		final LogicalType logicalType = dataType.getLogicalType();
+		if (logicalType instanceof RawType) {
+			final RawType<?> rawType = (RawType<?>) logicalType;
+			if (dataType.getConversionClass() == rawType.getOriginatingClass()) {
+				return (TypeSerializer<T>) rawType.getTypeSerializer();
+			}
+		}
+		throw new UnsupportedOperationException("External type information is not fully implemented yet.");
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	@Override
+	public DataType getDataType() {
+		return dataType;
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	@Override
+	public boolean isBasicType() {
+		return false;
+	}
+
+	@Override
+	public boolean isTupleType() {
+		return false;
+	}
+
+	@Override
+	public int getArity() {
+		return 1;
+	}
+
+	@Override
+	public int getTotalFields() {
+		return 1;
+	}
+
+	@Override
+	@SuppressWarnings("unchecked")
+	public Class<T> getTypeClass() {
+		return (Class<T>) dataType.getConversionClass();
+	}
+
+	@Override
+	public boolean isKeyType() {
+		return false;
+	}
+
+	@Override
+	public TypeSerializer<T> createSerializer(ExecutionConfig config) {
+		return typeSerializer;
+	}
+
+	@Override
+	public String toString() {
+		return String.format(
+			FORMAT,
+			dataType.getLogicalType().asSummaryString(),
+			dataType.getConversionClass().getName(),
+			typeSerializer.getClass().getName());
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (o == null || getClass() != o.getClass()) {
+			return false;
+		}
+		ExternalTypeInfo<?> that = (ExternalTypeInfo<?>) o;
+		return typeSerializer.equals(that.typeSerializer);
+	}
+
+	@Override
+	public int hashCode() {
+		return Objects.hash(typeSerializer);
+	}
+
+	@Override
+	public boolean canEqual(Object obj) {
+		return obj instanceof ExternalTypeInfo;
+	}
+}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/InternalSerializers.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalSerializers.java
similarity index 74%
rename from flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/InternalSerializers.java
rename to flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalSerializers.java
index 7c22c74d11218..a0bcf96e883ba 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/types/InternalSerializers.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalSerializers.java
@@ -16,8 +16,9 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.types;
+package org.apache.flink.table.runtime.typeutils;
 
+import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.base.BooleanSerializer;
@@ -28,13 +29,6 @@
 import org.apache.flink.api.common.typeutils.base.LongSerializer;
 import org.apache.flink.api.common.typeutils.base.ShortSerializer;
 import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;
-import org.apache.flink.table.runtime.typeutils.ArrayDataSerializer;
-import org.apache.flink.table.runtime.typeutils.DecimalDataSerializer;
-import org.apache.flink.table.runtime.typeutils.MapDataSerializer;
-import org.apache.flink.table.runtime.typeutils.RawValueDataSerializer;
-import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
-import org.apache.flink.table.runtime.typeutils.StringDataSerializer;
-import org.apache.flink.table.runtime.typeutils.TimestampDataSerializer;
 import org.apache.flink.table.types.logical.ArrayType;
 import org.apache.flink.table.types.logical.DistinctType;
 import org.apache.flink.table.types.logical.IntType;
@@ -48,25 +42,20 @@
 import static org.apache.flink.table.types.logical.utils.LogicalTypeChecks.getScale;
 
 /**
- * {@link TypeSerializer} of {@link LogicalType} for internal sql engine execution data formats.
+ * {@link TypeSerializer} of {@link LogicalType} for internal data structures.
  */
-public class InternalSerializers {
+@Internal
+public final class InternalSerializers {
 
 	/**
 	 * Creates a {@link TypeSerializer} for internal data structures of the given {@link LogicalType}.
 	 */
-	public static TypeSerializer<?> create(LogicalType type) {
-		return create(type, new ExecutionConfig());
+	@SuppressWarnings("unchecked")
+	public static <T> TypeSerializer<T> create(LogicalType type) {
+		return (TypeSerializer<T>) createInternal(type);
 	}
 
-	/**
-	 * Creates a {@link TypeSerializer} for internal data structures of the given {@link LogicalType}.
-	 *
-	 * @deprecated Use {@link #create(LogicalType)} instead. All types of the new type system have been
-	 *             fully resolved before.
-	 */
-	@Deprecated
-	public static TypeSerializer create(LogicalType type, ExecutionConfig config) {
+	private static TypeSerializer<?> createInternal(LogicalType type) {
 		// ordered by type root definition
 		switch (type.getTypeRoot()) {
 			case CHAR:
@@ -101,24 +90,24 @@ public static TypeSerializer create(LogicalType type, ExecutionConfig config) {
 			case TIMESTAMP_WITH_TIME_ZONE:
 				throw new UnsupportedOperationException();
 			case ARRAY:
-				return new ArrayDataSerializer(((ArrayType) type).getElementType(), config);
+				return new ArrayDataSerializer(((ArrayType) type).getElementType());
 			case MULTISET:
-				return new MapDataSerializer(((MultisetType) type).getElementType(), new IntType(false), config);
+				return new MapDataSerializer(((MultisetType) type).getElementType(), new IntType(false));
 			case MAP:
 				MapType mapType = (MapType) type;
-				return new MapDataSerializer(mapType.getKeyType(), mapType.getValueType(), config);
+				return new MapDataSerializer(mapType.getKeyType(), mapType.getValueType());
 			case ROW:
 			case STRUCTURED_TYPE:
-				return new RowDataSerializer(config, type.getChildren().toArray(new LogicalType[0]));
+				return new RowDataSerializer(type.getChildren().toArray(new LogicalType[0]));
 			case DISTINCT_TYPE:
-				return create(((DistinctType) type).getSourceType(), config);
+				return create(((DistinctType) type).getSourceType());
 			case RAW:
 				if (type instanceof RawType) {
 					final RawType<?> rawType = (RawType<?>) type;
 					return new RawValueDataSerializer<>(rawType.getTypeSerializer());
 				}
 				return new RawValueDataSerializer<>(
-					((TypeInformationRawType<?>) type).getTypeInformation().createSerializer(config));
+					((TypeInformationRawType<?>) type).getTypeInformation().createSerializer(new ExecutionConfig()));
 			case NULL:
 			case SYMBOL:
 			case UNRESOLVED:
@@ -127,4 +116,8 @@ public static TypeSerializer create(LogicalType type, ExecutionConfig config) {
 					"Unsupported type '" + type + "' to get internal serializer");
 		}
 	}
+
+	private InternalSerializers() {
+		// no instantiation
+	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfo.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfo.java
new file mode 100644
index 0000000000000..a56d85888cbdc
--- /dev/null
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfo.java
@@ -0,0 +1,234 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.DataTypeQueryable;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
+import org.apache.flink.table.types.utils.DataTypeUtils;
+import org.apache.flink.util.Preconditions;
+
+import java.util.Objects;
+
+/**
+ * Type information that wraps a serializer that originated from a {@link LogicalType}.
+ *
+ * <p>{@link TypeInformation} is a legacy class for the sole purpose of creating a {@link TypeSerializer}.
+ * Instances of {@link TypeInformation} are not required in the table ecosystem but sometimes enforced
+ * by interfaces of other modules (such as {@link org.apache.flink.api.dag.Transformation}). Therefore,
+ * this class acts as an adapter whenever type information is required.
+ *
+ * <p>Use {@link #of(LogicalType)} for type information of internal data structures.
+ *
+ * <p>Note: Instances of this class should only be created for passing it to interfaces that require
+ * type information. This class should not be used as a replacement for a {@link LogicalType}. Information
+ * such as the arity of a row type, field types, field names, etc. should be derived from the {@link LogicalType}
+ * directly.
+ *
+ * <p>The original {@link LogicalType} is stored in every instance (for access during pre-flight phase
+ * and planning) but has no effect on equality because only serialization matters during runtime.
+ *
+ * @param <T> internal data structure
+ */
+@Internal
+public final class InternalTypeInfo<T> extends TypeInformation<T> implements DataTypeQueryable {
+
+	private static final String FORMAT = "%s(%s, %s)";
+
+	private final LogicalType type;
+
+	private final Class<T> typeClass;
+
+	private final TypeSerializer<T> typeSerializer;
+
+	private InternalTypeInfo(LogicalType type, Class<T> typeClass, TypeSerializer<T> typeSerializer) {
+		this.type = Preconditions.checkNotNull(type);
+		this.typeClass = Preconditions.checkNotNull(typeClass);
+		this.typeSerializer = Preconditions.checkNotNull(typeSerializer);
+	}
+
+	/**
+	 * Creates type information for a {@link LogicalType} that is represented by internal data structures.
+	 */
+	@SuppressWarnings({"unchecked", "rawtypes"})
+	public static <T> InternalTypeInfo<T> of(LogicalType type) {
+		final Class<?> typeClass = LogicalTypeUtils.toInternalConversionClass(type);
+		final TypeSerializer<?> serializer = InternalSerializers.create(type);
+		return (InternalTypeInfo<T>) new InternalTypeInfo(type, typeClass, serializer);
+	}
+
+	/**
+	 * Creates type information for a {@link RowType} represented by internal data structures.
+	 */
+	public static InternalTypeInfo<RowData> of(RowType type) {
+		return of((LogicalType) type);
+	}
+
+	/**
+	 * Creates type information for {@link RowType} represented by internal data structures.
+	 */
+	public static InternalTypeInfo<RowData> ofFields(LogicalType... fieldTypes) {
+		return of(RowType.of(fieldTypes));
+	}
+
+	/**
+	 * Creates type information for {@link RowType} represented by internal data structures.
+	 */
+	public static InternalTypeInfo<RowData> ofFields(LogicalType[] fieldTypes, String[] fieldNames) {
+		return of(RowType.of(fieldTypes, fieldNames));
+	}
+
+	// --------------------------------------------------------------------------------------------
+	// Internal methods for common tasks
+	// --------------------------------------------------------------------------------------------
+
+	public LogicalType toLogicalType() {
+		return type;
+	}
+
+	public TypeSerializer<T> toSerializer() {
+		return typeSerializer;
+	}
+
+	public RowType toRowType() {
+		return (RowType) type;
+	}
+
+	public RowDataSerializer toRowSerializer() {
+		return (RowDataSerializer) typeSerializer;
+	}
+
+	/**
+	 * @deprecated {@link TypeInformation} should just be a thin wrapper of a serializer. This method
+	 *             only exists for legacy code. It is recommended to use the {@link RowType} instead
+	 *             for logical operations.
+	 */
+	@Deprecated
+	public LogicalType[] toRowFieldTypes() {
+		return toRowType()
+			.getFields().stream()
+			.map(RowType.RowField::getType)
+			.toArray(LogicalType[]::new);
+	}
+
+	/**
+	 * @deprecated {@link TypeInformation} should just be a thin wrapper of a serializer. This method
+	 *             only exists for legacy code. It is recommended to use the {@link RowType} instead
+	 *             for logical operations.
+	 */
+	@Deprecated
+	public String[] toRowFieldNames() {
+		return toRowType()
+			.getFields().stream()
+			.map(RowType.RowField::getName)
+			.toArray(String[]::new);
+	}
+
+	/**
+	 * @deprecated {@link TypeInformation} should just be a thin wrapper of a serializer. This method
+	 *             only exists for legacy code. It is recommended to use the {@link RowType} instead
+	 *             for logical operations.
+	 */
+	@Deprecated
+	public int toRowSize() {
+		return toRowType().getFieldCount();
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	@Override
+	public DataType getDataType() {
+		return DataTypeUtils.toInternalDataType(type);
+	}
+
+	// --------------------------------------------------------------------------------------------
+
+	@Override
+	public boolean isBasicType() {
+		return false;
+	}
+
+	@Override
+	public boolean isTupleType() {
+		return false;
+	}
+
+	@Override
+	public int getArity() {
+		return 1;
+	}
+
+	@Override
+	public int getTotalFields() {
+		return 1;
+	}
+
+	@Override
+	public Class<T> getTypeClass() {
+		return typeClass;
+	}
+
+	@Override
+	public boolean isKeyType() {
+		return false;
+	}
+
+	@Override
+	public TypeSerializer<T> createSerializer(ExecutionConfig config) {
+		return typeSerializer;
+	}
+
+	@Override
+	public String toString() {
+		return String.format(
+			FORMAT,
+			type.asSummaryString(),
+			typeClass.getName(),
+			typeSerializer.getClass().getName());
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (o == null || getClass() != o.getClass()) {
+			return false;
+		}
+		final InternalTypeInfo<?> that = (InternalTypeInfo<?>) o;
+		return typeSerializer.equals(that.typeSerializer);
+	}
+
+	@Override
+	public int hashCode() {
+		return Objects.hash(typeSerializer);
+	}
+
+	@Override
+	public boolean canEqual(Object obj) {
+		return obj instanceof InternalTypeInfo;
+	}
+}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/MapDataSerializer.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/MapDataSerializer.java
index 453a8a838da44..63a820795af7a 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/MapDataSerializer.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/MapDataSerializer.java
@@ -20,7 +20,6 @@
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
 import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
@@ -37,7 +36,6 @@
 import org.apache.flink.table.data.binary.BinarySegmentUtils;
 import org.apache.flink.table.data.writer.BinaryArrayWriter;
 import org.apache.flink.table.data.writer.BinaryWriter;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.InstantiationUtil;
 
@@ -60,12 +58,12 @@ public class MapDataSerializer extends TypeSerializer<MapData> {
 	private transient BinaryArrayWriter reuseKeyWriter;
 	private transient BinaryArrayWriter reuseValueWriter;
 
-	public MapDataSerializer(LogicalType keyType, LogicalType valueType, ExecutionConfig conf) {
+	public MapDataSerializer(LogicalType keyType, LogicalType valueType) {
 		this.keyType = keyType;
 		this.valueType = valueType;
 
-		this.keySerializer = InternalSerializers.create(keyType, conf);
-		this.valueSerializer = InternalSerializers.create(valueType, conf);
+		this.keySerializer = InternalSerializers.create(keyType);
+		this.valueSerializer = InternalSerializers.create(valueType);
 	}
 
 	private MapDataSerializer(
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataSerializer.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataSerializer.java
index bbc4a2ee78d49..fc1a26c618ebf 100644
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataSerializer.java
+++ b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataSerializer.java
@@ -19,7 +19,6 @@
 package org.apache.flink.table.runtime.typeutils;
 
 import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.CompositeTypeSerializerUtil;
 import org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
@@ -36,7 +35,6 @@
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.data.writer.BinaryRowWriter;
 import org.apache.flink.table.data.writer.BinaryWriter;
-import org.apache.flink.table.runtime.types.InternalSerializers;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.InstantiationUtil;
@@ -58,16 +56,16 @@ public class RowDataSerializer extends AbstractRowDataSerializer<RowData> {
 	private transient BinaryRowData reuseRow;
 	private transient BinaryRowWriter reuseWriter;
 
-	public RowDataSerializer(ExecutionConfig config, RowType rowType) {
+	public RowDataSerializer(RowType rowType) {
 		this(rowType.getChildren().toArray(new LogicalType[0]),
 			rowType.getChildren().stream()
-				.map((LogicalType type) -> InternalSerializers.create(type, config))
+				.map(InternalSerializers::create)
 				.toArray(TypeSerializer[]::new));
 	}
 
-	public RowDataSerializer(ExecutionConfig config, LogicalType... types) {
+	public RowDataSerializer(LogicalType... types) {
 		this(types, Arrays.stream(types)
-			.map((LogicalType type) -> InternalSerializers.create(type, config))
+			.map(InternalSerializers::create)
 			.toArray(TypeSerializer[]::new));
 	}
 
@@ -236,7 +234,7 @@ public RowData mapFromPages(
 	public boolean equals(Object obj) {
 		if (obj instanceof RowDataSerializer) {
 			RowDataSerializer other = (RowDataSerializer) obj;
-			return Arrays.equals(types, other.types);
+			return Arrays.equals(fieldSerializers, other.fieldSerializers);
 		}
 
 		return false;
@@ -244,7 +242,7 @@ public boolean equals(Object obj) {
 
 	@Override
 	public int hashCode() {
-		return Arrays.hashCode(types);
+		return Arrays.hashCode(fieldSerializers);
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfo.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfo.java
deleted file mode 100644
index 6c0d6658d9040..0000000000000
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfo.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.operators.Keys;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeutils.CompositeType;
-import org.apache.flink.api.common.typeutils.TypeComparator;
-import org.apache.flink.api.java.typeutils.TupleTypeInfoBase;
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.RowType;
-
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import static org.apache.flink.util.Preconditions.checkArgument;
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * TypeInformation for {@link RowData}.
- *
- * @deprecated Use {@link WrapperTypeInfo#of(LogicalType)} to represent all kinds of {@link LogicalType}
- *             as instances of {@link TypeInformation}.
- */
-@Internal
-@Deprecated
-public class RowDataTypeInfo extends TupleTypeInfoBase<RowData> {
-
-	private static final long serialVersionUID = 1L;
-
-	private static final String REGEX_INT_FIELD = "[0-9]+";
-	private static final String REGEX_STR_FIELD = "[\\p{L}_\\$][\\p{L}\\p{Digit}_\\$]*";
-	private static final String REGEX_FIELD = REGEX_STR_FIELD + "|" + REGEX_INT_FIELD;
-	private static final String REGEX_NESTED_FIELDS = "(" + REGEX_FIELD + ")(\\.(.+))?";
-	private static final Pattern PATTERN_NESTED_FIELDS = Pattern.compile(REGEX_NESTED_FIELDS);
-	private static final Pattern PATTERN_INT_FIELD = Pattern.compile(REGEX_INT_FIELD);
-
-	private final String[] fieldNames;
-	private final LogicalType[] logicalTypes;
-
-	public RowDataTypeInfo(RowType rowType) {
-		this(
-			rowType.getFields().stream().map(RowType.RowField::getType).toArray(LogicalType[]::new),
-			rowType.getFieldNames().toArray(new String[0]));
-	}
-
-	public RowDataTypeInfo(LogicalType... logicalTypes) {
-		this(logicalTypes, generateDefaultFieldNames(logicalTypes.length));
-	}
-
-	public RowDataTypeInfo(LogicalType[] logicalTypes, String[] fieldNames) {
-		super(RowData.class, Arrays.stream(logicalTypes)
-			.map(TypeInfoLogicalTypeConverter::fromLogicalTypeToTypeInfo)
-			.toArray(TypeInformation[]::new));
-		this.logicalTypes = logicalTypes;
-		checkNotNull(fieldNames, "FieldNames should not be null.");
-		checkArgument(logicalTypes.length == fieldNames.length,
-			"Number of field types and names is different.");
-		checkArgument(!hasDuplicateFieldNames(fieldNames),
-			"Field names are not unique.");
-		this.fieldNames = Arrays.copyOf(fieldNames, fieldNames.length);
-	}
-
-	public static String[] generateDefaultFieldNames(int length) {
-		String[] fieldNames = new String[length];
-		for (int i = 0; i < length; i++) {
-			fieldNames[i] = "f" + i;
-		}
-		return fieldNames;
-	}
-
-	@Override
-	public <X> TypeInformation<X> getTypeAt(String fieldExpression) {
-		Matcher matcher = PATTERN_NESTED_FIELDS.matcher(fieldExpression);
-		if (!matcher.matches()) {
-			if (fieldExpression.equals(Keys.ExpressionKeys.SELECT_ALL_CHAR) ||
-				fieldExpression.equals(Keys.ExpressionKeys.SELECT_ALL_CHAR_SCALA)) {
-				throw new InvalidFieldReferenceException("Wildcard expressions are not allowed here.");
-			} else {
-				throw new InvalidFieldReferenceException(
-					"Invalid format of Row field expression \"" + fieldExpression + "\".");
-			}
-		}
-
-		String field = matcher.group(1);
-
-		Matcher intFieldMatcher = PATTERN_INT_FIELD.matcher(field);
-		int fieldIndex;
-		if (intFieldMatcher.matches()) {
-			// field expression is an integer
-			fieldIndex = Integer.valueOf(field);
-		} else {
-			fieldIndex = this.getFieldIndex(field);
-		}
-		// fetch the field type will throw exception if the index is illegal
-		TypeInformation<X> fieldType = this.getTypeAt(fieldIndex);
-
-		String tail = matcher.group(3);
-		if (tail == null) {
-			// found the type
-			return fieldType;
-		} else {
-			if (fieldType instanceof CompositeType) {
-				return ((CompositeType<?>) fieldType).getTypeAt(tail);
-			} else {
-				throw new InvalidFieldReferenceException(
-					"Nested field expression \"" + tail + "\" not possible on atomic type " + fieldType + ".");
-			}
-		}
-	}
-
-	@Override
-	public TypeComparator<RowData> createComparator(
-		int[] logicalKeyFields,
-		boolean[] orders,
-		int logicalFieldOffset,
-		ExecutionConfig config) {
-		// TODO support it.
-		throw new UnsupportedOperationException("Not support yet!");
-	}
-
-	@Override
-	public String[] getFieldNames() {
-		return fieldNames;
-	}
-
-	@Override
-	public int getFieldIndex(String fieldName) {
-		for (int i = 0; i < fieldNames.length; i++) {
-			if (fieldNames[i].equals(fieldName)) {
-				return i;
-			}
-		}
-		return -1;
-	}
-
-	@Override
-	public boolean canEqual(Object obj) {
-		return obj instanceof RowDataTypeInfo;
-	}
-
-	@Override
-	public int hashCode() {
-		return 31 * super.hashCode() + Arrays.hashCode(fieldNames);
-	}
-
-	@Override
-	public String toString() {
-		StringBuilder bld = new StringBuilder("RowData");
-		if (logicalTypes.length > 0) {
-			bld.append('(').append(fieldNames[0]).append(": ").append(logicalTypes[0]);
-
-			for (int i = 1; i < logicalTypes.length; i++) {
-				bld.append(", ").append(fieldNames[i]).append(": ").append(logicalTypes[i]);
-			}
-
-			bld.append(')');
-		}
-		return bld.toString();
-	}
-
-	/**
-	 * Returns the field types of the row. The order matches the order of the field names.
-	 */
-	public TypeInformation<?>[] getFieldTypes() {
-		return types;
-	}
-
-	private boolean hasDuplicateFieldNames(String[] fieldNames) {
-		HashSet<String> names = new HashSet<>();
-		for (String field : fieldNames) {
-			if (!names.add(field)) {
-				return true;
-			}
-		}
-		return false;
-	}
-
-	@Override
-	public TypeComparatorBuilder<RowData> createTypeComparatorBuilder() {
-		throw new UnsupportedOperationException("Not support!");
-	}
-
-	@Override
-	public RowDataSerializer createSerializer(ExecutionConfig config) {
-		return new RowDataSerializer(config, logicalTypes);
-	}
-
-	public LogicalType[] getLogicalTypes() {
-		return logicalTypes;
-	}
-
-	public RowType toRowType() {
-		return RowType.of(logicalTypes, fieldNames);
-	}
-
-	public static RowDataTypeInfo of(RowType rowType) {
-		return new RowDataTypeInfo(
-			rowType.getChildren().toArray(new LogicalType[0]),
-			rowType.getFieldNames().toArray(new String[0]));
-	}
-}
diff --git a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfo.java b/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfo.java
deleted file mode 100644
index 27ace14eda46d..0000000000000
--- a/flink-table/flink-table-runtime-blink/src/main/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfo.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.table.runtime.types.InternalSerializers;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
-import org.apache.flink.util.Preconditions;
-
-import java.util.Objects;
-
-/**
- * Type information that wraps a serializer that originated from a {@link LogicalType}.
- *
- * <p>Use {@link #of(LogicalType)} for type information of internal data structures.
- */
-@Internal
-public final class WrapperTypeInfo<T> extends TypeInformation<T> {
-
-	private static final String FORMAT = "%s(%s, %s)";
-
-	private final LogicalType type;
-
-	private final Class<T> typeClass;
-
-	private final TypeSerializer<T> typeSerializer;
-
-	public WrapperTypeInfo(LogicalType type, Class<T> typeClass, TypeSerializer<T> typeSerializer) {
-		this.type = Preconditions.checkNotNull(type);
-		this.typeClass = Preconditions.checkNotNull(typeClass);
-		this.typeSerializer = Preconditions.checkNotNull(typeSerializer);
-	}
-
-	/**
-	 * Creates type information for a logical type represented by internal data structures.
-	 */
-	@SuppressWarnings({"unchecked", "rawtypes"})
-	public static <T> WrapperTypeInfo<T> of(LogicalType type) {
-		final Class<?> typeClass = LogicalTypeUtils.toInternalConversionClass(type);
-		final TypeSerializer<?> serializer = InternalSerializers.create(type);
-		return (WrapperTypeInfo<T>) new WrapperTypeInfo(type, typeClass, serializer);
-	}
-
-	public LogicalType toLogicalType() {
-		return type;
-	}
-
-	public RowType toRowType() {
-		return (RowType) type;
-	}
-
-	@Override
-	public boolean isBasicType() {
-		return false;
-	}
-
-	@Override
-	public boolean isTupleType() {
-		return false;
-	}
-
-	@Override
-	public int getArity() {
-		return 1;
-	}
-
-	@Override
-	public int getTotalFields() {
-		return 1;
-	}
-
-	@Override
-	public Class<T> getTypeClass() {
-		return typeClass;
-	}
-
-	@Override
-	public boolean isKeyType() {
-		return false;
-	}
-
-	@Override
-	public TypeSerializer<T> createSerializer(ExecutionConfig config) {
-		return typeSerializer;
-	}
-
-	@Override
-	public String toString() {
-		return String.format(
-			FORMAT,
-			type.asSummaryString(),
-			typeClass.getName(),
-			typeSerializer.getClass().getName());
-	}
-
-	@Override
-	public boolean equals(Object o) {
-		if (this == o) {
-			return true;
-		}
-		if (o == null || getClass() != o.getClass()) {
-			return false;
-		}
-		final WrapperTypeInfo<?> that = (WrapperTypeInfo<?>) o;
-		return type.equals(that.type) &&
-			typeClass.equals(that.typeClass) &&
-			typeSerializer.equals(that.typeSerializer);
-	}
-
-	@Override
-	public int hashCode() {
-		return Objects.hash(type, typeClass, typeSerializer);
-	}
-
-	@Override
-	public boolean canEqual(Object obj) {
-		return obj instanceof WrapperTypeInfo;
-	}
-}
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryArrayDataTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryArrayDataTest.java
index 2ffbe4b05c1be..b1f9cbbc87061 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryArrayDataTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryArrayDataTest.java
@@ -73,7 +73,7 @@ public void testArray() {
 		{
 			BinaryRowData row2 = new BinaryRowData(1);
 			BinaryRowWriter writer2 = new BinaryRowWriter(row2);
-			writer2.writeArray(0, array, new ArrayDataSerializer(DataTypes.INT().getLogicalType(), null));
+			writer2.writeArray(0, array, new ArrayDataSerializer(DataTypes.INT().getLogicalType()));
 			writer2.complete();
 
 			BinaryArrayData array2 = (BinaryArrayData) row2.getArray(0);
@@ -89,7 +89,7 @@ public void testArray() {
 
 			BinaryRowData row2 = new BinaryRowData(1);
 			BinaryRowWriter writer2 = new BinaryRowWriter(row2);
-			writer2.writeArray(0, array3, new ArrayDataSerializer(DataTypes.INT().getLogicalType(), null));
+			writer2.writeArray(0, array3, new ArrayDataSerializer(DataTypes.INT().getLogicalType()));
 			writer2.complete();
 
 			BinaryArrayData array2 = (BinaryArrayData) row2.getArray(0);
@@ -318,7 +318,7 @@ public void testArrayTypes() {
 			BinaryArrayData array = new BinaryArrayData();
 			BinaryArrayWriter writer = new BinaryArrayWriter(array, 2, 8);
 			writer.setNullAt(0);
-			writer.writeArray(1, subArray, new ArrayDataSerializer(DataTypes.INT().getLogicalType(), null));
+			writer.writeArray(1, subArray, new ArrayDataSerializer(DataTypes.INT().getLogicalType()));
 			writer.complete();
 
 			assertTrue(array.isNullAt(0));
@@ -335,7 +335,7 @@ public void testArrayTypes() {
 			BinaryArrayWriter writer = new BinaryArrayWriter(array, 2, 8);
 			writer.setNullAt(0);
 			writer.writeMap(1, BinaryMapData.valueOf(subArray, subArray),
-					new MapDataSerializer(DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType(), null));
+					new MapDataSerializer(DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType()));
 			writer.complete();
 
 			assertTrue(array.isNullAt(0));
@@ -368,7 +368,7 @@ public void testMap() {
 		BinaryRowData row = new BinaryRowData(1);
 		BinaryRowWriter rowWriter = new BinaryRowWriter(row);
 		rowWriter.writeMap(0, binaryMap,
-				new MapDataSerializer(DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType(), null));
+				new MapDataSerializer(DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType()));
 		rowWriter.complete();
 
 		BinaryMapData map = (BinaryMapData) row.getMap(0);
@@ -487,7 +487,7 @@ public void testNested() {
 		BinaryArrayData array = new BinaryArrayData();
 		BinaryArrayWriter writer = new BinaryArrayWriter(array, 2, 8);
 		writer.writeRow(0, GenericRowData.of(fromString("1"), 1),
-				new RowDataSerializer(null, RowType.of(new VarCharType(VarCharType.MAX_LENGTH), new IntType())));
+				new RowDataSerializer(RowType.of(new VarCharType(VarCharType.MAX_LENGTH), new IntType())));
 		writer.setNullAt(1);
 		writer.complete();
 
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java
index 273964498740d..c9326b1558a04 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/BinaryRowDataTest.java
@@ -475,7 +475,7 @@ public void testNested() {
 		BinaryRowData row = new BinaryRowData(2);
 		BinaryRowWriter writer = new BinaryRowWriter(row);
 		writer.writeRow(0, GenericRowData.of(fromString("1"), 1),
-				new RowDataSerializer(null, RowType.of(new VarCharType(VarCharType.MAX_LENGTH), new IntType())));
+				new RowDataSerializer(RowType.of(new VarCharType(VarCharType.MAX_LENGTH), new IntType())));
 		writer.setNullAt(1);
 		writer.complete();
 
@@ -518,8 +518,7 @@ public void testBinaryArray() {
 		// 2. test write array to binary row
 		BinaryRowData row = new BinaryRowData(1);
 		BinaryRowWriter rowWriter = new BinaryRowWriter(row);
-		ArrayDataSerializer serializer = new ArrayDataSerializer(
-			DataTypes.INT().getLogicalType(), new ExecutionConfig());
+		ArrayDataSerializer serializer = new ArrayDataSerializer(DataTypes.INT().getLogicalType());
 		rowWriter.writeArray(0, array, serializer);
 		rowWriter.complete();
 
@@ -543,8 +542,7 @@ public void testGenericArray() {
 		// 2. test write array to binary row
 		BinaryRowData row2 = new BinaryRowData(1);
 		BinaryRowWriter writer2 = new BinaryRowWriter(row2);
-		ArrayDataSerializer serializer = new ArrayDataSerializer(
-			DataTypes.INT().getLogicalType(), new ExecutionConfig());
+		ArrayDataSerializer serializer = new ArrayDataSerializer(DataTypes.INT().getLogicalType());
 		writer2.writeArray(0, array, serializer);
 		writer2.complete();
 
@@ -580,8 +578,7 @@ public void testBinaryMap() {
 		BinaryRowWriter rowWriter = new BinaryRowWriter(row);
 		MapDataSerializer serializer = new MapDataSerializer(
 			DataTypes.STRING().getLogicalType(),
-			DataTypes.INT().getLogicalType(),
-			new ExecutionConfig());
+			DataTypes.INT().getLogicalType());
 		rowWriter.writeMap(0, binaryMap, serializer);
 		rowWriter.complete();
 
@@ -613,8 +610,7 @@ public void testGenericMap() {
 		BinaryRowWriter rowWriter = new BinaryRowWriter(row);
 		MapDataSerializer serializer = new MapDataSerializer(
 			DataTypes.INT().getLogicalType(),
-			DataTypes.STRING().getLogicalType(),
-			new ExecutionConfig());
+			DataTypes.STRING().getLogicalType());
 		rowWriter.writeMap(0, genericMap, serializer);
 		rowWriter.complete();
 
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/DataFormatConvertersTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/DataFormatConvertersTest.java
index ac53aa2ff1664..fbcce6e75cd66 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/DataFormatConvertersTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/DataFormatConvertersTest.java
@@ -37,8 +37,8 @@
 import org.apache.flink.table.data.util.DataFormatConverters.DataFormatConverter;
 import org.apache.flink.table.runtime.functions.SqlDateTimeUtils;
 import org.apache.flink.table.runtime.typeutils.DecimalDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.LegacyTimestampTypeInfo;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
 import org.apache.flink.table.runtime.typeutils.StringDataTypeInfo;
 import org.apache.flink.table.types.AtomicDataType;
 import org.apache.flink.table.types.DataType;
@@ -180,9 +180,9 @@ public void testTypes() {
 		}
 		test(new RowTypeInfo(simpleTypes), new Row(simpleTypes.length));
 		test(new RowTypeInfo(simpleTypes), Row.ofKind(RowKind.DELETE, simpleValues));
-		test(new RowDataTypeInfo(new VarCharType(VarCharType.MAX_LENGTH), new IntType()),
+		test(InternalTypeInfo.ofFields(new VarCharType(VarCharType.MAX_LENGTH), new IntType()),
 				GenericRowData.of(StringData.fromString("hehe"), 111));
-		test(new RowDataTypeInfo(new VarCharType(VarCharType.MAX_LENGTH), new IntType()), GenericRowData.of(null, null));
+		test(InternalTypeInfo.ofFields(new VarCharType(VarCharType.MAX_LENGTH), new IntType()), GenericRowData.of(null, null));
 
 		test(new DecimalDataTypeInfo(10, 5), null);
 		test(new DecimalDataTypeInfo(10, 5), DecimalDataUtils.castFrom(5.555, 10, 5));
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/RowDataTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/RowDataTest.java
index eea2277a1f399..c3cb3387c3fa8 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/RowDataTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/data/RowDataTest.java
@@ -124,10 +124,9 @@ private BinaryRowData getBinaryRow() {
 		writer.writeRawValue(9, generic, genericSerializer);
 		writer.writeDecimal(10, decimal1, 5);
 		writer.writeDecimal(11, decimal2, 20);
-		writer.writeArray(12, array, new ArrayDataSerializer(DataTypes.INT().getLogicalType(), null));
-		writer.writeMap(13, map, new MapDataSerializer(
-			DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType(), null));
-		writer.writeRow(14, underRow, new RowDataSerializer(null, RowType.of(new IntType(), new IntType())));
+		writer.writeArray(12, array, new ArrayDataSerializer(DataTypes.INT().getLogicalType()));
+		writer.writeMap(13, map, new MapDataSerializer(DataTypes.INT().getLogicalType(), DataTypes.INT().getLogicalType()));
+		writer.writeRow(14, underRow, new RowDataSerializer(RowType.of(new IntType(), new IntType())));
 		writer.writeBinary(15, bytes);
 		writer.writeTimestamp(16, timestamp1, 3);
 		writer.writeTimestamp(17, timestamp2, 9);
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionTestBase.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionTestBase.java
index 4da8d1b683935..6072be5c433b3 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionTestBase.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/deduplicate/DeduplicateFunctionTestBase.java
@@ -19,7 +19,8 @@
 package org.apache.flink.table.runtime.operators.deduplicate;
 
 import org.apache.flink.api.common.time.Time;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;
 import org.apache.flink.table.runtime.util.GenericRowRecordSortComparator;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
@@ -33,15 +34,16 @@
 abstract class DeduplicateFunctionTestBase {
 
 	Time minTime = Time.milliseconds(10);
-	RowDataTypeInfo inputRowType = new RowDataTypeInfo(new VarCharType(VarCharType.MAX_LENGTH), new BigIntType(),
+	InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(new VarCharType(VarCharType.MAX_LENGTH), new BigIntType(),
 			new IntType());
 
 	int rowKeyIdx = 1;
-	BinaryRowDataKeySelector rowKeySelector = new BinaryRowDataKeySelector(new int[] { rowKeyIdx },
-			inputRowType.getLogicalTypes());
+	BinaryRowDataKeySelector rowKeySelector = new BinaryRowDataKeySelector(
+		new int[] { rowKeyIdx },
+		inputRowType.toRowFieldTypes());
 
 	RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(
-			inputRowType.getFieldTypes(),
-			new GenericRowRecordSortComparator(rowKeyIdx, inputRowType.getLogicalTypes()[rowKeyIdx]));
+		inputRowType.toRowFieldTypes(),
+		new GenericRowRecordSortComparator(rowKeyIdx, inputRowType.toRowFieldTypes()[rowKeyIdx]));
 
 }
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/AsyncLookupJoinHarnessTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/AsyncLookupJoinHarnessTest.java
index 2b122fdf72f86..80fa8bea52a9d 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/AsyncLookupJoinHarnessTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/AsyncLookupJoinHarnessTest.java
@@ -18,11 +18,9 @@
 
 package org.apache.flink.table.runtime.operators.join;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.functions.AbstractRichFunction;
 import org.apache.flink.api.common.functions.FlatMapFunction;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.streaming.api.datastream.AsyncDataStream;
@@ -31,6 +29,7 @@
 import org.apache.flink.streaming.api.functions.async.RichAsyncFunction;
 import org.apache.flink.streaming.api.operators.async.AsyncWaitOperatorFactory;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryStringData;
@@ -42,11 +41,10 @@
 import org.apache.flink.table.runtime.operators.join.lookup.AsyncLookupJoinWithCalcRunner;
 import org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner;
 import org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
-import org.apache.flink.table.types.logical.IntType;
-import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
 import org.junit.Test;
@@ -75,18 +73,19 @@ public class AsyncLookupJoinHarnessTest {
 	private static final int ASYNC_TIMEOUT_MS = 3000;
 
 	private final TypeSerializer<RowData> inSerializer = new RowDataSerializer(
-		new ExecutionConfig(),
-		new IntType(),
-		new VarCharType(VarCharType.MAX_LENGTH));
-
-	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new TypeInformation[]{
-		Types.INT,
-		Types.STRING,
-		Types.INT,
-		Types.STRING
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType());
+
+	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new LogicalType[]{
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType()
 	});
 
-	private RowDataTypeInfo rightRowTypeInfo = new RowDataTypeInfo(new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
+	private InternalTypeInfo<RowData> rightRowTypeInfo = InternalTypeInfo.ofFields(
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType());
 	private TypeInformation<?> fetcherReturnType = rightRowTypeInfo;
 
 	@Test
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/Int2HashJoinOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/Int2HashJoinOperatorTest.java
index c363bbc5221e8..2d6a1e2f1b374 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/Int2HashJoinOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/Int2HashJoinOperatorTest.java
@@ -34,7 +34,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.JoinCondition;
 import org.apache.flink.table.runtime.generated.Projection;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.UniformBinaryRowGenerator;
 import org.apache.flink.table.types.logical.IntType;
 import org.apache.flink.table.types.logical.RowType;
@@ -250,8 +250,8 @@ static void joinAndAssert(
 			int expectOutKeySize,
 			int expectOutVal,
 			boolean semiJoin) throws Exception {
-		RowDataTypeInfo typeInfo = new RowDataTypeInfo(new IntType(), new IntType());
-		RowDataTypeInfo rowDataTypeInfo = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(new IntType(), new IntType());
+		InternalTypeInfo<RowData> rowDataTypeInfo = InternalTypeInfo.ofFields(
 				new IntType(), new IntType(), new IntType(), new IntType());
 		TwoInputStreamTaskTestHarness<BinaryRowData, BinaryRowData, JoinedRowData> testHarness =
 			new TwoInputStreamTaskTestHarness<>(
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/LookupJoinHarnessTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/LookupJoinHarnessTest.java
index 7ea0f753a75a0..016d2168b1f76 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/LookupJoinHarnessTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/LookupJoinHarnessTest.java
@@ -18,14 +18,12 @@
 
 package org.apache.flink.table.runtime.operators.join;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.functions.FlatMapFunction;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.common.typeinfo.Types;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.streaming.api.functions.ProcessFunction;
 import org.apache.flink.streaming.api.operators.ProcessOperator;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
+import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.JoinedRowData;
 import org.apache.flink.table.data.RowData;
@@ -37,8 +35,7 @@
 import org.apache.flink.table.runtime.operators.join.lookup.LookupJoinWithCalcRunner;
 import org.apache.flink.table.runtime.typeutils.RowDataSerializer;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
-import org.apache.flink.table.types.logical.IntType;
-import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Collector;
 
 import org.junit.Test;
@@ -59,15 +56,14 @@
 public class LookupJoinHarnessTest {
 
 	private final TypeSerializer<RowData> inSerializer = new RowDataSerializer(
-		new ExecutionConfig(),
-			new IntType(),
-			new VarCharType(VarCharType.MAX_LENGTH));
-
-	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new TypeInformation[]{
-		Types.INT,
-		Types.STRING,
-		Types.INT,
-		Types.STRING
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType());
+
+	private final RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(new LogicalType[]{
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType(),
+		DataTypes.INT().getLogicalType(),
+		DataTypes.STRING().getLogicalType()
 	});
 
 	@Test
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/RandomSortMergeInnerJoinTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/RandomSortMergeInnerJoinTest.java
index f231b0a53fcd2..2d09a42148046 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/RandomSortMergeInnerJoinTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/RandomSortMergeInnerJoinTest.java
@@ -41,7 +41,7 @@
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.data.writer.BinaryRowWriter;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.IntType;
 import org.apache.flink.table.types.logical.VarCharType;
 import org.apache.flink.util.MutableObjectIterator;
@@ -237,8 +237,8 @@ public static LinkedBlockingQueue<Object> join(
 			MutableObjectIterator<Tuple2<Integer, String>> input1,
 			MutableObjectIterator<Tuple2<Integer, String>> input2,
 			boolean input1First) throws Exception {
-		RowDataTypeInfo typeInfo = new RowDataTypeInfo(new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
-		RowDataTypeInfo joinedInfo = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
+		InternalTypeInfo<RowData> joinedInfo = InternalTypeInfo.ofFields(
 				new IntType(), new VarCharType(VarCharType.MAX_LENGTH), new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
 		final TwoInputStreamTaskTestHarness<BinaryRowData, BinaryRowData, JoinedRowData> testHarness =
 			new TwoInputStreamTaskTestHarness<>(
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2HashJoinOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2HashJoinOperatorTest.java
index 08399071635fc..ae4b4303bc168 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2HashJoinOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2HashJoinOperatorTest.java
@@ -33,7 +33,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.JoinCondition;
 import org.apache.flink.table.runtime.generated.Projection;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.VarCharType;
 
@@ -48,8 +48,8 @@
  */
 public class String2HashJoinOperatorTest implements Serializable {
 
-	private RowDataTypeInfo typeInfo = new RowDataTypeInfo(new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH));
-	private RowDataTypeInfo joinedInfo = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH));
+	private InternalTypeInfo<RowData> joinedInfo = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH));
 	private transient TwoInputStreamTaskTestHarness<BinaryRowData, BinaryRowData, JoinedRowData> testHarness;
 	private ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2SortMergeJoinOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2SortMergeJoinOperatorTest.java
index 188c53c8d1328..50ec03bb381da 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2SortMergeJoinOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/String2SortMergeJoinOperatorTest.java
@@ -26,6 +26,7 @@
 import org.apache.flink.streaming.runtime.tasks.TwoInputStreamTaskTestHarness;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 import org.apache.flink.table.data.JoinedRowData;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.runtime.generated.GeneratedJoinCondition;
 import org.apache.flink.table.runtime.generated.GeneratedNormalizedKeyComputer;
@@ -38,7 +39,7 @@
 import org.apache.flink.table.runtime.operators.join.String2HashJoinOperatorTest.MyProjection;
 import org.apache.flink.table.runtime.operators.sort.StringNormalizedKeyComputer;
 import org.apache.flink.table.runtime.operators.sort.StringRecordComparator;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.VarCharType;
 
 import org.junit.Test;
@@ -59,9 +60,9 @@
 public class String2SortMergeJoinOperatorTest {
 
 	private boolean leftIsSmall;
-	RowDataTypeInfo typeInfo = new RowDataTypeInfo(
+	InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH));
-	private RowDataTypeInfo joinedInfo = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> joinedInfo = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH), new VarCharType(VarCharType.MAX_LENGTH));
 
 	public String2SortMergeJoinOperatorTest(boolean leftIsSmall) {
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoinTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoinTest.java
index c2df11b0b1cd7..fa4c78fec8442 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoinTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/ProcTimeIntervalJoinTest.java
@@ -23,7 +23,7 @@
 import org.apache.flink.streaming.util.KeyedTwoInputStreamOperatorTestHarness;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;
 
 import org.junit.Test;
@@ -40,9 +40,10 @@
 public class ProcTimeIntervalJoinTest extends TimeIntervalStreamJoinTestBase {
 
 	private int keyIdx = 0;
-	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(new int[] { keyIdx },
-			rowType.getLogicalTypes());
-	private TypeInformation<RowData> keyType = new RowDataTypeInfo();
+	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(
+		new int[] { keyIdx },
+		rowType.toRowFieldTypes());
+	private TypeInformation<RowData> keyType = InternalTypeInfo.ofFields();
 
 
 	/** a.proctime >= b.proctime - 10 and a.proctime <= b.proctime + 20. **/
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoinTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoinTest.java
index b733f0bb3b8eb..712fabaf26450 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoinTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/RowTimeIntervalJoinTest.java
@@ -25,7 +25,7 @@
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType;
 import org.apache.flink.table.runtime.operators.join.KeyedCoProcessOperatorWithWatermarkDelay;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;
 
 import org.junit.Test;
@@ -42,9 +42,10 @@
 public class RowTimeIntervalJoinTest extends TimeIntervalStreamJoinTestBase {
 
 	private int keyIdx = 1;
-	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(new int[] { keyIdx },
-			rowType.getLogicalTypes());
-	private TypeInformation<RowData> keyType = new RowDataTypeInfo();
+	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(
+		new int[] { keyIdx },
+		rowType.toRowFieldTypes());
+	private TypeInformation<RowData> keyType = InternalTypeInfo.ofFields();
 
 	/** a.rowtime >= b.rowtime - 10 and a.rowtime <= b.rowtime + 20. **/
 	@Test
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalStreamJoinTestBase.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalStreamJoinTestBase.java
index 3849f2a3077de..257892bda6c2b 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalStreamJoinTestBase.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/join/interval/TimeIntervalStreamJoinTestBase.java
@@ -21,7 +21,7 @@
 import org.apache.flink.api.common.functions.FlatJoinFunction;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.runtime.generated.GeneratedFunction;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.VarCharType;
@@ -30,11 +30,11 @@
  * Base Test for all subclass of {@link TimeIntervalJoin}.
  */
 abstract class TimeIntervalStreamJoinTestBase {
-	RowDataTypeInfo rowType = new RowDataTypeInfo(new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH));
+	InternalTypeInfo<RowData> rowType = InternalTypeInfo.ofFields(new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH));
 
-	private RowDataTypeInfo outputRowType = new RowDataTypeInfo(new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH),
+	private InternalTypeInfo<RowData> outputRowType = InternalTypeInfo.ofFields(new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH),
 			new BigIntType(), new VarCharType(VarCharType.MAX_LENGTH));
-	RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(outputRowType.getFieldTypes());
+	RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(outputRowType.toRowFieldTypes());
 
 	private String funcCode =
 			"public class IntervalJoinFunction\n" +
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/over/NonBufferOverWindowOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/over/NonBufferOverWindowOperatorTest.java
index 7fcae5eb22674..6012a8a2ee8fa 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/over/NonBufferOverWindowOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/over/NonBufferOverWindowOperatorTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.table.runtime.operators.over;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.streaming.api.graph.StreamConfig;
 import org.apache.flink.streaming.api.operators.Output;
 import org.apache.flink.streaming.api.operators.StreamingRuntimeContext;
@@ -68,7 +67,7 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 		}
 	};
 	static RowType inputType = RowType.of(new IntType(), new BigIntType(), new BigIntType());
-	static RowDataSerializer inputSer = new RowDataSerializer(new ExecutionConfig(), inputType);
+	static RowDataSerializer inputSer = new RowDataSerializer(inputType);
 
 	private static GeneratedAggsHandleFunction[] functions;
 
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/rank/TopNFunctionTestBase.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/rank/TopNFunctionTestBase.java
index 2a4f43f3473ce..7c082874596e5 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/rank/TopNFunctionTestBase.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/rank/TopNFunctionTestBase.java
@@ -29,7 +29,7 @@
 import org.apache.flink.table.runtime.generated.RecordComparator;
 import org.apache.flink.table.runtime.generated.RecordEqualiser;
 import org.apache.flink.table.runtime.operators.sort.IntRecordComparator;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;
 import org.apache.flink.table.runtime.util.GenericRowRecordSortComparator;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
@@ -57,7 +57,7 @@ abstract class TopNFunctionTestBase {
 	Time maxTime = Time.milliseconds(20);
 	long cacheSize = 10000L;
 
-	RowDataTypeInfo inputRowType = new RowDataTypeInfo(
+	InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH),
 			new BigIntType(),
 			new IntType());
@@ -75,8 +75,9 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 
 	private int sortKeyIdx = 2;
 
-	BinaryRowDataKeySelector sortKeySelector = new BinaryRowDataKeySelector(new int[] { sortKeyIdx },
-			inputRowType.getLogicalTypes());
+	BinaryRowDataKeySelector sortKeySelector = new BinaryRowDataKeySelector(
+		new int[] { sortKeyIdx },
+		inputRowType.toRowFieldTypes());
 
 	static GeneratedRecordEqualiser generatedEqualiser = new GeneratedRecordEqualiser("", "", new Object[0]) {
 
@@ -90,29 +91,30 @@ public RecordEqualiser newInstance(ClassLoader classLoader) {
 
 	private int partitionKeyIdx = 0;
 
-	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(new int[] { partitionKeyIdx },
-			inputRowType.getLogicalTypes());
+	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(
+		new int[] { partitionKeyIdx },
+		inputRowType.toRowFieldTypes());
 
-	private RowDataTypeInfo outputTypeWithoutRowNumber = inputRowType;
+	private InternalTypeInfo<RowData> outputTypeWithoutRowNumber = inputRowType;
 
-	private RowDataTypeInfo outputTypeWithRowNumber = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> outputTypeWithRowNumber = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH),
 			new BigIntType(),
 			new IntType(),
 			new BigIntType());
 
 	RowDataHarnessAssertor assertorWithoutRowNumber = new RowDataHarnessAssertor(
-			outputTypeWithoutRowNumber.getFieldTypes(),
-			new GenericRowRecordSortComparator(sortKeyIdx, outputTypeWithoutRowNumber.getLogicalTypes()[sortKeyIdx]));
+			outputTypeWithoutRowNumber.toRowFieldTypes(),
+			new GenericRowRecordSortComparator(sortKeyIdx, outputTypeWithoutRowNumber.toRowFieldTypes()[sortKeyIdx]));
 
 	RowDataHarnessAssertor assertorWithRowNumber = new RowDataHarnessAssertor(
-			outputTypeWithRowNumber.getFieldTypes(),
-			new GenericRowRecordSortComparator(sortKeyIdx, outputTypeWithRowNumber.getLogicalTypes()[sortKeyIdx]));
+			outputTypeWithRowNumber.toRowFieldTypes(),
+			new GenericRowRecordSortComparator(sortKeyIdx, outputTypeWithRowNumber.toRowFieldTypes()[sortKeyIdx]));
 
 	// rowKey only used in UpdateRankFunction
 	private int rowKeyIdx = 1;
 	BinaryRowDataKeySelector rowKeySelector = new BinaryRowDataKeySelector(new int[] { rowKeyIdx },
-			inputRowType.getLogicalTypes());
+			inputRowType.toRowFieldTypes());
 
 	/** RankEnd column must be long, int or short type, but could not be string type yet. */
 	@Test(expected = UnsupportedOperationException.class)
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperatorTest.java
index 9d1196eb3f6c2..a719dea7f021d 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/ProcTimeSortOperatorTest.java
@@ -25,7 +25,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.IntType;
@@ -43,7 +43,7 @@
  */
 public class ProcTimeSortOperatorTest {
 
-	private RowDataTypeInfo inputRowType = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(
 			new IntType(),
 			new BigIntType(),
 			new VarCharType(VarCharType.MAX_LENGTH),
@@ -60,7 +60,7 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 		}
 	};
 
-	private RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.getFieldTypes());
+	private RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.toRowFieldTypes());
 
 	@Test
 	public void test() throws Exception {
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperatorTest.java
index d6a80e86eb1f7..e9c01a60626dc 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/RowTimeSortOperatorTest.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
 import org.apache.flink.table.runtime.keyselector.EmptyRowDataKeySelector;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.IntType;
@@ -46,7 +46,7 @@ public class RowTimeSortOperatorTest {
 
 	@Test
 	public void testSortOnTwoFields() throws Exception {
-		RowDataTypeInfo inputRowType = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(
 				new IntType(),
 				new BigIntType(),
 				new VarCharType(VarCharType.MAX_LENGTH),
@@ -65,7 +65,7 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 			}
 		};
 
-		RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.getFieldTypes());
+		RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.toRowFieldTypes());
 
 		RowTimeSortOperator operator = createSortOperator(inputRowType, rowTimeIdx, gComparator);
 		OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);
@@ -129,13 +129,13 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 
 	@Test
 	public void testOnlySortOnRowTime() throws Exception {
-		RowDataTypeInfo inputRowType = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(
 				new BigIntType(),
 				new BigIntType(),
 				new VarCharType(VarCharType.MAX_LENGTH),
 				new IntType());
 		int rowTimeIdx = 0;
-		RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.getFieldTypes());
+		RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.toRowFieldTypes());
 		RowTimeSortOperator operator = createSortOperator(inputRowType, rowTimeIdx, null);
 		OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);
 		testHarness.open();
@@ -194,7 +194,7 @@ public void testOnlySortOnRowTime() throws Exception {
 		assertor.assertOutputEquals("output wrong.", expectedOutput, testHarness.getOutput());
 	}
 
-	private RowTimeSortOperator createSortOperator(RowDataTypeInfo inputRowType, int rowTimeIdx,
+	private RowTimeSortOperator createSortOperator(InternalTypeInfo<RowData> inputRowType, int rowTimeIdx,
 			GeneratedRecordComparator gComparator) {
 		return new RowTimeSortOperator(inputRowType, rowTimeIdx, gComparator);
 	}
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperatorTest.java
index 4375f244ca95f..4ed3f77e4ce72 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/sort/StreamSortOperatorTest.java
@@ -24,7 +24,7 @@
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.runtime.generated.GeneratedRecordComparator;
 import org.apache.flink.table.runtime.generated.RecordComparator;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
 import org.apache.flink.table.types.logical.IntType;
 import org.apache.flink.table.types.logical.VarCharType;
@@ -41,7 +41,7 @@
  */
 public class StreamSortOperatorTest {
 
-	private RowDataTypeInfo inputRowType = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> inputRowType = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH),
 			new IntType());
 
@@ -56,7 +56,7 @@ public RecordComparator newInstance(ClassLoader classLoader) {
 		}
 	};
 
-	private RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.getFieldTypes());
+	private RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(inputRowType.toRowFieldTypes());
 
 	@Test
 	public void test() throws Exception {
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/window/WindowOperatorTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/window/WindowOperatorTest.java
index 283ef9be89722..aa068b1b4fa76 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/window/WindowOperatorTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/operators/window/WindowOperatorTest.java
@@ -39,7 +39,7 @@
 import org.apache.flink.table.runtime.operators.window.triggers.ElementTriggers;
 import org.apache.flink.table.runtime.operators.window.triggers.EventTimeTriggers;
 import org.apache.flink.table.runtime.operators.window.triggers.ProcessingTimeTriggers;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.runtime.util.BinaryRowDataKeySelector;
 import org.apache.flink.table.runtime.util.GenericRowRecordSortComparator;
 import org.apache.flink.table.runtime.util.RowDataHarnessAssertor;
@@ -115,7 +115,7 @@ private NamespaceAggsHandleFunctionBase getCountWindowAggFunction() {
 			new IntType(),
 			new BigIntType()};
 
-	private RowDataTypeInfo outputType = new RowDataTypeInfo(
+	private InternalTypeInfo<RowData> outputType = InternalTypeInfo.ofFields(
 			new VarCharType(VarCharType.MAX_LENGTH),
 			new BigIntType(),
 			new BigIntType(),
@@ -130,7 +130,7 @@ private NamespaceAggsHandleFunctionBase getCountWindowAggFunction() {
 	private BinaryRowDataKeySelector keySelector = new BinaryRowDataKeySelector(new int[] { 0 }, inputFieldTypes);
 	private TypeInformation<RowData> keyType = keySelector.getProducedType();
 	private RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(
-			outputType.getFieldTypes(),
+			outputType.toRowFieldTypes(),
 			new GenericRowRecordSortComparator(0, new VarCharType(VarCharType.MAX_LENGTH)));
 
 	private ConcurrentLinkedQueue<Object> doubleRecord(boolean isDouble, StreamRecord<RowData> record) {
@@ -751,7 +751,8 @@ public void testProcessingTimeSessionWindows() throws Throwable {
 		OneInputStreamOperatorTestHarness<RowData, RowData> testHarness = createTestHarness(operator);
 
 		RowDataHarnessAssertor assertor = new RowDataHarnessAssertor(
-				outputType.getFieldTypes(), new GenericRowRecordSortComparator(0, new VarCharType(VarCharType.MAX_LENGTH)));
+			outputType.toRowFieldTypes(),
+			new GenericRowRecordSortComparator(0, new VarCharType(VarCharType.MAX_LENGTH)));
 
 		ConcurrentLinkedQueue<Object> expectedOutput = new ConcurrentLinkedQueue<>();
 
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializerTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializerTest.java
index dcf58028949df..2f8721ec0e3f1 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializerTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ArrayDataSerializerTest.java
@@ -18,35 +18,19 @@
 
 package org.apache.flink.table.runtime.typeutils;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.SerializerTestBase;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.ArrayData;
 import org.apache.flink.table.data.ColumnarArrayData;
 import org.apache.flink.table.data.GenericArrayData;
-import org.apache.flink.table.data.RawValueData;
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.data.binary.BinaryArrayData;
 import org.apache.flink.table.data.vector.heap.HeapBytesVector;
 import org.apache.flink.table.data.writer.BinaryArrayWriter;
 import org.apache.flink.testutils.DeeplyEqualsChecker;
 
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
 import java.nio.charset.StandardCharsets;
 
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.MyObj;
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.MyObjSerializer;
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.snapshotAndReconfigure;
-import static org.junit.Assert.assertEquals;
-
 /**
  * A test for the {@link ArrayDataSerializer}.
  */
@@ -77,51 +61,9 @@ public ArrayDataSerializerTest() {
 		));
 	}
 
-	@Test
-	public void testExecutionConfigWithKryo() throws Exception {
-		// serialize base array
-		ExecutionConfig config = new ExecutionConfig();
-		config.enableForceKryo();
-		config.registerTypeWithKryoSerializer(MyObj.class, new MyObjSerializer());
-		final ArrayDataSerializer serializer = createSerializerWithConfig(config);
-
-		MyObj inputObj = new MyObj(114514, 1919810);
-		ArrayData inputArray = new GenericArrayData(new RawValueData[] {
-			RawValueData.fromObject(inputObj)
-		});
-
-		byte[] serialized;
-		try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {
-			serializer.serialize(inputArray, new DataOutputViewStreamWrapper(out));
-			serialized = out.toByteArray();
-		}
-
-		// deserialize base array using restored serializer
-		final ArrayDataSerializer restoreSerializer =
-			(ArrayDataSerializer) snapshotAndReconfigure(serializer, () -> createSerializerWithConfig(config));
-
-		ArrayData outputArray;
-		try (ByteArrayInputStream in = new ByteArrayInputStream(serialized)) {
-			outputArray = restoreSerializer.deserialize(new DataInputViewStreamWrapper(in));
-		}
-
-		TypeSerializer restoreEleSer = restoreSerializer.getEleSer();
-		assertEquals(serializer.getEleSer(), restoreEleSer);
-
-		MyObj outputObj = outputArray
-			.<MyObj>getRawValue(0)
-			.toObject(new KryoSerializer<>(MyObj.class, config));
-		assertEquals(inputObj, outputObj);
-	}
-
-	private ArrayDataSerializer createSerializerWithConfig(ExecutionConfig config) {
-		return new ArrayDataSerializer(
-			DataTypes.RAW(TypeInformation.of(MyObj.class)).getLogicalType(), config);
-	}
-
 	@Override
 	protected ArrayDataSerializer createSerializer() {
-		return new ArrayDataSerializer(DataTypes.STRING().getLogicalType(), new ExecutionConfig());
+		return new ArrayDataSerializer(DataTypes.STRING().getLogicalType());
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfoTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfoTest.java
similarity index 61%
rename from flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfoTest.java
rename to flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfoTest.java
index 6a850ec585e17..7acae67588e40 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/WrapperTypeInfoTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/ExternalTypeInfoTest.java
@@ -20,32 +20,24 @@
 
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeInformationTestBase;
-import org.apache.flink.api.common.typeutils.base.IntSerializer;
 import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
-import org.apache.flink.table.types.logical.IntType;
+import org.apache.flink.table.api.DataTypes;
 
+import java.nio.ByteBuffer;
 import java.time.DayOfWeek;
 
 /**
- * Test for {@link WrapperTypeInfo}.
+ * Test for {@link ExternalTypeInfo}.
  */
-public class WrapperTypeInfoTest extends TypeInformationTestBase<WrapperTypeInfo<?>> {
+public class ExternalTypeInfoTest extends TypeInformationTestBase<ExternalTypeInfo<?>> {
 
 	@Override
-	protected WrapperTypeInfo<?>[] getTestData() {
-		return new WrapperTypeInfo<?>[] {
-				new WrapperTypeInfo<>(
-					new IntType(),
-					Object.class,
-					new KryoSerializer<>(Object.class, new ExecutionConfig())),
-				new WrapperTypeInfo<>(
-					new IntType(),
-					DayOfWeek.class,
-					new KryoSerializer<>(DayOfWeek.class, new ExecutionConfig())),
-				new WrapperTypeInfo<>(
-					new IntType(),
-					Integer.class,
-					IntSerializer.INSTANCE)
+	protected ExternalTypeInfo<?>[] getTestData() {
+		return new ExternalTypeInfo<?>[] {
+				ExternalTypeInfo.of(
+					DataTypes.RAW(DayOfWeek.class, new KryoSerializer<>(DayOfWeek.class, new ExecutionConfig()))),
+				ExternalTypeInfo.of(
+					DataTypes.RAW(ByteBuffer.class, new KryoSerializer<>(ByteBuffer.class, new ExecutionConfig()))),
 		};
 	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfoTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfoTest.java
index a6d882ab81b6d..58f7ed70358c7 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfoTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfoTest.java
@@ -18,23 +18,31 @@
 
 package org.apache.flink.table.runtime.typeutils;
 
-import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeInformationTestBase;
+import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
+import org.apache.flink.table.api.DataTypes;
+
+import java.nio.ByteBuffer;
+import java.time.DayOfWeek;
 
 /**
- * Test for {@link StringDataTypeInfo}, {@link DecimalDataTypeInfo}.
+ * Test for {@link InternalTypeInfo}.
  */
-public class InternalTypeInfoTest extends TypeInformationTestBase<TypeInformation<?>> {
+public class InternalTypeInfoTest extends TypeInformationTestBase<InternalTypeInfo<?>> {
 
 	@Override
-	protected TypeInformation[] getTestData() {
-		return new TypeInformation[] {
-			StringDataTypeInfo.INSTANCE,
-			new DecimalDataTypeInfo(5, 2),
-			new TimestampDataTypeInfo(0),
-			new TimestampDataTypeInfo(3),
-			new TimestampDataTypeInfo(6),
-			new TimestampDataTypeInfo(9)
+	protected InternalTypeInfo<?>[] getTestData() {
+		return new InternalTypeInfo<?>[] {
+				InternalTypeInfo.of(
+					DataTypes.INT()
+						.getLogicalType()),
+				InternalTypeInfo.of(
+					DataTypes.RAW(DayOfWeek.class, new KryoSerializer<>(DayOfWeek.class, new ExecutionConfig()))
+						.getLogicalType()),
+				InternalTypeInfo.of(
+					DataTypes.RAW(ByteBuffer.class, new KryoSerializer<>(ByteBuffer.class, new ExecutionConfig()))
+						.getLogicalType()),
 		};
 	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfosTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfosTest.java
new file mode 100644
index 0000000000000..7ec0a4b746aea
--- /dev/null
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/InternalTypeInfosTest.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils;
+
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeInformationTestBase;
+
+/**
+ * Test for {@link StringDataTypeInfo}, {@link DecimalDataTypeInfo}.
+ */
+public class InternalTypeInfosTest extends TypeInformationTestBase<TypeInformation<?>> {
+
+	@Override
+	protected TypeInformation[] getTestData() {
+		return new TypeInformation[] {
+			StringDataTypeInfo.INSTANCE,
+			new DecimalDataTypeInfo(5, 2),
+			new TimestampDataTypeInfo(0),
+			new TimestampDataTypeInfo(3),
+			new TimestampDataTypeInfo(6),
+			new TimestampDataTypeInfo(9)
+		};
+	}
+}
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/MapDataSerializerTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/MapDataSerializerTest.java
index 0d56396d240c0..52da37884346a 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/MapDataSerializerTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/MapDataSerializerTest.java
@@ -18,17 +18,10 @@
 
 package org.apache.flink.table.runtime.typeutils;
 
-import org.apache.flink.api.common.ExecutionConfig;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.SerializerTestBase;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.table.api.DataTypes;
 import org.apache.flink.table.data.GenericMapData;
 import org.apache.flink.table.data.MapData;
-import org.apache.flink.table.data.RawValueData;
 import org.apache.flink.table.data.StringData;
 import org.apache.flink.table.data.binary.BinaryArrayData;
 import org.apache.flink.table.data.binary.BinaryMapData;
@@ -36,18 +29,10 @@
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.testutils.DeeplyEqualsChecker;
 
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
 import java.util.HashMap;
 import java.util.Map;
 
 import static org.apache.flink.table.data.util.MapDataUtil.convertToJavaMap;
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.MyObj;
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.MyObjSerializer;
-import static org.apache.flink.table.runtime.typeutils.SerializerTestUtil.snapshotAndReconfigure;
-import static org.junit.Assert.assertEquals;
 
 /**
  * A test for the {@link MapDataSerializer}.
@@ -72,63 +57,9 @@ public MapDataSerializerTest() {
 		));
 	}
 
-	@Test
-	public void testExecutionConfigWithKryo() throws Exception {
-		// serialize base array
-		ExecutionConfig config = new ExecutionConfig();
-		config.enableForceKryo();
-		config.registerTypeWithKryoSerializer(MyObj.class, new MyObjSerializer());
-		final MapDataSerializer serializer = createSerializerWithConfig(config);
-
-		int inputKey = 998244353;
-		MyObj inputObj = new MyObj(114514, 1919810);
-		Map<Object, Object> javaMap = new HashMap<>();
-		javaMap.put(inputKey, RawValueData.fromObject(inputObj));
-		MapData inputMap = new GenericMapData(javaMap);
-
-		byte[] serialized;
-		try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {
-			serializer.serialize(inputMap, new DataOutputViewStreamWrapper(out));
-			serialized = out.toByteArray();
-		}
-
-		// deserialize base array using restored serializer
-		final MapDataSerializer restoreSerializer =
-			(MapDataSerializer) snapshotAndReconfigure(serializer, () -> createSerializerWithConfig(config));
-
-		MapData outputMap;
-		try (ByteArrayInputStream in = new ByteArrayInputStream(serialized)) {
-			outputMap = restoreSerializer.deserialize(new DataInputViewStreamWrapper(in));
-		}
-
-		TypeSerializer restoreKeySer = restoreSerializer.getKeySerializer();
-		TypeSerializer restoreValueSer = restoreSerializer.getValueSerializer();
-		assertEquals(serializer.getKeySerializer(), restoreKeySer);
-		assertEquals(serializer.getValueSerializer(), restoreValueSer);
-
-		Map<Object, Object> outputJavaMap = convertToJavaMap(
-			outputMap,
-			DataTypes.INT().getLogicalType(),
-			DataTypes.RAW(TypeInformation.of(MyObj.class)).getLogicalType());
-		MyObj outputObj = ((RawValueData<MyObj>) outputJavaMap.get(inputKey))
-			.toObject(new KryoSerializer<>(MyObj.class, config));
-		assertEquals(inputObj, outputObj);
-	}
-
-	private MapDataSerializer createSerializerWithConfig(ExecutionConfig config) {
-		return new MapDataSerializer(
-			DataTypes.INT().getLogicalType(),
-			DataTypes.RAW(TypeInformation.of(MyObj.class)).getLogicalType(),
-			config);
-	}
-
-	private static MapDataSerializer newSer() {
-		return new MapDataSerializer(INT, STRING, new ExecutionConfig());
-	}
-
 	@Override
 	protected MapDataSerializer createSerializer() {
-		return newSer();
+		return new MapDataSerializer(INT, STRING);
 	}
 
 	@Override
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataSerializerTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataSerializerTest.java
index 7c4ec98df1baa..38c1f4d06acb6 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataSerializerTest.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataSerializerTest.java
@@ -90,7 +90,7 @@ public static Collection<Object[]> parameters() {
 	}
 
 	private static Object[] testRowDataSerializer() {
-		RowDataTypeInfo typeInfo = new RowDataTypeInfo(new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
+		InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(new IntType(), new VarCharType(VarCharType.MAX_LENGTH));
 		GenericRowData row1 = new GenericRowData(2);
 		row1.setField(0, 1);
 		row1.setField(1, fromString("a"));
@@ -99,12 +99,12 @@ private static Object[] testRowDataSerializer() {
 		row2.setField(0, 2);
 		row2.setField(1, null);
 
-		RowDataSerializer serializer = typeInfo.createSerializer(new ExecutionConfig());
+		RowDataSerializer serializer = typeInfo.toRowSerializer();
 		return new Object[] {serializer, new RowData[]{row1, row2}};
 	}
 
 	private static Object[] testLargeRowDataSerializer() {
-		RowDataTypeInfo typeInfo = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(
 			new IntType(),
 			new IntType(),
 			new IntType(),
@@ -133,12 +133,12 @@ private static Object[] testLargeRowDataSerializer() {
 		row.setField(11, null);
 		row.setField(12, fromString("Test"));
 
-		RowDataSerializer serializer = typeInfo.createSerializer(new ExecutionConfig());
+		RowDataSerializer serializer = typeInfo.toRowSerializer();
 		return new Object[] {serializer, new RowData[]{row}};
 	}
 
 	private static Object[] testRowDataSerializerWithComplexTypes() {
-		RowDataTypeInfo typeInfo = new RowDataTypeInfo(
+		InternalTypeInfo<RowData> typeInfo = InternalTypeInfo.ofFields(
 			new IntType(),
 			new DoubleType(),
 			new VarCharType(VarCharType.MAX_LENGTH),
@@ -161,7 +161,7 @@ private static Object[] testRowDataSerializerWithComplexTypes() {
 			createRow(1, 1.0, fromString("b"), createArray(1, 2, 3, 4, 5, 6), createMap(new int[]{1, 8}, new int[]{1, 6}))
 		};
 
-		RowDataSerializer serializer = typeInfo.createSerializer(new ExecutionConfig());
+		RowDataSerializer serializer = typeInfo.toRowSerializer();
 		return new Object[] {serializer, data};
 	}
 
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfoTest.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfoTest.java
deleted file mode 100644
index 30b5dbb64f08b..0000000000000
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/typeutils/RowDataTypeInfoTest.java
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils;
-
-import org.apache.flink.table.types.logical.BooleanType;
-import org.apache.flink.table.types.logical.IntType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.VarCharType;
-
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotEquals;
-
-/**
- * Test for {@link RowDataTypeInfo}.
- */
-public class RowDataTypeInfoTest {
-	private static LogicalType[] typeList = new LogicalType[]{
-			new IntType(),
-			new VarCharType(VarCharType.MAX_LENGTH)
-	};
-
-	@Test(expected = IllegalArgumentException.class)
-	public void testWrongNumberOfFieldNames() {
-		new RowDataTypeInfo(typeList, new String[]{"int", "string", "int"});
-		// number of field names should be equal to number of types, go fail
-	}
-
-	@Test(expected = IllegalArgumentException.class)
-	public void testDuplicateCustomFieldNames() {
-		new RowDataTypeInfo(typeList, new String[]{"int", "int"});
-		// field names should not be the same, go fail
-	}
-
-	@Test
-	public void testBinaryRowTypeInfoEquality() {
-		RowDataTypeInfo typeInfo1 = new RowDataTypeInfo(
-				new IntType(),
-				new VarCharType(VarCharType.MAX_LENGTH));
-
-		RowDataTypeInfo typeInfo2 = new RowDataTypeInfo(
-				new IntType(),
-				new VarCharType(VarCharType.MAX_LENGTH));
-
-		assertEquals(typeInfo1, typeInfo2);
-		assertEquals(typeInfo1.hashCode(), typeInfo2.hashCode());
-	}
-
-	@Test
-	public void testBinaryRowTypeInfoInequality() {
-		RowDataTypeInfo typeInfo1 = new RowDataTypeInfo(
-				new IntType(),
-				new VarCharType(VarCharType.MAX_LENGTH));
-
-		RowDataTypeInfo typeInfo2 = new RowDataTypeInfo(
-				new IntType(),
-				new BooleanType());
-
-		assertNotEquals(typeInfo1, typeInfo2);
-		assertNotEquals(typeInfo1.hashCode(), typeInfo2.hashCode());
-	}
-}
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/BinaryRowDataKeySelector.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/BinaryRowDataKeySelector.java
index 5bc7ecad107c4..7e756e1cea90a 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/BinaryRowDataKeySelector.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/BinaryRowDataKeySelector.java
@@ -18,15 +18,14 @@
 
 package org.apache.flink.table.runtime.util;
 
-import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.binary.BinaryRowData;
 import org.apache.flink.table.data.writer.BinaryRowWriter;
 import org.apache.flink.table.data.writer.BinaryWriter;
 import org.apache.flink.table.runtime.keyselector.RowDataKeySelector;
-import org.apache.flink.table.runtime.types.InternalSerializers;
-import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.runtime.typeutils.InternalSerializers;
+import org.apache.flink.table.runtime.typeutils.InternalTypeInfo;
 import org.apache.flink.table.types.logical.LogicalType;
 
 /**
@@ -46,10 +45,9 @@ public BinaryRowDataKeySelector(int[] keyFields, LogicalType[] inputFieldTypes)
 		this.inputFieldTypes = inputFieldTypes;
 		this.keyFieldTypes = new LogicalType[keyFields.length];
 		this.keySers = new TypeSerializer[keyFields.length];
-		ExecutionConfig conf = new ExecutionConfig();
 		for (int i = 0; i < keyFields.length; ++i) {
 			keyFieldTypes[i] = inputFieldTypes[keyFields[i]];
-			keySers[i] = InternalSerializers.create(keyFieldTypes[i], conf);
+			keySers[i] = InternalSerializers.create(keyFieldTypes[i]);
 		}
 	}
 
@@ -74,7 +72,7 @@ public RowData getKey(RowData value) throws Exception {
 	}
 
 	@Override
-	public RowDataTypeInfo getProducedType() {
-		return new RowDataTypeInfo(keyFieldTypes);
+	public InternalTypeInfo<RowData> getProducedType() {
+		return InternalTypeInfo.ofFields(keyFieldTypes);
 	}
 }
diff --git a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/RowDataHarnessAssertor.java b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/RowDataHarnessAssertor.java
index 4c89449a78e71..92fddbfd68f07 100644
--- a/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/RowDataHarnessAssertor.java
+++ b/flink-table/flink-table-runtime-blink/src/test/java/org/apache/flink/table/runtime/util/RowDataHarnessAssertor.java
@@ -18,13 +18,11 @@
 
 package org.apache.flink.table.runtime.util;
 
-import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.streaming.api.watermark.Watermark;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.data.util.RowDataUtil;
-import org.apache.flink.table.runtime.types.TypeInfoLogicalTypeConverter;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.util.Preconditions;
 
@@ -44,16 +42,16 @@
  */
 public class RowDataHarnessAssertor {
 
-	private final TypeInformation[] typeInfos;
+	private final LogicalType[] types;
 	private final Comparator<GenericRowData> comparator;
 
-	public RowDataHarnessAssertor(TypeInformation[] typeInfos, Comparator<GenericRowData> comparator) {
-		this.typeInfos = typeInfos;
+	public RowDataHarnessAssertor(LogicalType[] types, Comparator<GenericRowData> comparator) {
+		this.types = types;
 		this.comparator = comparator;
 	}
 
-	public RowDataHarnessAssertor(TypeInformation[] typeInfos) {
-		this(typeInfos, new StringComparator());
+	public RowDataHarnessAssertor(LogicalType[] types) {
+		this(types, new StringComparator());
 	}
 
 	/**
@@ -110,9 +108,7 @@ private void assertOutputEquals(
 				} else {
 					GenericRowData genericRow = RowDataUtil.toGenericRow(
 						row,
-						Arrays.stream(typeInfos)
-							.map(TypeInfoLogicalTypeConverter::fromTypeInfoToLogicalType)
-							.toArray(LogicalType[]::new));
+						types);
 					expectedRecords.add(genericRow);
 				}
 			}
@@ -124,9 +120,7 @@ private void assertOutputEquals(
 				// joined row can't equals to generic row, so cast joined row to generic row first
 				GenericRowData actualRow = RowDataUtil.toGenericRow(
 						actualOutput,
-						Arrays.stream(typeInfos)
-								.map(TypeInfoLogicalTypeConverter::fromTypeInfoToLogicalType)
-								.toArray(LogicalType[]::new));
+						types);
 				actualRecords.add(actualRow);
 			}
 		}
