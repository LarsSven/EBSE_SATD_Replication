diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java
index 5213bdd45f841..a13b9f466ace2 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormat.java
@@ -22,7 +22,6 @@
 import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.connector.jdbc.JdbcInputFormat;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
 import org.apache.flink.types.Row;
 import org.apache.flink.util.Preconditions;
@@ -151,11 +150,6 @@ public JDBCInputFormatBuilder setRowTypeInfo(RowTypeInfo rowTypeInfo) {
 			return this;
 		}
 
-		public JDBCInputFormatBuilder setRowConverter(JdbcRowConverter rowConverter) {
-			format.rowConverter = rowConverter;
-			return this;
-		}
-
 		public JDBCInputFormatBuilder setFetchSize(int fetchSize) {
 			Preconditions.checkArgument(fetchSize == Integer.MIN_VALUE || fetchSize > 0,
 				"Illegal value %s for fetchSize, has to be positive or Integer.MIN_VALUE.", fetchSize);
@@ -187,9 +181,6 @@ public JDBCInputFormat finish() {
 			if (format.rowTypeInfo == null) {
 				throw new IllegalArgumentException("No " + RowTypeInfo.class.getSimpleName() + " supplied");
 			}
-			if (format.rowConverter == null) {
-				throw new IllegalArgumentException("No row converter supplied");
-			}
 			if (format.parameterValues == null) {
 				LOG.debug("No input splitting configured (data will be read with parallelism 1).");
 			}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcInputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcInputFormat.java
index 7a6895ee0147e..76656005c220c 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcInputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcInputFormat.java
@@ -27,7 +27,6 @@
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
 import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
 import org.apache.flink.core.io.GenericInputSplit;
 import org.apache.flink.core.io.InputSplit;
@@ -114,7 +113,6 @@ public class JdbcInputFormat extends RichInputFormat<Row, InputSplit> implements
 	protected int resultSetType;
 	protected int resultSetConcurrency;
 	protected RowTypeInfo rowTypeInfo;
-	protected JdbcRowConverter rowConverter;
 
 	protected transient Connection dbConn;
 	protected transient PreparedStatement statement;
@@ -296,10 +294,12 @@ public Row nextRecord(Row reuse) throws IOException {
 			if (!hasNext) {
 				return null;
 			}
-			Row row = rowConverter.convert(resultSet, reuse);
+			for (int pos = 0; pos < reuse.getArity(); pos++) {
+				reuse.setField(pos, resultSet.getObject(pos + 1));
+			}
 			//update hasNext after we've read the record
 			hasNext = resultSet.next();
-			return row;
+			return reuse;
 		} catch (SQLException se) {
 			throw new IOException("Couldn't read data - " + se.getMessage(), se);
 		} catch (NullPointerException npe) {
@@ -406,11 +406,6 @@ public JdbcInputFormatBuilder setRowTypeInfo(RowTypeInfo rowTypeInfo) {
 			return this;
 		}
 
-		public JdbcInputFormatBuilder setRowConverter(JdbcRowConverter rowConverter) {
-			format.rowConverter = rowConverter;
-			return this;
-		}
-
 		public JdbcInputFormatBuilder setFetchSize(int fetchSize) {
 			Preconditions.checkArgument(fetchSize == Integer.MIN_VALUE || fetchSize > 0,
 				"Illegal value %s for fetchSize, has to be positive or Integer.MIN_VALUE.", fetchSize);
@@ -442,9 +437,6 @@ public JdbcInputFormat finish() {
 			if (format.rowTypeInfo == null) {
 				throw new IllegalArgumentException("No " + RowTypeInfo.class.getSimpleName() + " supplied");
 			}
-			if (format.rowConverter == null) {
-				throw new IllegalArgumentException("No row converter supplied");
-			}
 			if (format.parameterValues == null) {
 				LOG.debug("No input splitting configured (data will be read with parallelism 1).");
 			}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java
index b57541c996b8b..23536d4da78d0 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/JdbcOutputFormat.java
@@ -68,13 +68,13 @@ public static JdbcOutputFormatBuilder buildJdbcOutputFormat() {
 	 * Builder for {@link JdbcOutputFormat}.
 	 */
 	public static class JdbcOutputFormatBuilder {
-		protected String username;
-		protected String password;
-		protected String drivername;
-		protected String dbURL;
-		protected String query;
-		protected int batchSize = JdbcExecutionOptions.DEFAULT_SIZE;
-		protected int[] typesArray;
+		private String username;
+		private String password;
+		private String drivername;
+		private String dbURL;
+		private String query;
+		private int batchSize = JdbcExecutionOptions.DEFAULT_SIZE;
+		private int[] typesArray;
 
 		private JdbcOutputFormatBuilder() {}
 
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/AbstractJdbcCatalog.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/AbstractJdbcCatalog.java
index 0bc199b2f7910..9e27816ee8951 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/AbstractJdbcCatalog.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/AbstractJdbcCatalog.java
@@ -18,7 +18,7 @@
 
 package org.apache.flink.connector.jdbc.catalog;
 
-import org.apache.flink.connector.jdbc.table.JdbcTableSourceSinkFactory;
+import org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory;
 import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.catalog.AbstractCatalog;
 import org.apache.flink.table.catalog.CatalogBaseTable;
@@ -43,7 +43,7 @@
 import org.apache.flink.table.catalog.stats.CatalogColumnStatistics;
 import org.apache.flink.table.catalog.stats.CatalogTableStatistics;
 import org.apache.flink.table.expressions.Expression;
-import org.apache.flink.table.factories.TableFactory;
+import org.apache.flink.table.factories.Factory;
 import org.apache.flink.util.StringUtils;
 
 import org.slf4j.Logger;
@@ -119,8 +119,9 @@ public String getBaseUrl() {
 
 	// ------ table factory ------
 
-	public Optional<TableFactory> getTableFactory() {
-		return Optional.of(new JdbcTableSourceSinkFactory());
+	@Override
+	public Optional<Factory> getFactory() {
+		return Optional.of(new JdbcDynamicTableSourceSinkFactory());
 	}
 
 	// ------ databases ------
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
index 427616eb4bdd6..a83fe075fec78 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
@@ -20,6 +20,7 @@
 
 import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+import org.apache.flink.connector.jdbc.dialect.PostgresDialect;
 
 import static org.apache.flink.util.Preconditions.checkArgument;
 
@@ -43,7 +44,7 @@ public static void validateJdbcUrl(String url) {
 	public static AbstractJdbcCatalog createCatalog(String catalogName, String defaultDatabase, String username, String pwd, String baseUrl) {
 		JdbcDialect dialect = JdbcDialects.get(baseUrl).get();
 
-		if (dialect instanceof JdbcDialects.PostgresDialect) {
+		if (dialect instanceof PostgresDialect) {
 			return new PostgresCatalog(catalogName, defaultDatabase, username, pwd, baseUrl);
 		} else {
 			throw new UnsupportedOperationException(
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
index 0dd00155b1ad7..c9b11246632db 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
@@ -49,13 +49,12 @@
 import java.util.Map;
 import java.util.Set;
 
-import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR_PROPERTY_VERSION;
-import static org.apache.flink.table.descriptors.ConnectorDescriptorValidator.CONNECTOR_TYPE;
-import static org.apache.flink.table.descriptors.JdbcValidator.CONNECTOR_PASSWORD;
-import static org.apache.flink.table.descriptors.JdbcValidator.CONNECTOR_TABLE;
-import static org.apache.flink.table.descriptors.JdbcValidator.CONNECTOR_TYPE_VALUE_JDBC;
-import static org.apache.flink.table.descriptors.JdbcValidator.CONNECTOR_URL;
-import static org.apache.flink.table.descriptors.JdbcValidator.CONNECTOR_USERNAME;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.IDENTIFIER;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.PASSWORD;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.TABLE_NAME;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.URL;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.USERNAME;
+import static org.apache.flink.table.factories.FactoryUtil.CONNECTOR;
 
 /**
  * Catalog for PostgreSQL.
@@ -198,13 +197,11 @@ public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistExcep
 			TableSchema tableSchema = new TableSchema.Builder().fields(names, types).build();
 
 			Map<String, String> props = new HashMap<>();
-			props.put(CONNECTOR_TYPE, CONNECTOR_TYPE_VALUE_JDBC);
-			props.put(CONNECTOR_PROPERTY_VERSION, "1");
-
-			props.put(CONNECTOR_URL, dbUrl);
-			props.put(CONNECTOR_TABLE, pgPath.getFullPath());
-			props.put(CONNECTOR_USERNAME, username);
-			props.put(CONNECTOR_PASSWORD, pwd);
+			props.put(CONNECTOR.key(), IDENTIFIER);
+			props.put(URL.key(), dbUrl);
+			props.put(TABLE_NAME.key(), pgPath.getFullPath());
+			props.put(USERNAME.key(), username);
+			props.put(PASSWORD.key(), pwd);
 
 			return new CatalogTableImpl(
 				tableSchema,
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java
new file mode 100644
index 0000000000000..4023f2a2a14f9
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+
+import java.util.List;
+
+abstract class AbstractDialect implements JdbcDialect {
+
+	@Override
+	public void validate(TableSchema schema) throws ValidationException {
+		for (int i = 0; i < schema.getFieldCount(); i++) {
+			DataType dt = schema.getFieldDataType(i).get();
+			String fieldName = schema.getFieldName(i).get();
+
+			// TODO: We can't convert VARBINARY(n) data type to
+			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter
+			//  when n is smaller than Integer.MAX_VALUE
+			if (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||
+				(dt.getLogicalType() instanceof VarBinaryType
+					&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {
+				throw new ValidationException(
+					String.format("The %s dialect doesn't support type: %s.",
+						dialectName(),
+						dt.toString()));
+			}
+
+			// only validate precision of DECIMAL type for blink planner
+			if (dt.getLogicalType() instanceof DecimalType) {
+				int precision = ((DecimalType) dt.getLogicalType()).getPrecision();
+				if (precision > maxDecimalPrecision()
+					|| precision < minDecimalPrecision()) {
+					throw new ValidationException(
+						String.format("The precision of field '%s' is out of the DECIMAL " +
+								"precision range [%d, %d] supported by %s dialect.",
+							fieldName,
+							minDecimalPrecision(),
+							maxDecimalPrecision(),
+							dialectName()));
+				}
+			}
+
+			// only validate precision of DECIMAL type for blink planner
+			if (dt.getLogicalType() instanceof TimestampType) {
+				int precision = ((TimestampType) dt.getLogicalType()).getPrecision();
+				if (precision > maxTimestampPrecision()
+					|| precision < minTimestampPrecision()) {
+					throw new ValidationException(
+						String.format("The precision of field '%s' is out of the TIMESTAMP " +
+								"precision range [%d, %d] supported by %s dialect.",
+							fieldName,
+							minTimestampPrecision(),
+							maxTimestampPrecision(),
+							dialectName()));
+				}
+			}
+		}
+	}
+
+	public abstract int maxDecimalPrecision();
+
+	public abstract int minDecimalPrecision();
+
+	public abstract int maxTimestampPrecision();
+
+	public abstract int minTimestampPrecision();
+
+	/**
+	 * Defines the unsupported types for the dialect.
+	 *
+	 * @return a list of logical type roots.
+	 */
+	public abstract List<LogicalTypeRoot> unsupportedTypes();
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java
new file mode 100644
index 0000000000000..55f110841e94e
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.DerbyRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+
+class DerbyDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to derby docs:
+	// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html
+	private static final int MAX_TIMESTAMP_PRECISION = 9;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of DECIMAL type according to derby docs:
+	// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html
+	private static final int MAX_DECIMAL_PRECISION = 31;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:derby:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new DerbyRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("org.apache.derby.jdbc.EmbeddedDriver");
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return identifier;
+	}
+
+	@Override
+	public String dialectName() {
+		return "Derby";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in Derby are list at
+		// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.ARRAY,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
index 6ba85eb4eec4d..3d311be00bd76 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
@@ -35,6 +35,12 @@
 @Internal
 public interface JdbcDialect extends Serializable {
 
+	/**
+	 * Get the name of jdbc dialect.
+	 * @return the dialect name.
+	 */
+	String dialectName();
+
 	/**
 	 * Check if this dialect instance can handle a certain jdbc url.
 	 * @param url the jdbc url.
@@ -43,7 +49,7 @@ public interface JdbcDialect extends Serializable {
 	boolean canHandle(String url);
 
 	/**
-	 * Get a row converter for the database according to the given row type.
+	 * Get converter that convert jdbc object and Flink internal object each other.
 	 * @param rowType the given row type
 	 * @return a row converter for the database
 	 */
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
index 211bb02ed4ca4..c2cc579c81325 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
@@ -18,23 +18,9 @@
 
 package org.apache.flink.connector.jdbc.dialect;
 
-import org.apache.flink.connector.jdbc.internal.converter.DerbyRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.MySQLRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter;
-import org.apache.flink.table.api.TableSchema;
-import org.apache.flink.table.api.ValidationException;
-import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.DecimalType;
-import org.apache.flink.table.types.logical.LogicalTypeRoot;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.TimestampType;
-import org.apache.flink.table.types.logical.VarBinaryType;
-
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
-import java.util.stream.Collectors;
 
 /**
  * Default JDBC dialects.
@@ -58,369 +44,4 @@ public static Optional<JdbcDialect> get(String url) {
 		}
 		return Optional.empty();
 	}
-
-	private abstract static class AbstractDialect implements JdbcDialect {
-
-		@Override
-		public void validate(TableSchema schema) throws ValidationException {
-			for (int i = 0; i < schema.getFieldCount(); i++) {
-				DataType dt = schema.getFieldDataType(i).get();
-				String fieldName = schema.getFieldName(i).get();
-
-				// TODO: We can't convert VARBINARY(n) data type to
-				//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter
-				//  when n is smaller than Integer.MAX_VALUE
-				if (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||
-						(dt.getLogicalType() instanceof VarBinaryType
-							&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {
-					throw new ValidationException(
-							String.format("The %s dialect doesn't support type: %s.",
-									dialectName(),
-									dt.toString()));
-				}
-
-				// only validate precision of DECIMAL type for blink planner
-				if (dt.getLogicalType() instanceof DecimalType) {
-					int precision = ((DecimalType) dt.getLogicalType()).getPrecision();
-					if (precision > maxDecimalPrecision()
-							|| precision < minDecimalPrecision()) {
-						throw new ValidationException(
-								String.format("The precision of field '%s' is out of the DECIMAL " +
-												"precision range [%d, %d] supported by %s dialect.",
-										fieldName,
-										minDecimalPrecision(),
-										maxDecimalPrecision(),
-										dialectName()));
-					}
-				}
-
-				// only validate precision of DECIMAL type for blink planner
-				if (dt.getLogicalType() instanceof TimestampType) {
-					int precision = ((TimestampType) dt.getLogicalType()).getPrecision();
-					if (precision > maxTimestampPrecision()
-							|| precision < minTimestampPrecision()) {
-						throw new ValidationException(
-								String.format("The precision of field '%s' is out of the TIMESTAMP " +
-												"precision range [%d, %d] supported by %s dialect.",
-										fieldName,
-										minTimestampPrecision(),
-										maxTimestampPrecision(),
-										dialectName()));
-					}
-				}
-			}
-		}
-
-		public abstract String dialectName();
-
-		public abstract int maxDecimalPrecision();
-
-		public abstract int minDecimalPrecision();
-
-		public abstract int maxTimestampPrecision();
-
-		public abstract int minTimestampPrecision();
-
-		/**
-		 * Defines the unsupported types for the dialect.
-		 * @return a list of logical type roots.
-		 */
-		public abstract List<LogicalTypeRoot> unsupportedTypes();
-	}
-
-	private static class DerbyDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to derby docs:
-		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html
-		private static final int MAX_TIMESTAMP_PRECISION = 9;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of DECIMAL type according to derby docs:
-		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html
-		private static final int MAX_DECIMAL_PRECISION = 31;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:derby:");
-		}
-
-		@Override
-		public JdbcRowConverter getRowConverter(RowType rowType) {
-			return new DerbyRowConverter(rowType);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("org.apache.derby.jdbc.EmbeddedDriver");
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return identifier;
-		}
-
-		@Override
-		public String dialectName() {
-			return "derby";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in Derby are list at
-			// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.ARRAY,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED);
-		}
-	}
-
-	/**
-	 * MySQL dialect.
-	 */
-	public static class MySQLDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:
-		// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html
-		private static final int MAX_TIMESTAMP_PRECISION = 6;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of DECIMAL type according to Mysql docs:
-		// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html
-		private static final int MAX_DECIMAL_PRECISION = 65;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:mysql:");
-		}
-
-		@Override
-		public JdbcRowConverter getRowConverter(RowType rowType) {
-			return new MySQLRowConverter(rowType);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("com.mysql.jdbc.Driver");
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return "`" + identifier + "`";
-		}
-
-		/**
-		 * Mysql upsert query use DUPLICATE KEY UPDATE.
-		 *
-		 * <p>NOTE: It requires Mysql's primary key to be consistent with pkFields.
-		 *
-		 * <p>We don't use REPLACE INTO, if there are other fields, we can keep their previous values.
-		 */
-		@Override
-		public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
-			String updateClause = Arrays.stream(fieldNames)
-					.map(f -> quoteIdentifier(f) + "=VALUES(" + quoteIdentifier(f) + ")")
-					.collect(Collectors.joining(", "));
-			return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
-					" ON DUPLICATE KEY UPDATE " + updateClause
-			);
-		}
-
-		@Override
-		public String dialectName() {
-			return "mysql";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in Mysql are list at:
-			// https://dev.mysql.com/doc/refman/8.0/en/data-types.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.ARRAY,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED
-			);
-		}
-	}
-
-	/**
-	 * Postgres dialect.
-	 */
-	public static class PostgresDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
-		// https://www.postgresql.org/docs/12/datatype-datetime.html
-		private static final int MAX_TIMESTAMP_PRECISION = 6;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
-		// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
-		private static final int MAX_DECIMAL_PRECISION = 1000;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:postgresql:");
-		}
-
-		@Override
-		public JdbcRowConverter getRowConverter(RowType rowType) {
-			return new PostgresRowConverter(rowType);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("org.postgresql.Driver");
-		}
-
-		/**
-		 * Postgres upsert query. It use ON CONFLICT ... DO UPDATE SET.. to replace into Postgres.
-		 */
-		@Override
-		public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
-			String uniqueColumns = Arrays.stream(uniqueKeyFields)
-					.map(this::quoteIdentifier)
-					.collect(Collectors.joining(", "));
-			String updateClause = Arrays.stream(fieldNames)
-					.map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
-					.collect(Collectors.joining(", "));
-			return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
-							" ON CONFLICT (" + uniqueColumns + ")" +
-							" DO UPDATE SET " + updateClause
-			);
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return identifier;
-		}
-
-		@Override
-		public String dialectName() {
-			return "postgresql";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in PostgreSQL are list at:
-			// https://www.postgresql.org/docs/12/datatype.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED
-			);
-
-		}
-	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java
new file mode 100644
index 0000000000000..5d6504833e4e3
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java
@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.MySQLRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * JDBC dialect for MySQL.
+ */
+public class MySQLDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:
+	// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html
+	private static final int MAX_TIMESTAMP_PRECISION = 6;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of DECIMAL type according to Mysql docs:
+	// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html
+	private static final int MAX_DECIMAL_PRECISION = 65;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:mysql:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new MySQLRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("com.mysql.jdbc.Driver");
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return "`" + identifier + "`";
+	}
+
+	/**
+	 * Mysql upsert query use DUPLICATE KEY UPDATE.
+	 *
+	 * <p>NOTE: It requires Mysql's primary key to be consistent with pkFields.
+	 *
+	 * <p>We don't use REPLACE INTO, if there are other fields, we can keep their previous values.
+	 */
+	@Override
+	public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
+		String updateClause = Arrays.stream(fieldNames)
+			.map(f -> quoteIdentifier(f) + "=VALUES(" + quoteIdentifier(f) + ")")
+			.collect(Collectors.joining(", "));
+		return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
+			" ON DUPLICATE KEY UPDATE " + updateClause
+		);
+	}
+
+	@Override
+	public String dialectName() {
+		return "MySQL";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in Mysql are list at:
+		// https://dev.mysql.com/doc/refman/8.0/en/data-types.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.ARRAY,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED
+		);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java
new file mode 100644
index 0000000000000..d62e9baf7dbf6
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java
@@ -0,0 +1,134 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * JDBC dialect for PostgreSQL.
+ */
+public class PostgresDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+	// https://www.postgresql.org/docs/12/datatype-datetime.html
+	private static final int MAX_TIMESTAMP_PRECISION = 6;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+	// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
+	private static final int MAX_DECIMAL_PRECISION = 1000;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:postgresql:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new PostgresRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("org.postgresql.Driver");
+	}
+
+	/**
+	 * Postgres upsert query. It use ON CONFLICT ... DO UPDATE SET.. to replace into Postgres.
+	 */
+	@Override
+	public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
+		String uniqueColumns = Arrays.stream(uniqueKeyFields)
+			.map(this::quoteIdentifier)
+			.collect(Collectors.joining(", "));
+		String updateClause = Arrays.stream(fieldNames)
+			.map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
+			.collect(Collectors.joining(", "));
+		return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
+			" ON CONFLICT (" + uniqueColumns + ")" +
+			" DO UPDATE SET " + updateClause
+		);
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return identifier;
+	}
+
+	@Override
+	public String dialectName() {
+		return "PostgreSQL";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in PostgreSQL are list at:
+		// https://www.postgresql.org/docs/12/datatype.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED
+		);
+	}
+}
+
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/AbstractJdbcOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/AbstractJdbcOutputFormat.java
index 4a002ac9ee2f4..2316401618b59 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/AbstractJdbcOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/AbstractJdbcOutputFormat.java
@@ -38,7 +38,7 @@
 
 	private static final long serialVersionUID = 1L;
 	public static final int DEFAULT_FLUSH_MAX_SIZE = 5000;
-	public static final long DEFAULT_FLUSH_INTERVAL_MILLS = 0;
+	public static final long DEFAULT_FLUSH_INTERVAL_MILLS = 0L;
 
 	private static final Logger LOG = LoggerFactory.getLogger(AbstractJdbcOutputFormat.class);
 	protected transient Connection connection;
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
index 0af3d73843908..5d6c7bb4ecc54 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/JdbcBatchingOutputFormat.java
@@ -157,7 +157,7 @@ public final synchronized void writeRecord(In record) throws IOException {
 		}
 	}
 
-	void addToBatch(In original, JdbcIn extracted) throws SQLException {
+	protected void addToBatch(In original, JdbcIn extracted) throws SQLException {
 		jdbcStatementExecutor.addToBatch(extracted);
 	}
 
@@ -185,7 +185,7 @@ public synchronized void flush() throws IOException {
 		}
 	}
 
-	void attemptFlush() throws SQLException {
+	protected void attemptFlush() throws SQLException {
 		jdbcStatementExecutor.executeBatch();
 	}
 
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
index 41708b433f73a..05b5315d75cb6 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/TableJdbcUpsertOutputFormat.java
@@ -69,7 +69,7 @@ private JdbcBatchStatementExecutor<Row> createDeleteExecutor() {
 	}
 
 	@Override
-	void addToBatch(Tuple2<Boolean, Row> original, Row extracted) throws SQLException {
+	protected void addToBatch(Tuple2<Boolean, Row> original, Row extracted) throws SQLException {
 		if (original.f0) {
 			super.addToBatch(original, extracted);
 		} else {
@@ -91,7 +91,7 @@ public synchronized void close() {
 	}
 
 	@Override
-	void attemptFlush() throws SQLException {
+	protected void attemptFlush() throws SQLException {
 		super.attemptFlush();
 		deleteExecutor.executeBatch();
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
index 1a2175abc05b5..be2b6ede28dc4 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
@@ -18,54 +18,226 @@
 
 package org.apache.flink.connector.jdbc.internal.converter;
 
+import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
+import org.apache.flink.table.data.DecimalData;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.StringData;
+import org.apache.flink.table.data.TimestampData;
+import org.apache.flink.table.types.logical.DecimalType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.types.Row;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.utils.TypeConversions;
 
+import java.io.Serializable;
+import java.math.BigDecimal;
+import java.sql.Date;
+import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.time.LocalDate;
+import java.time.LocalTime;
 
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
 /**
- * Base for all row converters.
+ * Base class for all converters that convert between JDBC object and Flink internal object.
  */
 public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
 
 	protected final RowType rowType;
-	protected final JdbcFieldConverter[] converters;
+	protected final JdbcDeserializationConverter[] toInternalConverters;
+	protected final JdbcSerializationConverter[] toExternalConverters;
+	protected final LogicalType[] fieldTypes;
+
+	public abstract String converterName();
 
 	public AbstractJdbcRowConverter(RowType rowType) {
 		this.rowType = checkNotNull(rowType);
-		converters = new JdbcFieldConverter[rowType.getFieldCount()];
-
-		for (int i = 0; i < converters.length; i++) {
-			converters[i] = createConverter(rowType.getTypeAt(i));
+		this.fieldTypes = rowType.getFields().stream()
+			.map(RowType.RowField::getType)
+			.toArray(LogicalType[]::new);
+		this.toInternalConverters = new JdbcDeserializationConverter[rowType.getFieldCount()];
+		this.toExternalConverters = new JdbcSerializationConverter[rowType.getFieldCount()];
+		for (int i = 0; i < rowType.getFieldCount(); i++) {
+			toInternalConverters[i] = createNullableInternalConverter(rowType.getTypeAt(i));
+			toExternalConverters[i] = createNullableExternalConverter(fieldTypes[i]);
 		}
 	}
 
 	@Override
-	public Row convert(ResultSet resultSet, Row reuse) throws SQLException {
+	public RowData toInternal(ResultSet resultSet) throws SQLException {
+		GenericRowData genericRowData = new GenericRowData(rowType.getFieldCount());
 		for (int pos = 0; pos < rowType.getFieldCount(); pos++) {
-			reuse.setField(pos, converters[pos].convert(resultSet.getObject(pos + 1)));
+			genericRowData.setField(pos, toInternalConverters[pos].deserialize(resultSet.getObject(pos + 1)));
 		}
+		return genericRowData;
+	}
+
+	@Override
+	public PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException {
+		for (int index = 0; index < rowData.getArity(); index++) {
+			toExternalConverters[index].serialize(rowData, index, statement);
+		}
+		return statement;
+	}
+
+	/**
+	 * Runtime converter to convert JDBC field to {@link RowData} type object.
+	 */
+	@FunctionalInterface
+	interface JdbcDeserializationConverter extends Serializable {
+		/**
+		 * Convert a jdbc field object of {@link ResultSet} to the internal data structure object.
+		 * @param jdbcField
+		 */
+		Object deserialize(Object jdbcField) throws SQLException;
+	}
+
+	/**
+	 * Runtime converter to convert {@link RowData} field to java object and fill into the {@link PreparedStatement}.
+	 */
+	@FunctionalInterface
+	interface JdbcSerializationConverter extends Serializable {
+		void serialize(RowData rowData, int index, PreparedStatement statement) throws SQLException;
+	}
 
-		return reuse;
+	/**
+	 * Create a nullable runtime {@link JdbcDeserializationConverter} from given {@link LogicalType}.
+	 */
+	protected JdbcDeserializationConverter createNullableInternalConverter(LogicalType type) {
+		return wrapIntoNullableInternalConverter(createInternalConverter(type));
+	}
+
+	protected JdbcDeserializationConverter wrapIntoNullableInternalConverter(JdbcDeserializationConverter jdbcDeserializationConverter) {
+		return v -> {
+			if (v == null) {
+				return null;
+			} else {
+				return jdbcDeserializationConverter.deserialize(v);
+			}
+		};
+	}
+
+	protected JdbcDeserializationConverter createInternalConverter(LogicalType type) {
+		switch (type.getTypeRoot()) {
+			case NULL:
+				return v -> null;
+			case BOOLEAN:
+			case TINYINT:
+			case FLOAT:
+			case DOUBLE:
+			case INTEGER:
+			case INTERVAL_YEAR_MONTH:
+			case BIGINT:
+			case INTERVAL_DAY_TIME:
+				return v -> v;
+			case SMALLINT:
+				// Converter for small type that casts value to int and then return short value, since
+				// JDBC 1.0 use int type for small values.
+				return v -> (Integer.valueOf(v.toString())).shortValue();
+			case DATE:
+				return v -> (int) (((Date) v).toLocalDate().toEpochDay());
+			case TIME_WITHOUT_TIME_ZONE:
+				return v -> (int) (((Time) v).toLocalTime().toNanoOfDay() / 1_000_000L);
+			case TIMESTAMP_WITH_TIME_ZONE:
+			case TIMESTAMP_WITHOUT_TIME_ZONE:
+				return v -> TimestampData.fromTimestamp((Timestamp) v);
+			case CHAR:
+			case VARCHAR:
+				return v -> StringData.fromString((String) v);
+			case BINARY:
+			case VARBINARY:
+				return v -> (byte[]) v;
+			case DECIMAL:
+				final int precision = ((DecimalType) type).getPrecision();
+				final int scale = ((DecimalType) type).getScale();
+				return v -> DecimalData.fromBigDecimal((BigDecimal) v, precision, scale);
+			case ARRAY:
+			case ROW:
+			case MAP:
+			case MULTISET:
+			case RAW:
+			default:
+				throw new UnsupportedOperationException("Unsupported type:" + type);
+		}
 	}
 
 	/**
-	 * Create a runtime JDBC field converter from given {@link LogicalType}.
+	 * Create a nullable JDBC f{@link JdbcSerializationConverter} from given sql type.
 	 */
-	public JdbcFieldConverter createConverter(LogicalType type) {
-		LogicalTypeRoot root = type.getTypeRoot();
-
-		if (root == LogicalTypeRoot.SMALLINT) {
-			// Converter for small type that casts value to int and then return short value, since
-	        // JDBC 1.0 use int type for small values.
-			return v -> ((Integer) v).shortValue();
-		} else {
-			return v -> v;
+	protected JdbcSerializationConverter createNullableExternalConverter(LogicalType type) {
+		return wrapIntoNullableExternalConverter(createExternalConverter(type), type);
+	}
+
+	protected JdbcSerializationConverter wrapIntoNullableExternalConverter(JdbcSerializationConverter jdbcSerializationConverter, LogicalType type) {
+		final int sqlType = JdbcTypeUtil.typeInformationToSqlType(TypeConversions.fromDataTypeToLegacyInfo(
+			TypeConversions.fromLogicalToDataType(type)));
+		return (val, index, statement)  -> {
+			if (val == null || val.isNullAt(index) || LogicalTypeRoot.NULL.equals(type.getTypeRoot())) {
+				statement.setNull(index + 1, sqlType);
+			} else {
+				jdbcSerializationConverter.serialize(val, index, statement);
+			}
+		};
+	}
+
+	protected JdbcSerializationConverter createExternalConverter(LogicalType type) {
+		switch (type.getTypeRoot()) {
+			case BOOLEAN:
+				return (val, index, statement) -> statement.setBoolean(index + 1, val.getBoolean(index));
+			case TINYINT:
+				return (val, index, statement) -> statement.setByte(index + 1, val.getByte(index));
+			case SMALLINT:
+				return (val, index, statement) -> statement.setShort(index + 1, val.getShort(index));
+			case INTEGER:
+			case INTERVAL_YEAR_MONTH:
+				return (val, index, statement) -> statement.setInt(index + 1, val.getInt(index));
+			case BIGINT:
+			case INTERVAL_DAY_TIME:
+				return (val, index, statement) -> statement.setLong(index + 1, val.getLong(index));
+			case FLOAT:
+				return (val, index, statement) -> statement.setFloat(index + 1, val.getFloat(index));
+			case DOUBLE:
+				return (val, index, statement) -> statement.setDouble(index + 1, val.getDouble(index));
+			case CHAR:
+			case VARCHAR:
+				// value is BinaryString
+				return (val, index, statement) -> statement.setString(index + 1, val.getString(index).toString());
+			case BINARY:
+			case VARBINARY:
+				return (val, index, statement) -> statement.setBytes(index + 1, val.getBinary(index));
+			case DATE:
+				return (val, index, statement) ->
+					statement.setDate(index + 1, Date.valueOf(LocalDate.ofEpochDay(val.getInt(index))));
+			case TIME_WITHOUT_TIME_ZONE:
+				return (val, index, statement) ->
+					statement.setTime(index + 1, Time.valueOf(LocalTime.ofNanoOfDay(val.getInt(index) * 1_000_000L)));
+			case TIMESTAMP_WITH_TIME_ZONE:
+			case TIMESTAMP_WITHOUT_TIME_ZONE:
+				final int timestampPrecision = ((TimestampType) type).getPrecision();
+				return (val, index, statement) ->
+					statement.setTimestamp(
+						index + 1,
+						val.getTimestamp(index, timestampPrecision).toTimestamp());
+			case DECIMAL:
+				final int decimalPrecision = ((DecimalType) type).getPrecision();
+				final int decimalScale = ((DecimalType) type).getScale();
+				return (val, index, statement) ->
+					statement.setBigDecimal(
+						index + 1,
+						val.getDecimal(index, decimalPrecision, decimalScale).toBigDecimal());
+			case ARRAY:
+			case MAP:
+			case MULTISET:
+			case ROW:
+			case RAW:
+			default:
+				throw new UnsupportedOperationException("Unsupported type:" + type);
 		}
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
index 9e5010194fa8f..cbb1256ec37ed 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
@@ -21,10 +21,17 @@
 import org.apache.flink.table.types.logical.RowType;
 
 /**
- * Row converter for Derby.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for Derby.
  */
 public class DerbyRowConverter extends AbstractJdbcRowConverter {
 
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "Derby";
+	}
+
 	public DerbyRowConverter(RowType rowType) {
 		super(rowType);
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
index 9baa9adcbecd8..18e6f613a5052 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
@@ -18,31 +18,31 @@
 
 package org.apache.flink.connector.jdbc.internal.converter;
 
-import org.apache.flink.types.Row;
+import org.apache.flink.table.data.RowData;
 
 import java.io.Serializable;
+import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 
 /**
- * Convert row from JDBC result set to a Flink row.
+ * Converter that is responsible to convert between JDBC object and Flink SQL internal data structure {@link RowData}.
  */
-@FunctionalInterface
 public interface JdbcRowConverter extends Serializable {
 
 	/**
-	 * Convert data retrieved from {@link ResultSet} to {@link Row}.
+	 * Convert data retrieved from {@link ResultSet} to internal {@link RowData}.
 	 *
 	 * @param resultSet ResultSet from JDBC
-	 * @param reuse The row to set
 	 */
-	Row convert(ResultSet resultSet, Row reuse) throws SQLException;
+	RowData toInternal(ResultSet resultSet) throws SQLException;
 
 	/**
-	 * Runtime converter to convert JDBC field to Java objects.
+	 * Convert data retrieved from Flink internal RowData to JDBC Object.
+	 *
+	 * @param rowData The given internal {@link RowData}.
+	 * @param statement The statement to be filled.
+	 * @return The filled statement.
 	 */
-	@FunctionalInterface
-	interface JdbcFieldConverter extends Serializable {
-		Object convert(Object value) throws SQLException;
-	}
+	PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException;
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
index cb8238a693499..0068089fbef64 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
@@ -21,10 +21,17 @@
 import org.apache.flink.table.types.logical.RowType;
 
 /**
- * Row converter for MySQL.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for MySQL.
  */
 public class MySQLRowConverter extends AbstractJdbcRowConverter {
 
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "MySQL";
+	}
+
 	public MySQLRowConverter(RowType rowType) {
 		super(rowType);
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
index b59a1151402ea..106464a801edc 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
@@ -18,57 +18,95 @@
 
 package org.apache.flink.connector.jdbc.internal.converter;
 
+import org.apache.flink.table.data.GenericArrayData;
 import org.apache.flink.table.types.logical.ArrayType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.LogicalTypeFamily;
 import org.apache.flink.table.types.logical.LogicalTypeRoot;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.utils.LogicalTypeChecks;
+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
 
 import org.postgresql.jdbc.PgArray;
 import org.postgresql.util.PGobject;
 
+import java.lang.reflect.Array;
+
 /**
- * Row converter for Postgres.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for PostgreSQL.
  */
 public class PostgresRowConverter extends AbstractJdbcRowConverter {
 
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "PostgreSQL";
+	}
+
 	public PostgresRowConverter(RowType rowType) {
 		super(rowType);
 	}
 
 	@Override
-	public JdbcFieldConverter createConverter(LogicalType type) {
+	public JdbcDeserializationConverter createNullableInternalConverter(LogicalType type) {
 		LogicalTypeRoot root = type.getTypeRoot();
 
 		if (root == LogicalTypeRoot.ARRAY) {
 			ArrayType arrayType = (ArrayType) type;
+			return createPostgresArrayConverter(arrayType);
+		} else {
+			return createPrimitiveConverter(type);
+		}
+	}
 
-			// PG's bytea[] is wrapped in PGobject, rather than primitive byte arrays
-			if (LogicalTypeChecks.hasFamily(arrayType.getElementType(), LogicalTypeFamily.BINARY_STRING)) {
-
-				return v -> {
-					PgArray pgArray = (PgArray) v;
-					Object[] in = (Object[]) pgArray.getArray();
+	@Override
+	protected JdbcSerializationConverter createNullableExternalConverter(LogicalType type) {
+		LogicalTypeRoot root = type.getTypeRoot();
+		if (root == LogicalTypeRoot.ARRAY) {
+			//note:Writing ARRAY type is not yet supported by PostgreSQL dialect now.
+			return (val, index, statement) -> {
+				throw new IllegalStateException(
+					String.format("Writing ARRAY type is not yet supported in JDBC:%s.", converterName()));
+			};
+		} else {
+			return super.createNullableExternalConverter(type);
+		}
+	}
 
-					Object[] out = new Object[in.length];
-					for (int i = 0; i < in.length; i++) {
-						out[i] = ((PGobject) in[i]).getValue().getBytes();
-					}
+	private JdbcDeserializationConverter createPostgresArrayConverter(ArrayType arrayType) {
+		// PG's bytea[] is wrapped in PGobject, rather than primitive byte arrays
+		if (LogicalTypeChecks.hasFamily(arrayType.getElementType(), LogicalTypeFamily.BINARY_STRING)) {
+			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
+			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
 
-					return out;
-				};
-			} else {
-				return v -> ((PgArray) v).getArray();
-			}
+			return v -> {
+				PgArray pgArray = (PgArray) v;
+				Object[] in = (Object[]) pgArray.getArray();
+				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
+				for (int i = 0; i < in.length; i++) {
+					array[i] = elementConverter.deserialize(((PGobject) in[i]).getValue().getBytes());
+				}
+				return new GenericArrayData(array);
+			};
 		} else {
-			return createPrimitiveConverter(type);
+			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
+			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
+			return v -> {
+				PgArray pgArray = (PgArray) v;
+				Object[] in = (Object[]) pgArray.getArray();
+				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
+				for (int i = 0; i < in.length; i++) {
+					array[i] = elementConverter.deserialize(in[i]);
+				}
+				return new GenericArrayData(array);
+			};
 		}
 	}
 
 	// Have its own method so that Postgres can support primitives that super class doesn't support in the future
-	private JdbcFieldConverter createPrimitiveConverter(LogicalType type) {
-		return super.createConverter(type);
+	private JdbcDeserializationConverter createPrimitiveConverter(LogicalType type) {
+		return super.createNullableInternalConverter(type);
 	}
 
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
index 8364d8f674b1d..fac2e79a65245 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
@@ -32,7 +32,7 @@ public class JdbcLookupOptions implements Serializable {
 	private final long cacheExpireMs;
 	private final int maxRetryTimes;
 
-	protected JdbcLookupOptions(long cacheMaxSize, long cacheExpireMs, int maxRetryTimes) {
+	public JdbcLookupOptions(long cacheMaxSize, long cacheExpireMs, int maxRetryTimes) {
 		this.cacheMaxSize = cacheMaxSize;
 		this.cacheExpireMs = cacheExpireMs;
 		this.maxRetryTimes = maxRetryTimes;
@@ -70,9 +70,9 @@ public boolean equals(Object o) {
 	 * Builder of {@link JdbcLookupOptions}.
 	 */
 	public static class Builder {
-		protected long cacheMaxSize = -1L;
-		protected long cacheExpireMs = -1L;
-		protected int maxRetryTimes = JdbcExecutionOptions.DEFAULT_MAX_RETRY_TIMES;
+		private long cacheMaxSize = -1L;
+		private long cacheExpireMs = -1L;
+		private int maxRetryTimes = JdbcExecutionOptions.DEFAULT_MAX_RETRY_TIMES;
 
 		/**
 		 * optional, lookup cache max size, over this value, the old data will be eliminated.
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcOptions.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcOptions.java
index 6e07a14c9ea6e..6e253207b0dd5 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcOptions.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcOptions.java
@@ -37,7 +37,7 @@ public class JdbcOptions extends JdbcConnectionOptions {
 	private String tableName;
 	private JdbcDialect dialect;
 
-	protected JdbcOptions(String dbURL, String tableName, String driverName, String username,
+	private JdbcOptions(String dbURL, String tableName, String driverName, String username,
 						String password, JdbcDialect dialect) {
 		super(dbURL, driverName, username, password);
 		this.tableName = tableName;
@@ -75,11 +75,11 @@ public boolean equals(Object o) {
 	 * Builder of {@link JdbcOptions}.
 	 */
 	public static class Builder {
-		protected String dbURL;
-		protected String tableName;
-		protected String driverName;
-		protected String username;
-		protected String password;
+		private String dbURL;
+		private String tableName;
+		private String driverName;
+		private String username;
+		private String password;
 		private JdbcDialect dialect;
 
 		/**
@@ -138,7 +138,7 @@ public JdbcOptions build() {
 			if (this.dialect == null) {
 				Optional<JdbcDialect> optional = JdbcDialects.get(dbURL);
 				this.dialect = optional.orElseGet(() -> {
-					throw new NullPointerException("No dialect supplied.");
+					throw new NullPointerException("Unknown dbURL,can not find proper dialect.");
 				});
 			}
 			if (this.driverName == null) {
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcReadOptions.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcReadOptions.java
index e18677eeef6a3..a1350ab35eb75 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcReadOptions.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcReadOptions.java
@@ -34,7 +34,7 @@ public class JdbcReadOptions implements Serializable {
 
 	private final int fetchSize;
 
-	protected JdbcReadOptions(
+	private JdbcReadOptions(
 			String partitionColumnName,
 			Long partitionLowerBound,
 			Long partitionUpperBound,
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
new file mode 100644
index 0000000000000..7b002241661bf
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.connector.ChangelogMode;
+import org.apache.flink.table.connector.sink.DynamicTableSink;
+import org.apache.flink.table.connector.sink.OutputFormatProvider;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.types.RowKind;
+
+import java.util.Objects;
+
+import static org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.DynamicOutputFormatBuilder;
+import static org.apache.flink.util.Preconditions.checkState;
+
+/**
+ * A {@link DynamicTableSink} for JDBC.
+ */
+@Internal
+public class JdbcDynamicTableSink implements DynamicTableSink {
+
+	private final JdbcOptions jdbcOptions;
+	private final JdbcExecutionOptions executionOptions;
+	private final JdbcDmlOptions dmlOptions;
+	private final TableSchema tableSchema;
+	private final String dialectName;
+
+	public JdbcDynamicTableSink(
+			JdbcOptions jdbcOptions,
+			JdbcExecutionOptions executionOptions,
+			JdbcDmlOptions dmlOptions,
+			TableSchema tableSchema) {
+		this.jdbcOptions = jdbcOptions;
+		this.executionOptions = executionOptions;
+		this.dmlOptions = dmlOptions;
+		this.tableSchema = tableSchema;
+		this.dialectName = dmlOptions.getDialect().dialectName();
+	}
+
+	@Override
+	public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {
+		validatePrimaryKey(requestedMode);
+		return ChangelogMode.newBuilder()
+			.addContainedKind(RowKind.INSERT)
+			.addContainedKind(RowKind.DELETE)
+			.addContainedKind(RowKind.UPDATE_AFTER)
+			.build();
+	}
+
+	private void validatePrimaryKey(ChangelogMode requestedMode) {
+		checkState(ChangelogMode.insertOnly().equals(requestedMode) || dmlOptions.getKeyFields().isPresent(),
+			"please declare primary key for sink table when query contains update/delete record.");
+	}
+
+	@Override
+	@SuppressWarnings("unchecked")
+	public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
+		final TypeInformation<RowData> rowDataTypeInformation = (TypeInformation<RowData>) context
+			.createTypeInformation(tableSchema.toRowDataType());
+		final DynamicOutputFormatBuilder builder = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder();
+
+		builder.setJdbcOptions(jdbcOptions);
+		builder.setJdbcDmlOptions(dmlOptions);
+		builder.setJdbcExecutionOptions(executionOptions);
+		builder.setRowDataTypeInfo(rowDataTypeInformation);
+		builder.setFieldDataTypes(tableSchema.getFieldDataTypes());
+		return OutputFormatProvider.of(builder.build());
+	}
+
+	@Override
+	public DynamicTableSink copy() {
+		return new JdbcDynamicTableSink(jdbcOptions, executionOptions, dmlOptions, tableSchema);
+	}
+
+	@Override
+	public String asSummaryString() {
+		return "JDBC:" + dialectName;
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (!(o instanceof JdbcDynamicTableSink)) {
+			return false;
+		}
+		JdbcDynamicTableSink that = (JdbcDynamicTableSink) o;
+		return Objects.equals(jdbcOptions, that.jdbcOptions) &&
+			Objects.equals(executionOptions, that.executionOptions) &&
+			Objects.equals(dmlOptions, that.dmlOptions) &&
+			Objects.equals(tableSchema, that.tableSchema) &&
+			Objects.equals(dialectName, that.dialectName);
+	}
+
+	@Override
+	public int hashCode() {
+		return Objects.hash(jdbcOptions, executionOptions, dmlOptions, tableSchema, dialectName);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java
new file mode 100644
index 0000000000000..248ffe1b10b62
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java
@@ -0,0 +1,162 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcReadOptions;
+import org.apache.flink.connector.jdbc.split.JdbcNumericBetweenParametersProvider;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.connector.ChangelogMode;
+import org.apache.flink.table.connector.source.DynamicTableSource;
+import org.apache.flink.table.connector.source.InputFormatProvider;
+import org.apache.flink.table.connector.source.LookupTableSource;
+import org.apache.flink.table.connector.source.ScanTableSource;
+import org.apache.flink.table.connector.source.TableFunctionProvider;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.util.Preconditions;
+
+import java.util.Arrays;
+import java.util.Objects;
+
+/**
+ * A {@link DynamicTableSource} for JDBC.
+ */
+@Internal
+public class JdbcDynamicTableSource implements ScanTableSource, LookupTableSource {
+
+	private final JdbcOptions options;
+	private final JdbcReadOptions readOptions;
+	private final JdbcLookupOptions lookupOptions;
+	private final TableSchema schema;
+	private final int[] selectFields;
+	private final String dialectName;
+
+	public JdbcDynamicTableSource(
+			JdbcOptions options,
+			JdbcReadOptions readOptions,
+			JdbcLookupOptions lookupOptions,
+			TableSchema schema,
+			int[] selectFields) {
+		this.options = options;
+		this.readOptions = readOptions;
+		this.lookupOptions = lookupOptions;
+		this.schema = schema;
+		this.selectFields = selectFields;
+		this.dialectName = options.getDialect().dialectName();
+	}
+
+	@Override
+	public LookupRuntimeProvider getLookupRuntimeProvider(LookupTableSource.Context context) {
+		// JDBC only support non-nested look up keys
+		String[] keyNames = new String[context.getKeys().length];
+		for (int i = 0; i < keyNames.length; i++) {
+			int[] innerKeyArr = context.getKeys()[i];
+			Preconditions.checkArgument(innerKeyArr.length == 1,
+				"JDBC only support non-nested look up keys");
+			keyNames[i] = schema.getFieldNames()[innerKeyArr[0]];
+		}
+		final RowType rowType = (RowType) schema.toRowDataType().getLogicalType();
+
+		return TableFunctionProvider.of(new JdbcRowDataLookupFunction(
+			options,
+			lookupOptions,
+			schema.getFieldNames(),
+			schema.getFieldDataTypes(),
+			keyNames,
+			rowType));
+	}
+
+	@Override
+	@SuppressWarnings("unchecked")
+	public ScanRuntimeProvider getScanRuntimeProvider(ScanTableSource.Context runtimeProviderContext) {
+		final JdbcRowDataInputFormat.Builder builder = JdbcRowDataInputFormat.builder()
+			.setDrivername(options.getDriverName())
+			.setDBUrl(options.getDbURL())
+			.setUsername(options.getUsername().orElse(null))
+			.setPassword(options.getPassword().orElse(null));
+
+		if (readOptions.getFetchSize() != 0) {
+			builder.setFetchSize(readOptions.getFetchSize());
+		}
+		final JdbcDialect dialect = options.getDialect();
+		String query = dialect.getSelectFromStatement(
+			options.getTableName(), schema.getFieldNames(), new String[0]);
+		if (readOptions.getPartitionColumnName().isPresent()) {
+			long lowerBound = readOptions.getPartitionLowerBound().get();
+			long upperBound = readOptions.getPartitionUpperBound().get();
+			int numPartitions = readOptions.getNumPartitions().get();
+			builder.setParametersProvider(
+				new JdbcNumericBetweenParametersProvider(lowerBound, upperBound).ofBatchNum(numPartitions));
+			query += " WHERE " +
+				dialect.quoteIdentifier(readOptions.getPartitionColumnName().get()) +
+				" BETWEEN ? AND ?";
+		}
+		builder.setQuery(query);
+		final RowType rowType = (RowType) schema.toRowDataType().getLogicalType();
+		builder.setRowConverter(dialect.getRowConverter(rowType));
+		builder.setRowDataTypeInfo((TypeInformation<RowData>) runtimeProviderContext
+			.createTypeInformation(schema.toRowDataType()));
+
+		return InputFormatProvider.of(builder.build());
+	}
+
+	@Override
+	public ChangelogMode getChangelogMode() {
+		return ChangelogMode.insertOnly();
+	}
+
+	@Override
+	public DynamicTableSource copy() {
+		return new JdbcDynamicTableSource(options, readOptions, lookupOptions, schema, selectFields);
+	}
+
+	@Override
+	public String asSummaryString() {
+		return "JDBC:" + dialectName;
+	}
+
+	@Override
+	public boolean equals(Object o) {
+		if (this == o) {
+			return true;
+		}
+		if (!(o instanceof JdbcDynamicTableSource)) {
+			return false;
+		}
+		JdbcDynamicTableSource that = (JdbcDynamicTableSource) o;
+		return Objects.equals(options, that.options) &&
+			Objects.equals(readOptions, that.readOptions) &&
+			Objects.equals(lookupOptions, that.lookupOptions) &&
+			Objects.equals(schema, that.schema) &&
+			Arrays.equals(selectFields, that.selectFields) &&
+			Objects.equals(dialectName, that.dialectName);
+	}
+
+	@Override
+	public int hashCode() {
+		int result = Objects.hash(options, readOptions, lookupOptions, schema, dialectName);
+		result = 31 * result + Arrays.hashCode(selectFields);
+		return result;
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java
new file mode 100644
index 0000000000000..28a129df2d641
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java
@@ -0,0 +1,320 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.configuration.ConfigOption;
+import org.apache.flink.configuration.ConfigOptions;
+import org.apache.flink.configuration.ReadableConfig;
+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcReadOptions;
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.connector.sink.DynamicTableSink;
+import org.apache.flink.table.connector.source.DynamicTableSource;
+import org.apache.flink.table.factories.DynamicTableSinkFactory;
+import org.apache.flink.table.factories.DynamicTableSourceFactory;
+import org.apache.flink.table.factories.FactoryUtil;
+import org.apache.flink.table.utils.TableSchemaUtils;
+import org.apache.flink.util.Preconditions;
+
+import java.time.Duration;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Optional;
+import java.util.Set;
+
+import static org.apache.flink.util.Preconditions.checkState;
+
+/**
+ * Factory for creating configured instances of {@link JdbcDynamicTableSource}
+ * and {@link JdbcDynamicTableSink}.
+ */
+@Internal
+public class JdbcDynamicTableSourceSinkFactory implements DynamicTableSourceFactory, DynamicTableSinkFactory {
+
+	public static final String IDENTIFIER = "jdbc";
+	public static final ConfigOption<String> URL = ConfigOptions
+		.key("url")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the jdbc database url.");
+	public static final ConfigOption<String> TABLE_NAME = ConfigOptions
+		.key("table-name")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the jdbc table name.");
+	public static final ConfigOption<String> USERNAME = ConfigOptions
+		.key("username")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the jdbc user name.");
+	public static final ConfigOption<String> PASSWORD = ConfigOptions
+		.key("password")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the jdbc password.");
+	private static final ConfigOption<String> DRIVER = ConfigOptions
+		.key("driver")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the class name of the JDBC driver to use to connect to this URL. " +
+			"If not set, it will automatically be derived from the URL.");
+
+	// read config options
+	private static final ConfigOption<String> SCAN_PARTITION_COLUMN = ConfigOptions
+		.key("scan.partition.column")
+		.stringType()
+		.noDefaultValue()
+		.withDescription("the column name used for partitioning the input.");
+	private static final ConfigOption<Integer> SCAN_PARTITION_NUM = ConfigOptions
+		.key("scan.partition.num")
+		.intType()
+		.noDefaultValue()
+		.withDescription("the number of partitions.");
+	private static final ConfigOption<Long> SCAN_PARTITION_LOWER_BOUND = ConfigOptions
+		.key("scan.partition.lower-bound")
+		.longType()
+		.noDefaultValue()
+		.withDescription("the smallest value of the first partition.");
+	private static final ConfigOption<Long> SCAN_PARTITION_UPPER_BOUND = ConfigOptions
+		.key("scan.partition.upper-bound")
+		.longType()
+		.noDefaultValue()
+		.withDescription("the largest value of the last partition.");
+	private static final ConfigOption<Integer> SCAN_FETCH_SIZE = ConfigOptions
+		.key("scan.fetch-size")
+		.intType()
+		.defaultValue(0)
+		.withDescription("gives the reader a hint as to the number of rows that should be fetched, from" +
+			" the database when reading per round trip. If the value specified is zero, then the hint is ignored. The" +
+			" default value is zero.");
+
+	// look up config options
+	private static final ConfigOption<Long> LOOKUP_CACHE_MAX_ROWS = ConfigOptions
+		.key("lookup.cache.max-rows")
+		.longType()
+		.defaultValue(-1L)
+		.withDescription("the max number of rows of lookup cache, over this value, the oldest rows will " +
+			"be eliminated. \"cache.max-rows\" and \"cache.ttl\" options must all be specified if any of them is " +
+			"specified. Cache is not enabled as default.");
+	private static final ConfigOption<Duration> LOOKUP_CACHE_TTL = ConfigOptions
+		.key("lookup.cache.ttl")
+		.durationType()
+		.defaultValue(Duration.ofSeconds(10))
+		.withDescription("the cache time to live.");
+	private static final ConfigOption<Integer> LOOKUP_MAX_RETRIES = ConfigOptions
+		.key("lookup.max-retries")
+		.intType()
+		.defaultValue(3)
+		.withDescription("the max retry times if lookup database failed.");
+
+	// write config options
+	private static final ConfigOption<Integer> SINK_BUFFER_FLUSH_MAX_ROWS = ConfigOptions
+		.key("sink.buffer-flush.max-rows")
+		.intType()
+		.defaultValue(5000)
+		.withDescription("the flush max size (includes all append, upsert and delete records), over this number" +
+			" of records, will flush data. The default value is 5000.");
+	private static final ConfigOption<Long> SINK_BUFFER_FLUSH_INTERVAL = ConfigOptions
+		.key("sink.buffer-flush.interval")
+		.longType()
+		.defaultValue(0L)
+		.withDescription("the flush interval mills, over this time, asynchronous threads will flush data. The " +
+			"default value is 0, which means no asynchronous flush thread will be scheduled.");
+	private static final ConfigOption<Integer> SINK_MAX_RETRIES = ConfigOptions
+		.key("sink.max-retries")
+		.intType()
+		.defaultValue(3)
+		.withDescription("the max retry times if writing records to database failed.");
+
+	@Override
+	public DynamicTableSink createDynamicTableSink(Context context) {
+		final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);
+		final ReadableConfig config = helper.getOptions();
+
+		helper.validate();
+		validateConfigOptions(config);
+		JdbcOptions jdbcOptions = getJdbcOptions(config);
+		TableSchema physicalSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
+
+		return new JdbcDynamicTableSink(
+			jdbcOptions,
+			getJdbcExecutionOptions(config),
+			getJdbcDmlOptions(jdbcOptions, physicalSchema),
+			physicalSchema);
+	}
+
+	@Override
+	public DynamicTableSource createDynamicTableSource(Context context) {
+		final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);
+		final ReadableConfig config = helper.getOptions();
+
+		helper.validate();
+		validateConfigOptions(config);
+		TableSchema physicalSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
+		int[] selectFields = new int[physicalSchema.getFieldNames().length];
+		for (int i = 0; i < selectFields.length; i++) {
+			selectFields[i] = i;
+		}
+		return new JdbcDynamicTableSource(
+			getJdbcOptions(helper.getOptions()),
+			getJdbcReadOptions(helper.getOptions()),
+			getJdbcLookupOptions(helper.getOptions()),
+			physicalSchema,
+			selectFields);
+	}
+
+	private JdbcOptions getJdbcOptions(ReadableConfig readableConfig) {
+		final String url = readableConfig.get(URL);
+		final JdbcOptions.Builder builder = JdbcOptions.builder()
+			.setDBUrl(url)
+			.setTableName(readableConfig.get(TABLE_NAME))
+			.setDialect(JdbcDialects.get(url).get());
+
+		readableConfig.getOptional(DRIVER).ifPresent(builder::setDriverName);
+		readableConfig.getOptional(USERNAME).ifPresent(builder::setUsername);
+		readableConfig.getOptional(PASSWORD).ifPresent(builder::setPassword);
+		return builder.build();
+	}
+
+	private JdbcReadOptions getJdbcReadOptions(ReadableConfig readableConfig) {
+		final Optional<String> partitionColumnName = readableConfig.getOptional(SCAN_PARTITION_COLUMN);
+		final JdbcReadOptions.Builder builder = JdbcReadOptions.builder();
+		if (partitionColumnName.isPresent()) {
+			builder.setPartitionColumnName(partitionColumnName.get());
+			builder.setPartitionLowerBound(readableConfig.get(SCAN_PARTITION_LOWER_BOUND));
+			builder.setPartitionUpperBound(readableConfig.get(SCAN_PARTITION_UPPER_BOUND));
+			builder.setNumPartitions(readableConfig.get(SCAN_PARTITION_NUM));
+		}
+		readableConfig.getOptional(SCAN_FETCH_SIZE).ifPresent(builder::setFetchSize);
+		return builder.build();
+	}
+
+	private JdbcLookupOptions getJdbcLookupOptions(ReadableConfig readableConfig) {
+		return new JdbcLookupOptions(
+			readableConfig.get(LOOKUP_CACHE_MAX_ROWS),
+			readableConfig.get(LOOKUP_CACHE_TTL).toMillis(),
+			readableConfig.get(LOOKUP_MAX_RETRIES));
+	}
+
+	private JdbcExecutionOptions getJdbcExecutionOptions(ReadableConfig config) {
+		final JdbcExecutionOptions.Builder builder = new JdbcExecutionOptions.Builder();
+		builder.withBatchSize(config.get(SINK_BUFFER_FLUSH_MAX_ROWS));
+		builder.withBatchIntervalMs(config.get(SINK_BUFFER_FLUSH_INTERVAL));
+		builder.withMaxRetries(config.get(SINK_MAX_RETRIES));
+		return builder.build();
+	}
+
+	private JdbcDmlOptions getJdbcDmlOptions(JdbcOptions jdbcOptions, TableSchema schema) {
+		String[] keyFields = schema.getPrimaryKey()
+			.map(pk -> pk.getColumns().toArray(new String[0]))
+			.orElse(null);
+
+		return JdbcDmlOptions.builder()
+			.withTableName(jdbcOptions.getTableName())
+			.withDialect(jdbcOptions.getDialect())
+			.withFieldNames(schema.getFieldNames())
+			.withKeyFields(keyFields)
+			.build();
+	}
+
+	@Override
+	public String factoryIdentifier() {
+		return IDENTIFIER;
+	}
+
+	@Override
+	public Set<ConfigOption<?>> requiredOptions() {
+		Set<ConfigOption<?>> requiredOptions = new HashSet<>();
+		requiredOptions.add(URL);
+		requiredOptions.add(TABLE_NAME);
+		return requiredOptions;
+	}
+
+	@Override
+	public Set<ConfigOption<?>> optionalOptions() {
+		Set<ConfigOption<?>> optionalOptions = new HashSet<>();
+		optionalOptions.add(DRIVER);
+		optionalOptions.add(USERNAME);
+		optionalOptions.add(PASSWORD);
+		optionalOptions.add(SCAN_PARTITION_COLUMN);
+		optionalOptions.add(SCAN_PARTITION_LOWER_BOUND);
+		optionalOptions.add(SCAN_PARTITION_UPPER_BOUND);
+		optionalOptions.add(SCAN_PARTITION_NUM);
+		optionalOptions.add(SCAN_FETCH_SIZE);
+		optionalOptions.add(LOOKUP_CACHE_MAX_ROWS);
+		optionalOptions.add(LOOKUP_CACHE_TTL);
+		optionalOptions.add(LOOKUP_MAX_RETRIES);
+		optionalOptions.add(SINK_BUFFER_FLUSH_MAX_ROWS);
+		optionalOptions.add(SINK_BUFFER_FLUSH_INTERVAL);
+		optionalOptions.add(SINK_MAX_RETRIES);
+		return optionalOptions;
+	}
+
+	private void validateConfigOptions(ReadableConfig config) {
+		config.getOptional(URL).orElseThrow(() -> new IllegalArgumentException(
+			String.format("Could not find required option: %s", URL.key())));
+		config.getOptional(TABLE_NAME).orElseThrow(() -> new IllegalArgumentException(
+			String.format("Could not find required option: %s", TABLE_NAME.key())));
+
+		String jdbcUrl = config.get(URL);
+		final Optional<JdbcDialect> dialect = JdbcDialects.get(jdbcUrl);
+		checkState(dialect.isPresent(), "Cannot handle such jdbc url: " + jdbcUrl);
+
+		if (config.getOptional(USERNAME).isPresent()) {
+			checkState(config.getOptional(PASSWORD).isPresent(),
+				"Database username must be provided when database password is provided");
+		}
+
+		checkAllOrNone(config, new ConfigOption[]{
+			SCAN_PARTITION_COLUMN,
+			SCAN_PARTITION_NUM,
+			SCAN_PARTITION_LOWER_BOUND,
+			SCAN_PARTITION_UPPER_BOUND
+		});
+
+		if (config.getOptional(SCAN_PARTITION_LOWER_BOUND).isPresent() &&
+			config.getOptional(SCAN_PARTITION_UPPER_BOUND).isPresent()) {
+			checkState(config.get(SCAN_PARTITION_LOWER_BOUND) <= config.get(SCAN_PARTITION_UPPER_BOUND),
+				String.format("%s must not be larger than %s", SCAN_PARTITION_LOWER_BOUND.key(), SCAN_PARTITION_UPPER_BOUND.key()));
+		}
+
+		checkAllOrNone(config, new ConfigOption[]{
+			LOOKUP_CACHE_MAX_ROWS,
+			LOOKUP_CACHE_TTL
+		});
+	}
+
+	private void checkAllOrNone(ReadableConfig config, ConfigOption<?>[] configOptions) {
+		int presentCount = 0;
+		for (ConfigOption configOption : configOptions) {
+			if (config.getOptional(configOption).isPresent()) {
+				presentCount++;
+			}
+		}
+		String[] propertyNames = Arrays.stream(configOptions).map(ConfigOption::key).toArray(String[]::new);
+		Preconditions.checkArgument(configOptions.length == presentCount || presentCount == 0,
+			"Either all or none of the following options should be provided:\n" + String.join("\n", propertyNames));
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java
new file mode 100644
index 0000000000000..06820d1007aa7
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java
@@ -0,0 +1,403 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;
+import org.apache.flink.api.common.io.InputFormat;
+import org.apache.flink.api.common.io.RichInputFormat;
+import org.apache.flink.api.common.io.statistics.BaseStatistics;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
+import org.apache.flink.core.io.GenericInputSplit;
+import org.apache.flink.core.io.InputSplit;
+import org.apache.flink.core.io.InputSplitAssigner;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.util.Preconditions;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.sql.Array;
+import java.sql.Connection;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.util.Arrays;
+
+/**
+ * InputFormat for {@link JdbcDynamicTableSource}.
+ */
+@Internal
+public class JdbcRowDataInputFormat extends RichInputFormat<RowData, InputSplit> implements ResultTypeQueryable<RowData> {
+
+	private static final long serialVersionUID = 1L;
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataInputFormat.class);
+
+	private JdbcConnectionOptions connectionOptions;
+	private int fetchSize;
+	private Boolean autoCommit;
+	private Object[][] parameterValues;
+	private String queryTemplate;
+	private int resultSetType;
+	private int resultSetConcurrency;
+	private JdbcRowConverter rowConverter;
+	private TypeInformation<RowData> rowDataTypeInfo;
+
+	private transient Connection dbConn;
+	private transient PreparedStatement statement;
+	private transient ResultSet resultSet;
+	private transient boolean hasNext;
+
+	private JdbcRowDataInputFormat(
+			JdbcConnectionOptions connectionOptions,
+			int fetchSize,
+			Boolean autoCommit,
+			Object[][] parameterValues,
+			String queryTemplate,
+			int resultSetType,
+			int resultSetConcurrency,
+			JdbcRowConverter rowConverter,
+			TypeInformation<RowData> rowDataTypeInfo) {
+		this.connectionOptions = connectionOptions;
+		this.fetchSize = fetchSize;
+		this.autoCommit = autoCommit;
+		this.parameterValues = parameterValues;
+		this.queryTemplate = queryTemplate;
+		this.resultSetType = resultSetType;
+		this.resultSetConcurrency = resultSetConcurrency;
+		this.rowConverter = rowConverter;
+		this.rowDataTypeInfo = rowDataTypeInfo;
+	}
+
+	@Override
+	public void configure(Configuration parameters) {
+		//do nothing here
+	}
+
+	@Override
+	public void openInputFormat() {
+		//called once per inputFormat (on open)
+		try {
+			dbConn = new SimpleJdbcConnectionProvider(connectionOptions).getConnection();
+			// set autoCommit mode only if it was explicitly configured.
+			// keep connection default otherwise.
+			if (autoCommit != null) {
+				dbConn.setAutoCommit(autoCommit);
+			}
+			statement = dbConn.prepareStatement(queryTemplate, resultSetType, resultSetConcurrency);
+			if (fetchSize == Integer.MIN_VALUE || fetchSize > 0) {
+				statement.setFetchSize(fetchSize);
+			}
+		} catch (SQLException se) {
+			throw new IllegalArgumentException("open() failed." + se.getMessage(), se);
+		} catch (ClassNotFoundException cnfe) {
+			throw new IllegalArgumentException("JDBC-Class not found. - " + cnfe.getMessage(), cnfe);
+		}
+	}
+
+	@Override
+	public void closeInputFormat() {
+		//called once per inputFormat (on close)
+		try {
+			if (statement != null) {
+				statement.close();
+			}
+		} catch (SQLException se) {
+			LOG.info("Inputformat Statement couldn't be closed - " + se.getMessage());
+		} finally {
+			statement = null;
+		}
+
+		try {
+			if (dbConn != null) {
+				dbConn.close();
+			}
+		} catch (SQLException se) {
+			LOG.info("Inputformat couldn't be closed - " + se.getMessage());
+		} finally {
+			dbConn = null;
+		}
+
+		parameterValues = null;
+	}
+
+	/**
+	 * Connects to the source database and executes the query in a <b>parallel
+	 * fashion</b> if
+	 * this {@link InputFormat} is built using a parameterized query (i.e. using
+	 * a {@link PreparedStatement})
+	 * and a proper {@link JdbcParameterValuesProvider}, in a <b>non-parallel
+	 * fashion</b> otherwise.
+	 *
+	 * @param inputSplit which is ignored if this InputFormat is executed as a
+	 *                   non-parallel source,
+	 *                   a "hook" to the query parameters otherwise (using its
+	 *                   <i>splitNumber</i>)
+	 * @throws IOException if there's an error during the execution of the query
+	 */
+	@Override
+	public void open(InputSplit inputSplit) throws IOException {
+		try {
+			if (inputSplit != null && parameterValues != null) {
+				for (int i = 0; i < parameterValues[inputSplit.getSplitNumber()].length; i++) {
+					Object param = parameterValues[inputSplit.getSplitNumber()][i];
+					if (param instanceof String) {
+						statement.setString(i + 1, (String) param);
+					} else if (param instanceof Long) {
+						statement.setLong(i + 1, (Long) param);
+					} else if (param instanceof Integer) {
+						statement.setInt(i + 1, (Integer) param);
+					} else if (param instanceof Double) {
+						statement.setDouble(i + 1, (Double) param);
+					} else if (param instanceof Boolean) {
+						statement.setBoolean(i + 1, (Boolean) param);
+					} else if (param instanceof Float) {
+						statement.setFloat(i + 1, (Float) param);
+					} else if (param instanceof BigDecimal) {
+						statement.setBigDecimal(i + 1, (BigDecimal) param);
+					} else if (param instanceof Byte) {
+						statement.setByte(i + 1, (Byte) param);
+					} else if (param instanceof Short) {
+						statement.setShort(i + 1, (Short) param);
+					} else if (param instanceof Date) {
+						statement.setDate(i + 1, (Date) param);
+					} else if (param instanceof Time) {
+						statement.setTime(i + 1, (Time) param);
+					} else if (param instanceof Timestamp) {
+						statement.setTimestamp(i + 1, (Timestamp) param);
+					} else if (param instanceof Array) {
+						statement.setArray(i + 1, (Array) param);
+					} else {
+						//extends with other types if needed
+						throw new IllegalArgumentException("open() failed. Parameter " + i + " of type " + param.getClass() + " is not handled (yet).");
+					}
+				}
+				if (LOG.isDebugEnabled()) {
+					LOG.debug(String.format("Executing '%s' with parameters %s", queryTemplate, Arrays.deepToString(parameterValues[inputSplit.getSplitNumber()])));
+				}
+			}
+			resultSet = statement.executeQuery();
+			hasNext = resultSet.next();
+		} catch (SQLException se) {
+			throw new IllegalArgumentException("open() failed." + se.getMessage(), se);
+		}
+	}
+
+	/**
+	 * Closes all resources used.
+	 *
+	 * @throws IOException Indicates that a resource could not be closed.
+	 */
+	@Override
+	public void close() throws IOException {
+		if (resultSet == null) {
+			return;
+		}
+		try {
+			resultSet.close();
+		} catch (SQLException se) {
+			LOG.info("Inputformat ResultSet couldn't be closed - " + se.getMessage());
+		}
+	}
+
+	@Override
+	public TypeInformation<RowData> getProducedType() {
+		return rowDataTypeInfo;
+	}
+
+	/**
+	 * Checks whether all data has been read.
+	 *
+	 * @return boolean value indication whether all data has been read.
+	 * @throws IOException
+	 */
+	@Override
+	public boolean reachedEnd() throws IOException {
+		return !hasNext;
+	}
+
+	/**
+	 * Stores the next resultSet row in a tuple.
+	 *
+	 * @param reuse row to be reused.
+	 * @return row containing next {@link RowData}
+	 * @throws IOException
+	 */
+	@Override
+	public RowData nextRecord(RowData reuse) throws IOException {
+		try {
+			if (!hasNext) {
+				return null;
+			}
+			RowData row = rowConverter.toInternal(resultSet);
+			//update hasNext after we've read the record
+			hasNext = resultSet.next();
+			return row;
+		} catch (SQLException se) {
+			throw new IOException("Couldn't read data - " + se.getMessage(), se);
+		} catch (NullPointerException npe) {
+			throw new IOException("Couldn't access resultSet", npe);
+		}
+	}
+
+	@Override
+	public BaseStatistics getStatistics(BaseStatistics cachedStatistics) throws IOException {
+		return cachedStatistics;
+	}
+
+	@Override
+	public InputSplit[] createInputSplits(int minNumSplits) throws IOException {
+		if (parameterValues == null) {
+			return new GenericInputSplit[]{new GenericInputSplit(0, 1)};
+		}
+		GenericInputSplit[] ret = new GenericInputSplit[parameterValues.length];
+		for (int i = 0; i < ret.length; i++) {
+			ret[i] = new GenericInputSplit(i, ret.length);
+		}
+		return ret;
+	}
+
+	@Override
+	public InputSplitAssigner getInputSplitAssigner(InputSplit[] inputSplits) {
+		return new DefaultInputSplitAssigner(inputSplits);
+	}
+
+	/**
+	 * A builder used to set parameters to the output format's configuration in a fluent way.
+	 *
+	 * @return builder
+	 */
+	public static Builder builder() {
+		return new Builder();
+	}
+
+	/**
+	 * Builder for {@link JdbcRowDataInputFormat}.
+	 */
+	public static class Builder {
+		private JdbcConnectionOptions.JdbcConnectionOptionsBuilder connOptionsBuilder;
+		private int fetchSize;
+		private Boolean autoCommit;
+		private Object[][] parameterValues;
+		private String queryTemplate;
+		private JdbcRowConverter rowConverter;
+		private TypeInformation<RowData> rowDataTypeInfo;
+		private int resultSetType = ResultSet.TYPE_FORWARD_ONLY;
+		private int resultSetConcurrency = ResultSet.CONCUR_READ_ONLY;
+
+		public Builder() {
+			this.connOptionsBuilder = new JdbcConnectionOptions.JdbcConnectionOptionsBuilder();
+		}
+
+		public Builder setDrivername(String drivername) {
+			this.connOptionsBuilder.withDriverName(drivername);
+			return this;
+		}
+
+		public Builder setDBUrl(String dbURL) {
+			this.connOptionsBuilder.withUrl(dbURL);
+			return this;
+		}
+
+		public Builder setUsername(String username) {
+			this.connOptionsBuilder.withUsername(username);
+			return this;
+		}
+
+		public Builder setPassword(String password) {
+			this.connOptionsBuilder.withPassword(password);
+			return this;
+		}
+
+		public Builder setQuery(String query) {
+			this.queryTemplate = query;
+			return this;
+		}
+
+		public Builder setParametersProvider(JdbcParameterValuesProvider parameterValuesProvider) {
+			this.parameterValues = parameterValuesProvider.getParameterValues();
+			return this;
+		}
+
+		public Builder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {
+			this.rowDataTypeInfo = rowDataTypeInfo;
+			return this;
+		}
+
+		public Builder setRowConverter(JdbcRowConverter rowConverter) {
+			this.rowConverter = rowConverter;
+			return this;
+		}
+
+		public Builder setFetchSize(int fetchSize) {
+			Preconditions.checkArgument(fetchSize == Integer.MIN_VALUE || fetchSize > 0,
+				"Illegal value %s for fetchSize, has to be positive or Integer.MIN_VALUE.", fetchSize);
+			this.fetchSize = fetchSize;
+			return this;
+		}
+
+		public Builder setAutoCommit(Boolean autoCommit) {
+			this.autoCommit = autoCommit;
+			return this;
+		}
+
+		public Builder setResultSetType(int resultSetType) {
+			this.resultSetType = resultSetType;
+			return this;
+		}
+
+		public Builder setResultSetConcurrency(int resultSetConcurrency) {
+			this.resultSetConcurrency = resultSetConcurrency;
+			return this;
+		}
+
+		public JdbcRowDataInputFormat build() {
+			if (this.queryTemplate == null) {
+				throw new IllegalArgumentException("No query supplied");
+			}
+			if (this.rowConverter == null) {
+				throw new IllegalArgumentException("No row converter supplied");
+			}
+			if (this.parameterValues == null) {
+				LOG.debug("No input splitting configured (data will be read with parallelism 1).");
+			}
+			return new JdbcRowDataInputFormat(
+				connOptionsBuilder.build(),
+				this.fetchSize,
+				this.autoCommit,
+				this.parameterValues,
+				this.queryTemplate,
+				this.resultSetType,
+				this.resultSetConcurrency,
+				this.rowConverter,
+				this.rowDataTypeInfo);
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
new file mode 100644
index 0000000000000..a075aeeae2882
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
@@ -0,0 +1,218 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.functions.FunctionContext;
+import org.apache.flink.table.functions.TableFunction;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+
+import org.apache.flink.shaded.guava18.com.google.common.cache.Cache;
+import org.apache.flink.shaded.guava18.com.google.common.cache.CacheBuilder;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+
+import static org.apache.flink.util.Preconditions.checkArgument;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * A lookup function for {@link JdbcDynamicTableSource}.
+ */
+@Internal
+public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
+
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataLookupFunction.class);
+	private static final long serialVersionUID = 1L;
+
+	private final String query;
+	private final String drivername;
+	private final String dbURL;
+	private final String username;
+	private final String password;
+	private final DataType[] keyTypes;
+	private final long cacheMaxSize;
+	private final long cacheExpireMs;
+	private final int maxRetryTimes;
+	private final JdbcDialect jdbcDialect;
+	private final JdbcRowConverter jdbcRowConverter;
+	private final JdbcRowConverter lookupKeyRowConverter;
+
+	private transient Connection dbConn;
+	private transient PreparedStatement statement;
+	private transient Cache<RowData, List<RowData>> cache;
+
+	public JdbcRowDataLookupFunction(
+			JdbcOptions options,
+			JdbcLookupOptions lookupOptions,
+			String[] fieldNames,
+			DataType[] fieldTypes,
+			String[] keyNames,
+			RowType rowType) {
+		checkNotNull(options, "No JdbcOptions supplied.");
+		checkNotNull(fieldNames, "No fieldNames supplied.");
+		checkNotNull(fieldTypes, "No fieldTypes supplied.");
+		checkNotNull(keyNames, "No keyNames supplied.");
+		this.drivername = options.getDriverName();
+		this.dbURL = options.getDbURL();
+		this.username = options.getUsername().orElse(null);
+		this.password = options.getPassword().orElse(null);
+		List<String> nameList = Arrays.asList(fieldNames);
+		this.keyTypes = Arrays.stream(keyNames)
+			.map(s -> {
+				checkArgument(nameList.contains(s),
+					"keyName %s can't find in fieldNames %s.", s, nameList);
+				return fieldTypes[nameList.indexOf(s)];
+			})
+			.toArray(DataType[]::new);
+		this.cacheMaxSize = lookupOptions.getCacheMaxSize();
+		this.cacheExpireMs = lookupOptions.getCacheExpireMs();
+		this.maxRetryTimes = lookupOptions.getMaxRetryTimes();
+		this.query = options.getDialect().getSelectFromStatement(
+			options.getTableName(), fieldNames, keyNames);
+		this.jdbcDialect = JdbcDialects.get(dbURL)
+			.orElseThrow(() -> new UnsupportedOperationException(String.format("Unknown dbUrl:%s", dbURL)));
+		this.jdbcRowConverter = jdbcDialect.getRowConverter(rowType);
+		this.lookupKeyRowConverter = jdbcDialect.getRowConverter(RowType.of(Arrays.stream(keyTypes).map(DataType::getLogicalType).toArray(LogicalType[]::new)));
+	}
+
+	@Override
+	public void open(FunctionContext context) throws Exception {
+		try {
+			establishConnection();
+			statement = dbConn.prepareStatement(query);
+			this.cache = cacheMaxSize == -1 || cacheExpireMs == -1 ? null : CacheBuilder.newBuilder()
+				.expireAfterWrite(cacheExpireMs, TimeUnit.MILLISECONDS)
+				.maximumSize(cacheMaxSize)
+				.build();
+		} catch (SQLException sqe) {
+			throw new IllegalArgumentException("open() failed.", sqe);
+		} catch (ClassNotFoundException cnfe) {
+			throw new IllegalArgumentException("JDBC driver class not found.", cnfe);
+		}
+	}
+
+	/**
+	 * This is a lookup method which is called by Flink framework in runtime.
+	 * @param keys lookup keys
+	 */
+	public void eval(Object... keys) {
+		RowData keyRow = GenericRowData.of(keys);
+		if (cache != null) {
+			List<RowData> cachedRows = cache.getIfPresent(keyRow);
+			if (cachedRows != null) {
+				for (RowData cachedRow : cachedRows) {
+					collect(cachedRow);
+				}
+				return;
+			}
+		}
+
+		for (int retry = 1; retry <= maxRetryTimes; retry++) {
+			try {
+				statement.clearParameters();
+				statement = lookupKeyRowConverter.toExternal(keyRow, statement);
+				try (ResultSet resultSet = statement.executeQuery()) {
+					if (cache == null) {
+						while (resultSet.next()) {
+							collect(jdbcRowConverter.toInternal(resultSet));
+						}
+					} else {
+						ArrayList<RowData> rows = new ArrayList<>();
+						while (resultSet.next()) {
+							RowData row = jdbcRowConverter.toInternal(resultSet);
+							rows.add(row);
+							collect(row);
+						}
+						rows.trimToSize();
+						cache.put(keyRow, rows);
+					}
+				}
+				break;
+			} catch (SQLException e) {
+				LOG.error(String.format("JDBC executeBatch error, retry times = %d", retry), e);
+				if (retry >= maxRetryTimes) {
+					throw new RuntimeException("Execution of JDBC statement failed.", e);
+				}
+
+				try {
+					Thread.sleep(1000 * retry);
+				} catch (InterruptedException e1) {
+					throw new RuntimeException(e1);
+				}
+			}
+		}
+	}
+
+	private void establishConnection() throws SQLException, ClassNotFoundException {
+		Class.forName(drivername);
+		if (username == null) {
+			dbConn = DriverManager.getConnection(dbURL);
+		} else {
+			dbConn = DriverManager.getConnection(dbURL, username, password);
+		}
+	}
+
+	@Override
+	public void close() throws IOException {
+		if (cache != null) {
+			cache.cleanUp();
+			cache = null;
+		}
+		if (statement != null) {
+			try {
+				statement.close();
+			} catch (SQLException e) {
+				LOG.info("JDBC statement could not be closed: " + e.getMessage());
+			} finally {
+				statement = null;
+			}
+		}
+
+		if (dbConn != null) {
+			try {
+				dbConn.close();
+			} catch (SQLException se) {
+				LOG.info("JDBC connection could not be closed: " + se.getMessage());
+			} finally {
+				dbConn = null;
+			}
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
new file mode 100644
index 0000000000000..b78eacaa65c33
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
@@ -0,0 +1,296 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.functions.RuntimeContext;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
+import org.apache.flink.connector.jdbc.internal.connection.JdbcConnectionProvider;
+import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
+import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.utils.JdbcUtils;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.RowData.FieldGetter;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.types.Row;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.sql.SQLException;
+import java.util.Arrays;
+import java.util.function.Function;
+
+import static org.apache.flink.table.data.RowData.createFieldGetter;
+import static org.apache.flink.util.Preconditions.checkArgument;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * OutputFormat for {@link JdbcDynamicTableSource}.
+ */
+@Internal
+public class JdbcRowDataOutputFormat extends JdbcBatchingOutputFormat<RowData, RowData, JdbcBatchStatementExecutor<RowData>> {
+
+	private static final long serialVersionUID = 1L;
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataOutputFormat.class);
+
+	private JdbcBatchStatementExecutor<RowData> deleteExecutor;
+	private final JdbcDmlOptions dmlOptions;
+	private final LogicalType[] logicalTypes;
+
+	private JdbcRowDataOutputFormat(
+			JdbcConnectionProvider connectionProvider,
+			JdbcDmlOptions dmlOptions,
+			JdbcExecutionOptions batchOptions,
+			TypeInformation<RowData> rowDataTypeInfo,
+			LogicalType[] logicalTypes) {
+		super(
+			connectionProvider,
+			batchOptions,
+			ctx -> createUpsertRowExecutor(dmlOptions, ctx, rowDataTypeInfo, logicalTypes),
+			RecordExtractor.identity());
+		this.dmlOptions = dmlOptions;
+		this.logicalTypes = logicalTypes;
+	}
+
+	private JdbcRowDataOutputFormat(
+			JdbcConnectionProvider connectionProvider,
+			JdbcDmlOptions dmlOptions,
+			JdbcExecutionOptions batchOptions,
+			TypeInformation<RowData> rowDataTypeInfo,
+			LogicalType[] logicalTypes,
+			String sql) {
+		super(connectionProvider,
+			batchOptions,
+			ctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInfo),
+			RecordExtractor.identity());
+		this.dmlOptions = dmlOptions;
+		this.logicalTypes = logicalTypes;
+	}
+
+	@Override
+	public void open(int taskNumber, int numTasks) throws IOException {
+		deleteExecutor = createDeleteExecutor();
+		super.open(taskNumber, numTasks);
+		try {
+			deleteExecutor.open(connection);
+		} catch (SQLException e) {
+			throw new IOException(e);
+		}
+	}
+
+	private JdbcBatchStatementExecutor<RowData> createDeleteExecutor() {
+		int[] pkFields = Arrays.stream(dmlOptions.getFieldNames()).mapToInt(Arrays.asList(dmlOptions.getFieldNames())::indexOf).toArray();
+		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> logicalTypes[f]).toArray(LogicalType[]::new);
+		String deleteSql = dmlOptions.getDialect().getDeleteStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
+		return createKeyedRowExecutor(dmlOptions.getDialect(), pkFields, pkTypes, deleteSql, logicalTypes);
+	}
+
+	@Override
+	protected void addToBatch(RowData original, RowData extracted) throws SQLException {
+		switch (original.getRowKind()) {
+			case INSERT:
+			case UPDATE_AFTER:
+				super.addToBatch(original, extracted);
+				break;
+			case DELETE:
+			case UPDATE_BEFORE:
+				deleteExecutor.addToBatch(extracted);
+				break;
+			default:
+				throw new UnsupportedOperationException(
+					String.format("unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER," +
+						" DELETE, but get: %s.", original.getRowKind()));
+		}
+	}
+
+	@Override
+	public synchronized void close() {
+		try {
+			super.close();
+		} finally {
+			try {
+				if (deleteExecutor != null) {
+					deleteExecutor.close();
+				}
+			} catch (SQLException e) {
+				LOG.warn("unable to close delete statement runner", e);
+			}
+		}
+	}
+
+	@Override
+	protected void attemptFlush() throws SQLException {
+		super.attemptFlush();
+		deleteExecutor.executeBatch();
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(JdbcDialect dialect, int[] pkFields, LogicalType[] pkTypes, String sql, LogicalType[] logicalTypes) {
+		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(logicalTypes));
+		final Function<RowData, RowData>  keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);
+		return JdbcBatchStatementExecutor.keyed(
+			sql,
+			keyExtractor,
+			(st, record) -> rowConverter
+				.toExternal(keyExtractor.apply(record), st));
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(JdbcDmlOptions opt, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo, LogicalType[] logicalTypes) {
+		checkArgument(opt.getKeyFields().isPresent());
+
+		int[] pkFields = Arrays.stream(opt.getKeyFields().get()).mapToInt(Arrays.asList(opt.getFieldNames())::indexOf).toArray();
+		LogicalType[] pkTypes = Arrays.stream(pkFields).mapToObj(f -> logicalTypes[f]).toArray(LogicalType[]::new);
+		JdbcDialect dialect = opt.getDialect();
+		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
+		return opt.getDialect()
+			.getUpsertStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get())
+			.map(sql -> createSimpleRowDataExecutor(dialect, sql, logicalTypes, ctx, rowDataTypeInfo))
+			.orElseGet(() ->
+				new InsertOrUpdateJdbcExecutor<>(
+					opt.getDialect().getRowExistsStatement(opt.getTableName(), opt.getKeyFields().get()),
+					opt.getDialect().getInsertIntoStatement(opt.getTableName(), opt.getFieldNames()),
+					opt.getDialect().getUpdateStatement(opt.getTableName(), opt.getFieldNames(), opt.getKeyFields().get()),
+					createRowDataJdbcStatementBuilder(dialect, pkTypes),
+					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
+					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
+					createRowKeyExtractor(logicalTypes, pkFields),
+					ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : r -> r));
+	}
+
+	private static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {
+		final FieldGetter[] fieldGetters = new FieldGetter[pkFields.length];
+		for (int i = 0; i < pkFields.length; i++) {
+			fieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);
+		}
+		return row -> getPrimaryKey(row, fieldGetters);
+	}
+
+	private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {
+		final TypeSerializer<RowData> typeSerializer = rowDataTypeInfo.createSerializer(ctx.getExecutionConfig());
+		return JdbcBatchStatementExecutor.simple(
+			sql,
+			createRowDataJdbcStatementBuilder(dialect, fieldTypes),
+			ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : Function.identity());
+	}
+
+	/**
+	 * Creates a {@link JdbcStatementBuilder} for {@link Row} using the provided SQL types array.
+	 * Uses {@link JdbcUtils#setRecordToStatement}
+	 */
+	private static JdbcStatementBuilder<RowData> createRowDataJdbcStatementBuilder(JdbcDialect dialect, LogicalType[] types) {
+		final JdbcRowConverter converter = dialect.getRowConverter(RowType.of(types));
+		return (st, record) -> converter.toExternal(record, st);
+	}
+
+	private static RowData getPrimaryKey(RowData row, FieldGetter[] fieldGetters) {
+		GenericRowData pkRow = new GenericRowData(fieldGetters.length);
+		for (int i = 0; i < fieldGetters.length; i++) {
+			pkRow.setField(i, fieldGetters[i].getFieldOrNull(row));
+		}
+		return pkRow;
+	}
+
+	public static DynamicOutputFormatBuilder dynamicOutputFormatBuilder() {
+		return new DynamicOutputFormatBuilder();
+	}
+
+	/**
+	 * Builder for {@link JdbcRowDataOutputFormat}.
+	 */
+	public static class DynamicOutputFormatBuilder {
+		private JdbcOptions jdbcOptions;
+		private JdbcExecutionOptions executionOptions;
+		private JdbcDmlOptions dmlOptions;
+		private TypeInformation<RowData> rowDataTypeInformation;
+		private DataType[] fieldDataTypes;
+
+		private DynamicOutputFormatBuilder() {
+		}
+
+		public DynamicOutputFormatBuilder setJdbcOptions(JdbcOptions jdbcOptions) {
+			this.jdbcOptions = jdbcOptions;
+			return this;
+		}
+
+		public DynamicOutputFormatBuilder setJdbcExecutionOptions(JdbcExecutionOptions executionOptions) {
+			this.executionOptions = executionOptions;
+			return this;
+		}
+
+		public DynamicOutputFormatBuilder setJdbcDmlOptions(JdbcDmlOptions dmlOptions) {
+			this.dmlOptions = dmlOptions;
+			return this;
+		}
+
+		public DynamicOutputFormatBuilder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {
+			this.rowDataTypeInformation = rowDataTypeInfo;
+			return this;
+		}
+
+		public DynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {
+			this.fieldDataTypes = fieldDataTypes;
+			return this;
+		}
+
+		public JdbcRowDataOutputFormat build() {
+			checkNotNull(jdbcOptions, "jdbc options can not be null");
+			checkNotNull(dmlOptions, "jdbc dml options can not be null");
+			checkNotNull(executionOptions, "jdbc execution options can not be null");
+
+			final LogicalType[] logicalTypes = Arrays.stream(fieldDataTypes)
+				.map(DataType::getLogicalType)
+				.toArray(LogicalType[]::new);
+			if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
+				//upsert query
+				return new JdbcRowDataOutputFormat(
+					new SimpleJdbcConnectionProvider(jdbcOptions),
+					dmlOptions,
+					executionOptions,
+					rowDataTypeInformation,
+					logicalTypes);
+			} else {
+				// append only query
+				final String sql = dmlOptions
+					.getDialect()
+					.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
+				return new JdbcRowDataOutputFormat(
+					new SimpleJdbcConnectionProvider(jdbcOptions),
+					dmlOptions,
+					executionOptions,
+					rowDataTypeInformation,
+					logicalTypes,
+					sql);
+			}
+		}
+	}
+}
+
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
index 1135991653466..a599f2fb71a6f 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcTableSource.java
@@ -36,7 +36,6 @@
 import org.apache.flink.table.sources.StreamTableSource;
 import org.apache.flink.table.sources.TableSource;
 import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.utils.TableConnectorUtils;
 import org.apache.flink.types.Row;
 
@@ -54,21 +53,21 @@ public class JdbcTableSource implements
 		ProjectableTableSource<Row>,
 		LookupableTableSource<Row> {
 
-	protected final JdbcOptions options;
-	protected final JdbcReadOptions readOptions;
-	protected final JdbcLookupOptions lookupOptions;
-	protected final TableSchema schema;
+	private final JdbcOptions options;
+	private final JdbcReadOptions readOptions;
+	private final JdbcLookupOptions lookupOptions;
+	private final TableSchema schema;
 
 	// index of fields selected, null means that all fields are selected
 	private final int[] selectFields;
 	private final DataType producedDataType;
 
-	protected JdbcTableSource(
+	private JdbcTableSource(
 		JdbcOptions options, JdbcReadOptions readOptions, JdbcLookupOptions lookupOptions, TableSchema schema) {
 		this(options, readOptions, lookupOptions, schema, null);
 	}
 
-	protected JdbcTableSource(
+	private JdbcTableSource(
 		JdbcOptions options, JdbcReadOptions readOptions, JdbcLookupOptions lookupOptions,
 		TableSchema schema, int[] selectFields) {
 		this.options = options;
@@ -182,7 +181,6 @@ private JdbcInputFormat getInputFormat() {
 				" BETWEEN ? AND ?";
 		}
 		builder.setQuery(query);
-		builder.setRowConverter(dialect.getRowConverter((RowType) producedDataType.getLogicalType()));
 
 		return builder.finish();
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcUpsertTableSink.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcUpsertTableSink.java
index d6b8531a94d90..51e2766b07afa 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcUpsertTableSink.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcUpsertTableSink.java
@@ -59,7 +59,7 @@ public class JdbcUpsertTableSink implements UpsertStreamTableSink<Row> {
 	private String[] keyFields;
 	private boolean isAppendOnly;
 
-	protected JdbcUpsertTableSink(
+	private JdbcUpsertTableSink(
 			TableSchema schema,
 			JdbcOptions options,
 			int flushMaxSize,
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/utils/JdbcTypeUtil.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/utils/JdbcTypeUtil.java
index a4313eceed5fd..1a45b3ee7e2cd 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/utils/JdbcTypeUtil.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/utils/JdbcTypeUtil.java
@@ -19,6 +19,7 @@
 package org.apache.flink.connector.jdbc.utils;
 
 import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeinfo.LocalTimeTypeInfo;
 import org.apache.flink.api.common.typeinfo.PrimitiveArrayTypeInfo;
 import org.apache.flink.api.common.typeinfo.SqlTimeTypeInfo;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
@@ -65,6 +66,9 @@ public class JdbcTypeUtil {
 		m.put(SqlTimeTypeInfo.DATE, Types.DATE);
 		m.put(SqlTimeTypeInfo.TIME, Types.TIME);
 		m.put(SqlTimeTypeInfo.TIMESTAMP, Types.TIMESTAMP);
+		m.put(LocalTimeTypeInfo.LOCAL_DATE, Types.DATE);
+		m.put(LocalTimeTypeInfo.LOCAL_TIME, Types.TIME);
+		m.put(LocalTimeTypeInfo.LOCAL_DATE_TIME, Types.TIMESTAMP);
 		m.put(BIG_DEC_TYPE_INFO, Types.DECIMAL);
 		m.put(BYTE_PRIMITIVE_ARRAY_TYPE_INFO, Types.BINARY);
 		TYPE_MAPPING = Collections.unmodifiableMap(m);
diff --git a/flink-connectors/flink-connector-jdbc/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory b/flink-connectors/flink-connector-jdbc/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory
new file mode 100644
index 0000000000000..9fb9fe0eeede8
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/resources/META-INF/services/org.apache.flink.table.factories.Factory
@@ -0,0 +1,16 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java
index a3f44d81ccb0b..73c2d1a6cea80 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/api/java/io/jdbc/JDBCInputFormatTest.java
@@ -22,7 +22,6 @@
 import org.apache.flink.api.java.io.jdbc.split.NumericBetweenParametersProvider;
 import org.apache.flink.api.java.io.jdbc.split.ParameterValuesProvider;
 import org.apache.flink.connector.jdbc.JdbcDataTestBase;
-import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
 import org.apache.flink.core.io.InputSplit;
 import org.apache.flink.types.Row;
 
@@ -37,7 +36,6 @@
 import java.sql.SQLException;
 
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
-import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE_INFO;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR;
@@ -133,8 +131,6 @@ public void testValidFetchSizeIntegerMin() {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(Integer.MIN_VALUE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 	}
 
@@ -145,8 +141,6 @@ public void testDefaultFetchSizeIsUsedIfNotConfiguredOtherwise() throws SQLExcep
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 
@@ -165,8 +159,6 @@ public void testFetchSizeCanBeConfigured() throws SQLException {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(desiredFetchSize)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 		Assert.assertEquals(desiredFetchSize, jdbcInputFormat.getStatement().getFetchSize());
@@ -180,8 +172,6 @@ public void testDefaultAutoCommitIsUsedIfNotConfiguredOtherwise() throws SQLExce
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 
@@ -202,8 +192,6 @@ public void testAutoCommitCanBeConfigured() throws SQLException {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setAutoCommit(desiredAutoCommit)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -219,8 +207,6 @@ public void testJDBCInputFormatWithoutParallelism() throws IOException {
 				.setQuery(SELECT_ALL_BOOKS)
 				.setRowTypeInfo(ROW_TYPE_INFO)
 				.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-				.setRowConverter(
-					JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 				.finish();
 		//this query does not exploit parallelism
 		Assert.assertEquals(1, jdbcInputFormat.createInputSplits(1).length);
@@ -253,8 +239,6 @@ public void testJDBCInputFormatWithParallelismAndNumericColumnSplitting() throws
 				.setRowTypeInfo(ROW_TYPE_INFO)
 				.setParametersProvider(pramProvider)
 				.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-				.setRowConverter(
-					JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 				.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -291,8 +275,6 @@ public void testJDBCInputFormatWithoutParallelismAndNumericColumnSplitting() thr
 				.setRowTypeInfo(ROW_TYPE_INFO)
 				.setParametersProvider(pramProvider)
 				.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-				.setRowConverter(
-					JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 				.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -329,8 +311,6 @@ public void testJDBCInputFormatWithParallelismAndGenericSplitting() throws IOExc
 				.setRowTypeInfo(ROW_TYPE_INFO)
 				.setParametersProvider(paramProvider)
 				.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-				.setRowConverter(
-					JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 				.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -370,8 +350,6 @@ public void testEmptyResults() throws IOException {
 				.setQuery(SELECT_EMPTY)
 				.setRowTypeInfo(ROW_TYPE_INFO)
 				.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-				.setRowConverter(
-					JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 				.finish();
 		try {
 			jdbcInputFormat.openInputFormat();
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTestBase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTestBase.java
index 444c4f1b83808..f47fd3570cbff 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTestBase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTestBase.java
@@ -20,6 +20,9 @@
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.functions.RuntimeContext;
 import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.StringData;
 import org.apache.flink.types.Row;
 
 import org.junit.Before;
@@ -54,11 +57,24 @@ public static Row toRow(JdbcTestFixture.TestEntry entry) {
 		return row;
 	}
 
-	public static void setRuntimeContext(JdbcBatchingOutputFormat format) {
+	// utils function to build a RowData, note: only support primitive type and String from now
+	public static RowData buildGenericData(Object... args) {
+		GenericRowData row = new GenericRowData(args.length);
+		for (int i = 0; i < args.length; i++) {
+			if (args[i] instanceof String) {
+				row.setField(i, StringData.fromString((String) args[i]));
+			} else {
+				row.setField(i, args[i]);
+			}
+		}
+		return row;
+	}
+
+	public static void setRuntimeContext(JdbcBatchingOutputFormat format, Boolean reused) {
 		RuntimeContext context = Mockito.mock(RuntimeContext.class);
 		ExecutionConfig config = Mockito.mock(ExecutionConfig.class);
 		doReturn(config).when(context).getExecutionConfig();
-		doReturn(true).when(config).isObjectReuseEnabled();
+		doReturn(reused).when(config).isObjectReuseEnabled();
 		format.setRuntimeContext(context);
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
index 197cf18d07ee9..f5654b4865d74 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
@@ -103,26 +103,26 @@ public static List<TestItem> testData() {
 			createTestItem("postgresql", "ARRAY<INTEGER>"),
 
 			// Unsupported types throws errors.
-			createTestItem("derby", "BINARY", "The derby dialect doesn't support type: BINARY(1)."),
-			createTestItem("derby", "VARBINARY(10)", "The derby dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("derby", "BINARY", "The Derby dialect doesn't support type: BINARY(1)."),
+			createTestItem("derby", "VARBINARY(10)", "The Derby dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("derby", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The derby dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+					"The Derby dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
 			createTestItem("derby", "DECIMAL(38, 18)",
-					"The precision of field 'f0' is out of the DECIMAL precision range [1, 31] supported by derby dialect."),
+					"The precision of field 'f0' is out of the DECIMAL precision range [1, 31] supported by Derby dialect."),
 
-			createTestItem("mysql", "BINARY", "The mysql dialect doesn't support type: BINARY(1)."),
-			createTestItem("mysql", "VARBINARY(10)", "The mysql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("mysql", "BINARY", "The MySQL dialect doesn't support type: BINARY(1)."),
+			createTestItem("mysql", "VARBINARY(10)", "The MySQL dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("mysql", "TIMESTAMP(9) WITHOUT TIME ZONE",
-					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by mysql dialect."),
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by MySQL dialect."),
 			createTestItem("mysql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The mysql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+					"The MySQL dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
 
-			createTestItem("postgresql", "BINARY", "The postgresql dialect doesn't support type: BINARY(1)."),
-			createTestItem("postgresql", "VARBINARY(10)", "The postgresql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("postgresql", "BINARY", "The PostgreSQL dialect doesn't support type: BINARY(1)."),
+			createTestItem("postgresql", "VARBINARY(10)", "The PostgreSQL dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("postgresql", "TIMESTAMP(9) WITHOUT TIME ZONE",
-					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by postgresql dialect."),
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by PostgreSQL dialect."),
 			createTestItem("postgresql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The postgresql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE.")
+					"The PostgreSQL dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE.")
 		);
 	}
 
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcInputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcInputFormatTest.java
index e40f6c82371dc..9834c12314690 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcInputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcInputFormatTest.java
@@ -18,7 +18,6 @@
 
 package org.apache.flink.connector.jdbc;
 
-import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
 import org.apache.flink.connector.jdbc.split.JdbcGenericParameterValuesProvider;
 import org.apache.flink.connector.jdbc.split.JdbcNumericBetweenParametersProvider;
 import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
@@ -36,7 +35,6 @@
 import java.sql.SQLException;
 
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
-import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE_INFO;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR;
@@ -132,8 +130,6 @@ public void testValidFetchSizeIntegerMin() {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(Integer.MIN_VALUE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 	}
 
@@ -144,8 +140,6 @@ public void testDefaultFetchSizeIsUsedIfNotConfiguredOtherwise() throws SQLExcep
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 
@@ -164,8 +158,6 @@ public void testFetchSizeCanBeConfigured() throws SQLException {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(desiredFetchSize)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 		Assert.assertEquals(desiredFetchSize, jdbcInputFormat.getStatement().getFetchSize());
@@ -179,8 +171,6 @@ public void testDefaultAutoCommitIsUsedIfNotConfiguredOtherwise() throws SQLExce
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		jdbcInputFormat.openInputFormat();
 
@@ -201,8 +191,6 @@ public void testAutoCommitCanBeConfigured() throws SQLException {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setAutoCommit(desiredAutoCommit)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -218,8 +206,6 @@ public void testJdbcInputFormatWithoutParallelism() throws IOException {
 			.setQuery(SELECT_ALL_BOOKS)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		//this query does not exploit parallelism
 		Assert.assertEquals(1, jdbcInputFormat.createInputSplits(1).length);
@@ -252,8 +238,6 @@ public void testJdbcInputFormatWithParallelismAndNumericColumnSplitting() throws
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(pramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -290,8 +274,6 @@ public void testJdbcInputFormatWithoutParallelismAndNumericColumnSplitting() thr
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(pramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -328,8 +310,6 @@ public void testJdbcInputFormatWithParallelismAndGenericSplitting() throws IOExc
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(paramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 
 		jdbcInputFormat.openInputFormat();
@@ -369,8 +349,6 @@ public void testEmptyResults() throws IOException {
 			.setQuery(SELECT_EMPTY)
 			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(
-				JdbcDialects.get(DERBY_EBOOKSHOP_DB.getUrl()).get().getRowConverter(ROW_TYPE))
 			.finish();
 		try {
 			jdbcInputFormat.openInputFormat();
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcOutputFormatTest.java
index d59026db93474..519b35756ccb0 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcOutputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcOutputFormatTest.java
@@ -104,7 +104,7 @@ public void testInvalidQuery() {
 				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 				.setQuery("iamnotsql")
 				.finish();
-			setRuntimeContext(jdbcOutputFormat);
+			setRuntimeContext(jdbcOutputFormat, true);
 			jdbcOutputFormat.open(0, 1);
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, IOException.class).isPresent());
@@ -135,7 +135,7 @@ public void testIncompatibleTypes() {
 				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 				.setQuery(String.format(INSERT_TEMPLATE, INPUT_TABLE))
 				.finish();
-			setRuntimeContext(jdbcOutputFormat);
+			setRuntimeContext(jdbcOutputFormat, true);
 			jdbcOutputFormat.open(0, 1);
 
 			Row row = new Row(5);
@@ -168,7 +168,7 @@ public void testExceptionOnInvalidType() {
 					Types.DOUBLE,
 					Types.INTEGER})
 				.finish();
-			setRuntimeContext(jdbcOutputFormat);
+			setRuntimeContext(jdbcOutputFormat, true);
 			jdbcOutputFormat.open(0, 1);
 
 			TestEntry entry = TEST_DATA[0];
@@ -201,7 +201,7 @@ public void testExceptionOnClose() {
 					Types.DOUBLE,
 					Types.INTEGER})
 				.finish();
-			setRuntimeContext(jdbcOutputFormat);
+			setRuntimeContext(jdbcOutputFormat, true);
 			jdbcOutputFormat.open(0, 1);
 
 			TestEntry entry = TEST_DATA[0];
@@ -228,7 +228,7 @@ public void testJdbcOutputFormat() throws IOException, SQLException {
 				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 				.setQuery(String.format(INSERT_TEMPLATE, OUTPUT_TABLE))
 				.finish();
-		setRuntimeContext(jdbcOutputFormat);
+		setRuntimeContext(jdbcOutputFormat, true);
 		jdbcOutputFormat.open(0, 1);
 
 		for (TestEntry entry : TEST_DATA) {
@@ -264,7 +264,7 @@ public void testFlush() throws SQLException, IOException {
 			.setQuery(String.format(INSERT_TEMPLATE, OUTPUT_TABLE_2))
 			.setBatchSize(3)
 			.finish();
-		setRuntimeContext(jdbcOutputFormat);
+		setRuntimeContext(jdbcOutputFormat, true);
 		try (
 			Connection dbConn = DriverManager.getConnection(DERBY_EBOOKSHOP_DB.getUrl());
 			PreparedStatement statement = dbConn.prepareStatement(SELECT_ALL_NEWBOOKS_2)
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java
index e0d1e163b6a62..f2aec19e3b6c5 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/internal/JdbcFullTest.java
@@ -28,7 +28,6 @@
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
 import org.apache.flink.connector.jdbc.JdbcInputFormat;
 import org.apache.flink.connector.jdbc.JdbcStatementBuilder;
-import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
 import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
 import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
@@ -50,7 +49,6 @@
 
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.INSERT_TEMPLATE;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.OUTPUT_TABLE;
-import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE_INFO;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_ID;
@@ -112,8 +110,7 @@ private void runTest(boolean exploitParallelism) throws Exception {
 				.setDrivername(getDbMetadata().getDriverClass())
 				.setDBUrl(getDbMetadata().getUrl())
 				.setQuery(SELECT_ALL_BOOKS)
-				.setRowTypeInfo(ROW_TYPE_INFO)
-				.setRowConverter(JdbcDialects.get(getDbMetadata().getUrl()).get().getRowConverter(ROW_TYPE));
+				.setRowTypeInfo(ROW_TYPE_INFO);
 
 		if (exploitParallelism) {
 			final int fetchSize = 1;
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
new file mode 100644
index 0000000000000..a2f7f77f21bbf
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
@@ -0,0 +1,276 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.api.java.tuple.Tuple4;
+import org.apache.flink.connector.jdbc.JdbcTestFixture;
+import org.apache.flink.streaming.api.TimeCharacteristic;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
+import org.apache.flink.table.api.EnvironmentSettings;
+import org.apache.flink.table.api.Table;
+import org.apache.flink.table.api.TableEnvironment;
+import org.apache.flink.table.api.TableResult;
+import org.apache.flink.table.api.java.StreamTableEnvironment;
+import org.apache.flink.test.util.AbstractTestBase;
+import org.apache.flink.types.Row;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.sql.Timestamp;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
+import static org.apache.flink.connector.jdbc.internal.JdbcTableOutputFormatTest.check;
+import static org.apache.flink.table.api.Expressions.$;
+
+/**
+ * The ITCase for {@link JdbcDynamicTableSink}.
+ */
+public class JdbcDynamicTableSinkITCase extends AbstractTestBase {
+
+	public static final String DB_URL = "jdbc:derby:memory:upsert";
+	public static final String OUTPUT_TABLE1 = "dynamicSinkForUpsert";
+	public static final String OUTPUT_TABLE2 = "dynamicSinkForAppend";
+	public static final String OUTPUT_TABLE3 = "dynamicSinkForBatch";
+	public static final String OUTPUT_TABLE4 = "REAL_TABLE";
+
+	@Before
+	public void before() throws ClassNotFoundException, SQLException {
+		System.setProperty("derby.stream.error.field", JdbcTestFixture.class.getCanonicalName() + ".DEV_NULL");
+
+		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL + ";create=true");
+			Statement stat = conn.createStatement()) {
+			stat.executeUpdate("CREATE TABLE " + OUTPUT_TABLE1 + " (" +
+				"cnt BIGINT NOT NULL DEFAULT 0," +
+				"lencnt BIGINT NOT NULL DEFAULT 0," +
+				"cTag INT NOT NULL DEFAULT 0," +
+				"ts TIMESTAMP," +
+				"PRIMARY KEY (cnt, cTag))");
+
+			stat.executeUpdate("CREATE TABLE " + OUTPUT_TABLE2 + " (" +
+				"id INT NOT NULL DEFAULT 0," +
+				"num BIGINT NOT NULL DEFAULT 0," +
+				"ts TIMESTAMP)");
+
+			stat.executeUpdate("CREATE TABLE " + OUTPUT_TABLE3 + " (" +
+				"NAME VARCHAR(20) NOT NULL," +
+				"SCORE BIGINT NOT NULL DEFAULT 0)");
+
+			stat.executeUpdate("CREATE TABLE " + OUTPUT_TABLE4 + " (real_data REAL)");
+		}
+	}
+
+	@After
+	public void clearOutputTable() throws Exception {
+		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL);
+			Statement stat = conn.createStatement()) {
+			stat.execute("DROP TABLE " + OUTPUT_TABLE1);
+			stat.execute("DROP TABLE " + OUTPUT_TABLE2);
+			stat.execute("DROP TABLE " + OUTPUT_TABLE3);
+			stat.execute("DROP TABLE " + OUTPUT_TABLE4);
+		}
+	}
+
+	public static DataStream<Tuple4<Integer, Long, String, Timestamp>> get4TupleDataStream(StreamExecutionEnvironment env) {
+		List<Tuple4<Integer, Long, String, Timestamp>> data = new ArrayList<>();
+		data.add(new Tuple4<>(1, 1L, "Hi", Timestamp.valueOf("1970-01-01 00:00:00.001")));
+		data.add(new Tuple4<>(2, 2L, "Hello", Timestamp.valueOf("1970-01-01 00:00:00.002")));
+		data.add(new Tuple4<>(3, 2L, "Hello world", Timestamp.valueOf("1970-01-01 00:00:00.003")));
+		data.add(new Tuple4<>(4, 3L, "Hello world, how are you?", Timestamp.valueOf("1970-01-01 00:00:00.004")));
+		data.add(new Tuple4<>(5, 3L, "I am fine.", Timestamp.valueOf("1970-01-01 00:00:00.005")));
+		data.add(new Tuple4<>(6, 3L, "Luke Skywalker", Timestamp.valueOf("1970-01-01 00:00:00.006")));
+		data.add(new Tuple4<>(7, 4L, "Comment#1", Timestamp.valueOf("1970-01-01 00:00:00.007")));
+		data.add(new Tuple4<>(8, 4L, "Comment#2", Timestamp.valueOf("1970-01-01 00:00:00.008")));
+		data.add(new Tuple4<>(9, 4L, "Comment#3", Timestamp.valueOf("1970-01-01 00:00:00.009")));
+		data.add(new Tuple4<>(10, 4L, "Comment#4", Timestamp.valueOf("1970-01-01 00:00:00.010")));
+		data.add(new Tuple4<>(11, 5L, "Comment#5", Timestamp.valueOf("1970-01-01 00:00:00.011")));
+		data.add(new Tuple4<>(12, 5L, "Comment#6", Timestamp.valueOf("1970-01-01 00:00:00.012")));
+		data.add(new Tuple4<>(13, 5L, "Comment#7", Timestamp.valueOf("1970-01-01 00:00:00.013")));
+		data.add(new Tuple4<>(14, 5L, "Comment#8", Timestamp.valueOf("1970-01-01 00:00:00.014")));
+		data.add(new Tuple4<>(15, 5L, "Comment#9", Timestamp.valueOf("1970-01-01 00:00:00.015")));
+		data.add(new Tuple4<>(16, 6L, "Comment#10", Timestamp.valueOf("1970-01-01 00:00:00.016")));
+		data.add(new Tuple4<>(17, 6L, "Comment#11", Timestamp.valueOf("1970-01-01 00:00:00.017")));
+		data.add(new Tuple4<>(18, 6L, "Comment#12", Timestamp.valueOf("1970-01-01 00:00:00.018")));
+		data.add(new Tuple4<>(19, 6L, "Comment#13", Timestamp.valueOf("1970-01-01 00:00:00.019")));
+		data.add(new Tuple4<>(20, 6L, "Comment#14", Timestamp.valueOf("1970-01-01 00:00:00.020")));
+		data.add(new Tuple4<>(21, 6L, "Comment#15", Timestamp.valueOf("1970-01-01 00:00:00.021")));
+
+		Collections.shuffle(data);
+		return env.fromCollection(data);
+	}
+
+	@Test
+	public void testReal() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.getConfig().enableObjectReuse();
+		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner()
+			.inStreamingMode()
+			.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.executeSql(
+			"CREATE TABLE upsertSink (" +
+				"  real_data float" +
+				") WITH (" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + OUTPUT_TABLE4 + "'" +
+				")");
+
+		TableResult tableResult = tEnv.executeSql("INSERT INTO upsertSink SELECT CAST(1.0 as FLOAT)");
+		// wait to finish
+		tableResult.getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();
+		check(new Row[] {Row.of(1.0f)}, DB_URL, "REAL_TABLE", new String[]{"real_data"});
+	}
+
+	@Test
+	public void testUpsert() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.getConfig().enableObjectReuse();
+		env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner()
+			.inStreamingMode()
+			.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		Table t = tEnv.fromDataStream(get4TupleDataStream(env).assignTimestampsAndWatermarks(
+			new AscendingTimestampExtractor<Tuple4<Integer, Long, String, Timestamp>>() {
+				@Override
+				public long extractAscendingTimestamp(Tuple4<Integer, Long, String, Timestamp> element) {
+					return element.f0;
+				}
+			}), $("id"), $("num"), $("text"), $("ts"));
+
+		tEnv.createTemporaryView("T", t);
+		tEnv.executeSql(
+			"CREATE TABLE upsertSink (" +
+				"  cnt BIGINT," +
+				"  lencnt BIGINT," +
+				"  cTag INT," +
+				"  ts TIMESTAMP(3)," +
+				"  PRIMARY KEY (cnt, cTag) NOT ENFORCED" +
+				") WITH (" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + OUTPUT_TABLE1 + "'" +
+				")");
+
+		TableResult tableResult = tEnv.executeSql("INSERT INTO upsertSink \n" +
+			"SELECT cnt, COUNT(len) AS lencnt, cTag, MAX(ts) AS ts\n" +
+			"FROM (\n" +
+			"  SELECT len, COUNT(id) as cnt, cTag, MAX(ts) AS ts\n" +
+			"  FROM (SELECT id, CHAR_LENGTH(text) AS len, (CASE WHEN id > 0 THEN 1 ELSE 0 END) cTag, ts FROM T)\n" +
+			"  GROUP BY len, cTag\n" +
+			")\n" +
+			"GROUP BY cnt, cTag");
+		// wait to finish
+		tableResult.getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();
+		check(new Row[] {
+			Row.of(1, 5, 1, Timestamp.valueOf("1970-01-01 00:00:00.006")),
+			Row.of(7, 1, 1, Timestamp.valueOf("1970-01-01 00:00:00.021")),
+			Row.of(9, 1, 1, Timestamp.valueOf("1970-01-01 00:00:00.015"))
+		}, DB_URL, OUTPUT_TABLE1, new String[]{"cnt", "lencnt", "cTag", "ts"});
+	}
+
+	@Test
+	public void testAppend() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.getConfig().enableObjectReuse();
+		env.getConfig().setParallelism(1);
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
+
+		Table t = tEnv.fromDataStream(get4TupleDataStream(env), $("id"), $("num"), $("text"), $("ts"));
+
+		tEnv.registerTable("T", t);
+
+		tEnv.executeSql(
+			"CREATE TABLE upsertSink (" +
+				"  id INT," +
+				"  num BIGINT," +
+				"  ts TIMESTAMP(3)" +
+				") WITH (" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + OUTPUT_TABLE2 + "'" +
+				")");
+
+		TableResult tableResult = tEnv.executeSql(
+			"INSERT INTO upsertSink SELECT id, num, ts FROM T WHERE id IN (2, 10, 20)");
+		// wait to finish
+		tableResult.getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();
+		check(new Row[] {
+			Row.of(2, 2, Timestamp.valueOf("1970-01-01 00:00:00.002")),
+			Row.of(10, 4, Timestamp.valueOf("1970-01-01 00:00:00.01")),
+			Row.of(20, 6, Timestamp.valueOf("1970-01-01 00:00:00.02"))
+		}, DB_URL, OUTPUT_TABLE2, new String[]{"id", "num", "ts"});
+	}
+
+	@Test
+	public void testBatchSink() throws Exception {
+		EnvironmentSettings bsSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner().inBatchMode().build();
+		TableEnvironment tEnv = TableEnvironment.create(bsSettings);
+
+		tEnv.executeSql(
+			"CREATE TABLE USER_RESULT(" +
+				"NAME VARCHAR," +
+				"SCORE BIGINT" +
+				") WITH ( " +
+				"'connector' = 'jdbc'," +
+				"'url'='" + DB_URL + "'," +
+				"'table-name' = '" + OUTPUT_TABLE3 + "'," +
+				"'sink.buffer-flush.max-rows' = '2'," +
+				"'sink.buffer-flush.interval' = '3'," +
+				"'sink.max-retries' = '4'" +
+				")");
+
+		TableResult tableResult  = tEnv.executeSql("INSERT INTO USER_RESULT\n" +
+			"SELECT user_name, score " +
+			"FROM (VALUES (1, 'Bob'), (22, 'Tom'), (42, 'Kim'), " +
+			"(42, 'Kim'), (1, 'Bob')) " +
+			"AS UserCountTable(score, user_name)");
+		// wait to finish
+		tableResult.getJobClient().get().getJobExecutionResult(Thread.currentThread().getContextClassLoader()).get();
+
+		check(new Row[] {
+			Row.of("Bob", 1),
+			Row.of("Tom", 22),
+			Row.of("Kim", 42),
+			Row.of("Kim", 42),
+			Row.of("Bob", 1)
+		}, DB_URL, OUTPUT_TABLE3, new String[]{"NAME", "SCORE"});
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
new file mode 100644
index 0000000000000..6f93307aaac0d
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
@@ -0,0 +1,161 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.connector.jdbc.JdbcTestBase;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.table.api.EnvironmentSettings;
+import org.apache.flink.table.api.java.StreamTableEnvironment;
+import org.apache.flink.table.runtime.utils.StreamITCase;
+import org.apache.flink.test.util.AbstractTestBase;
+import org.apache.flink.types.Row;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.Arrays;
+import java.util.List;
+
+/**
+ * ITCase for {@link JdbcDynamicTableSource}.
+ */
+public class JdbcDynamicTableSourceITCase extends AbstractTestBase {
+
+	public static final String DRIVER_CLASS = "org.apache.derby.jdbc.EmbeddedDriver";
+	public static final String DB_URL = "jdbc:derby:memory:test";
+	public static final String INPUT_TABLE = "jdbDynamicTableSource";
+
+	@Before
+	public void before() throws ClassNotFoundException, SQLException {
+		System.setProperty("derby.stream.error.field", JdbcTestBase.class.getCanonicalName() + ".DEV_NULL");
+		Class.forName(DRIVER_CLASS);
+
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL + ";create=true");
+			Statement statement = conn.createStatement()) {
+			statement.executeUpdate("CREATE TABLE " + INPUT_TABLE + " (" +
+				"id BIGINT NOT NULL," +
+				"timestamp6_col TIMESTAMP, " +
+				"timestamp9_col TIMESTAMP, " +
+				"time_col TIME, " +
+				"real_col FLOAT(23), " +    // A precision of 23 or less makes FLOAT equivalent to REAL.
+				"double_col FLOAT(24)," +   // A precision of 24 or greater makes FLOAT equivalent to DOUBLE PRECISION.
+				"decimal_col DECIMAL(10, 4))");
+			statement.executeUpdate("INSERT INTO " + INPUT_TABLE + " VALUES (" +
+				"1, TIMESTAMP('2020-01-01 15:35:00.123456'), TIMESTAMP('2020-01-01 15:35:00.123456789'), " +
+				"TIME('15:35:00'), 1.175E-37, 1.79769E+308, 100.1234)");
+			statement.executeUpdate("INSERT INTO " + INPUT_TABLE + " VALUES (" +
+				"2, TIMESTAMP('2020-01-01 15:36:01.123456'), TIMESTAMP('2020-01-01 15:36:01.123456789'), " +
+				"TIME('15:36:01'), -1.175E-37, -1.79769E+308, 101.1234)");
+		}
+	}
+
+	@After
+	public void clearOutputTable() throws Exception {
+		Class.forName(DRIVER_CLASS);
+		try (
+			Connection conn = DriverManager.getConnection(DB_URL);
+			Statement stat = conn.createStatement()) {
+			stat.executeUpdate("DROP TABLE " + INPUT_TABLE);
+		}
+	}
+
+	@Test
+	public void testJdbcSource() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner()
+			.inStreamingMode()
+			.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.executeSql(
+			"CREATE TABLE " + INPUT_TABLE + "(" +
+				"id BIGINT," +
+				"timestamp6_col TIMESTAMP(6)," +
+				"timestamp9_col TIMESTAMP(9)," +
+				"time_col TIME," +
+				"real_col FLOAT," +
+				"double_col DOUBLE," +
+				"decimal_col DECIMAL(10, 4)" +
+				") WITH (" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + INPUT_TABLE + "'" +
+				")"
+		);
+
+		StreamITCase.clear();
+		tEnv.toAppendStream(tEnv.sqlQuery("SELECT * FROM " + INPUT_TABLE), Row.class)
+			.addSink(new StreamITCase.StringSink<>());
+		env.execute();
+
+		List<String> expected =
+			Arrays.asList(
+				"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234",
+				"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234");
+		StreamITCase.compareWithList(expected);
+	}
+
+	@Test
+	public void testProject() throws Exception {
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
+			.useBlinkPlanner()
+			.inStreamingMode()
+			.build();
+		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
+
+		tEnv.executeSql(
+			"CREATE TABLE " + INPUT_TABLE + "(" +
+				"id BIGINT," +
+				"timestamp6_col TIMESTAMP(6)," +
+				"timestamp9_col TIMESTAMP(9)," +
+				"time_col TIME," +
+				"real_col FLOAT," +
+				"double_col DOUBLE," +
+				"decimal_col DECIMAL(10, 4)" +
+				") WITH (" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + INPUT_TABLE + "'," +
+				"  'scan.partition.column'='id'," +
+				"  'scan.partition.num'='2'," +
+				"  'scan.partition.lower-bound'='0'," +
+				"  'scan.partition.upper-bound'='100'" +
+				")"
+		);
+
+		StreamITCase.clear();
+		tEnv.toAppendStream(tEnv.sqlQuery("SELECT id,timestamp6_col,decimal_col FROM " + INPUT_TABLE), Row.class)
+			.addSink(new StreamITCase.StringSink<>());
+		env.execute();
+
+		List<String> expected =
+			Arrays.asList(
+				"1,2020-01-01T15:35:00.123456,100.1234",
+				"2,2020-01-01T15:36:01.123456,101.1234");
+		StreamITCase.compareWithList(expected);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
index 28d53c2d5bf04..8d40cdd8889e0 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
@@ -60,15 +60,22 @@ public class JdbcLookupFunctionITCase extends AbstractTestBase {
 	public static final String DB_URL = "jdbc:derby:memory:lookup";
 	public static final String LOOKUP_TABLE = "lookup_table";
 
+	private final String tableFactory;
 	private final boolean useCache;
 
-	public JdbcLookupFunctionITCase(boolean useCache) {
+	public JdbcLookupFunctionITCase(String tableFactory, boolean useCache) {
 		this.useCache = useCache;
+		this.tableFactory = tableFactory;
 	}
 
-	@Parameterized.Parameters(name = "Table config = {0}")
-	public static Collection<Boolean> parameters() {
-		return Arrays.asList(true, false);
+	@Parameterized.Parameters(name = "Table factory = {0}, use cache {1}")
+	@SuppressWarnings("unchecked,rawtypes")
+	public static Collection<Object[]>  useCache() {
+		return Arrays.asList(new Object[][]{
+			{"legacyFactory", true},
+			{"legacyFactory", false},
+			{"dynamicFactory", true},
+			{"dynamicFactory", false}});
 	}
 
 	@Before
@@ -81,19 +88,19 @@ public void before() throws ClassNotFoundException, SQLException {
 			Statement stat = conn.createStatement()) {
 			stat.executeUpdate("CREATE TABLE " + LOOKUP_TABLE + " (" +
 					"id1 INT NOT NULL DEFAULT 0," +
-					"id2 INT NOT NULL DEFAULT 0," +
+					"id2 VARCHAR(20) NOT NULL," +
 					"comment1 VARCHAR(1000)," +
 					"comment2 VARCHAR(1000))");
 
 			Object[][] data = new Object[][] {
-					new Object[] {1, 1, "11-c1-v1", "11-c2-v1"},
-					new Object[] {1, 1, "11-c1-v2", "11-c2-v2"},
-					new Object[] {2, 3, null, "23-c2"},
-					new Object[] {2, 5, "25-c1", "25-c2"},
-					new Object[] {3, 8, "38-c1", "38-c2"}
+					new Object[] {1, "1", "11-c1-v1", "11-c2-v1"},
+					new Object[] {1, "1", "11-c1-v2", "11-c2-v2"},
+					new Object[] {2, "3", null, "23-c2"},
+					new Object[] {2, "5", "25-c1", "25-c2"},
+					new Object[] {3, "8", "38-c1", "38-c2"}
 			};
 			boolean[] surroundedByQuotes = new boolean[] {
-				false, false, true, true
+				false, true, true, true
 			};
 
 			StringBuilder sqlQueryBuilder = new StringBuilder(
@@ -141,50 +148,90 @@ public void test() throws Exception {
 		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
 		StreamITCase.clear();
 
+		if ("legacyFactory".equals(tableFactory)) {
+			useLegacyTableFactory(env, tEnv);
+		} else {
+			useDynamicTableFactory(env, tEnv);
+		}
+
+		List<String> expected = new ArrayList<>();
+		expected.add("1,1,11-c1-v1,11-c2-v1");
+		expected.add("1,1,11-c1-v1,11-c2-v1");
+		expected.add("1,1,11-c1-v2,11-c2-v2");
+		expected.add("1,1,11-c1-v2,11-c2-v2");
+		expected.add("2,3,null,23-c2");
+		expected.add("2,5,25-c1,25-c2");
+		expected.add("3,8,38-c1,38-c2");
+
+		StreamITCase.compareWithList(expected);
+	}
+
+	private void useLegacyTableFactory(StreamExecutionEnvironment env, StreamTableEnvironment tEnv) throws Exception {
 		Table t = tEnv.fromDataStream(env.fromCollection(Arrays.asList(
-					new Tuple2<>(1, 1),
-					new Tuple2<>(1, 1),
-					new Tuple2<>(2, 3),
-					new Tuple2<>(2, 5),
-					new Tuple2<>(3, 5),
-					new Tuple2<>(3, 8)
-				)), $("id1"), $("id2"));
+			new Tuple2<>(1, "1"),
+			new Tuple2<>(1, "1"),
+			new Tuple2<>(2, "3"),
+			new Tuple2<>(2, "5"),
+			new Tuple2<>(3, "5"),
+			new Tuple2<>(3, "8")
+		)), $("id1"), $("id2"));
 
 		tEnv.registerTable("T", t);
-
 		JdbcTableSource.Builder builder = JdbcTableSource.builder()
-				.setOptions(JdbcOptions.builder()
-						.setDBUrl(DB_URL)
-						.setTableName(LOOKUP_TABLE)
-						.build())
-				.setSchema(TableSchema.builder().fields(
-						new String[]{"id1", "id2", "comment1", "comment2"},
-						new DataType[]{DataTypes.INT(), DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING()})
-						.build());
+			.setOptions(JdbcOptions.builder()
+				.setDBUrl(DB_URL)
+				.setTableName(LOOKUP_TABLE)
+				.build())
+			.setSchema(TableSchema.builder().fields(
+				new String[]{"id1", "id2", "comment1", "comment2"},
+				new DataType[]{DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING(), DataTypes.STRING()})
+				.build());
 		if (useCache) {
 			builder.setLookupOptions(JdbcLookupOptions.builder()
-					.setCacheMaxSize(1000).setCacheExpireMs(1000 * 1000).build());
+				.setCacheMaxSize(1000).setCacheExpireMs(1000 * 1000).build());
 		}
 		tEnv.registerFunction("jdbcLookup",
-				builder.build().getLookupFunction(t.getSchema().getFieldNames()));
+			builder.build().getLookupFunction(t.getSchema().getFieldNames()));
 
 		String sqlQuery = "SELECT id1, id2, comment1, comment2 FROM T, " +
-				"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, l_id2, comment1, comment2)";
+			"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, l_id2, comment1, comment2)";
 		Table result = tEnv.sqlQuery(sqlQuery);
-
 		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
 		resultSet.addSink(new StreamITCase.StringSink<>());
 		env.execute();
+	}
 
-		List<String> expected = new ArrayList<>();
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("2,3,null,23-c2");
-		expected.add("2,5,25-c1,25-c2");
-		expected.add("3,8,38-c1,38-c2");
-
-		StreamITCase.compareWithList(expected);
+	private void useDynamicTableFactory(StreamExecutionEnvironment env, StreamTableEnvironment tEnv) throws Exception {
+		Table t = tEnv.fromDataStream(env.fromCollection(Arrays.asList(
+			new Tuple2<>(1, "1"),
+			new Tuple2<>(1, "1"),
+			new Tuple2<>(2, "3"),
+			new Tuple2<>(2, "5"),
+			new Tuple2<>(3, "5"),
+			new Tuple2<>(3, "8")
+		)), $("id1"), $("id2"), $("proctime").proctime());
+
+		tEnv.createTemporaryView("T", t);
+
+		String cacheConfig = ", 'lookup.cache.max-rows'='4', 'lookup.cache.ttl'='10000', 'lookup.max-retries'='5'";
+		tEnv.sqlUpdate(
+			String.format("create table lookup (" +
+				"  id1 INT," +
+				"  id2 VARCHAR," +
+				"  comment1 VARCHAR," +
+				"  comment2 VARCHAR" +
+				") with(" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + LOOKUP_TABLE + "'" +
+				"  %s)", useCache ? cacheConfig : ""));
+
+		String sqlQuery = "SELECT source.id1, source.id2, L.comment1, L.comment2 FROM T AS source " +
+			"JOIN lookup for system_time as of source.proctime AS L " +
+			"ON source.id1 = L.id1 and source.id2 = L.id2";
+		Table result = tEnv.sqlQuery(sqlQuery);
+		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
+		resultSet.addSink(new StreamITCase.StringSink<>());
+		env.execute();
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java
new file mode 100644
index 0000000000000..e72fdca37e436
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java
@@ -0,0 +1,326 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.connector.jdbc.JdbcDataTestBase;
+import org.apache.flink.connector.jdbc.JdbcTestFixture;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.connector.jdbc.split.JdbcGenericParameterValuesProvider;
+import org.apache.flink.connector.jdbc.split.JdbcNumericBetweenParametersProvider;
+import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
+import org.apache.flink.core.io.InputSplit;
+import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.IntType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+
+import org.junit.After;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.sql.ResultSet;
+import java.util.Arrays;
+
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.INPUT_TABLE;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_ID;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_EMPTY;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TEST_DATA;
+
+/**
+ * Test suite for {@link JdbcRowDataInputFormat}.
+ */
+public class JdbcRowDataInputFormatTest extends JdbcDataTestBase {
+
+	private JdbcRowDataInputFormat inputFormat;
+	private static String[] fieldNames = new String[]{"id", "title", "author", "price", "qty"};
+	private static DataType[] fieldDataTypes = new DataType[]{
+		DataTypes.INT(),
+		DataTypes.STRING(),
+		DataTypes.STRING(),
+		DataTypes.DOUBLE(),
+		DataTypes.INT()};
+	final JdbcDialect dialect = JdbcOptions.builder()
+		.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+		.setTableName(INPUT_TABLE)
+		.build()
+		.getDialect();
+	final RowType rowType = RowType.of(
+		Arrays.stream(fieldDataTypes)
+			.map(DataType::getLogicalType)
+			.toArray(LogicalType[]::new),
+		fieldNames);
+
+	@After
+	public void tearDown() throws IOException {
+		if (inputFormat != null) {
+			inputFormat.close();
+			inputFormat.closeInputFormat();
+		}
+		inputFormat = null;
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testUntypedRowInfo() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername("org.apache.derby.jdbc.idontexist")
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS)
+			.build();
+		inputFormat.openInputFormat();
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testInvalidDriver() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername("org.apache.derby.jdbc.idontexist")
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS)
+			.build();
+		inputFormat.openInputFormat();
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testInvalidURL() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl("jdbc:der:iamanerror:mory:ebookshop")
+			.setQuery(SELECT_ALL_BOOKS)
+			.build();
+		inputFormat.openInputFormat();
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testInvalidQuery() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery("iamnotsql")
+			.build();
+		inputFormat.openInputFormat();
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testIncompleteConfiguration() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setQuery(SELECT_ALL_BOOKS)
+			.build();
+	}
+
+	@Test(expected = IllegalArgumentException.class)
+	public void testInvalidFetchSize() {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS)
+			.setFetchSize(-7)
+			.build();
+	}
+
+	@Test
+	public void testValidFetchSizeIntegerMin() {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS)
+			.setFetchSize(Integer.MIN_VALUE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+	}
+
+	@Test
+	public void testJdbcInputFormatWithoutParallelism() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS)
+			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+		//this query does not exploit parallelism
+		Assert.assertEquals(1, inputFormat.createInputSplits(1).length);
+		inputFormat.openInputFormat();
+		inputFormat.open(null);
+		RowData row = new GenericRowData(5);
+		int recordCount = 0;
+		while (!inputFormat.reachedEnd()) {
+			RowData next = inputFormat.nextRecord(row);
+
+			assertEquals(TEST_DATA[recordCount], next);
+
+			recordCount++;
+		}
+		inputFormat.close();
+		inputFormat.closeInputFormat();
+		Assert.assertEquals(TEST_DATA.length, recordCount);
+	}
+
+	@Test
+	public void testJdbcInputFormatWithParallelismAndNumericColumnSplitting() throws IOException {
+		final int fetchSize = 1;
+		final long min = TEST_DATA[0].id;
+		final long max = TEST_DATA[TEST_DATA.length - fetchSize].id;
+		JdbcParameterValuesProvider pramProvider = new JdbcNumericBetweenParametersProvider(min, max).ofBatchSize(fetchSize);
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_ID)
+			.setParametersProvider(pramProvider)
+			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+
+		inputFormat.openInputFormat();
+		InputSplit[] splits = inputFormat.createInputSplits(1);
+		//this query exploit parallelism (1 split for every id)
+		Assert.assertEquals(TEST_DATA.length, splits.length);
+		int recordCount = 0;
+		RowData row = new GenericRowData(5);
+		for (InputSplit split : splits) {
+			inputFormat.open(split);
+			while (!inputFormat.reachedEnd()) {
+				RowData next = inputFormat.nextRecord(row);
+
+				assertEquals(TEST_DATA[recordCount], next);
+
+				recordCount++;
+			}
+			inputFormat.close();
+		}
+		inputFormat.closeInputFormat();
+		Assert.assertEquals(TEST_DATA.length, recordCount);
+	}
+
+	@Test
+	public void testJdbcInputFormatWithoutParallelismAndNumericColumnSplitting() throws IOException {
+		final long min = TEST_DATA[0].id;
+		final long max = TEST_DATA[TEST_DATA.length - 1].id;
+		final long fetchSize = max + 1; //generate a single split
+		JdbcParameterValuesProvider pramProvider = new JdbcNumericBetweenParametersProvider(min, max).ofBatchSize(fetchSize);
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_ID)
+			.setParametersProvider(pramProvider)
+			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+
+		inputFormat.openInputFormat();
+		InputSplit[] splits = inputFormat.createInputSplits(1);
+		//assert that a single split was generated
+		Assert.assertEquals(1, splits.length);
+		int recordCount = 0;
+		RowData row = new GenericRowData(5);
+		for (InputSplit split : splits) {
+			inputFormat.open(split);
+			while (!inputFormat.reachedEnd()) {
+				RowData next = inputFormat.nextRecord(row);
+
+				assertEquals(TEST_DATA[recordCount], next);
+
+				recordCount++;
+			}
+			inputFormat.close();
+		}
+		inputFormat.closeInputFormat();
+		Assert.assertEquals(TEST_DATA.length, recordCount);
+	}
+
+	@Test
+	public void testJdbcInputFormatWithParallelismAndGenericSplitting() throws IOException {
+		Serializable[][] queryParameters = new String[2][1];
+		queryParameters[0] = new String[]{TEST_DATA[3].author};
+		queryParameters[1] = new String[]{TEST_DATA[0].author};
+		JdbcParameterValuesProvider paramProvider = new JdbcGenericParameterValuesProvider(queryParameters);
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR)
+			.setParametersProvider(paramProvider)
+			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+
+		inputFormat.openInputFormat();
+		InputSplit[] splits = inputFormat.createInputSplits(1);
+		//this query exploit parallelism (1 split for every queryParameters row)
+		Assert.assertEquals(queryParameters.length, splits.length);
+
+		verifySplit(splits[0], TEST_DATA[3].id);
+		verifySplit(splits[1], TEST_DATA[0].id + TEST_DATA[1].id);
+
+		inputFormat.closeInputFormat();
+	}
+
+	private void verifySplit(InputSplit split, int expectedIDSum) throws IOException {
+		int sum = 0;
+
+		RowData row = new GenericRowData(5);
+		inputFormat.open(split);
+		while (!inputFormat.reachedEnd()) {
+			row = inputFormat.nextRecord(row);
+
+			int id = ((int) RowData.get(row, 0, new IntType()));
+			int testDataIndex = id - 1001;
+
+			assertEquals(TEST_DATA[testDataIndex], row);
+			sum += id;
+		}
+
+		Assert.assertEquals(expectedIDSum, sum);
+	}
+
+	@Test
+	public void testEmptyResults() throws IOException {
+		inputFormat = JdbcRowDataInputFormat.builder()
+			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setQuery(SELECT_EMPTY)
+			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
+			.setRowConverter(dialect.getRowConverter(rowType))
+			.build();
+
+		try {
+			inputFormat.openInputFormat();
+			inputFormat.open(null);
+			Assert.assertTrue(inputFormat.reachedEnd());
+		} finally {
+			inputFormat.close();
+			inputFormat.closeInputFormat();
+		}
+	}
+
+	private static void assertEquals(JdbcTestFixture.TestEntry expected, RowData actual) {
+		Assert.assertEquals(expected.id, actual.isNullAt(0) ? null : Integer.valueOf(actual.getInt(0)));
+		Assert.assertEquals(expected.title, actual.isNullAt(1) ? null : actual.getString(1).toString());
+		Assert.assertEquals(expected.author, actual.isNullAt(2) ? null : actual.getString(2).toString());
+		Assert.assertEquals(expected.price, actual.isNullAt(3) ? null : Double.valueOf(actual.getDouble(3)));
+		Assert.assertEquals(expected.qty, actual.isNullAt(4) ? null : Integer.valueOf(actual.getInt(4)));
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
new file mode 100644
index 0000000000000..21f12e072d2bd
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
@@ -0,0 +1,372 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.table;
+
+import org.apache.flink.connector.jdbc.JdbcDataTestBase;
+import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
+import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.table.api.DataTypes;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+
+import org.junit.After;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Statement;
+import java.util.Arrays;
+
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.INPUT_TABLE;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.OUTPUT_TABLE;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.OUTPUT_TABLE_2;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_NEWBOOKS;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_NEWBOOKS_2;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TEST_DATA;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TestEntry;
+import static org.apache.flink.util.ExceptionUtils.findThrowable;
+import static org.apache.flink.util.ExceptionUtils.findThrowableWithMessage;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+/**
+ * Test suite for {@link JdbcRowDataOutputFormat}.
+ */
+public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
+
+	private static JdbcRowDataOutputFormat outputFormat;
+	private static String[] fieldNames = new String[] {"id", "title", "author", "price", "qty"};
+	private static DataType[] fieldDataTypes = new DataType[]{
+		DataTypes.INT(),
+		DataTypes.STRING(),
+		DataTypes.STRING(),
+		DataTypes.DOUBLE(),
+		DataTypes.INT()};
+	private static RowType rowType = RowType.of(
+		Arrays.stream(fieldDataTypes)
+			.map(DataType::getLogicalType)
+			.toArray(LogicalType[]::new),
+		fieldNames);
+	private static RowDataTypeInfo rowDataTypeInfo = RowDataTypeInfo.of(rowType);
+
+	@After
+	public void tearDown() throws Exception {
+		if (outputFormat != null) {
+			outputFormat.close();
+		}
+		outputFormat = null;
+	}
+
+	@Test
+	public void testInvalidDriver() {
+		String expectedMsg = "unable to open JDBC writer";
+		try {
+			JdbcOptions jdbcOptions = JdbcOptions.builder()
+				.setDriverName("org.apache.derby.jdbc.idontexist")
+				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+				.setTableName(INPUT_TABLE)
+				.build();
+			JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+				.withTableName(jdbcOptions.getTableName())
+				.withDialect(jdbcOptions.getDialect())
+				.withFieldNames(fieldNames)
+				.build();
+
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+				.setJdbcOptions(jdbcOptions)
+				.setFieldDataTypes(fieldDataTypes)
+				.setJdbcDmlOptions(dmlOptions)
+				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+				.build();
+			outputFormat.open(0, 1);
+		} catch (Exception e) {
+			assertTrue(findThrowable(e, IOException.class).isPresent());
+			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
+		}
+	}
+
+	@Test
+	public void testInvalidURL() {
+		String expectedMsg = "Unknown dbURL,can not find proper dialect.";
+		try {
+			JdbcOptions jdbcOptions = JdbcOptions.builder()
+				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+				.setDBUrl("jdbc:der:iamanerror:mory:ebookshop")
+				.setTableName(INPUT_TABLE)
+				.build();
+			JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+				.withTableName(jdbcOptions.getTableName())
+				.withDialect(jdbcOptions.getDialect())
+				.withFieldNames(fieldNames)
+				.build();
+
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+				.setJdbcOptions(jdbcOptions)
+				.setFieldDataTypes(fieldDataTypes)
+				.setJdbcDmlOptions(dmlOptions)
+				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+				.build();
+			outputFormat.open(0, 1);
+		} catch (Exception e) {
+			assertTrue(findThrowable(e, NullPointerException.class).isPresent());
+			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
+		}
+	}
+
+	@Test
+	public void testIncompatibleTypes() {
+		try {
+			JdbcOptions jdbcOptions = JdbcOptions.builder()
+				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+				.setTableName(INPUT_TABLE)
+				.build();
+			JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+				.withTableName(jdbcOptions.getTableName())
+				.withDialect(jdbcOptions.getDialect())
+				.withFieldNames(fieldNames)
+				.build();
+
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+				.setJdbcOptions(jdbcOptions)
+				.setFieldDataTypes(fieldDataTypes)
+				.setJdbcDmlOptions(dmlOptions)
+				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+				.setRowDataTypeInfo(rowDataTypeInfo)
+				.build();
+
+			setRuntimeContext(outputFormat, false);
+			outputFormat.open(0, 1);
+
+			RowData row = buildGenericData(4, "hello", "world", 0.99, "imthewrongtype");
+			outputFormat.writeRecord(row);
+			outputFormat.close();
+		} catch (Exception e) {
+			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
+		}
+	}
+
+	@Test
+	public void testExceptionOnInvalidType() {
+		try {
+			JdbcOptions jdbcOptions = JdbcOptions.builder()
+				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+				.setTableName(OUTPUT_TABLE)
+				.build();
+			JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+				.withTableName(jdbcOptions.getTableName())
+				.withDialect(jdbcOptions.getDialect())
+				.withFieldNames(fieldNames)
+				.build();
+
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+				.setJdbcOptions(jdbcOptions)
+				.setFieldDataTypes(fieldDataTypes)
+				.setJdbcDmlOptions(dmlOptions)
+				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+				.setRowDataTypeInfo(rowDataTypeInfo)
+				.build();
+			setRuntimeContext(outputFormat, false);
+			outputFormat.open(0, 1);
+
+			TestEntry entry = TEST_DATA[0];
+			RowData row = buildGenericData(entry.id, entry.title, entry.author, 0L, entry.qty);
+			outputFormat.writeRecord(row);
+			outputFormat.close();
+		} catch (Exception e) {
+			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
+		}
+	}
+
+	@Test
+	public void testExceptionOnClose() {
+		String expectedMsg = "Writing records to JDBC failed.";
+		try {
+			JdbcOptions jdbcOptions = JdbcOptions.builder()
+				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+				.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+				.setTableName(OUTPUT_TABLE)
+				.build();
+			JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+				.withTableName(jdbcOptions.getTableName())
+				.withDialect(jdbcOptions.getDialect())
+				.withFieldNames(fieldNames)
+				.build();
+
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+				.setJdbcOptions(jdbcOptions)
+				.setFieldDataTypes(fieldDataTypes)
+				.setJdbcDmlOptions(dmlOptions)
+				.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+				.setRowDataTypeInfo(rowDataTypeInfo)
+				.build();
+			setRuntimeContext(outputFormat, true);
+			outputFormat.open(0, 1);
+
+			TestEntry entry = TEST_DATA[0];
+			RowData row = buildGenericData(entry.id, entry.title, entry.author, entry.price, entry.qty);
+
+			outputFormat.writeRecord(row);
+			outputFormat.writeRecord(row); // writing the same record twice must yield a unique key violation.
+
+			outputFormat.close();
+		} catch (Exception e) {
+			assertTrue(findThrowable(e, RuntimeException.class).isPresent());
+			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
+		}
+	}
+
+	@Test
+	public void testJdbcOutputFormat() throws IOException, SQLException {
+		JdbcOptions jdbcOptions = JdbcOptions.builder()
+			.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setTableName(OUTPUT_TABLE)
+			.build();
+		JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+			.withTableName(jdbcOptions.getTableName())
+			.withDialect(jdbcOptions.getDialect())
+			.withFieldNames(fieldNames)
+			.build();
+
+		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			.setJdbcOptions(jdbcOptions)
+			.setFieldDataTypes(fieldDataTypes)
+			.setJdbcDmlOptions(dmlOptions)
+			.setJdbcExecutionOptions(JdbcExecutionOptions.builder().build())
+			.setRowDataTypeInfo(rowDataTypeInfo)
+			.build();
+		setRuntimeContext(outputFormat, true);
+		outputFormat.open(0, 1);
+
+		setRuntimeContext(outputFormat, true);
+		outputFormat.open(0, 1);
+
+		for (TestEntry entry : TEST_DATA) {
+			outputFormat.writeRecord(buildGenericData(entry.id, entry.title, entry.author, entry.price, entry.qty));
+		}
+
+		outputFormat.close();
+
+		try (
+			Connection dbConn = DriverManager.getConnection(DERBY_EBOOKSHOP_DB.getUrl());
+			PreparedStatement statement = dbConn.prepareStatement(SELECT_ALL_NEWBOOKS);
+			ResultSet resultSet = statement.executeQuery()
+		) {
+			int recordCount = 0;
+			while (resultSet.next()) {
+				assertEquals(TEST_DATA[recordCount].id, resultSet.getObject("id"));
+				assertEquals(TEST_DATA[recordCount].title, resultSet.getObject("title"));
+				assertEquals(TEST_DATA[recordCount].author, resultSet.getObject("author"));
+				assertEquals(TEST_DATA[recordCount].price, resultSet.getObject("price"));
+				assertEquals(TEST_DATA[recordCount].qty, resultSet.getObject("qty"));
+
+				recordCount++;
+			}
+			assertEquals(TEST_DATA.length, recordCount);
+		}
+	}
+
+	@Test
+	public void testFlush() throws SQLException, IOException {
+		JdbcOptions jdbcOptions = JdbcOptions.builder()
+			.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
+			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
+			.setTableName(OUTPUT_TABLE_2)
+			.build();
+		JdbcDmlOptions dmlOptions = JdbcDmlOptions.builder()
+			.withTableName(jdbcOptions.getTableName())
+			.withDialect(jdbcOptions.getDialect())
+			.withFieldNames(fieldNames)
+			.build();
+		JdbcExecutionOptions executionOptions = JdbcExecutionOptions.builder()
+			.withBatchSize(3)
+			.build();
+
+		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
+			.setJdbcOptions(jdbcOptions)
+			.setFieldDataTypes(fieldDataTypes)
+			.setJdbcDmlOptions(dmlOptions)
+			.setJdbcExecutionOptions(executionOptions)
+			.setRowDataTypeInfo(rowDataTypeInfo)
+			.build();
+		setRuntimeContext(outputFormat, true);
+		outputFormat.open(0, 1);
+
+		try (
+			Connection dbConn = DriverManager.getConnection(DERBY_EBOOKSHOP_DB.getUrl());
+			PreparedStatement statement = dbConn.prepareStatement(SELECT_ALL_NEWBOOKS_2)
+		) {
+			outputFormat.open(0, 1);
+			for (int i = 0; i < 2; ++i) {
+				outputFormat.writeRecord(buildGenericData(
+					TEST_DATA[i].id,
+					TEST_DATA[i].title,
+					TEST_DATA[i].author,
+					TEST_DATA[i].price,
+					TEST_DATA[i].qty));
+			}
+			try (ResultSet resultSet = statement.executeQuery()) {
+				assertFalse(resultSet.next());
+			}
+			outputFormat.writeRecord(buildGenericData(
+				TEST_DATA[2].id,
+				TEST_DATA[2].title,
+				TEST_DATA[2].author,
+				TEST_DATA[2].price,
+				TEST_DATA[2].qty));
+			try (ResultSet resultSet = statement.executeQuery()) {
+				int recordCount = 0;
+				while (resultSet.next()) {
+					assertEquals(TEST_DATA[recordCount].id, resultSet.getObject("id"));
+					assertEquals(TEST_DATA[recordCount].title, resultSet.getObject("title"));
+					assertEquals(TEST_DATA[recordCount].author, resultSet.getObject("author"));
+					assertEquals(TEST_DATA[recordCount].price, resultSet.getObject("price"));
+					assertEquals(TEST_DATA[recordCount].qty, resultSet.getObject("qty"));
+					recordCount++;
+				}
+				assertEquals(3, recordCount);
+			}
+		} finally {
+			outputFormat.close();
+		}
+	}
+
+	@After
+	public void clearOutputTable() throws Exception {
+		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
+		try (
+			Connection conn = DriverManager.getConnection(DERBY_EBOOKSHOP_DB.getUrl());
+			Statement stat = conn.createStatement()) {
+			stat.execute("DELETE FROM " + OUTPUT_TABLE);
+		}
+	}
+
+}
diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/trait/ModifyKindSet.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/trait/ModifyKindSet.java
index e5db0b09606ba..af306d5ea3401 100644
--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/trait/ModifyKindSet.java
+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/plan/trait/ModifyKindSet.java
@@ -18,6 +18,9 @@
 
 package org.apache.flink.table.planner.plan.trait;
 
+import org.apache.flink.table.connector.ChangelogMode;
+import org.apache.flink.types.RowKind;
+
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.EnumSet;
@@ -112,6 +115,24 @@ public ModifyKindSet union(ModifyKindSet other) {
 		return union(this, other);
 	}
 
+	/**
+	 * Returns the default {@link ChangelogMode} from this {@link ModifyKindSet}.
+	 */
+	public ChangelogMode toChangelogMode() {
+		ChangelogMode.Builder builder = ChangelogMode.newBuilder();
+		if (this.contains(ModifyKind.INSERT)) {
+			builder.addContainedKind(RowKind.INSERT);
+		}
+		if (this.contains(ModifyKind.UPDATE)) {
+			builder.addContainedKind(RowKind.UPDATE_BEFORE);
+			builder.addContainedKind(RowKind.UPDATE_AFTER);
+		}
+		if (this.contains(ModifyKind.DELETE)) {
+			builder.addContainedKind(RowKind.DELETE);
+		}
+		return builder.build();
+	}
+
 	@Override
 	public boolean equals(Object o) {
 		if (this == o) {
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
index 2bced01551929..1751f93e80bbd 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/optimize/program/FlinkChangelogModeInferenceProgram.scala
@@ -19,10 +19,10 @@
 package org.apache.flink.table.planner.plan.optimize.program
 
 import org.apache.flink.table.api.TableException
+import org.apache.flink.table.connector.ChangelogMode
 import org.apache.flink.table.planner.plan.`trait`.UpdateKindTrait.{BEFORE_AND_AFTER, ONLY_UPDATE_AFTER, beforeAfterOrNone, onlyAfterOrNone}
 import org.apache.flink.table.planner.plan.`trait`._
 import org.apache.flink.table.planner.plan.nodes.physical.stream._
-import org.apache.flink.table.planner.plan.utils.ChangelogPlanUtils.FULL_CHANGELOG_MODE
 import org.apache.flink.table.planner.plan.utils._
 import org.apache.flink.table.planner.sinks.DataStreamTableSink
 import org.apache.flink.table.runtime.operators.join.FlinkJoinType
@@ -115,8 +115,9 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti
         requester: String): StreamPhysicalRel = rel match {
       case sink: StreamExecSink =>
         val name = s"Table sink '${sink.tableIdentifier.asSummaryString()}'"
+        val queryModifyKindSet = deriveQueryDefaultChangelogMode(sink.getInput, name)
         val sinkRequiredTrait = ModifyKindSetTrait.fromChangelogMode(
-          sink.tableSink.getChangelogMode(FULL_CHANGELOG_MODE))
+          sink.tableSink.getChangelogMode(queryModifyKindSet))
         val children = visitChildren(sink, sinkRequiredTrait, name)
         val sinkTrait = sink.getTraitSet.plus(ModifyKindSetTrait.EMPTY)
         // ignore required trait from context, because sink is the true root
@@ -323,6 +324,18 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti
       }
     }
 
+    /**
+     * Derives the [[ModifyKindSetTrait]] of query plan without required ModifyKindSet validation.
+     */
+    private def deriveQueryDefaultChangelogMode(
+        queryNode: RelNode, name: String): ChangelogMode = {
+      val newNode = visit(
+        queryNode.asInstanceOf[StreamPhysicalRel],
+        ModifyKindSetTrait.ALL_CHANGES,
+        name)
+      getModifyKindSet(newNode).toChangelogMode
+    }
+
     private def createNewNode(
         node: StreamPhysicalRel,
         children: List[StreamPhysicalRel],
@@ -385,7 +398,7 @@ class FlinkChangelogModeInferenceProgram extends FlinkOptimizeProgram[StreamOpti
         val onlyAfter = onlyAfterOrNone(childModifyKindSet)
         val beforeAndAfter = beforeAfterOrNone(childModifyKindSet)
         val sinkTrait = UpdateKindTrait.fromChangelogMode(
-          sink.tableSink.getChangelogMode(FULL_CHANGELOG_MODE))
+          sink.tableSink.getChangelogMode(childModifyKindSet.toChangelogMode))
         val sinkRequiredTraits = if (sinkTrait.equals(ONLY_UPDATE_AFTER)) {
           Seq(onlyAfter, beforeAndAfter)
         } else if (sinkTrait.equals(BEFORE_AND_AFTER)){
