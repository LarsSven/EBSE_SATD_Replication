diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
index 427616eb4bdd6..a83fe075fec78 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/JdbcCatalogUtils.java
@@ -20,6 +20,7 @@
 
 import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+import org.apache.flink.connector.jdbc.dialect.PostgresDialect;
 
 import static org.apache.flink.util.Preconditions.checkArgument;
 
@@ -43,7 +44,7 @@ public static void validateJdbcUrl(String url) {
 	public static AbstractJdbcCatalog createCatalog(String catalogName, String defaultDatabase, String username, String pwd, String baseUrl) {
 		JdbcDialect dialect = JdbcDialects.get(baseUrl).get();
 
-		if (dialect instanceof JdbcDialects.PostgresDialect) {
+		if (dialect instanceof PostgresDialect) {
 			return new PostgresCatalog(catalogName, defaultDatabase, username, pwd, baseUrl);
 		} else {
 			throw new UnsupportedOperationException(
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
index b9bba1e4bb38a..c9b11246632db 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/catalog/PostgresCatalog.java
@@ -51,9 +51,10 @@
 
 import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.IDENTIFIER;
 import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.PASSWORD;
-import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.TABLE;
+import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.TABLE_NAME;
 import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.URL;
 import static org.apache.flink.connector.jdbc.table.JdbcDynamicTableSourceSinkFactory.USERNAME;
+import static org.apache.flink.table.factories.FactoryUtil.CONNECTOR;
 
 /**
  * Catalog for PostgreSQL.
@@ -196,9 +197,9 @@ public CatalogBaseTable getTable(ObjectPath tablePath) throws TableNotExistExcep
 			TableSchema tableSchema = new TableSchema.Builder().fields(names, types).build();
 
 			Map<String, String> props = new HashMap<>();
-			props.put(IDENTIFIER.key(), IDENTIFIER.defaultValue());
+			props.put(CONNECTOR.key(), IDENTIFIER);
 			props.put(URL.key(), dbUrl);
-			props.put(TABLE.key(), pgPath.getFullPath());
+			props.put(TABLE_NAME.key(), pgPath.getFullPath());
 			props.put(USERNAME.key(), username);
 			props.put(PASSWORD.key(), pwd);
 
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java
new file mode 100644
index 0000000000000..4023f2a2a14f9
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/AbstractDialect.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.table.api.TableSchema;
+import org.apache.flink.table.api.ValidationException;
+import org.apache.flink.table.types.DataType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+
+import java.util.List;
+
+abstract class AbstractDialect implements JdbcDialect {
+
+	@Override
+	public void validate(TableSchema schema) throws ValidationException {
+		for (int i = 0; i < schema.getFieldCount(); i++) {
+			DataType dt = schema.getFieldDataType(i).get();
+			String fieldName = schema.getFieldName(i).get();
+
+			// TODO: We can't convert VARBINARY(n) data type to
+			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter
+			//  when n is smaller than Integer.MAX_VALUE
+			if (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||
+				(dt.getLogicalType() instanceof VarBinaryType
+					&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {
+				throw new ValidationException(
+					String.format("The %s dialect doesn't support type: %s.",
+						dialectName(),
+						dt.toString()));
+			}
+
+			// only validate precision of DECIMAL type for blink planner
+			if (dt.getLogicalType() instanceof DecimalType) {
+				int precision = ((DecimalType) dt.getLogicalType()).getPrecision();
+				if (precision > maxDecimalPrecision()
+					|| precision < minDecimalPrecision()) {
+					throw new ValidationException(
+						String.format("The precision of field '%s' is out of the DECIMAL " +
+								"precision range [%d, %d] supported by %s dialect.",
+							fieldName,
+							minDecimalPrecision(),
+							maxDecimalPrecision(),
+							dialectName()));
+				}
+			}
+
+			// only validate precision of DECIMAL type for blink planner
+			if (dt.getLogicalType() instanceof TimestampType) {
+				int precision = ((TimestampType) dt.getLogicalType()).getPrecision();
+				if (precision > maxTimestampPrecision()
+					|| precision < minTimestampPrecision()) {
+					throw new ValidationException(
+						String.format("The precision of field '%s' is out of the TIMESTAMP " +
+								"precision range [%d, %d] supported by %s dialect.",
+							fieldName,
+							minTimestampPrecision(),
+							maxTimestampPrecision(),
+							dialectName()));
+				}
+			}
+		}
+	}
+
+	public abstract int maxDecimalPrecision();
+
+	public abstract int minDecimalPrecision();
+
+	public abstract int maxTimestampPrecision();
+
+	public abstract int minTimestampPrecision();
+
+	/**
+	 * Defines the unsupported types for the dialect.
+	 *
+	 * @return a list of logical type roots.
+	 */
+	public abstract List<LogicalTypeRoot> unsupportedTypes();
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java
new file mode 100644
index 0000000000000..55f110841e94e
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/DerbyDialect.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.DerbyRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+
+class DerbyDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to derby docs:
+	// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html
+	private static final int MAX_TIMESTAMP_PRECISION = 9;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of DECIMAL type according to derby docs:
+	// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html
+	private static final int MAX_DECIMAL_PRECISION = 31;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:derby:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new DerbyRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("org.apache.derby.jdbc.EmbeddedDriver");
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return identifier;
+	}
+
+	@Override
+	public String dialectName() {
+		return "Derby";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in Derby are list at
+		// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.ARRAY,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
index 11adf4050fc02..3d311be00bd76 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialect.java
@@ -19,11 +19,9 @@
 package org.apache.flink.connector.jdbc.dialect;
 
 import org.apache.flink.annotation.Internal;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcToRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.RowToJdbcConverter;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.api.ValidationException;
-import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 
 import java.io.Serializable;
@@ -37,6 +35,12 @@
 @Internal
 public interface JdbcDialect extends Serializable {
 
+	/**
+	 * Get the name of jdbc dialect.
+	 * @return the dialect name.
+	 */
+	String dialectName();
+
 	/**
 	 * Check if this dialect instance can handle a certain jdbc url.
 	 * @param url the jdbc url.
@@ -45,18 +49,11 @@ public interface JdbcDialect extends Serializable {
 	boolean canHandle(String url);
 
 	/**
-	 * Get converter that convert the jdbc object to flink internal object according to the given row type.
+	 * Get converter that convert jdbc object and Flink internal object each other.
 	 * @param rowType the given row type
 	 * @return a row converter for the database
 	 */
-	JdbcToRowConverter getInputConverter(RowType rowType);
-
-	/**
-	 * Get converter that convert the flink internal object to jdbc object according to the given jdbc type.
-	 * @param jdbcTypes the given jdbc type
-	 * @return a row converter for the database
-	 */
-	RowToJdbcConverter getOutputConverter(LogicalType[] jdbcTypes);
+	JdbcRowConverter getRowConverter(RowType rowType);
 
 	/**
 	 * Check if this dialect instance support a specific data type in table schema.
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
index aa0a959524177..c2cc579c81325 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/JdbcDialects.java
@@ -18,28 +18,9 @@
 
 package org.apache.flink.connector.jdbc.dialect;
 
-import org.apache.flink.connector.jdbc.internal.converter.DerbyToJdbcConverter;
-import org.apache.flink.connector.jdbc.internal.converter.DerbyToRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcToRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.MySQLToJdbcConverter;
-import org.apache.flink.connector.jdbc.internal.converter.MySQLToRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.PostgresToJdbcConverter;
-import org.apache.flink.connector.jdbc.internal.converter.PostgresToRowConverter;
-import org.apache.flink.connector.jdbc.internal.converter.RowToJdbcConverter;
-import org.apache.flink.table.api.TableSchema;
-import org.apache.flink.table.api.ValidationException;
-import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.DecimalType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.LogicalTypeRoot;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.TimestampType;
-import org.apache.flink.table.types.logical.VarBinaryType;
-
 import java.util.Arrays;
 import java.util.List;
 import java.util.Optional;
-import java.util.stream.Collectors;
 
 /**
  * Default JDBC dialects.
@@ -63,384 +44,4 @@ public static Optional<JdbcDialect> get(String url) {
 		}
 		return Optional.empty();
 	}
-
-	private abstract static class AbstractDialect implements JdbcDialect {
-
-		@Override
-		public void validate(TableSchema schema) throws ValidationException {
-			for (int i = 0; i < schema.getFieldCount(); i++) {
-				DataType dt = schema.getFieldDataType(i).get();
-				String fieldName = schema.getFieldName(i).get();
-
-				// TODO: We can't convert VARBINARY(n) data type to
-				//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter
-				//  when n is smaller than Integer.MAX_VALUE
-				if (unsupportedTypes().contains(dt.getLogicalType().getTypeRoot()) ||
-						(dt.getLogicalType() instanceof VarBinaryType
-							&& Integer.MAX_VALUE != ((VarBinaryType) dt.getLogicalType()).getLength())) {
-					throw new ValidationException(
-							String.format("The %s dialect doesn't support type: %s.",
-									dialectName(),
-									dt.toString()));
-				}
-
-				// only validate precision of DECIMAL type for blink planner
-				if (dt.getLogicalType() instanceof DecimalType) {
-					int precision = ((DecimalType) dt.getLogicalType()).getPrecision();
-					if (precision > maxDecimalPrecision()
-							|| precision < minDecimalPrecision()) {
-						throw new ValidationException(
-								String.format("The precision of field '%s' is out of the DECIMAL " +
-												"precision range [%d, %d] supported by %s dialect.",
-										fieldName,
-										minDecimalPrecision(),
-										maxDecimalPrecision(),
-										dialectName()));
-					}
-				}
-
-				// only validate precision of DECIMAL type for blink planner
-				if (dt.getLogicalType() instanceof TimestampType) {
-					int precision = ((TimestampType) dt.getLogicalType()).getPrecision();
-					if (precision > maxTimestampPrecision()
-							|| precision < minTimestampPrecision()) {
-						throw new ValidationException(
-								String.format("The precision of field '%s' is out of the TIMESTAMP " +
-												"precision range [%d, %d] supported by %s dialect.",
-										fieldName,
-										minTimestampPrecision(),
-										maxTimestampPrecision(),
-										dialectName()));
-					}
-				}
-			}
-		}
-
-		public abstract String dialectName();
-
-		public abstract int maxDecimalPrecision();
-
-		public abstract int minDecimalPrecision();
-
-		public abstract int maxTimestampPrecision();
-
-		public abstract int minTimestampPrecision();
-
-		/**
-		 * Defines the unsupported types for the dialect.
-		 * @return a list of logical type roots.
-		 */
-		public abstract List<LogicalTypeRoot> unsupportedTypes();
-	}
-
-	private static class DerbyDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to derby docs:
-		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj27620.html
-		private static final int MAX_TIMESTAMP_PRECISION = 9;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of DECIMAL type according to derby docs:
-		// http://db.apache.org/derby/docs/10.14/ref/rrefsqlj15260.html
-		private static final int MAX_DECIMAL_PRECISION = 31;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:derby:");
-		}
-
-		@Override
-		public JdbcToRowConverter getInputConverter(RowType rowType) {
-			return new DerbyToRowConverter(rowType);
-		}
-
-		@Override
-		public RowToJdbcConverter getOutputConverter(LogicalType[] jdbcTypes) {
-			return new DerbyToJdbcConverter(jdbcTypes);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("org.apache.derby.jdbc.EmbeddedDriver");
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return identifier;
-		}
-
-		@Override
-		public String dialectName() {
-			return "derby";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in Derby are list at
-			// http://db.apache.org/derby/docs/10.14/ref/crefsqlj31068.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.ARRAY,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED);
-		}
-	}
-
-	/**
-	 * MySQL dialect.
-	 */
-	public static class MySQLDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:
-		// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html
-		private static final int MAX_TIMESTAMP_PRECISION = 6;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of DECIMAL type according to Mysql docs:
-		// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html
-		private static final int MAX_DECIMAL_PRECISION = 65;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:mysql:");
-		}
-
-		@Override
-		public JdbcToRowConverter getInputConverter(RowType rowType) {
-			return new MySQLToRowConverter(rowType);
-		}
-
-		@Override
-		public RowToJdbcConverter getOutputConverter(LogicalType[] jdbcTypes) {
-			return new MySQLToJdbcConverter(jdbcTypes);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("com.mysql.jdbc.Driver");
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return "`" + identifier + "`";
-		}
-
-		/**
-		 * Mysql upsert query use DUPLICATE KEY UPDATE.
-		 *
-		 * <p>NOTE: It requires Mysql's primary key to be consistent with pkFields.
-		 *
-		 * <p>We don't use REPLACE INTO, if there are other fields, we can keep their previous values.
-		 */
-		@Override
-		public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
-			String updateClause = Arrays.stream(fieldNames)
-					.map(f -> quoteIdentifier(f) + "=VALUES(" + quoteIdentifier(f) + ")")
-					.collect(Collectors.joining(", "));
-			return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
-					" ON DUPLICATE KEY UPDATE " + updateClause
-			);
-		}
-
-		@Override
-		public String dialectName() {
-			return "mysql";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in Mysql are list at:
-			// https://dev.mysql.com/doc/refman/8.0/en/data-types.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.ARRAY,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED
-			);
-		}
-	}
-
-	/**
-	 * Postgres dialect.
-	 */
-	public static class PostgresDialect extends AbstractDialect {
-
-		private static final long serialVersionUID = 1L;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
-		// https://www.postgresql.org/docs/12/datatype-datetime.html
-		private static final int MAX_TIMESTAMP_PRECISION = 6;
-		private static final int MIN_TIMESTAMP_PRECISION = 1;
-
-		// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
-		// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
-		private static final int MAX_DECIMAL_PRECISION = 1000;
-		private static final int MIN_DECIMAL_PRECISION = 1;
-
-		@Override
-		public boolean canHandle(String url) {
-			return url.startsWith("jdbc:postgresql:");
-		}
-
-		@Override
-		public JdbcToRowConverter getInputConverter(RowType rowType) {
-			return new PostgresToRowConverter(rowType);
-		}
-
-		@Override
-		public RowToJdbcConverter getOutputConverter(LogicalType[] jdbcTypes) {
-			return new PostgresToJdbcConverter(jdbcTypes);
-		}
-
-		@Override
-		public Optional<String> defaultDriverName() {
-			return Optional.of("org.postgresql.Driver");
-		}
-
-		/**
-		 * Postgres upsert query. It use ON CONFLICT ... DO UPDATE SET.. to replace into Postgres.
-		 */
-		@Override
-		public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
-			String uniqueColumns = Arrays.stream(uniqueKeyFields)
-					.map(this::quoteIdentifier)
-					.collect(Collectors.joining(", "));
-			String updateClause = Arrays.stream(fieldNames)
-					.map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
-					.collect(Collectors.joining(", "));
-			return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
-							" ON CONFLICT (" + uniqueColumns + ")" +
-							" DO UPDATE SET " + updateClause
-			);
-		}
-
-		@Override
-		public String quoteIdentifier(String identifier) {
-			return identifier;
-		}
-
-		@Override
-		public String dialectName() {
-			return "postgresql";
-		}
-
-		@Override
-		public int maxDecimalPrecision() {
-			return MAX_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int minDecimalPrecision() {
-			return MIN_DECIMAL_PRECISION;
-		}
-
-		@Override
-		public int maxTimestampPrecision() {
-			return MAX_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public int minTimestampPrecision() {
-			return MIN_TIMESTAMP_PRECISION;
-		}
-
-		@Override
-		public List<LogicalTypeRoot> unsupportedTypes() {
-			// The data types used in PostgreSQL are list at:
-			// https://www.postgresql.org/docs/12/datatype.html
-
-			// TODO: We can't convert BINARY data type to
-			//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
-			return Arrays.asList(
-					LogicalTypeRoot.BINARY,
-					LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
-					LogicalTypeRoot.INTERVAL_YEAR_MONTH,
-					LogicalTypeRoot.INTERVAL_DAY_TIME,
-					LogicalTypeRoot.MULTISET,
-					LogicalTypeRoot.MAP,
-					LogicalTypeRoot.ROW,
-					LogicalTypeRoot.DISTINCT_TYPE,
-					LogicalTypeRoot.STRUCTURED_TYPE,
-					LogicalTypeRoot.NULL,
-					LogicalTypeRoot.RAW,
-					LogicalTypeRoot.SYMBOL,
-					LogicalTypeRoot.UNRESOLVED
-			);
-
-		}
-	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java
new file mode 100644
index 0000000000000..5d6504833e4e3
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/MySQLDialect.java
@@ -0,0 +1,135 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.MySQLRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * JDBC dialect for MySQL.
+ */
+public class MySQLDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to Mysql docs:
+	// https://dev.mysql.com/doc/refman/8.0/en/fractional-seconds.html
+	private static final int MAX_TIMESTAMP_PRECISION = 6;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of DECIMAL type according to Mysql docs:
+	// https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html
+	private static final int MAX_DECIMAL_PRECISION = 65;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:mysql:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new MySQLRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("com.mysql.jdbc.Driver");
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return "`" + identifier + "`";
+	}
+
+	/**
+	 * Mysql upsert query use DUPLICATE KEY UPDATE.
+	 *
+	 * <p>NOTE: It requires Mysql's primary key to be consistent with pkFields.
+	 *
+	 * <p>We don't use REPLACE INTO, if there are other fields, we can keep their previous values.
+	 */
+	@Override
+	public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
+		String updateClause = Arrays.stream(fieldNames)
+			.map(f -> quoteIdentifier(f) + "=VALUES(" + quoteIdentifier(f) + ")")
+			.collect(Collectors.joining(", "));
+		return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
+			" ON DUPLICATE KEY UPDATE " + updateClause
+		);
+	}
+
+	@Override
+	public String dialectName() {
+		return "MySQL";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in Mysql are list at:
+		// https://dev.mysql.com/doc/refman/8.0/en/data-types.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_LOCAL_TIME_ZONE,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.ARRAY,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED
+		);
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java
new file mode 100644
index 0000000000000..2c60cff0af2ba
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/dialect/PostgresDialect.java
@@ -0,0 +1,134 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.dialect;
+
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.PostgresRowConverter;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+
+import java.util.Arrays;
+import java.util.List;
+import java.util.Optional;
+import java.util.stream.Collectors;
+
+/**
+ * JDBC dialect for Postgres.
+ */
+public class PostgresDialect extends AbstractDialect {
+
+	private static final long serialVersionUID = 1L;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+	// https://www.postgresql.org/docs/12/datatype-datetime.html
+	private static final int MAX_TIMESTAMP_PRECISION = 6;
+	private static final int MIN_TIMESTAMP_PRECISION = 1;
+
+	// Define MAX/MIN precision of TIMESTAMP type according to PostgreSQL docs:
+	// https://www.postgresql.org/docs/12/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL
+	private static final int MAX_DECIMAL_PRECISION = 1000;
+	private static final int MIN_DECIMAL_PRECISION = 1;
+
+	@Override
+	public boolean canHandle(String url) {
+		return url.startsWith("jdbc:postgresql:");
+	}
+
+	@Override
+	public JdbcRowConverter getRowConverter(RowType rowType) {
+		return new PostgresRowConverter(rowType);
+	}
+
+	@Override
+	public Optional<String> defaultDriverName() {
+		return Optional.of("org.postgresql.Driver");
+	}
+
+	/**
+	 * Postgres upsert query. It use ON CONFLICT ... DO UPDATE SET.. to replace into Postgres.
+	 */
+	@Override
+	public Optional<String> getUpsertStatement(String tableName, String[] fieldNames, String[] uniqueKeyFields) {
+		String uniqueColumns = Arrays.stream(uniqueKeyFields)
+			.map(this::quoteIdentifier)
+			.collect(Collectors.joining(", "));
+		String updateClause = Arrays.stream(fieldNames)
+			.map(f -> quoteIdentifier(f) + "=EXCLUDED." + quoteIdentifier(f))
+			.collect(Collectors.joining(", "));
+		return Optional.of(getInsertIntoStatement(tableName, fieldNames) +
+			" ON CONFLICT (" + uniqueColumns + ")" +
+			" DO UPDATE SET " + updateClause
+		);
+	}
+
+	@Override
+	public String quoteIdentifier(String identifier) {
+		return identifier;
+	}
+
+	@Override
+	public String dialectName() {
+		return "Postgres";
+	}
+
+	@Override
+	public int maxDecimalPrecision() {
+		return MAX_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int minDecimalPrecision() {
+		return MIN_DECIMAL_PRECISION;
+	}
+
+	@Override
+	public int maxTimestampPrecision() {
+		return MAX_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public int minTimestampPrecision() {
+		return MIN_TIMESTAMP_PRECISION;
+	}
+
+	@Override
+	public List<LogicalTypeRoot> unsupportedTypes() {
+		// The data types used in PostgreSQL are list at:
+		// https://www.postgresql.org/docs/12/datatype.html
+
+		// TODO: We can't convert BINARY data type to
+		//  PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO in LegacyTypeInfoDataTypeConverter.
+		return Arrays.asList(
+			LogicalTypeRoot.BINARY,
+			LogicalTypeRoot.TIMESTAMP_WITH_TIME_ZONE,
+			LogicalTypeRoot.INTERVAL_YEAR_MONTH,
+			LogicalTypeRoot.INTERVAL_DAY_TIME,
+			LogicalTypeRoot.MULTISET,
+			LogicalTypeRoot.MAP,
+			LogicalTypeRoot.ROW,
+			LogicalTypeRoot.DISTINCT_TYPE,
+			LogicalTypeRoot.STRUCTURED_TYPE,
+			LogicalTypeRoot.NULL,
+			LogicalTypeRoot.RAW,
+			LogicalTypeRoot.SYMBOL,
+			LogicalTypeRoot.UNRESOLVED
+		);
+	}
+}
+
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
new file mode 100644
index 0000000000000..4e5271a853e24
--- /dev/null
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcRowConverter.java
@@ -0,0 +1,272 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.connector.jdbc.internal.converter;
+
+import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
+import org.apache.flink.table.data.DecimalData;
+import org.apache.flink.table.data.GenericArrayData;
+import org.apache.flink.table.data.GenericRowData;
+import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.StringData;
+import org.apache.flink.table.data.TimestampData;
+import org.apache.flink.table.types.logical.ArrayType;
+import org.apache.flink.table.types.logical.DecimalType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.LogicalTypeRoot;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.TimestampType;
+import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
+import org.apache.flink.table.types.utils.TypeConversions;
+
+import java.io.Serializable;
+import java.lang.reflect.Array;
+import java.math.BigDecimal;
+import java.sql.Date;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.time.LocalDate;
+import java.time.LocalTime;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Base class for all converters that convert between JDBC object and Flink internal object.
+ */
+public abstract class AbstractJdbcRowConverter implements JdbcRowConverter {
+
+	protected final RowType rowType;
+	protected final JdbcDeserializationConverter[] toInternalConverters;
+	protected final JdbcSerializationConverter[] toExternalConverters;
+	protected final LogicalType[] fieldTypes;
+
+	public abstract String converterName();
+
+	public AbstractJdbcRowConverter(RowType rowType) {
+		this.rowType = checkNotNull(rowType);
+		this.fieldTypes = rowType.getFields().stream()
+			.map(RowType.RowField::getType)
+			.toArray(LogicalType[]::new);
+		this.toInternalConverters = new JdbcDeserializationConverter[rowType.getFieldCount()];
+		this.toExternalConverters = new JdbcSerializationConverter[rowType.getFieldCount()];
+		for (int i = 0; i < rowType.getFieldCount(); i++) {
+			toInternalConverters[i] = createNullableInternalConverter(rowType.getTypeAt(i));
+			toExternalConverters[i] = createNullableExternalConverter(fieldTypes[i]);
+		}
+	}
+
+	@Override
+	public RowData toInternal(ResultSet resultSet) throws SQLException {
+		GenericRowData genericRowData = new GenericRowData(rowType.getFieldCount());
+		for (int pos = 0; pos < rowType.getFieldCount(); pos++) {
+			genericRowData.setField(pos, toInternalConverters[pos].deserialize(resultSet.getObject(pos + 1)));
+		}
+		return genericRowData;
+	}
+
+	@Override
+	public PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException {
+		for (int index = 0; index < rowData.getArity(); index++) {
+			toExternalConverters[index].serialize(rowData, index, statement);
+		}
+		return statement;
+	}
+
+	/**
+	 * Runtime converter to convert JDBC field to {@link RowData} type object.
+	 */
+	@FunctionalInterface
+	interface JdbcDeserializationConverter extends Serializable {
+		/**
+		 * convert a jdbc field to java object, the field could be a simple type or array type.
+		 * @param jdbcField
+		 */
+		Object deserialize(Object jdbcField) throws SQLException;
+	}
+
+	/**
+	 * Runtime converter to convert {@link RowData} field to java object and fill into the {@link PreparedStatement}.
+	 */
+	@FunctionalInterface
+	interface JdbcSerializationConverter extends Serializable {
+		void serialize(RowData rowData, int index, PreparedStatement statement) throws SQLException;
+	}
+
+	/**
+	 * Create a nullable runtime {@link JdbcDeserializationConverter} from given {@link LogicalType}.
+	 */
+	protected JdbcDeserializationConverter createNullableInternalConverter(LogicalType type) {
+		return wrapIntoNullableInternalConverter(createInternalConverter(type));
+	}
+
+	protected JdbcDeserializationConverter wrapIntoNullableInternalConverter(JdbcDeserializationConverter jdbcDeserializationConverter) {
+		return v -> {
+			if (v == null) {
+				return null;
+			} else {
+				return jdbcDeserializationConverter.deserialize(v);
+			}
+		};
+	}
+
+	protected JdbcDeserializationConverter createInternalConverter(LogicalType type) {
+		switch (type.getTypeRoot()) {
+			case NULL:
+				return v -> null;
+			case BOOLEAN:
+				return v -> (boolean) v;
+			case TINYINT:
+				return v -> (byte) v;
+			case SMALLINT:
+				// Converter for small type that casts value to int and then return short value, since
+				// JDBC 1.0 use int type for small values.
+				return v -> (Integer.valueOf(v.toString())).shortValue();
+			case INTEGER:
+			case INTERVAL_YEAR_MONTH:
+				return v -> (int) v;
+			case BIGINT:
+			case INTERVAL_DAY_TIME:
+				return v -> (long) v;
+			case DATE:
+				return v -> (int) (((Date) v).toLocalDate().toEpochDay());
+			case TIME_WITHOUT_TIME_ZONE:
+				return v -> ((Time) v).toLocalTime().toSecondOfDay() * 1000;
+			case TIMESTAMP_WITH_TIME_ZONE:
+			case TIMESTAMP_WITHOUT_TIME_ZONE:
+				return v -> TimestampData.fromTimestamp((Timestamp) v);
+			case FLOAT:
+				return v -> (float) v;
+			case DOUBLE:
+				return v -> (double) v;
+			case CHAR:
+			case VARCHAR:
+				return v -> StringData.fromString((String) v);
+			case BINARY:
+			case VARBINARY:
+				return v -> (byte[]) v;
+			case DECIMAL:
+				final int precision = ((DecimalType) type).getPrecision();
+				final int scale = ((DecimalType) type).getScale();
+				return v -> DecimalData.fromBigDecimal((BigDecimal) v, precision, scale);
+			case ARRAY:
+				final JdbcDeserializationConverter arrayConverter = createToInternalArrayConverter((ArrayType) type);
+				return v -> arrayConverter.deserialize(v);
+			case ROW:
+			case MAP:
+			case MULTISET:
+			case RAW:
+			default:
+				throw new UnsupportedOperationException("Not support to parse type: " + type);
+		}
+	}
+
+	protected JdbcDeserializationConverter createToInternalArrayConverter(ArrayType arrayType) {
+		final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
+		final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
+		return v -> {
+			final Object[] objects = (Object[]) v;
+			final Object[] array = (Object[]) Array.newInstance(elementClass, objects.length);
+			for (int i = 0; i < objects.length; i++) {
+				array[i] = elementConverter.deserialize(objects[i]);
+			}
+			return new GenericArrayData(array);
+		};
+	}
+
+	/**
+	 * Create a nullable JDBC f{@link JdbcSerializationConverter} from given sql type.
+	 */
+	protected JdbcSerializationConverter createNullableExternalConverter(LogicalType type) {
+		return wrapIntoNullableExternalConverter(createExternalConverter(type), type);
+	}
+
+	protected JdbcSerializationConverter wrapIntoNullableExternalConverter(JdbcSerializationConverter jdbcSerializationConverter, LogicalType type) {
+		final int sqlType = JdbcTypeUtil.typeInformationToSqlType(TypeConversions.fromDataTypeToLegacyInfo(
+			TypeConversions.fromLogicalToDataType(type)));
+		return (val, index, statement)  -> {
+			if (val == null || val.isNullAt(index) || LogicalTypeRoot.NULL.equals(type.getTypeRoot())) {
+				statement.setNull(index + 1, sqlType);
+			} else {
+				jdbcSerializationConverter.serialize(val, index, statement);
+			}
+		};
+	}
+
+	protected JdbcSerializationConverter createExternalConverter(LogicalType type) {
+		switch (type.getTypeRoot()) {
+			case BOOLEAN:
+				return (val, index, statement) -> statement.setBoolean(index + 1, val.getBoolean(index));
+			case TINYINT:
+				return (val, index, statement) -> statement.setByte(index + 1, val.getByte(index));
+			case SMALLINT:
+				return (val, index, statement) -> statement.setShort(index + 1, val.getShort(index));
+			case INTEGER:
+			case INTERVAL_YEAR_MONTH:
+				return (val, index, statement) -> statement.setInt(index + 1, val.getInt(index));
+			case BIGINT:
+			case INTERVAL_DAY_TIME:
+				return (val, index, statement) -> statement.setLong(index + 1, val.getLong(index));
+			case FLOAT:
+				return (val, index, statement) -> statement.setFloat(index + 1, val.getFloat(index));
+			case DOUBLE:
+				return (val, index, statement) -> statement.setDouble(index + 1, val.getDouble(index));
+			case CHAR:
+			case VARCHAR:
+				// value is BinaryString
+				return (val, index, statement) -> statement.setString(index + 1, val.getString(index).toString());
+			case BINARY:
+			case VARBINARY:
+				return (val, index, statement) -> statement.setBytes(index + 1, val.getBinary(index));
+			case DATE:
+				return (val, index, statement) ->
+					statement.setDate(index + 1, Date.valueOf(LocalDate.ofEpochDay(val.getInt(index))));
+			case TIME_WITHOUT_TIME_ZONE:
+				return (val, index, statement) ->
+					statement.setTime(index + 1, Time.valueOf(LocalTime.ofSecondOfDay(val.getInt(index) / 1000L)));
+			case TIMESTAMP_WITH_TIME_ZONE:
+			case TIMESTAMP_WITHOUT_TIME_ZONE:
+				final int timestampPrecision = ((TimestampType) type).getPrecision();
+				return (val, index, statement) ->
+					statement.setTimestamp(index + 1, val
+						.getTimestamp(index, timestampPrecision)
+						.toTimestamp());
+			case DECIMAL:
+				final int decimalPrecision = ((DecimalType) type).getPrecision();
+				final int decimalScale = ((DecimalType) type).getScale();
+				return (val, index, statement) ->
+					statement.setBigDecimal(index + 1, val
+						.getDecimal(index, decimalPrecision, decimalScale)
+						.toBigDecimal());
+			case ARRAY:
+				//note: dialect need implements the conversion from ArrayData to JDBC Array if the dialect supports array.
+				return (val, index, statement) -> {
+					throw new IllegalStateException(
+						String.format("JDBC:%s do not support write ARRAY type.", converterName()));
+				};
+			case MAP:
+			case MULTISET:
+			case ROW:
+			case RAW:
+			default:
+				throw new UnsupportedOperationException("Not support to parse type: " + type);
+		}
+	}
+}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcToRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcToRowConverter.java
deleted file mode 100644
index 210105a130f0f..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractJdbcToRowConverter.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.table.data.DecimalData;
-import org.apache.flink.table.data.GenericArrayData;
-import org.apache.flink.table.data.GenericRowData;
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.data.StringData;
-import org.apache.flink.table.data.TimestampData;
-import org.apache.flink.table.types.logical.ArrayType;
-import org.apache.flink.table.types.logical.DecimalType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.utils.LogicalTypeUtils;
-
-import java.lang.reflect.Array;
-import java.math.BigDecimal;
-import java.sql.Date;
-import java.sql.ResultSet;
-import java.sql.SQLException;
-import java.sql.Time;
-import java.sql.Timestamp;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * Base class for all converters that convert JDBC object to Flink internal data structure.
- */
-public abstract class AbstractJdbcToRowConverter implements JdbcToRowConverter {
-
-	protected final RowType rowType;
-	protected final JdbcFieldConverter[] converters;
-
-	public AbstractJdbcToRowConverter(RowType rowType) {
-		this.rowType = checkNotNull(rowType);
-		converters = new JdbcFieldConverter[rowType.getFieldCount()];
-
-		for (int i = 0; i < converters.length; i++) {
-			converters[i] = createConverter(rowType.getTypeAt(i));
-		}
-	}
-
-	@Override
-	public RowData toInternal(ResultSet resultSet) throws SQLException {
-		GenericRowData genericRowData = new GenericRowData(rowType.getFieldCount());
-		try {
-			for (int pos = 0; pos < rowType.getFieldCount(); pos++) {
-				genericRowData.setField(pos, converters[pos].convert(resultSet.getObject(pos + 1)));
-			}
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-		return genericRowData;
-	}
-
-	/**
-	 * Create a runtime JDBC field converter from given {@link LogicalType}.
-	 */
-	public JdbcFieldConverter createConverter(LogicalType type) {
-		return v -> {
-			if (v == null) {
-				return null;
-			} else {
-				try {
-					switch (type.getTypeRoot()) {
-						case NULL:
-							return null;
-						case BOOLEAN:
-							return (boolean) v;
-						case TINYINT:
-							return (byte) v;
-						case SMALLINT:
-							// Converter for small type that casts value to int and then return short value, since
-							// JDBC 1.0 use int type for small values.
-							return (Integer.valueOf(v.toString())).shortValue();
-						case INTEGER:
-						case INTERVAL_YEAR_MONTH:
-							return (int) v;
-						case BIGINT:
-						case INTERVAL_DAY_TIME:
-							return (long) v;
-						case DATE:
-							return (int) (((Date) v).toLocalDate().toEpochDay());
-						case TIME_WITHOUT_TIME_ZONE:
-							return ((Time) v).toLocalTime().toSecondOfDay() * 1000;
-						case TIMESTAMP_WITH_TIME_ZONE:
-						case TIMESTAMP_WITHOUT_TIME_ZONE:
-							return TimestampData.fromTimestamp((Timestamp) v);
-						case FLOAT:
-							return (float) v;
-						case DOUBLE:
-							return (double) v;
-						case CHAR:
-						case VARCHAR:
-							return StringData.fromString((String) v);
-						case BINARY:
-						case VARBINARY:
-							return (byte[]) v;
-						case DECIMAL:
-							final int precision = ((DecimalType) type).getPrecision();
-							final int scale = ((DecimalType) type).getScale();
-							return DecimalData.fromBigDecimal((BigDecimal) v, precision, scale);
-						case ARRAY:
-							return createArrayConverter((ArrayType) type).convert(v);
-						case ROW:
-						case MAP:
-						case MULTISET:
-						case RAW:
-						default:
-							throw new UnsupportedOperationException("Unsupported type: " + type);
-					}
-				} catch (ClassCastException e) {
-					// enrich the exception with detailed information.
-					String errorMessage = String.format(
-						"%s, field type: %s, field value: %s.", e.getMessage(), type, v);
-					ClassCastException enrichedException = new ClassCastException(errorMessage);
-					enrichedException.setStackTrace(e.getStackTrace());
-					throw enrichedException;
-				}
-			}
-		};
-	}
-
-	private JdbcFieldConverter createArrayConverter(ArrayType arrayType) {
-		final JdbcFieldConverter elementConverter = createConverter(arrayType.getElementType());
-		final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
-		return v -> {
-			final Object[] objects = (Object[]) v;
-			final Object[] array = (Object[]) Array.newInstance(elementClass, objects.length);
-			for (int i = 0; i < objects.length; i++) {
-				array[i] = elementConverter.convert(objects[i]);
-			}
-			return new GenericArrayData(array);
-		};
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractRowToJdbcConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractRowToJdbcConverter.java
deleted file mode 100644
index a3b1b44da7535..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/AbstractRowToJdbcConverter.java
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.types.logical.DecimalType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.TimestampType;
-import org.apache.flink.table.types.utils.TypeConversions;
-
-import java.sql.Date;
-import java.sql.PreparedStatement;
-import java.sql.SQLException;
-import java.sql.Time;
-import java.time.LocalDate;
-import java.time.LocalTime;
-
-import static org.apache.flink.util.Preconditions.checkNotNull;
-
-/**
- * Base class for all converters that convert Flink internal data structure to JDBC object.
- */
-public abstract class AbstractRowToJdbcConverter implements RowToJdbcConverter {
-
-	protected final LogicalType[] logicalTypes;
-	protected final RowFieldConverter[] toExternalConverters;
-
-	public AbstractRowToJdbcConverter(LogicalType[] logicalTypes) {
-		checkNotNull(logicalTypes, "logical types types can not be null.");
-		this.logicalTypes = logicalTypes;
-		this.toExternalConverters = new RowToJdbcConverter.RowFieldConverter[logicalTypes.length];
-		for (int i = 0; i < toExternalConverters.length; i++) {
-			toExternalConverters[i] = createExternalConverter(logicalTypes[i]);
-		}
-	}
-
-	@Override
-	public PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException {
-		for (int index = 0; index < rowData.getArity(); index++) {
-			toExternalConverters[index].convert(statement, index, logicalTypes[index], rowData);
-		}
-		return statement;
-	}
-
-	/**
-	 * Create a runtime JDBC field converter from given sql type.
-	 */
-	public RowFieldConverter createExternalConverter(LogicalType logicalType) {
-		return (statement, index, type, val) -> {
-			final int sqlType = JdbcTypeUtil.typeInformationToSqlType(TypeConversions.fromDataTypeToLegacyInfo(
-					TypeConversions.fromLogicalToDataType(type)));
-			if (val.isNullAt(index)) {
-				statement.setNull(index + 1, sqlType);
-			} else {
-				try {
-					switch (sqlType) {
-						case java.sql.Types.NULL:
-							statement.setNull(index + 1, sqlType);
-							break;
-						case java.sql.Types.BOOLEAN:
-						case java.sql.Types.BIT:
-							statement.setBoolean(index + 1, val.getBoolean(index));
-							break;
-						case java.sql.Types.CHAR:
-						case java.sql.Types.NCHAR:
-						case java.sql.Types.VARCHAR:
-						case java.sql.Types.LONGVARCHAR:
-						case java.sql.Types.LONGNVARCHAR:
-							statement.setString(index + 1, val.getString(index).toString());
-							break;
-						case java.sql.Types.TINYINT:
-							statement.setByte(index + 1, val.getByte(index));
-							break;
-						case java.sql.Types.SMALLINT:
-							statement.setShort(index + 1, val.getShort(index));
-							break;
-						case java.sql.Types.INTEGER:
-							statement.setInt(index + 1, val.getInt(index));
-							break;
-						case java.sql.Types.BIGINT:
-							statement.setLong(index + 1, val.getLong(index));
-							break;
-						case java.sql.Types.REAL:
-							statement.setFloat(index + 1, val.getFloat(index));
-							break;
-						case java.sql.Types.FLOAT:
-						case java.sql.Types.DOUBLE:
-							statement.setDouble(index + 1, val.getDouble(index));
-							break;
-						case java.sql.Types.DECIMAL:
-						case java.sql.Types.NUMERIC:
-							// legacy decimal precision and scale is (38, 18).
-							final int precision = logicalType instanceof DecimalType ? ((DecimalType) logicalType).getPrecision() : 38;
-							final int scale = logicalType instanceof DecimalType ? ((DecimalType) logicalType).getScale() : 18;
-							statement.setBigDecimal(index + 1, (val.getDecimal(index, precision, scale)).toBigDecimal());
-							break;
-						case java.sql.Types.DATE:
-							statement.setDate(index + 1, Date.valueOf(LocalDate.ofEpochDay(val.getLong(index))));
-							break;
-						case java.sql.Types.TIME:
-							statement.setTime(index + 1, Time.valueOf(LocalTime.ofSecondOfDay(val.getInt(index) / 1000L)));
-							break;
-						case java.sql.Types.TIMESTAMP:
-							// default timestamp precision is 3.
-							final int precison1 = logicalType instanceof TimestampType ? ((TimestampType) logicalType).getPrecision() : 3;
-							statement.setTimestamp(index + 1, (val.getTimestamp(index, precison1)).toTimestamp());
-							break;
-						case java.sql.Types.BINARY:
-						case java.sql.Types.VARBINARY:
-						case java.sql.Types.LONGVARBINARY:
-							statement.setBytes(index + 1, val.getBinary(index));
-							break;
-						default:
-							statement.setObject(index + 1, RowData.get(val, index, logicalType));
-					}
-				} catch (ClassCastException e) {
-					// enrich the exception with detailed information.
-					String errorMessage = String.format(
-						"%s, field index: %s, field value: %s.", e.getMessage(), index, val);
-					ClassCastException enrichedException = new ClassCastException(errorMessage);
-					enrichedException.setStackTrace(e.getStackTrace());
-					throw enrichedException;
-				}
-			}
-		};
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
similarity index 74%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToRowConverter.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
index b22bef86e7d91..cbb1256ec37ed 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyRowConverter.java
@@ -21,11 +21,18 @@
 import org.apache.flink.table.types.logical.RowType;
 
 /**
- * JDBC object to Flink internal data structure converter for MySQL.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for Derby.
  */
-public class MySQLToRowConverter extends AbstractJdbcToRowConverter {
+public class DerbyRowConverter extends AbstractJdbcRowConverter {
 
-	public MySQLToRowConverter(RowType rowType) {
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "Derby";
+	}
+
+	public DerbyRowConverter(RowType rowType) {
 		super(rowType);
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToJdbcConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToJdbcConverter.java
deleted file mode 100644
index a13ba9fb33a2e..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToJdbcConverter.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.table.types.logical.LogicalType;
-
-/**
- * Flink internal data structure to JDBC object converter for Derby.
- */
-public class DerbyToJdbcConverter extends AbstractRowToJdbcConverter {
-
-	public DerbyToJdbcConverter(LogicalType[] externalTypes) {
-		super(externalTypes);
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcToRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
similarity index 70%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcToRowConverter.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
index 68948c303d360..18e6f613a5052 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcToRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/JdbcRowConverter.java
@@ -21,14 +21,14 @@
 import org.apache.flink.table.data.RowData;
 
 import java.io.Serializable;
+import java.sql.PreparedStatement;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 
 /**
- * Converter that converts JDBC object to Flink SQL internal data structure {@link RowData}.
+ * Converter that is responsible to convert between JDBC object and Flink SQL internal data structure {@link RowData}.
  */
-@FunctionalInterface
-public interface JdbcToRowConverter extends Serializable {
+public interface JdbcRowConverter extends Serializable {
 
 	/**
 	 * Convert data retrieved from {@link ResultSet} to internal {@link RowData}.
@@ -38,10 +38,11 @@ public interface JdbcToRowConverter extends Serializable {
 	RowData toInternal(ResultSet resultSet) throws SQLException;
 
 	/**
-	 * Runtime converter to convert JDBC field to {@link RowData} type object.
+	 * Convert data retrieved from Flink internal RowData to JDBC Object.
+	 *
+	 * @param rowData The given internal {@link RowData}.
+	 * @param statement The statement to be filled.
+	 * @return The filled statement.
 	 */
-	@FunctionalInterface
-	interface JdbcFieldConverter extends Serializable {
-		Object convert(Object field) throws SQLException;
-	}
+	PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException;
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
similarity index 74%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToRowConverter.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
index 6ef5964cfbf55..0068089fbef64 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/DerbyToRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLRowConverter.java
@@ -21,11 +21,18 @@
 import org.apache.flink.table.types.logical.RowType;
 
 /**
- * JDBC object to Flink internal data structure converter for Derby.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for MySQL.
  */
-public class DerbyToRowConverter extends AbstractJdbcToRowConverter {
+public class MySQLRowConverter extends AbstractJdbcRowConverter {
 
-	public DerbyToRowConverter(RowType rowType) {
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "MySQL";
+	}
+
+	public MySQLRowConverter(RowType rowType) {
 		super(rowType);
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToJdbcConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToJdbcConverter.java
deleted file mode 100644
index 79164e368c8f4..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/MySQLToJdbcConverter.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.table.types.logical.LogicalType;
-
-/**
- * Flink internal data structure to JDBC object converter for MySQL.
- */
-public class MySQLToJdbcConverter extends AbstractRowToJdbcConverter {
-	public MySQLToJdbcConverter(LogicalType[] externalTypes) {
-		super(externalTypes);
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToRowConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
similarity index 71%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToRowConverter.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
index 5d088502e1890..bcac3f4a20f20 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToRowConverter.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresRowConverter.java
@@ -33,50 +33,57 @@
 import java.lang.reflect.Array;
 
 /**
- * JDBC object to Flink internal data structure converter for Postgres.
+ * Runtime converter that responsible to convert between JDBC object and Flink internal object for Postgres.
  */
-public class PostgresToRowConverter extends AbstractJdbcToRowConverter {
+public class PostgresRowConverter extends AbstractJdbcRowConverter {
 
-	public PostgresToRowConverter(RowType rowType) {
+	private static final long serialVersionUID = 1L;
+
+	@Override
+	public String converterName() {
+		return "Postgres";
+	}
+
+	public PostgresRowConverter(RowType rowType) {
 		super(rowType);
 	}
 
 	@Override
-	public JdbcFieldConverter createConverter(LogicalType type) {
+	public JdbcDeserializationConverter createNullableInternalConverter(LogicalType type) {
 		LogicalTypeRoot root = type.getTypeRoot();
 
 		if (root == LogicalTypeRoot.ARRAY) {
 			ArrayType arrayType = (ArrayType) type;
-			return createArrayConverter(arrayType);
+			return createPostgresArrayConverter(arrayType);
 		} else {
 			return createPrimitiveConverter(type);
 		}
 	}
 
-	private JdbcFieldConverter createArrayConverter(ArrayType arrayType) {
+	private JdbcDeserializationConverter createPostgresArrayConverter(ArrayType arrayType) {
 		// PG's bytea[] is wrapped in PGobject, rather than primitive byte arrays
 		if (LogicalTypeChecks.hasFamily(arrayType.getElementType(), LogicalTypeFamily.BINARY_STRING)) {
 			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
-			final JdbcFieldConverter elementConverter = createConverter(arrayType.getElementType());
+			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
 
 			return v -> {
 				PgArray pgArray = (PgArray) v;
 				Object[] in = (Object[]) pgArray.getArray();
 				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
 				for (int i = 0; i < in.length; i++) {
-					array[i] = elementConverter.convert(((PGobject) in[i]).getValue().getBytes());
+					array[i] = elementConverter.deserialize(((PGobject) in[i]).getValue().getBytes());
 				}
 				return new GenericArrayData(array);
 			};
 		} else {
 			final Class<?> elementClass = LogicalTypeUtils.toInternalConversionClass(arrayType.getElementType());
-			final JdbcFieldConverter elementConverter = createConverter(arrayType.getElementType());
+			final JdbcDeserializationConverter elementConverter = createNullableInternalConverter(arrayType.getElementType());
 			return v -> {
 				PgArray pgArray = (PgArray) v;
 				Object[] in = (Object[]) pgArray.getArray();
 				final Object[] array = (Object[]) Array.newInstance(elementClass, in.length);
 				for (int i = 0; i < in.length; i++) {
-					array[i] = elementConverter.convert(in[i]);
+					array[i] = elementConverter.deserialize(in[i]);
 				}
 				return new GenericArrayData(array);
 			};
@@ -84,8 +91,8 @@ private JdbcFieldConverter createArrayConverter(ArrayType arrayType) {
 	}
 
 	// Have its own method so that Postgres can support primitives that super class doesn't support in the future
-	private JdbcFieldConverter createPrimitiveConverter(LogicalType type) {
-		return super.createConverter(type);
+	private JdbcDeserializationConverter createPrimitiveConverter(LogicalType type) {
+		return super.createNullableInternalConverter(type);
 	}
 
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToJdbcConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToJdbcConverter.java
deleted file mode 100644
index 3ada591df024d..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/PostgresToJdbcConverter.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.table.types.logical.LogicalType;
-
-/**
- * Flink internal data structure to JDBC object converter for Postgres.
- */
-public class PostgresToJdbcConverter extends AbstractRowToJdbcConverter{
-	public PostgresToJdbcConverter(LogicalType[] externalTypes) {
-		super(externalTypes);
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/RowToJdbcConverter.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/RowToJdbcConverter.java
deleted file mode 100644
index b8c52fb02dd3c..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/converter/RowToJdbcConverter.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.internal.converter;
-
-import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.types.logical.LogicalType;
-
-import java.io.Serializable;
-import java.sql.PreparedStatement;
-import java.sql.SQLException;
-
-/**
- * Converter that converts Flink internal data structure {@link RowData} to JDBC object.
- */
-public interface RowToJdbcConverter extends Serializable {
-	/**
-	 * Convert data retrieved from  internal RowData to JDBC Object.
-	 *
-	 * @param rowData The given internal {@link RowData}.
-	 * @param statement The statement to be filled.
-	 * @return The filled statement.
-	 */
-	PreparedStatement toExternal(RowData rowData, PreparedStatement statement) throws SQLException;
-
-	/**
-	 * Runtime converter that convert {@link RowData} field to Java object and fill into the {@link PreparedStatement}.
-	 */
-	@FunctionalInterface
-	interface RowFieldConverter extends Serializable {
-		void convert(PreparedStatement statement, int index, LogicalType logicalType, RowData rowData) throws SQLException;
-	}
-}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
index 5b826a65a17cc..fac2e79a65245 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/internal/options/JdbcLookupOptions.java
@@ -32,7 +32,7 @@ public class JdbcLookupOptions implements Serializable {
 	private final long cacheExpireMs;
 	private final int maxRetryTimes;
 
-	private JdbcLookupOptions(long cacheMaxSize, long cacheExpireMs, int maxRetryTimes) {
+	public JdbcLookupOptions(long cacheMaxSize, long cacheExpireMs, int maxRetryTimes) {
 		this.cacheMaxSize = cacheMaxSize;
 		this.cacheExpireMs = cacheExpireMs;
 		this.maxRetryTimes = maxRetryTimes;
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
index c5eef5a548f57..7b002241661bf 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSink.java
@@ -23,17 +23,17 @@
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
+import org.apache.flink.table.api.TableSchema;
 import org.apache.flink.table.connector.ChangelogMode;
 import org.apache.flink.table.connector.sink.DynamicTableSink;
 import org.apache.flink.table.connector.sink.OutputFormatProvider;
 import org.apache.flink.table.data.RowData;
-import org.apache.flink.table.types.DataType;
 import org.apache.flink.types.RowKind;
 
-import java.util.Arrays;
 import java.util.Objects;
 
-import static org.apache.flink.connector.jdbc.table.JdbcDynamicOutputFormat.DynamicOutputFormatBuilder;
+import static org.apache.flink.connector.jdbc.table.JdbcRowDataOutputFormat.DynamicOutputFormatBuilder;
+import static org.apache.flink.util.Preconditions.checkState;
 
 /**
  * A {@link DynamicTableSink} for JDBC.
@@ -41,23 +41,27 @@
 @Internal
 public class JdbcDynamicTableSink implements DynamicTableSink {
 
-	private static final String name = "JdbcTableSink";
 	private final JdbcOptions jdbcOptions;
 	private final JdbcExecutionOptions executionOptions;
 	private final JdbcDmlOptions dmlOptions;
-	private final DataType rowDataType;
-	private final DataType[] fieldTypes;
+	private final TableSchema tableSchema;
+	private final String dialectName;
 
-	public JdbcDynamicTableSink(JdbcOptions jdbcOptions, JdbcExecutionOptions executionOptions, JdbcDmlOptions dmlOptions, DataType rowDataType, DataType[] fieldTypes) {
+	public JdbcDynamicTableSink(
+			JdbcOptions jdbcOptions,
+			JdbcExecutionOptions executionOptions,
+			JdbcDmlOptions dmlOptions,
+			TableSchema tableSchema) {
 		this.jdbcOptions = jdbcOptions;
 		this.executionOptions = executionOptions;
 		this.dmlOptions = dmlOptions;
-		this.rowDataType = rowDataType;
-		this.fieldTypes = fieldTypes;
+		this.tableSchema = tableSchema;
+		this.dialectName = dmlOptions.getDialect().dialectName();
 	}
 
 	@Override
 	public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {
+		validatePrimaryKey(requestedMode);
 		return ChangelogMode.newBuilder()
 			.addContainedKind(RowKind.INSERT)
 			.addContainedKind(RowKind.DELETE)
@@ -65,27 +69,34 @@ public ChangelogMode getChangelogMode(ChangelogMode requestedMode) {
 			.build();
 	}
 
+	private void validatePrimaryKey(ChangelogMode requestedMode) {
+		checkState(ChangelogMode.insertOnly().equals(requestedMode) || dmlOptions.getKeyFields().isPresent(),
+			"please declare primary key for sink table when query contains update/delete record.");
+	}
+
 	@Override
+	@SuppressWarnings("unchecked")
 	public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {
-		final TypeInformation<RowData> rowDataTypeInformation = (TypeInformation<RowData>) context.createTypeInformation(rowDataType);
-		final DynamicOutputFormatBuilder builder = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder();
+		final TypeInformation<RowData> rowDataTypeInformation = (TypeInformation<RowData>) context
+			.createTypeInformation(tableSchema.toRowDataType());
+		final DynamicOutputFormatBuilder builder = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder();
 
 		builder.setJdbcOptions(jdbcOptions);
 		builder.setJdbcDmlOptions(dmlOptions);
 		builder.setJdbcExecutionOptions(executionOptions);
 		builder.setRowDataTypeInfo(rowDataTypeInformation);
-		builder.setFieldDataTypes(fieldTypes);
+		builder.setFieldDataTypes(tableSchema.getFieldDataTypes());
 		return OutputFormatProvider.of(builder.build());
 	}
 
 	@Override
 	public DynamicTableSink copy() {
-		return new JdbcDynamicTableSink(jdbcOptions, executionOptions, dmlOptions, rowDataType, fieldTypes);
+		return new JdbcDynamicTableSink(jdbcOptions, executionOptions, dmlOptions, tableSchema);
 	}
 
 	@Override
 	public String asSummaryString() {
-		return name;
+		return "JDBC:" + dialectName;
 	}
 
 	@Override
@@ -100,14 +111,12 @@ public boolean equals(Object o) {
 		return Objects.equals(jdbcOptions, that.jdbcOptions) &&
 			Objects.equals(executionOptions, that.executionOptions) &&
 			Objects.equals(dmlOptions, that.dmlOptions) &&
-			Objects.equals(rowDataType, that.rowDataType) &&
-			Arrays.equals(fieldTypes, that.fieldTypes);
+			Objects.equals(tableSchema, that.tableSchema) &&
+			Objects.equals(dialectName, that.dialectName);
 	}
 
 	@Override
 	public int hashCode() {
-		int result = Objects.hash(jdbcOptions, executionOptions, dmlOptions, rowDataType);
-		result = 31 * result + Arrays.hashCode(fieldTypes);
-		return result;
+		return Objects.hash(jdbcOptions, executionOptions, dmlOptions, tableSchema, dialectName);
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java
index 4a5db2a48381d..5060ee37d558c 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSource.java
@@ -19,7 +19,7 @@
 package org.apache.flink.connector.jdbc.table;
 
 import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.java.typeutils.RowTypeInfo;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
@@ -32,39 +32,40 @@
 import org.apache.flink.table.connector.source.LookupTableSource;
 import org.apache.flink.table.connector.source.ScanTableSource;
 import org.apache.flink.table.connector.source.TableFunctionProvider;
+import org.apache.flink.table.data.RowData;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.util.Preconditions;
 
 import java.util.Arrays;
 import java.util.Objects;
 
-import static org.apache.flink.table.types.utils.TypeConversions.fromDataTypeToLegacyInfo;
-
 /**
  * A {@link DynamicTableSource} for JDBC.
  */
 @Internal
 public class JdbcDynamicTableSource implements ScanTableSource, LookupTableSource {
 
-	private static final String name = "JdbcTableSource";
 	private final JdbcOptions options;
 	private final JdbcReadOptions readOptions;
 	private final JdbcLookupOptions lookupOptions;
 	private final TableSchema schema;
 	private final int[] selectFields;
+	private final String dialectName;
 
 	public JdbcDynamicTableSource(
-		JdbcOptions options,
-		JdbcReadOptions readOptions,
-		JdbcLookupOptions lookupOptions,
-		TableSchema schema,
-		int[] selectFields) {
+			JdbcOptions options,
+			JdbcReadOptions readOptions,
+			JdbcLookupOptions lookupOptions,
+			TableSchema schema,
+			int[] selectFields) {
 		this.options = options;
 		this.readOptions = readOptions;
 		this.lookupOptions = lookupOptions;
 		this.schema = schema;
 		this.selectFields = selectFields;
+		this.dialectName = options.getDialect().dialectName();
 	}
 
 	@Override
@@ -72,35 +73,37 @@ public LookupRuntimeProvider getLookupRuntimeProvider(LookupTableSource.Context
 		// JDBC only support non-nested look up keys
 		String[] keyNames = new String[context.getKeys().length];
 		for (int i = 0; i < keyNames.length; i++) {
-			int index = context.getKeys()[i][0];
-			keyNames[i] = schema.getFieldNames()[index];
+			int[] innerKeyArr = context.getKeys()[i];
+			Preconditions.checkArgument(innerKeyArr.length == 1,
+				"JDBC only support non-nested look up keys");
+			keyNames[i] = schema.getFieldNames()[innerKeyArr[0]];
 		}
-		return TableFunctionProvider.of(JdbcDynamicLookupFunction.builder()
-			.setFieldNames(schema.getFieldNames())
-			.setFieldTypes(schema.getFieldDataTypes())
-			.setKeyNames(keyNames)
-			.setOptions(options)
-			.setLookupOptions(lookupOptions)
-			.build());
+		final RowType rowType = (RowType) schema.toRowDataType().getLogicalType();
+
+		return TableFunctionProvider.of(new JdbcRowDataLookupFunction(
+			options,
+			lookupOptions,
+			schema.getFieldNames(),
+			schema.getFieldDataTypes(),
+			keyNames,
+			rowType));
 	}
 
 	@Override
+	@SuppressWarnings("unchecked")
 	public ScanRuntimeProvider getScanRuntimeProvider(ScanTableSource.Context runtimeProviderContext) {
-		final DataType rowDataType = schema.toPhysicalRowDataType();
-		final RowTypeInfo rowTypeInfo = (RowTypeInfo) fromDataTypeToLegacyInfo(rowDataType);
-		final JdbcDynamicInputFormat.Builder builder = JdbcDynamicInputFormat.builder()
+		final JdbcRowDataInputFormat.Builder builder = JdbcRowDataInputFormat.builder()
 			.setDrivername(options.getDriverName())
 			.setDBUrl(options.getDbURL())
 			.setUsername(options.getUsername().orElse(null))
-			.setPassword(options.getPassword().orElse(null))
-			.setRowTypeInfo(new RowTypeInfo(rowTypeInfo.getFieldTypes(), rowTypeInfo.getFieldNames()));
+			.setPassword(options.getPassword().orElse(null));
 
 		if (readOptions.getFetchSize() != 0) {
 			builder.setFetchSize(readOptions.getFetchSize());
 		}
 		final JdbcDialect dialect = options.getDialect();
 		String query = dialect.getSelectFromStatement(
-			options.getTableName(), rowTypeInfo.getFieldNames(), new String[0]);
+			options.getTableName(), schema.getFieldNames(), new String[0]);
 		if (readOptions.getPartitionColumnName().isPresent()) {
 			long lowerBound = readOptions.getPartitionLowerBound().get();
 			long upperBound = readOptions.getPartitionUpperBound().get();
@@ -117,7 +120,9 @@ public ScanRuntimeProvider getScanRuntimeProvider(ScanTableSource.Context runtim
 				.map(DataType::getLogicalType)
 				.toArray(LogicalType[]::new),
 			schema.getFieldNames());
-		builder.setRowConverter(dialect.getInputConverter(rowType));
+		builder.setRowConverter(dialect.getRowConverter(rowType));
+		builder.setRowDataTypeInfo((TypeInformation<RowData>) runtimeProviderContext
+			.createTypeInformation(schema.toRowDataType()));
 
 		return InputFormatProvider.of(builder.build());
 	}
@@ -134,7 +139,7 @@ public DynamicTableSource copy() {
 
 	@Override
 	public String asSummaryString() {
-		return name;
+		return "JDBC:" + dialectName;
 	}
 
 	@Override
@@ -150,12 +155,13 @@ public boolean equals(Object o) {
 			Objects.equals(readOptions, that.readOptions) &&
 			Objects.equals(lookupOptions, that.lookupOptions) &&
 			Objects.equals(schema, that.schema) &&
-			Arrays.equals(selectFields, that.selectFields);
+			Arrays.equals(selectFields, that.selectFields) &&
+			Objects.equals(dialectName, that.dialectName);
 	}
 
 	@Override
 	public int hashCode() {
-		int result = Objects.hash(options, readOptions, lookupOptions, schema);
+		int result = Objects.hash(options, readOptions, lookupOptions, schema, dialectName);
 		result = 31 * result + Arrays.hashCode(selectFields);
 		return result;
 	}
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java
index e49cf6cc68050..19fbd0546e9c7 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceSinkFactory.java
@@ -23,6 +23,7 @@
 import org.apache.flink.configuration.ConfigOptions;
 import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
+import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
@@ -34,14 +35,17 @@
 import org.apache.flink.table.factories.DynamicTableSinkFactory;
 import org.apache.flink.table.factories.DynamicTableSourceFactory;
 import org.apache.flink.table.factories.FactoryUtil;
-import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.utils.TableSchemaUtils;
+import org.apache.flink.util.Preconditions;
 
 import java.time.Duration;
+import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Optional;
 import java.util.Set;
 
+import static org.apache.flink.util.Preconditions.checkState;
+
 /**
  * Factory for creating configured instances of {@link JdbcDynamicTableSource}
  * and {@link JdbcDynamicTableSink}.
@@ -49,64 +53,60 @@
 @Internal
 public class JdbcDynamicTableSourceSinkFactory implements DynamicTableSourceFactory, DynamicTableSinkFactory {
 
-	public static final ConfigOption<String> IDENTIFIER = ConfigOptions
-		.key("connector")
-		.stringType()
-		.defaultValue("jdbc")
-		.withDescription("-- required: specify this table type is jdbc.");
+	public static final String IDENTIFIER = "jdbc";
 	public static final ConfigOption<String> URL = ConfigOptions
 		.key("url")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- required: the jdbc database url.");
-	public static final ConfigOption<String> TABLE = ConfigOptions
-		.key("table")
+		.withDescription("the jdbc database url.");
+	public static final ConfigOption<String> TABLE_NAME = ConfigOptions
+		.key("table-name")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- required: the jdbc table name.");
+		.withDescription("the jdbc table name.");
 	public static final ConfigOption<String> USERNAME = ConfigOptions
 		.key("username")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- optional: the jdbc user name.");
+		.withDescription("the jdbc user name.");
 	public static final ConfigOption<String> PASSWORD = ConfigOptions
 		.key("password")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- optional: the jdbc password.");
+		.withDescription("the jdbc password.");
 	private static final ConfigOption<String> DRIVER = ConfigOptions
 		.key("driver")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- optional: the class name of the JDBC driver to use to connect to this URL. " +
+		.withDescription("the class name of the JDBC driver to use to connect to this URL. " +
 			"If not set, it will automatically be derived from the URL.");
 
 	// read config options
-	private static final ConfigOption<String> READ_PARTITION_COLUMN = ConfigOptions
-		.key("read.partition.column")
+	private static final ConfigOption<String> SCAN_PARTITION_COLUMN = ConfigOptions
+		.key("scan.partition.column")
 		.stringType()
 		.noDefaultValue()
-		.withDescription("-- optional: the column name used for partitioning the input.");
-	private static final ConfigOption<Integer> READ_PARTITION_NUM = ConfigOptions
-		.key("read.partition.num")
+		.withDescription("the column name used for partitioning the input.");
+	private static final ConfigOption<Integer> SCAN_PARTITION_NUM = ConfigOptions
+		.key("scan.partition.num")
 		.intType()
 		.noDefaultValue()
-		.withDescription("-- optional: the number of partitions.");
-	private static final ConfigOption<Long> READ_PARTITION_LOWER_BOUND = ConfigOptions
-		.key("read.partition.lower-bound")
+		.withDescription("the number of partitions.");
+	private static final ConfigOption<Long> SCAN_PARTITION_LOWER_BOUND = ConfigOptions
+		.key("scan.partition.lower-bound")
 		.longType()
 		.noDefaultValue()
-		.withDescription("-- optional: the smallest value of the first partition.");
-	private static final ConfigOption<Long> READ_PARTITION_UPPER_BOUND = ConfigOptions
-		.key("read.partition.upper-bound")
+		.withDescription("the smallest value of the first partition.");
+	private static final ConfigOption<Long> SCAN_PARTITION_UPPER_BOUND = ConfigOptions
+		.key("scan.partition.upper-bound")
 		.longType()
 		.noDefaultValue()
-		.withDescription("-- optional: the largest value of the last partition.");
-	private static final ConfigOption<Integer> READ_FETCH_SIZE = ConfigOptions
-		.key("read.fetch-size")
+		.withDescription("the largest value of the last partition.");
+	private static final ConfigOption<Integer> SCAN_FETCH_SIZE = ConfigOptions
+		.key("scan.fetch-size")
 		.intType()
 		.defaultValue(0)
-		.withDescription("-- optional, Gives the reader a hint as to the number of rows that should be fetched, from" +
+		.withDescription("gives the reader a hint as to the number of rows that should be fetched, from" +
 			" the database when reading per round trip. If the value specified is zero, then the hint is ignored. The" +
 			" default value is zero.");
 
@@ -115,63 +115,65 @@ public class JdbcDynamicTableSourceSinkFactory implements DynamicTableSourceFact
 		.key("lookup.cache.max-rows")
 		.longType()
 		.defaultValue(-1L)
-		.withDescription("-- optional, max number of rows of lookup cache, over this value, the oldest rows will " +
+		.withDescription("the max number of rows of lookup cache, over this value, the oldest rows will " +
 			"be eliminated. \"cache.max-rows\" and \"cache.ttl\" options must all be specified if any of them is " +
 			"specified. Cache is not enabled as default.");
 	private static final ConfigOption<Duration> LOOKUP_CACHE_TTL = ConfigOptions
 		.key("lookup.cache.ttl")
 		.durationType()
 		.defaultValue(Duration.ofSeconds(10))
-		.withDescription("-- optional, the cache time to live.");
+		.withDescription("the cache time to live.");
 	private static final ConfigOption<Integer> LOOKUP_MAX_RETRIES = ConfigOptions
 		.key("lookup.max-retries")
 		.intType()
 		.defaultValue(3)
-		.withDescription("-- optional, max retry times if lookup database failed.");
+		.withDescription("the max retry times if lookup database failed.");
 
 	// write config options
-	private static final ConfigOption<Integer> WRITE_FLUSH_MAX_ROWS = ConfigOptions
-		.key("write.flush.max-rows")
+	private static final ConfigOption<Integer> SINK_FLUSH_MAX_ROWS = ConfigOptions
+		.key("sink.flush.max-rows")
 		.intType()
 		.defaultValue(5000)
-		.withDescription("-- optional, flush max size (includes all append, upsert and delete records), over this number" +
+		.withDescription("the flush max size (includes all append, upsert and delete records), over this number" +
 			" of records, will flush data. The default value is 5000.");
-	private static final ConfigOption<Long> WRITE_FLUSH_INTERVAL = ConfigOptions
-		.key("write.flush.interval")
+	private static final ConfigOption<Long> SINK_FLUSH_INTERVAL = ConfigOptions
+		.key("sink.flush.interval")
 		.longType()
 		.defaultValue(0L)
-		.withDescription("-- optional, flush interval mills, over this time, asynchronous threads will flush data. The " +
+		.withDescription("the flush interval mills, over this time, asynchronous threads will flush data. The " +
 			"default value is 0, which means no asynchronous flush thread will be scheduled.");
-	private static final ConfigOption<Integer> WRITE_MAX_RETRIES = ConfigOptions
-		.key("write.max-retries")
+	private static final ConfigOption<Integer> SINK_MAX_RETRIES = ConfigOptions
+		.key("sink.max-retries")
 		.intType()
 		.defaultValue(3)
-		.withDescription("-- optional, max retry times if writing records to database failed.");
+		.withDescription("the max retry times if writing records to database failed.");
 
 	@Override
 	public DynamicTableSink createDynamicTableSink(Context context) {
 		final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);
+		final ReadableConfig config = helper.getOptions();
+
 		helper.validate();
-		final JdbcOptions jdbcOptions = getJdbcOptions(helper.getOptions());
-		final DataType rowDataType = context.getCatalogTable().getSchema().toPhysicalRowDataType();
-		final TableSchema formatSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
-		final DataType[] fieldDataTypes = formatSchema.getFieldDataTypes();
+		validateConfigOptions(config);
+		JdbcOptions jdbcOptions = getJdbcOptions(config);
+		TableSchema physicalSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
 
 		return new JdbcDynamicTableSink(
 			jdbcOptions,
-			getJdbcExecutionOptions(helper.getOptions()),
+			getJdbcExecutionOptions(config),
 			getJdbcDmlOptions(jdbcOptions, context.getCatalogTable().getSchema()),
-			rowDataType,
-			fieldDataTypes);
+			physicalSchema);
 	}
 
 	@Override
 	public DynamicTableSource createDynamicTableSource(Context context) {
 		final FactoryUtil.TableFactoryHelper helper = FactoryUtil.createTableFactoryHelper(this, context);
-		helper.validate();
-		TableSchema formatSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
+		final ReadableConfig config = helper.getOptions();
 
-		int[] selectFields = new int[formatSchema.getFieldNames().length];
+		helper.validate();
+		validateConfigOptions(config);
+		TableSchema physicalSchema = TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());
+		int[] selectFields = new int[physicalSchema.getFieldNames().length];
 		for (int i = 0; i < selectFields.length; i++) {
 			selectFields[i] = i;
 		}
@@ -179,7 +181,7 @@ public DynamicTableSource createDynamicTableSource(Context context) {
 			getJdbcOptions(helper.getOptions()),
 			getJdbcReadOptions(helper.getOptions()),
 			getJdbcLookupOptions(helper.getOptions()),
-			formatSchema,
+			physicalSchema,
 			selectFields);
 	}
 
@@ -187,7 +189,7 @@ private JdbcOptions getJdbcOptions(ReadableConfig readableConfig) {
 		final String url = readableConfig.get(URL);
 		final JdbcOptions.Builder builder = JdbcOptions.builder()
 			.setDBUrl(url)
-			.setTableName(readableConfig.get(TABLE))
+			.setTableName(readableConfig.get(TABLE_NAME))
 			.setDialect(JdbcDialects.get(url).get());
 
 		readableConfig.getOptional(DRIVER).ifPresent(builder::setDriverName);
@@ -197,41 +199,30 @@ private JdbcOptions getJdbcOptions(ReadableConfig readableConfig) {
 	}
 
 	private JdbcReadOptions getJdbcReadOptions(ReadableConfig readableConfig) {
-		final Optional<String> partitionColumnName = readableConfig.getOptional(READ_PARTITION_COLUMN);
-		final Optional<Long> partitionLower = readableConfig.getOptional(READ_PARTITION_LOWER_BOUND);
-		final Optional<Long> partitionUpper = readableConfig.getOptional(READ_PARTITION_UPPER_BOUND);
-		final Optional<Integer> numPartitions = readableConfig.getOptional(READ_PARTITION_NUM);
-
+		final Optional<String> partitionColumnName = readableConfig.getOptional(SCAN_PARTITION_COLUMN);
 		final JdbcReadOptions.Builder builder = JdbcReadOptions.builder();
 		if (partitionColumnName.isPresent()) {
 			builder.setPartitionColumnName(partitionColumnName.get());
-			builder.setPartitionLowerBound(partitionLower.get());
-			builder.setPartitionUpperBound(partitionUpper.get());
-			builder.setNumPartitions(numPartitions.get());
+			builder.setPartitionLowerBound(readableConfig.get(SCAN_PARTITION_LOWER_BOUND));
+			builder.setPartitionUpperBound(readableConfig.get(SCAN_PARTITION_UPPER_BOUND));
+			builder.setNumPartitions(readableConfig.get(SCAN_PARTITION_NUM));
 		}
-		readableConfig.getOptional(READ_FETCH_SIZE).ifPresent(builder::setFetchSize);
+		readableConfig.getOptional(SCAN_FETCH_SIZE).ifPresent(builder::setFetchSize);
 		return builder.build();
 	}
 
 	private JdbcLookupOptions getJdbcLookupOptions(ReadableConfig readableConfig) {
-		final JdbcLookupOptions.Builder builder = JdbcLookupOptions.builder();
-
-		readableConfig.getOptional(LOOKUP_CACHE_MAX_ROWS).ifPresent(builder::setCacheMaxSize);
-		readableConfig.getOptional(LOOKUP_CACHE_TTL).ifPresent(
-			s -> builder.setCacheExpireMs(s.toMillis()));
-		readableConfig.getOptional(LOOKUP_MAX_RETRIES).ifPresent(builder::setMaxRetryTimes);
-
-		return builder.build();
+		return new JdbcLookupOptions(
+			readableConfig.get(LOOKUP_CACHE_MAX_ROWS),
+			readableConfig.get(LOOKUP_CACHE_TTL).toMillis(),
+			readableConfig.get(LOOKUP_MAX_RETRIES));
 	}
 
-	private JdbcExecutionOptions getJdbcExecutionOptions(ReadableConfig readableConfig) {
+	private JdbcExecutionOptions getJdbcExecutionOptions(ReadableConfig config) {
 		final JdbcExecutionOptions.Builder builder = new JdbcExecutionOptions.Builder();
-		readableConfig.getOptional(WRITE_FLUSH_MAX_ROWS)
-			.ifPresent(builder::withBatchSize);
-		readableConfig.getOptional(WRITE_FLUSH_INTERVAL)
-			.ifPresent(builder::withBatchIntervalMs);
-		readableConfig.getOptional(WRITE_MAX_RETRIES)
-			.ifPresent(builder::withMaxRetries);
+		builder.withBatchSize(config.get(SINK_FLUSH_MAX_ROWS));
+		builder.withBatchIntervalMs(config.get(SINK_FLUSH_INTERVAL));
+		builder.withMaxRetries(config.get(SINK_MAX_RETRIES));
 		return builder.build();
 	}
 
@@ -250,14 +241,14 @@ private JdbcDmlOptions getJdbcDmlOptions(JdbcOptions jdbcOptions, TableSchema sc
 
 	@Override
 	public String factoryIdentifier() {
-		return IDENTIFIER.defaultValue();
+		return IDENTIFIER;
 	}
 
 	@Override
 	public Set<ConfigOption<?>> requiredOptions() {
 		Set<ConfigOption<?>> requiredOptions = new HashSet<>();
 		requiredOptions.add(URL);
-		requiredOptions.add(TABLE);
+		requiredOptions.add(TABLE_NAME);
 		return requiredOptions;
 	}
 
@@ -267,17 +258,63 @@ public Set<ConfigOption<?>> optionalOptions() {
 		optionalOptions.add(DRIVER);
 		optionalOptions.add(USERNAME);
 		optionalOptions.add(PASSWORD);
-		optionalOptions.add(READ_PARTITION_COLUMN);
-		optionalOptions.add(READ_PARTITION_LOWER_BOUND);
-		optionalOptions.add(READ_PARTITION_UPPER_BOUND);
-		optionalOptions.add(READ_PARTITION_NUM);
-		optionalOptions.add(READ_FETCH_SIZE);
+		optionalOptions.add(SCAN_PARTITION_COLUMN);
+		optionalOptions.add(SCAN_PARTITION_LOWER_BOUND);
+		optionalOptions.add(SCAN_PARTITION_UPPER_BOUND);
+		optionalOptions.add(SCAN_PARTITION_NUM);
+		optionalOptions.add(SCAN_FETCH_SIZE);
 		optionalOptions.add(LOOKUP_CACHE_MAX_ROWS);
 		optionalOptions.add(LOOKUP_CACHE_TTL);
 		optionalOptions.add(LOOKUP_MAX_RETRIES);
-		optionalOptions.add(WRITE_FLUSH_MAX_ROWS);
-		optionalOptions.add(WRITE_FLUSH_INTERVAL);
-		optionalOptions.add(WRITE_MAX_RETRIES);
+		optionalOptions.add(SINK_FLUSH_MAX_ROWS);
+		optionalOptions.add(SINK_FLUSH_INTERVAL);
+		optionalOptions.add(SINK_MAX_RETRIES);
 		return optionalOptions;
 	}
+
+	private void validateConfigOptions(ReadableConfig config) {
+		config.getOptional(URL).orElseThrow(() -> new IllegalArgumentException(
+			String.format("Could not find required option: %s", URL.key())));
+		config.getOptional(TABLE_NAME).orElseThrow(() -> new IllegalArgumentException(
+			String.format("Could not find required option: %s", TABLE_NAME.key())));
+
+		String jdbcUrl = config.get(URL);
+		final Optional<JdbcDialect> dialect = JdbcDialects.get(jdbcUrl);
+		checkState(dialect.isPresent(), "Cannot handle such jdbc url: " + jdbcUrl);
+
+		if (config.getOptional(USERNAME).isPresent()) {
+			checkState(config.getOptional(PASSWORD).isPresent(),
+				"Database username must be provided when database password is provided");
+		}
+
+		checkAllOrNone(config, new ConfigOption[]{
+			SCAN_PARTITION_COLUMN,
+			SCAN_PARTITION_NUM,
+			SCAN_PARTITION_LOWER_BOUND,
+			SCAN_PARTITION_UPPER_BOUND
+		});
+
+		if (config.getOptional(SCAN_PARTITION_LOWER_BOUND).isPresent() &&
+			config.getOptional(SCAN_PARTITION_UPPER_BOUND).isPresent()) {
+			checkState(config.get(SCAN_PARTITION_LOWER_BOUND) <= config.get(SCAN_PARTITION_UPPER_BOUND),
+				String.format("%s must not be larger than %s", SCAN_PARTITION_LOWER_BOUND.key(), SCAN_PARTITION_UPPER_BOUND.key()));
+		}
+
+		checkAllOrNone(config, new ConfigOption[]{
+			LOOKUP_CACHE_MAX_ROWS,
+			LOOKUP_CACHE_TTL
+		});
+	}
+
+	private void checkAllOrNone(ReadableConfig config, ConfigOption<?>[] configOptions) {
+		int presentCount = 0;
+		for (ConfigOption configOption : configOptions) {
+			if (config.getOptional(configOption).isPresent()) {
+				presentCount++;
+			}
+		}
+		String[] propertyNames = Arrays.stream(configOptions).map(ConfigOption::key).toArray(String[]::new);
+		Preconditions.checkArgument(configOptions.length == presentCount || presentCount == 0,
+			"Either all or none of the following options should be provided:\n" + String.join("\n", propertyNames));
+	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java
similarity index 91%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormat.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java
index 35e7dc1d0dff4..06820d1007aa7 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormat.java
@@ -24,13 +24,11 @@
 import org.apache.flink.api.common.io.RichInputFormat;
 import org.apache.flink.api.common.io.statistics.BaseStatistics;
 import org.apache.flink.api.common.typeinfo.TypeInformation;
-import org.apache.flink.api.java.typeutils.GenericTypeInfo;
 import org.apache.flink.api.java.typeutils.ResultTypeQueryable;
-import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
 import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
-import org.apache.flink.connector.jdbc.internal.converter.JdbcToRowConverter;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.split.JdbcParameterValuesProvider;
 import org.apache.flink.core.io.GenericInputSplit;
 import org.apache.flink.core.io.InputSplit;
@@ -57,10 +55,10 @@
  * InputFormat for {@link JdbcDynamicTableSource}.
  */
 @Internal
-public class JdbcDynamicInputFormat extends RichInputFormat<RowData, InputSplit> implements ResultTypeQueryable<RowData> {
+public class JdbcRowDataInputFormat extends RichInputFormat<RowData, InputSplit> implements ResultTypeQueryable<RowData> {
 
 	private static final long serialVersionUID = 1L;
-	private static final Logger LOG = LoggerFactory.getLogger(JdbcDynamicInputFormat.class);
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataInputFormat.class);
 
 	private JdbcConnectionOptions connectionOptions;
 	private int fetchSize;
@@ -69,22 +67,24 @@ public class JdbcDynamicInputFormat extends RichInputFormat<RowData, InputSplit>
 	private String queryTemplate;
 	private int resultSetType;
 	private int resultSetConcurrency;
-	private JdbcToRowConverter rowConverter;
+	private JdbcRowConverter rowConverter;
+	private TypeInformation<RowData> rowDataTypeInfo;
 
 	private transient Connection dbConn;
 	private transient PreparedStatement statement;
 	private transient ResultSet resultSet;
 	private transient boolean hasNext;
 
-	private JdbcDynamicInputFormat(
-		JdbcConnectionOptions connectionOptions,
-		int fetchSize,
-		Boolean autoCommit,
-		Object[][] parameterValues,
-		String queryTemplate,
-		int resultSetType,
-		int resultSetConcurrency,
-		JdbcToRowConverter rowConverter) {
+	private JdbcRowDataInputFormat(
+			JdbcConnectionOptions connectionOptions,
+			int fetchSize,
+			Boolean autoCommit,
+			Object[][] parameterValues,
+			String queryTemplate,
+			int resultSetType,
+			int resultSetConcurrency,
+			JdbcRowConverter rowConverter,
+			TypeInformation<RowData> rowDataTypeInfo) {
 		this.connectionOptions = connectionOptions;
 		this.fetchSize = fetchSize;
 		this.autoCommit = autoCommit;
@@ -93,6 +93,7 @@ private JdbcDynamicInputFormat(
 		this.resultSetType = resultSetType;
 		this.resultSetConcurrency = resultSetConcurrency;
 		this.rowConverter = rowConverter;
+		this.rowDataTypeInfo = rowDataTypeInfo;
 	}
 
 	@Override
@@ -228,7 +229,7 @@ public void close() throws IOException {
 
 	@Override
 	public TypeInformation<RowData> getProducedType() {
-		return new GenericTypeInfo<RowData>(RowData.class);
+		return rowDataTypeInfo;
 	}
 
 	/**
@@ -298,7 +299,7 @@ public static Builder builder() {
 	}
 
 	/**
-	 * Builder for {@link JdbcDynamicInputFormat}.
+	 * Builder for {@link JdbcRowDataInputFormat}.
 	 */
 	public static class Builder {
 		private JdbcConnectionOptions.JdbcConnectionOptionsBuilder connOptionsBuilder;
@@ -306,8 +307,8 @@ public static class Builder {
 		private Boolean autoCommit;
 		private Object[][] parameterValues;
 		private String queryTemplate;
-		private RowTypeInfo rowTypeInfo;
-		private JdbcToRowConverter rowConverter;
+		private JdbcRowConverter rowConverter;
+		private TypeInformation<RowData> rowDataTypeInfo;
 		private int resultSetType = ResultSet.TYPE_FORWARD_ONLY;
 		private int resultSetConcurrency = ResultSet.CONCUR_READ_ONLY;
 
@@ -345,12 +346,12 @@ public Builder setParametersProvider(JdbcParameterValuesProvider parameterValues
 			return this;
 		}
 
-		public Builder setRowTypeInfo(RowTypeInfo rowTypeInfo) {
-			this.rowTypeInfo = rowTypeInfo;
+		public Builder setRowDataTypeInfo(TypeInformation<RowData> rowDataTypeInfo) {
+			this.rowDataTypeInfo = rowDataTypeInfo;
 			return this;
 		}
 
-		public Builder setRowConverter(JdbcToRowConverter rowConverter) {
+		public Builder setRowConverter(JdbcRowConverter rowConverter) {
 			this.rowConverter = rowConverter;
 			return this;
 		}
@@ -377,7 +378,7 @@ public Builder setResultSetConcurrency(int resultSetConcurrency) {
 			return this;
 		}
 
-		public JdbcDynamicInputFormat build() {
+		public JdbcRowDataInputFormat build() {
 			if (this.queryTemplate == null) {
 				throw new IllegalArgumentException("No query supplied");
 			}
@@ -387,7 +388,7 @@ public JdbcDynamicInputFormat build() {
 			if (this.parameterValues == null) {
 				LOG.debug("No input splitting configured (data will be read with parallelism 1).");
 			}
-			return new JdbcDynamicInputFormat(
+			return new JdbcRowDataInputFormat(
 				connOptionsBuilder.build(),
 				this.fetchSize,
 				this.autoCommit,
@@ -395,8 +396,8 @@ public JdbcDynamicInputFormat build() {
 				this.queryTemplate,
 				this.resultSetType,
 				this.resultSetConcurrency,
-				this.rowConverter
-			);
+				this.rowConverter,
+				this.rowDataTypeInfo);
 		}
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicLookupFunction.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
similarity index 73%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicLookupFunction.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
index 17ab5d5a6e0c2..1c66ec64ab559 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicLookupFunction.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataLookupFunction.java
@@ -21,6 +21,7 @@
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialect;
 import org.apache.flink.connector.jdbc.dialect.JdbcDialects;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.internal.options.JdbcLookupOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
 import org.apache.flink.connector.jdbc.utils.JdbcTypeUtil;
@@ -30,7 +31,6 @@
 import org.apache.flink.table.functions.FunctionContext;
 import org.apache.flink.table.functions.TableFunction;
 import org.apache.flink.table.types.DataType;
-import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.utils.TypeConversions;
 
@@ -58,9 +58,9 @@
  * A lookup function for {@link JdbcDynamicTableSource}.
  */
 @Internal
-public class JdbcDynamicLookupFunction extends TableFunction<RowData> {
+public class JdbcRowDataLookupFunction extends TableFunction<RowData> {
 
-	private static final Logger LOG = LoggerFactory.getLogger(JdbcDynamicLookupFunction.class);
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataLookupFunction.class);
 	private static final long serialVersionUID = 1L;
 
 	private final String query;
@@ -74,15 +74,23 @@ public class JdbcDynamicLookupFunction extends TableFunction<RowData> {
 	private final long cacheExpireMs;
 	private final int maxRetryTimes;
 	private final JdbcDialect jdbcDialect;
-	private final RowType rowType;
+	private final JdbcRowConverter jdbcRowConverter;
 
 	private transient Connection dbConn;
 	private transient PreparedStatement statement;
 	private transient Cache<RowData, List<RowData>> cache;
 
-	private JdbcDynamicLookupFunction(
-		JdbcOptions options, JdbcLookupOptions lookupOptions,
-		String[] fieldNames, DataType[] fieldTypes, String[] keyNames, RowType rowType) {
+	public JdbcRowDataLookupFunction(
+			JdbcOptions options,
+			JdbcLookupOptions lookupOptions,
+			String[] fieldNames,
+			DataType[] fieldTypes,
+			String[] keyNames,
+			RowType rowType) {
+		checkNotNull(options, "No JdbcOptions supplied.");
+		checkNotNull(fieldNames, "No fieldNames supplied.");
+		checkNotNull(fieldTypes, "No fieldTypes supplied.");
+		checkNotNull(keyNames, "No keyNames supplied.");
 		this.drivername = options.getDriverName();
 		this.dbURL = options.getDbURL();
 		this.username = options.getUsername().orElse(null);
@@ -105,11 +113,7 @@ private JdbcDynamicLookupFunction(
 			options.getTableName(), fieldNames, keyNames);
 		this.jdbcDialect = JdbcDialects.get(dbURL)
 			.orElseThrow(() -> new UnsupportedOperationException(String.format("Unknown dbUrl:%s", dbURL)));
-		this.rowType = rowType;
-	}
-
-	public static Builder builder() {
-		return new Builder();
+		this.jdbcRowConverter = jdbcDialect.getRowConverter(rowType);
 	}
 
 	@Override
@@ -128,6 +132,10 @@ public void open(FunctionContext context) throws Exception {
 		}
 	}
 
+	/**
+	 * This is a lookup method which is called by Flink framework in runtime.
+	 * @param keys lookup keys
+	 */
 	public void eval(Object... keys) {
 		RowData keyRow = GenericRowData.of(keys);
 		if (cache != null) {
@@ -179,7 +187,7 @@ public void eval(Object... keys) {
 	}
 
 	private RowData convertToRowFromResultSet(ResultSet resultSet) throws SQLException {
-		return jdbcDialect.getInputConverter(rowType).toInternal(resultSet);
+		return jdbcRowConverter.toInternal(resultSet);
 	}
 
 	private void establishConnection() throws SQLException, ClassNotFoundException {
@@ -217,77 +225,4 @@ public void close() throws IOException {
 			}
 		}
 	}
-
-	/**
-	 * Builder for a {@link JdbcDynamicLookupFunction}.
-	 */
-	public static class Builder {
-		private JdbcOptions options;
-		private JdbcLookupOptions lookupOptions;
-		protected String[] fieldNames;
-		protected DataType[] fieldTypes;
-		protected String[] keyNames;
-
-		/**
-		 * required, jdbc options.
-		 */
-		public Builder setOptions(JdbcOptions options) {
-			this.options = options;
-			return this;
-		}
-
-		/**
-		 * optional, lookup related options.
-		 */
-		public Builder setLookupOptions(JdbcLookupOptions lookupOptions) {
-			this.lookupOptions = lookupOptions;
-			return this;
-		}
-
-		/**
-		 * required, field names of this jdbc table.
-		 */
-		public Builder setFieldNames(String[] fieldNames) {
-			this.fieldNames = fieldNames;
-			return this;
-		}
-
-		/**
-		 * required, field types of this jdbc table.
-		 */
-		public Builder setFieldTypes(DataType[] fieldTypes) {
-			this.fieldTypes = fieldTypes;
-			return this;
-		}
-
-		/**
-		 * required, key names to query this jdbc table.
-		 */
-		public Builder setKeyNames(String[] keyNames) {
-			this.keyNames = keyNames;
-			return this;
-		}
-
-		/**
-		 * Finalizes the configuration and checks validity.
-		 *
-		 * @return Configured JdbcLookupFunction
-		 */
-		public JdbcDynamicLookupFunction build() {
-			checkNotNull(options, "No JdbcOptions supplied.");
-			if (lookupOptions == null) {
-				lookupOptions = JdbcLookupOptions.builder().build();
-			}
-			checkNotNull(fieldNames, "No fieldNames supplied.");
-			checkNotNull(fieldTypes, "No fieldTypes supplied.");
-			checkNotNull(keyNames, "No keyNames supplied.");
-
-			RowType rowType = RowType.of(
-				Arrays.stream(fieldTypes).map(DataType::getLogicalType)
-					.toArray(LogicalType[]::new),
-				fieldNames);
-
-			return new JdbcDynamicLookupFunction(options, lookupOptions, fieldNames, fieldTypes, keyNames, rowType);
-		}
-	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormat.java b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
similarity index 78%
rename from flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormat.java
rename to flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
index 78224d6d95ade..6d3a1794c183d 100644
--- a/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormat.java
+++ b/flink-connectors/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormat.java
@@ -28,6 +28,7 @@
 import org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat;
 import org.apache.flink.connector.jdbc.internal.connection.JdbcConnectionProvider;
 import org.apache.flink.connector.jdbc.internal.connection.SimpleJdbcConnectionProvider;
+import org.apache.flink.connector.jdbc.internal.converter.JdbcRowConverter;
 import org.apache.flink.connector.jdbc.internal.executor.InsertOrUpdateJdbcExecutor;
 import org.apache.flink.connector.jdbc.internal.executor.JdbcBatchStatementExecutor;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
@@ -35,8 +36,10 @@
 import org.apache.flink.connector.jdbc.utils.JdbcUtils;
 import org.apache.flink.table.data.GenericRowData;
 import org.apache.flink.table.data.RowData;
+import org.apache.flink.table.data.RowData.FieldGetter;
 import org.apache.flink.table.types.DataType;
 import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
 
 import org.slf4j.Logger;
@@ -47,6 +50,7 @@
 import java.util.Arrays;
 import java.util.function.Function;
 
+import static org.apache.flink.table.data.RowData.createFieldGetter;
 import static org.apache.flink.util.Preconditions.checkArgument;
 import static org.apache.flink.util.Preconditions.checkNotNull;
 
@@ -54,21 +58,39 @@
  * OutputFormat for {@link JdbcDynamicTableSource}.
  */
 @Internal
-public class JdbcDynamicOutputFormat extends JdbcBatchingOutputFormat<RowData, RowData, JdbcBatchStatementExecutor<RowData>> {
-	private static final Logger LOG = LoggerFactory.getLogger(JdbcDynamicOutputFormat.class);
+public class JdbcRowDataOutputFormat extends JdbcBatchingOutputFormat<RowData, RowData, JdbcBatchStatementExecutor<RowData>> {
+
+	private static final long serialVersionUID = 1L;
+	private static final Logger LOG = LoggerFactory.getLogger(JdbcRowDataOutputFormat.class);
 
 	private JdbcBatchStatementExecutor<RowData> deleteExecutor;
 	private final JdbcDmlOptions dmlOptions;
 	private final LogicalType[] logicalTypes;
 
-	private JdbcDynamicOutputFormat(JdbcConnectionProvider connectionProvider, JdbcDmlOptions dmlOptions, JdbcExecutionOptions batchOptions, TypeInformation<RowData> rowDataTypeInfo, LogicalType[] logicalTypes) {
-		super(connectionProvider, batchOptions, ctx -> createUpsertRowExecutor(dmlOptions, ctx, rowDataTypeInfo, logicalTypes), RecordExtractor.identity());
+	private JdbcRowDataOutputFormat(
+			JdbcConnectionProvider connectionProvider,
+			JdbcDmlOptions dmlOptions,
+			JdbcExecutionOptions batchOptions,
+			TypeInformation<RowData> rowDataTypeInfo,
+			LogicalType[] logicalTypes) {
+		super(
+			connectionProvider,
+			batchOptions,
+			ctx -> createUpsertRowExecutor(dmlOptions, ctx, rowDataTypeInfo, logicalTypes),
+			RecordExtractor.identity());
 		this.dmlOptions = dmlOptions;
 		this.logicalTypes = logicalTypes;
 	}
 
-	private JdbcDynamicOutputFormat(JdbcConnectionProvider connectionProvider, JdbcDmlOptions dmlOptions, JdbcExecutionOptions batchOptions, TypeInformation<RowData> rowDataTypeInfo, LogicalType[] logicalTypes, String sql) {
-		super(connectionProvider, batchOptions,
+	private JdbcRowDataOutputFormat(
+			JdbcConnectionProvider connectionProvider,
+			JdbcDmlOptions dmlOptions,
+			JdbcExecutionOptions batchOptions,
+			TypeInformation<RowData> rowDataTypeInfo,
+			LogicalType[] logicalTypes,
+			String sql) {
+		super(connectionProvider,
+			batchOptions,
 			ctx -> createSimpleRowDataExecutor(dmlOptions.getDialect(), sql, logicalTypes, ctx, rowDataTypeInfo),
 			RecordExtractor.identity());
 		this.dmlOptions = dmlOptions;
@@ -77,8 +99,8 @@ private JdbcDynamicOutputFormat(JdbcConnectionProvider connectionProvider, JdbcD
 
 	@Override
 	public void open(int taskNumber, int numTasks) throws IOException {
-		super.open(taskNumber, numTasks);
 		deleteExecutor = createDeleteExecutor();
+		super.open(taskNumber, numTasks);
 		try {
 			deleteExecutor.open(connection);
 		} catch (SQLException e) {
@@ -96,15 +118,18 @@ private JdbcBatchStatementExecutor<RowData> createDeleteExecutor() {
 	@Override
 	protected void addToBatch(RowData original, RowData extracted) throws SQLException {
 		switch (original.getRowKind()) {
-			case DELETE:
-				deleteExecutor.addToBatch(extracted);
-				break;
 			case INSERT:
 			case UPDATE_AFTER:
 				super.addToBatch(original, extracted);
 				break;
+			case DELETE:
+			case UPDATE_BEFORE:
+				deleteExecutor.addToBatch(extracted);
+				break;
 			default:
-				return;
+				throw new UnsupportedOperationException(
+					String.format("unknown row kind, the supported row kinds is: INSERT, UPDATE_BEFORE, UPDATE_AFTER," +
+						" DELETE, but get: %s.", original.getRowKind()));
 		}
 	}
 
@@ -130,12 +155,13 @@ protected void attemptFlush() throws SQLException {
 	}
 
 	private static JdbcBatchStatementExecutor<RowData> createKeyedRowExecutor(JdbcDialect dialect, int[] pkFields, LogicalType[] pkTypes, String sql, LogicalType[] logicalTypes) {
+		final JdbcRowConverter rowConverter = dialect.getRowConverter(RowType.of(logicalTypes));
+		final Function<RowData, RowData>  keyExtractor = createRowKeyExtractor(logicalTypes, pkFields);
 		return JdbcBatchStatementExecutor.keyed(
 			sql,
-			createRowKeyExtractor(pkFields, logicalTypes),
-			(st, record) -> dialect
-				.getOutputConverter(logicalTypes)
-				.toExternal(createRowKeyExtractor(pkFields, logicalTypes).apply(record), st));
+			keyExtractor,
+			(st, record) -> rowConverter
+				.toExternal(keyExtractor.apply(record), st));
 	}
 
 	private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(JdbcDmlOptions opt, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo, LogicalType[] logicalTypes) {
@@ -156,12 +182,16 @@ private static JdbcBatchStatementExecutor<RowData> createUpsertRowExecutor(JdbcD
 					createRowDataJdbcStatementBuilder(dialect, pkTypes),
 					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
 					createRowDataJdbcStatementBuilder(dialect, logicalTypes),
-					createRowKeyExtractor(pkFields, logicalTypes),
+					createRowKeyExtractor(logicalTypes, pkFields),
 					ctx.getExecutionConfig().isObjectReuseEnabled() ? typeSerializer::copy : r -> r));
 	}
 
-	private static Function<RowData, RowData> createRowKeyExtractor(int[] pkFields, LogicalType[] logicalTypes) {
-		return row -> getPrimaryKey(row, pkFields, logicalTypes);
+	private static Function<RowData, RowData> createRowKeyExtractor(LogicalType[] logicalTypes, int[] pkFields) {
+		final FieldGetter[] fieldGetters = new FieldGetter[pkFields.length];
+		for (int i = 0; i < pkFields.length; i++) {
+			fieldGetters[i] = createFieldGetter(logicalTypes[pkFields[i]], pkFields[i]);
+		}
+		return row -> getPrimaryKey(row, fieldGetters);
 	}
 
 	private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(JdbcDialect dialect, String sql, LogicalType[] fieldTypes, RuntimeContext ctx, TypeInformation<RowData> rowDataTypeInfo) {
@@ -177,13 +207,13 @@ private static JdbcBatchStatementExecutor<RowData> createSimpleRowDataExecutor(J
 	 * Uses {@link JdbcUtils#setRecordToStatement}
 	 */
 	private static JdbcStatementBuilder<RowData> createRowDataJdbcStatementBuilder(JdbcDialect dialect, LogicalType[] types) {
-		return (st, record) -> dialect.getOutputConverter(types).toExternal(record, st);
+		return (st, record) -> dialect.getRowConverter(RowType.of(types)).toExternal(record, st);
 	}
 
-	private static RowData getPrimaryKey(RowData row, int[] pkFields, LogicalType[] logicalTypes) {
-		GenericRowData pkRow = new GenericRowData(pkFields.length);
-		for (int i = 0; i < pkFields.length; i++) {
-			pkRow.setField(i, RowData.get(row, pkFields[i], logicalTypes[pkFields[i]]));
+	private static RowData getPrimaryKey(RowData row, FieldGetter[] fieldGetters) {
+		GenericRowData pkRow = new GenericRowData(fieldGetters.length);
+		for (int i = 0; i < fieldGetters.length; i++) {
+			pkRow.setField(i, fieldGetters[i].getFieldOrNull(row));
 		}
 		return pkRow;
 	}
@@ -193,7 +223,7 @@ public static DynamicOutputFormatBuilder dynamicOutputFormatBuilder() {
 	}
 
 	/**
-	 * Builder for {@link JdbcDynamicOutputFormat}.
+	 * Builder for {@link JdbcRowDataOutputFormat}.
 	 */
 	public static class DynamicOutputFormatBuilder {
 		private JdbcOptions jdbcOptions;
@@ -230,7 +260,7 @@ public DynamicOutputFormatBuilder setFieldDataTypes(DataType[] fieldDataTypes) {
 			return this;
 		}
 
-		public JdbcDynamicOutputFormat build() {
+		public JdbcRowDataOutputFormat build() {
 			checkNotNull(jdbcOptions, "jdbc options can not be null");
 			checkNotNull(dmlOptions, "jdbc dml options can not be null");
 			checkNotNull(executionOptions, "jdbc execution options can not be null");
@@ -240,7 +270,7 @@ public JdbcDynamicOutputFormat build() {
 				.toArray(LogicalType[]::new);
 			if (dmlOptions.getKeyFields().isPresent() && dmlOptions.getKeyFields().get().length > 0) {
 				//upsert query
-				return new JdbcDynamicOutputFormat(
+				return new JdbcRowDataOutputFormat(
 					new SimpleJdbcConnectionProvider(jdbcOptions),
 					dmlOptions,
 					executionOptions,
@@ -251,7 +281,7 @@ public JdbcDynamicOutputFormat build() {
 				final String sql = dmlOptions
 					.getDialect()
 					.getInsertIntoStatement(dmlOptions.getTableName(), dmlOptions.getFieldNames());
-				return new JdbcDynamicOutputFormat(
+				return new JdbcRowDataOutputFormat(
 					new SimpleJdbcConnectionProvider(jdbcOptions),
 					dmlOptions,
 					executionOptions,
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
index 197cf18d07ee9..1fefb6e4a9b48 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/JdbcDataTypeTest.java
@@ -103,26 +103,26 @@ public static List<TestItem> testData() {
 			createTestItem("postgresql", "ARRAY<INTEGER>"),
 
 			// Unsupported types throws errors.
-			createTestItem("derby", "BINARY", "The derby dialect doesn't support type: BINARY(1)."),
-			createTestItem("derby", "VARBINARY(10)", "The derby dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("derby", "BINARY", "The Derby dialect doesn't support type: BINARY(1)."),
+			createTestItem("derby", "VARBINARY(10)", "The Derby dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("derby", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The derby dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+					"The Derby dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
 			createTestItem("derby", "DECIMAL(38, 18)",
-					"The precision of field 'f0' is out of the DECIMAL precision range [1, 31] supported by derby dialect."),
+					"The precision of field 'f0' is out of the DECIMAL precision range [1, 31] supported by Derby dialect."),
 
-			createTestItem("mysql", "BINARY", "The mysql dialect doesn't support type: BINARY(1)."),
-			createTestItem("mysql", "VARBINARY(10)", "The mysql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("mysql", "BINARY", "The MySQL dialect doesn't support type: BINARY(1)."),
+			createTestItem("mysql", "VARBINARY(10)", "The MySQL dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("mysql", "TIMESTAMP(9) WITHOUT TIME ZONE",
-					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by mysql dialect."),
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by MySQL dialect."),
 			createTestItem("mysql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The mysql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
+					"The MySQL dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE."),
 
-			createTestItem("postgresql", "BINARY", "The postgresql dialect doesn't support type: BINARY(1)."),
-			createTestItem("postgresql", "VARBINARY(10)", "The postgresql dialect doesn't support type: VARBINARY(10)."),
+			createTestItem("postgresql", "BINARY", "The Postgres dialect doesn't support type: BINARY(1)."),
+			createTestItem("postgresql", "VARBINARY(10)", "The Postgres dialect doesn't support type: VARBINARY(10)."),
 			createTestItem("postgresql", "TIMESTAMP(9) WITHOUT TIME ZONE",
-					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by postgresql dialect."),
+					"The precision of field 'f0' is out of the TIMESTAMP precision range [1, 6] supported by Postgres dialect."),
 			createTestItem("postgresql", "TIMESTAMP(3) WITH LOCAL TIME ZONE",
-					"The postgresql dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE.")
+					"The Postgres dialect doesn't support type: TIMESTAMP(3) WITH LOCAL TIME ZONE.")
 		);
 	}
 
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
index e7f79254436e4..5591c97abb9d5 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSinkITCase.java
@@ -146,7 +146,7 @@ public void testReal() throws Exception {
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table'='" + OUTPUT_TABLE4 + "'" +
+				"  'table-name'='" + OUTPUT_TABLE4 + "'" +
 				")");
 
 		TableResult tableResult = tEnv.executeSql("INSERT INTO upsertSink SELECT CAST(1.0 as FLOAT)");
@@ -185,7 +185,7 @@ public long extractAscendingTimestamp(Tuple4<Integer, Long, String, Timestamp> e
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table'='" + OUTPUT_TABLE1 + "'" +
+				"  'table-name'='" + OUTPUT_TABLE1 + "'" +
 				")");
 
 		TableResult tableResult = tEnv.executeSql("INSERT INTO upsertSink \n" +
@@ -224,7 +224,7 @@ public void testAppend() throws Exception {
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table'='" + OUTPUT_TABLE2 + "'" +
+				"  'table-name'='" + OUTPUT_TABLE2 + "'" +
 				")");
 
 		TableResult tableResult = tEnv.executeSql(
@@ -251,7 +251,10 @@ public void testBatchSink() throws Exception {
 				") WITH ( " +
 				"'connector' = 'jdbc'," +
 				"'url'='" + DB_URL + "'," +
-				"'table' = '" + OUTPUT_TABLE3 + "'" +
+				"'table-name' = '" + OUTPUT_TABLE3 + "'," +
+				"'sink.flush.max-rows' = '2'," +
+				"'sink.flush.interval' = '3'," +
+				"'sink.max-retries' = '4'" +
 				")");
 
 		TableResult tableResult  = tEnv.executeSql("INSERT INTO USER_RESULT\n" +
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
index 3e6048ad3e43e..6f93307aaac0d 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicTableSourceITCase.java
@@ -102,7 +102,7 @@ public void testJdbcSource() throws Exception {
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table'='" + INPUT_TABLE + "'" +
+				"  'table-name'='" + INPUT_TABLE + "'" +
 				")"
 		);
 
@@ -119,7 +119,7 @@ public void testJdbcSource() throws Exception {
 	}
 
 	@Test
-	public void testJdbcLookUpSource() throws Exception {
+	public void testProject() throws Exception {
 		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
 			.useBlinkPlanner()
@@ -139,19 +139,23 @@ public void testJdbcLookUpSource() throws Exception {
 				") WITH (" +
 				"  'connector'='jdbc'," +
 				"  'url'='" + DB_URL + "'," +
-				"  'table'='" + INPUT_TABLE + "'" +
+				"  'table-name'='" + INPUT_TABLE + "'," +
+				"  'scan.partition.column'='id'," +
+				"  'scan.partition.num'='2'," +
+				"  'scan.partition.lower-bound'='0'," +
+				"  'scan.partition.upper-bound'='100'" +
 				")"
 		);
 
 		StreamITCase.clear();
-		tEnv.toAppendStream(tEnv.sqlQuery("SELECT * FROM " + INPUT_TABLE), Row.class)
+		tEnv.toAppendStream(tEnv.sqlQuery("SELECT id,timestamp6_col,decimal_col FROM " + INPUT_TABLE), Row.class)
 			.addSink(new StreamITCase.StringSink<>());
 		env.execute();
 
 		List<String> expected =
 			Arrays.asList(
-				"1,2020-01-01T15:35:00.123456,2020-01-01T15:35:00.123456789,15:35,1.175E-37,1.79769E308,100.1234",
-				"2,2020-01-01T15:36:01.123456,2020-01-01T15:36:01.123456789,15:36:01,-1.175E-37,-1.79769E308,101.1234");
+				"1,2020-01-01T15:35:00.123456,100.1234",
+				"2,2020-01-01T15:36:01.123456,101.1234");
 		StreamITCase.compareWithList(expected);
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
index 28d53c2d5bf04..0a67a3cf1def7 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcLookupFunctionITCase.java
@@ -60,15 +60,22 @@ public class JdbcLookupFunctionITCase extends AbstractTestBase {
 	public static final String DB_URL = "jdbc:derby:memory:lookup";
 	public static final String LOOKUP_TABLE = "lookup_table";
 
+	private final String tableFactory;
 	private final boolean useCache;
 
-	public JdbcLookupFunctionITCase(boolean useCache) {
+	public JdbcLookupFunctionITCase(String tableFactory, boolean useCache) {
 		this.useCache = useCache;
+		this.tableFactory = tableFactory;
 	}
 
-	@Parameterized.Parameters(name = "Table config = {0}")
-	public static Collection<Boolean> parameters() {
-		return Arrays.asList(true, false);
+	@Parameterized.Parameters(name = "Table factory = {0}, use cache {1}")
+	@SuppressWarnings("unchecked,rawtypes")
+	public static Collection<Object[]>  useCache() {
+		return Arrays.asList(new Object[][]{
+			{"legacyFactory", true},
+			{"legacyFactory", false},
+			{"dynamicFactory", true},
+			{"dynamicFactory", false}});
 	}
 
 	@Before
@@ -141,50 +148,90 @@ public void test() throws Exception {
 		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
 		StreamITCase.clear();
 
+		if ("legacyFactory".equals(tableFactory)) {
+			useLegacyTableFactory(env, tEnv);
+		} else {
+			useDynamicTableFactory(env, tEnv);
+		}
+
+		List<String> expected = new ArrayList<>();
+		expected.add("1,1,11-c1-v1,11-c2-v1");
+		expected.add("1,1,11-c1-v1,11-c2-v1");
+		expected.add("1,1,11-c1-v2,11-c2-v2");
+		expected.add("1,1,11-c1-v2,11-c2-v2");
+		expected.add("2,3,null,23-c2");
+		expected.add("2,5,25-c1,25-c2");
+		expected.add("3,8,38-c1,38-c2");
+
+		StreamITCase.compareWithList(expected);
+	}
+
+	private void useLegacyTableFactory(StreamExecutionEnvironment env, StreamTableEnvironment tEnv) throws Exception {
 		Table t = tEnv.fromDataStream(env.fromCollection(Arrays.asList(
-					new Tuple2<>(1, 1),
-					new Tuple2<>(1, 1),
-					new Tuple2<>(2, 3),
-					new Tuple2<>(2, 5),
-					new Tuple2<>(3, 5),
-					new Tuple2<>(3, 8)
-				)), $("id1"), $("id2"));
+			new Tuple2<>(1, 1),
+			new Tuple2<>(1, 1),
+			new Tuple2<>(2, 3),
+			new Tuple2<>(2, 5),
+			new Tuple2<>(3, 5),
+			new Tuple2<>(3, 8)
+		)), $("id1"), $("id2"));
 
 		tEnv.registerTable("T", t);
-
 		JdbcTableSource.Builder builder = JdbcTableSource.builder()
-				.setOptions(JdbcOptions.builder()
-						.setDBUrl(DB_URL)
-						.setTableName(LOOKUP_TABLE)
-						.build())
-				.setSchema(TableSchema.builder().fields(
-						new String[]{"id1", "id2", "comment1", "comment2"},
-						new DataType[]{DataTypes.INT(), DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING()})
-						.build());
+			.setOptions(JdbcOptions.builder()
+				.setDBUrl(DB_URL)
+				.setTableName(LOOKUP_TABLE)
+				.build())
+			.setSchema(TableSchema.builder().fields(
+				new String[]{"id1", "id2", "comment1", "comment2"},
+				new DataType[]{DataTypes.INT(), DataTypes.INT(), DataTypes.STRING(), DataTypes.STRING()})
+				.build());
 		if (useCache) {
 			builder.setLookupOptions(JdbcLookupOptions.builder()
-					.setCacheMaxSize(1000).setCacheExpireMs(1000 * 1000).build());
+				.setCacheMaxSize(1000).setCacheExpireMs(1000 * 1000).build());
 		}
 		tEnv.registerFunction("jdbcLookup",
-				builder.build().getLookupFunction(t.getSchema().getFieldNames()));
+			builder.build().getLookupFunction(t.getSchema().getFieldNames()));
 
 		String sqlQuery = "SELECT id1, id2, comment1, comment2 FROM T, " +
-				"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, l_id2, comment1, comment2)";
+			"LATERAL TABLE(jdbcLookup(id1, id2)) AS S(l_id1, l_id2, comment1, comment2)";
 		Table result = tEnv.sqlQuery(sqlQuery);
-
 		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
 		resultSet.addSink(new StreamITCase.StringSink<>());
 		env.execute();
+	}
 
-		List<String> expected = new ArrayList<>();
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("2,3,null,23-c2");
-		expected.add("2,5,25-c1,25-c2");
-		expected.add("3,8,38-c1,38-c2");
-
-		StreamITCase.compareWithList(expected);
+	private void useDynamicTableFactory(StreamExecutionEnvironment env, StreamTableEnvironment tEnv) throws Exception {
+		Table t = tEnv.fromDataStream(env.fromCollection(Arrays.asList(
+			new Tuple2<>(1, 1),
+			new Tuple2<>(1, 1),
+			new Tuple2<>(2, 3),
+			new Tuple2<>(2, 5),
+			new Tuple2<>(3, 5),
+			new Tuple2<>(3, 8)
+		)), $("id1"), $("id2"), $("proctime").proctime());
+
+		tEnv.createTemporaryView("T", t);
+
+		String cacheConfig = ", 'lookup.cache.max-rows'='4', 'lookup.cache.ttl'='10000', 'lookup.max-retries'='5'";
+		tEnv.sqlUpdate(
+			String.format("create table lookup (" +
+				"  id1 INT," +
+				"  id2 INT," +
+				"  comment1 VARCHAR," +
+				"  comment2 VARCHAR" +
+				") with(" +
+				"  'connector'='jdbc'," +
+				"  'url'='" + DB_URL + "'," +
+				"  'table-name'='" + LOOKUP_TABLE + "'" +
+				"  %s)", useCache ? cacheConfig : ""));
+
+		String sqlQuery = "SELECT source.id1, source.id2, L.comment1, L.comment2 FROM T AS source " +
+			"JOIN lookup for system_time as of source.proctime AS L " +
+			"ON source.id1 = L.id1 and source.id2 = L.id2";
+		Table result = tEnv.sqlQuery(sqlQuery);
+		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
+		resultSet.addSink(new StreamITCase.StringSink<>());
+		env.execute();
 	}
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java
similarity index 87%
rename from flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormatTest.java
rename to flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java
index 21d401bee49c5..e72fdca37e436 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicInputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataInputFormatTest.java
@@ -45,7 +45,6 @@
 
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.INPUT_TABLE;
-import static org.apache.flink.connector.jdbc.JdbcTestFixture.ROW_TYPE_INFO;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_BOOKS_SPLIT_BY_ID;
@@ -53,11 +52,11 @@
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.TEST_DATA;
 
 /**
- * Test suite for {@link JdbcDynamicInputFormat}.
+ * Test suite for {@link JdbcRowDataInputFormat}.
  */
-public class JdbcDynamicInputFormatTest extends JdbcDataTestBase {
+public class JdbcRowDataInputFormatTest extends JdbcDataTestBase {
 
-	private JdbcDynamicInputFormat inputFormat;
+	private JdbcRowDataInputFormat inputFormat;
 	private static String[] fieldNames = new String[]{"id", "title", "author", "price", "qty"};
 	private static DataType[] fieldDataTypes = new DataType[]{
 		DataTypes.INT(),
@@ -87,7 +86,7 @@ public void tearDown() throws IOException {
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testUntypedRowInfo() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername("org.apache.derby.jdbc.idontexist")
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
@@ -97,78 +96,71 @@ public void testUntypedRowInfo() throws IOException {
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testInvalidDriver() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername("org.apache.derby.jdbc.idontexist")
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.build();
 		inputFormat.openInputFormat();
 	}
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testInvalidURL() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl("jdbc:der:iamanerror:mory:ebookshop")
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.build();
 		inputFormat.openInputFormat();
 	}
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testInvalidQuery() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery("iamnotsql")
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.build();
 		inputFormat.openInputFormat();
 	}
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testIncompleteConfiguration() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.build();
 	}
 
 	@Test(expected = IllegalArgumentException.class)
 	public void testInvalidFetchSize() {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(-7)
 			.build();
 	}
 
 	@Test
 	public void testValidFetchSizeIntegerMin() {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setFetchSize(Integer.MIN_VALUE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 	}
 
 	@Test
 	public void testJdbcInputFormatWithoutParallelism() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 		//this query does not exploit parallelism
 		Assert.assertEquals(1, inputFormat.createInputSplits(1).length);
@@ -194,14 +186,13 @@ public void testJdbcInputFormatWithParallelismAndNumericColumnSplitting() throws
 		final long min = TEST_DATA[0].id;
 		final long max = TEST_DATA[TEST_DATA.length - fetchSize].id;
 		JdbcParameterValuesProvider pramProvider = new JdbcNumericBetweenParametersProvider(min, max).ofBatchSize(fetchSize);
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_ID)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(pramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 
 		inputFormat.openInputFormat();
@@ -231,14 +222,13 @@ public void testJdbcInputFormatWithoutParallelismAndNumericColumnSplitting() thr
 		final long max = TEST_DATA[TEST_DATA.length - 1].id;
 		final long fetchSize = max + 1; //generate a single split
 		JdbcParameterValuesProvider pramProvider = new JdbcNumericBetweenParametersProvider(min, max).ofBatchSize(fetchSize);
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_ID)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(pramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 
 		inputFormat.openInputFormat();
@@ -268,14 +258,13 @@ public void testJdbcInputFormatWithParallelismAndGenericSplitting() throws IOExc
 		queryParameters[0] = new String[]{TEST_DATA[3].author};
 		queryParameters[1] = new String[]{TEST_DATA[0].author};
 		JdbcParameterValuesProvider paramProvider = new JdbcGenericParameterValuesProvider(queryParameters);
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_ALL_BOOKS_SPLIT_BY_AUTHOR)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setParametersProvider(paramProvider)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 
 		inputFormat.openInputFormat();
@@ -309,13 +298,12 @@ private void verifySplit(InputSplit split, int expectedIDSum) throws IOException
 
 	@Test
 	public void testEmptyResults() throws IOException {
-		inputFormat = JdbcDynamicInputFormat.builder()
+		inputFormat = JdbcRowDataInputFormat.builder()
 			.setDrivername(DERBY_EBOOKSHOP_DB.getDriverClass())
 			.setDBUrl(DERBY_EBOOKSHOP_DB.getUrl())
 			.setQuery(SELECT_EMPTY)
-			.setRowTypeInfo(ROW_TYPE_INFO)
 			.setResultSetType(ResultSet.TYPE_SCROLL_INSENSITIVE)
-			.setRowConverter(dialect.getInputConverter(rowType))
+			.setRowConverter(dialect.getRowConverter(rowType))
 			.build();
 
 		try {
@@ -335,5 +323,4 @@ private static void assertEquals(JdbcTestFixture.TestEntry expected, RowData act
 		Assert.assertEquals(expected.price, actual.isNullAt(3) ? null : Double.valueOf(actual.getDouble(3)));
 		Assert.assertEquals(expected.qty, actual.isNullAt(4) ? null : Integer.valueOf(actual.getInt(4)));
 	}
-
 }
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
similarity index 91%
rename from flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
rename to flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
index 0644545b5c475..21f12e072d2bd 100644
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcDynamicOutputFormatTest.java
+++ b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcRowDataOutputFormatTest.java
@@ -20,7 +20,6 @@
 
 import org.apache.flink.connector.jdbc.JdbcDataTestBase;
 import org.apache.flink.connector.jdbc.JdbcExecutionOptions;
-import org.apache.flink.connector.jdbc.JdbcTestFixture;
 import org.apache.flink.connector.jdbc.internal.options.JdbcDmlOptions;
 import org.apache.flink.connector.jdbc.internal.options.JdbcOptions;
 import org.apache.flink.table.api.DataTypes;
@@ -49,6 +48,7 @@
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_NEWBOOKS;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.SELECT_ALL_NEWBOOKS_2;
 import static org.apache.flink.connector.jdbc.JdbcTestFixture.TEST_DATA;
+import static org.apache.flink.connector.jdbc.JdbcTestFixture.TestEntry;
 import static org.apache.flink.util.ExceptionUtils.findThrowable;
 import static org.apache.flink.util.ExceptionUtils.findThrowableWithMessage;
 import static org.junit.Assert.assertEquals;
@@ -56,11 +56,11 @@
 import static org.junit.Assert.assertTrue;
 
 /**
- * Test suite for {@link JdbcDynamicOutputFormat}.
+ * Test suite for {@link JdbcRowDataOutputFormat}.
  */
-public class JdbcDynamicOutputFormatTest extends JdbcDataTestBase {
+public class JdbcRowDataOutputFormatTest extends JdbcDataTestBase {
 
-	private static JdbcDynamicOutputFormat outputFormat;
+	private static JdbcRowDataOutputFormat outputFormat;
 	private static String[] fieldNames = new String[] {"id", "title", "author", "price", "qty"};
 	private static DataType[] fieldDataTypes = new DataType[]{
 		DataTypes.INT(),
@@ -98,7 +98,7 @@ public void testInvalidDriver() {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -126,7 +126,7 @@ public void testInvalidURL() {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -141,7 +141,6 @@ public void testInvalidURL() {
 
 	@Test
 	public void testIncompatibleTypes() {
-		String expectedMsg = "field index: 4, field value: +I(4,hello,world,0.99,imthewrongtype).";
 		try {
 			JdbcOptions jdbcOptions = JdbcOptions.builder()
 				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
@@ -154,7 +153,7 @@ public void testIncompatibleTypes() {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -170,13 +169,11 @@ public void testIncompatibleTypes() {
 			outputFormat.close();
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
-			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
 		}
 	}
 
 	@Test
 	public void testExceptionOnInvalidType() {
-		String expectedMsg = "field index: 3, field value: +I(1001,Java public for dummies,Tan Ah Teck,0,11)";
 		try {
 			JdbcOptions jdbcOptions = JdbcOptions.builder()
 				.setDriverName(DERBY_EBOOKSHOP_DB.getDriverClass())
@@ -189,7 +186,7 @@ public void testExceptionOnInvalidType() {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -199,13 +196,12 @@ public void testExceptionOnInvalidType() {
 			setRuntimeContext(outputFormat, false);
 			outputFormat.open(0, 1);
 
-			JdbcTestFixture.TestEntry entry = TEST_DATA[0];
+			TestEntry entry = TEST_DATA[0];
 			RowData row = buildGenericData(entry.id, entry.title, entry.author, 0L, entry.qty);
 			outputFormat.writeRecord(row);
 			outputFormat.close();
 		} catch (Exception e) {
 			assertTrue(findThrowable(e, ClassCastException.class).isPresent());
-			assertTrue(findThrowableWithMessage(e, expectedMsg).isPresent());
 		}
 	}
 
@@ -224,7 +220,7 @@ public void testExceptionOnClose() {
 				.withFieldNames(fieldNames)
 				.build();
 
-			outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+			outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 				.setJdbcOptions(jdbcOptions)
 				.setFieldDataTypes(fieldDataTypes)
 				.setJdbcDmlOptions(dmlOptions)
@@ -234,7 +230,7 @@ public void testExceptionOnClose() {
 			setRuntimeContext(outputFormat, true);
 			outputFormat.open(0, 1);
 
-			JdbcTestFixture.TestEntry entry = TEST_DATA[0];
+			TestEntry entry = TEST_DATA[0];
 			RowData row = buildGenericData(entry.id, entry.title, entry.author, entry.price, entry.qty);
 
 			outputFormat.writeRecord(row);
@@ -260,7 +256,7 @@ public void testJdbcOutputFormat() throws IOException, SQLException {
 			.withFieldNames(fieldNames)
 			.build();
 
-		outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 			.setJdbcOptions(jdbcOptions)
 			.setFieldDataTypes(fieldDataTypes)
 			.setJdbcDmlOptions(dmlOptions)
@@ -273,7 +269,7 @@ public void testJdbcOutputFormat() throws IOException, SQLException {
 		setRuntimeContext(outputFormat, true);
 		outputFormat.open(0, 1);
 
-		for (JdbcTestFixture.TestEntry entry : TEST_DATA) {
+		for (TestEntry entry : TEST_DATA) {
 			outputFormat.writeRecord(buildGenericData(entry.id, entry.title, entry.author, entry.price, entry.qty));
 		}
 
@@ -314,7 +310,7 @@ public void testFlush() throws SQLException, IOException {
 			.withBatchSize(3)
 			.build();
 
-		outputFormat = JdbcDynamicOutputFormat.dynamicOutputFormatBuilder()
+		outputFormat = JdbcRowDataOutputFormat.dynamicOutputFormatBuilder()
 			.setJdbcOptions(jdbcOptions)
 			.setFieldDataTypes(fieldDataTypes)
 			.setJdbcDmlOptions(dmlOptions)
diff --git a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcTemporayTableITCase.java b/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcTemporayTableITCase.java
deleted file mode 100644
index 764f3d0c3e8e3..0000000000000
--- a/flink-connectors/flink-connector-jdbc/src/test/java/org/apache/flink/connector/jdbc/table/JdbcTemporayTableITCase.java
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.connector.jdbc.table;
-
-import org.apache.flink.api.java.tuple.Tuple2;
-import org.apache.flink.connector.jdbc.JdbcTestFixture;
-import org.apache.flink.streaming.api.datastream.DataStream;
-import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
-import org.apache.flink.table.api.EnvironmentSettings;
-import org.apache.flink.table.api.Table;
-import org.apache.flink.table.api.java.StreamTableEnvironment;
-import org.apache.flink.table.runtime.utils.StreamITCase;
-import org.apache.flink.test.util.AbstractTestBase;
-import org.apache.flink.types.Row;
-
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import java.sql.Connection;
-import java.sql.DriverManager;
-import java.sql.SQLException;
-import java.sql.Statement;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import static org.apache.flink.connector.jdbc.JdbcTestFixture.DERBY_EBOOKSHOP_DB;
-import static org.apache.flink.table.api.Expressions.$;
-
-/**
- * Test suite for {@link JdbcDynamicLookupFunction}.
- */
-@RunWith(Parameterized.class)
-public class JdbcTemporayTableITCase extends AbstractTestBase {
-
-	public static final String DB_URL = "jdbc:derby:memory:lookup";
-	public static final String LOOKUP_TABLE = "lookup_table";
-
-	private final boolean useCache;
-
-	public JdbcTemporayTableITCase(boolean useCache) {
-		this.useCache = useCache;
-	}
-
-	@Parameterized.Parameters(name = "Table config = {0}")
-	public static Collection<Boolean> parameters() {
-		return Arrays.asList(true, false);
-	}
-
-	@Before
-	public void before() throws ClassNotFoundException, SQLException {
-		System.setProperty("derby.stream.error.field", JdbcTestFixture.class.getCanonicalName() + ".DEV_NULL");
-
-		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
-		try (
-			Connection conn = DriverManager.getConnection(DB_URL + ";create=true");
-			Statement stat = conn.createStatement()) {
-			stat.executeUpdate("CREATE TABLE " + LOOKUP_TABLE + " (" +
-				"id1 INT NOT NULL DEFAULT 0," +
-				"id2 INT NOT NULL DEFAULT 0," +
-				"comment1 VARCHAR(1000)," +
-				"comment2 VARCHAR(1000))");
-
-			Object[][] data = new Object[][]{
-				new Object[]{1, 1, "11-c1-v1", "11-c2-v1"},
-				new Object[]{1, 1, "11-c1-v2", "11-c2-v2"},
-				new Object[]{2, 3, null, "23-c2"},
-				new Object[]{2, 5, "25-c1", "25-c2"},
-				new Object[]{3, 8, "38-c1", "38-c2"}
-			};
-			boolean[] surroundedByQuotes = new boolean[]{
-				false, false, true, true
-			};
-
-			StringBuilder sqlQueryBuilder = new StringBuilder(
-				"INSERT INTO " + LOOKUP_TABLE + " (id1, id2, comment1, comment2) VALUES ");
-			for (int i = 0; i < data.length; i++) {
-				sqlQueryBuilder.append("(");
-				for (int j = 0; j < data[i].length; j++) {
-					if (data[i][j] == null) {
-						sqlQueryBuilder.append("null");
-					} else {
-						if (surroundedByQuotes[j]) {
-							sqlQueryBuilder.append("'");
-						}
-						sqlQueryBuilder.append(data[i][j]);
-						if (surroundedByQuotes[j]) {
-							sqlQueryBuilder.append("'");
-						}
-					}
-					if (j < data[i].length - 1) {
-						sqlQueryBuilder.append(", ");
-					}
-				}
-				sqlQueryBuilder.append(")");
-				if (i < data.length - 1) {
-					sqlQueryBuilder.append(", ");
-				}
-			}
-			stat.execute(sqlQueryBuilder.toString());
-		}
-	}
-
-	@After
-	public void clearOutputTable() throws Exception {
-		Class.forName(DERBY_EBOOKSHOP_DB.getDriverClass());
-		try (
-			Connection conn = DriverManager.getConnection(DB_URL);
-			Statement stat = conn.createStatement()) {
-			stat.execute("DROP TABLE " + LOOKUP_TABLE);
-		}
-	}
-
-	@Test
-	public void test() throws Exception {
-		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
-		EnvironmentSettings envSettings = EnvironmentSettings.newInstance()
-			.useBlinkPlanner()
-			.inStreamingMode()
-			.build();
-		StreamTableEnvironment tEnv = StreamTableEnvironment.create(env, envSettings);
-		StreamITCase.clear();
-
-		Table t = tEnv.fromDataStream(env.fromCollection(Arrays.asList(
-			new Tuple2<>(1, 1),
-			new Tuple2<>(1, 1),
-			new Tuple2<>(2, 3),
-			new Tuple2<>(2, 5),
-			new Tuple2<>(3, 5),
-			new Tuple2<>(3, 8)
-		)), $("id1"), $("id2"), $("proctime").proctime());
-		tEnv.createTemporaryView("T", t);
-
-		String cacheConfig = ", 'lookup.cache.max-rows'='5000', 'lookup.cache.ttl'='1000000'";
-		tEnv.sqlUpdate(
-			String.format("create table lookup (" +
-			"  id1 INT," +
-			"  id2 INT," +
-			"  comment1 VARCHAR," +
-			"  comment2 VARCHAR" +
-			") with(" +
-			"  'connector'='jdbc'," +
-			"  'url'='" + DB_URL + "'," +
-			"  'table'='" + LOOKUP_TABLE + "'" +
-			"  %s)", useCache ? cacheConfig : ""));
-
-		String sqlQuery = "SELECT source.id1, source.id2, L.comment1, L.comment2 FROM T AS source " +
-			"JOIN lookup for system_time as of source.proctime AS L " +
-			"ON source.id1 = L.id1 and source.id2 = L.id2";
-		Table result = tEnv.sqlQuery(sqlQuery);
-
-		DataStream<Row> resultSet = tEnv.toAppendStream(result, Row.class);
-		resultSet.addSink(new StreamITCase.StringSink<>());
-		env.execute();
-
-		List<String> expected = new ArrayList<>();
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v1,11-c2-v1");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("1,1,11-c1-v2,11-c2-v2");
-		expected.add("2,3,null,23-c2");
-		expected.add("2,5,25-c1,25-c2");
-		expected.add("3,8,38-c1,38-c2");
-
-		StreamITCase.compareWithList(expected);
-	}
-}
