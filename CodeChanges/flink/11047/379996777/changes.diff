diff --git a/azure-pipelines.yml b/azure-pipelines.yml
index cc41f084b2df3..7ffc7d0a5897f 100644
--- a/azure-pipelines.yml
+++ b/azure-pipelines.yml
@@ -13,23 +13,47 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+#
+# This file defines an Azure Pipeline build for testing Flink. It is intended to be used
+# with a free Azure Pipelines account.
+# It has the following features:
+#  - default builds for pushes / pull requests
+#  - end-to-end tests
+#
+#
+# For the "apache/flink" repository, we are using the pipeline definition located in
+#   tools/azure-pipelines/build-apache-repo.yml
+# That file points to custom, self-hosted build agents for faster pull request build processing and 
+# integration with Flinkbot.
+# The custom pipeline definition file is configured in the "Pipeline settings" screen
+# of the Azure Pipelines web ui.
+#
 
-trigger:
-  branches:
-    include:
-    - '*' 
 
 resources:
   containers:
-  # Container with Maven 3.2.5 to have the same environment everywhere.
+  # Container with Maven 3.2.5, SSL to have the same environment everywhere.
   - container: flink-build-container
-    image: rmetzger/flink-ci:3
-  repositories:
-    - repository: templates
-      type: github
-      name: flink-ci/flink-azure-builds
-      endpoint: flink-ci
+    image: rmetzger/flink-ci:ubuntu-jdk8-amd64-2a765ab
+
+# See tools/azure-pipelines/jobs-template.yml for a short summary of the caching
+variables:
+  MAVEN_CACHE_FOLDER: $(Pipeline.Workspace)/.m2/repository
+  MAVEN_OPTS: '-Dmaven.repo.local=$(MAVEN_CACHE_FOLDER)'
+  CACHE_KEY: maven | $(Agent.OS) | **/pom.xml, !**/target/**
+  CACHE_FALLBACK_KEY: maven | $(Agent.OS)
+  CACHE_FLINK_DIR: $(Pipeline.Workspace)/flink_cache
+
 
 jobs:
-- template: flink-build-jobs.yml@templates
+  - template: tools/azure-pipelines/jobs-template.yml
+    parameters: # see template file for a definition of the parameters.
+      stage_name: ci_build
+      test_pool_definition:
+        vmImage: 'ubuntu-latest'
+      e2e_pool_definition:
+        vmImage: 'ubuntu-16.04'
+      environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
+
+
 
diff --git a/docs/_includes/generated/kubernetes_config_configuration.html b/docs/_includes/generated/kubernetes_config_configuration.html
index 5ebcc3d4976ec..967b23e022d5c 100644
--- a/docs/_includes/generated/kubernetes_config_configuration.html
+++ b/docs/_includes/generated/kubernetes_config_configuration.html
@@ -8,12 +8,6 @@
         </tr>
     </thead>
     <tbody>
-        <tr>
-            <td><h5>kubernetes.context</h5></td>
-            <td style="word-wrap: break-word;">(none)</td>
-            <td>String</td>
-            <td>The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts.</td>
-        </tr>
         <tr>
             <td><h5>kubernetes.cluster-id</h5></td>
             <td style="word-wrap: break-word;">(none)</td>
@@ -44,6 +38,12 @@
             <td>String</td>
             <td>Kubernetes image pull policy. Valid values are Always, Never, and IfNotPresent. The default policy is IfNotPresent to avoid putting pressure to image repository.</td>
         </tr>
+        <tr>
+            <td><h5>kubernetes.context</h5></td>
+            <td style="word-wrap: break-word;">(none)</td>
+            <td>String</td>
+            <td>The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts.</td>
+        </tr>
         <tr>
             <td><h5>kubernetes.entry.path</h5></td>
             <td style="word-wrap: break-word;">"/opt/flink/bin/kubernetes-entry.sh"</td>
diff --git a/docs/dev/best_practices.md b/docs/dev/best_practices.md
deleted file mode 100644
index e16f396db80fd..0000000000000
--- a/docs/dev/best_practices.md
+++ /dev/null
@@ -1,298 +0,0 @@
----
-title: "Best Practices"
-nav-parent_id: dev
-nav-pos: 90
----
-<!--
-Licensed to the Apache Software Foundation (ASF) under one
-or more contributor license agreements.  See the NOTICE file
-distributed with this work for additional information
-regarding copyright ownership.  The ASF licenses this file
-to you under the Apache License, Version 2.0 (the
-"License"); you may not use this file except in compliance
-with the License.  You may obtain a copy of the License at
-
-  http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing,
-software distributed under the License is distributed on an
-"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied.  See the License for the
-specific language governing permissions and limitations
-under the License.
--->
-
-This page contains a collection of best practices for Flink programmers on how to solve frequently encountered problems.
-
-
-* This will be replaced by the TOC
-{:toc}
-
-## Parsing command line arguments and passing them around in your Flink application
-
-Almost all Flink applications, both batch and streaming, rely on external configuration parameters.
-They are used to specify input and output sources (like paths or addresses), system parameters (parallelism, runtime configuration), and application specific parameters (typically used within user functions).
-
-Flink provides a simple utility called `ParameterTool` to provide some basic tooling for solving these problems.
-Please note that you don't have to use the `ParameterTool` described here. Other frameworks such as [Commons CLI](https://commons.apache.org/proper/commons-cli/) and
-[argparse4j](http://argparse4j.sourceforge.net/) also work well with Flink.
-
-
-### Getting your configuration values into the `ParameterTool`
-
-The `ParameterTool` provides a set of predefined static methods for reading the configuration. The tool is internally expecting a `Map<String, String>`, so it's very easy to integrate it with your own configuration style.
-
-
-#### From `.properties` files
-
-The following method will read a [Properties](https://docs.oracle.com/javase/tutorial/essential/environment/properties.html) file and provide the key/value pairs:
-{% highlight java %}
-String propertiesFilePath = "/home/sam/flink/myjob.properties";
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath);
-
-File propertiesFile = new File(propertiesFilePath);
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile);
-
-InputStream propertiesFileInputStream = new FileInputStream(file);
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFileInputStream);
-{% endhighlight %}
-
-
-#### From the command line arguments
-
-This allows getting arguments like `--input hdfs:///mydata --elements 42` from the command line.
-{% highlight java %}
-public static void main(String[] args) {
-    ParameterTool parameter = ParameterTool.fromArgs(args);
-    // .. regular code ..
-{% endhighlight %}
-
-
-#### From system properties
-
-When starting a JVM, you can pass system properties to it: `-Dinput=hdfs:///mydata`. You can also initialize the `ParameterTool` from these system properties:
-
-{% highlight java %}
-ParameterTool parameter = ParameterTool.fromSystemProperties();
-{% endhighlight %}
-
-
-### Using the parameters in your Flink program
-
-Now that we've got the parameters from somewhere (see above) we can use them in various ways.
-
-**Directly from the `ParameterTool`**
-
-The `ParameterTool` itself has methods for accessing the values.
-{% highlight java %}
-ParameterTool parameters = // ...
-parameter.getRequired("input");
-parameter.get("output", "myDefaultValue");
-parameter.getLong("expectedCount", -1L);
-parameter.getNumberOfParameters()
-// .. there are more methods available.
-{% endhighlight %}
-
-You can use the return values of these methods directly in the `main()` method of the client submitting the application.
-For example, you could set the parallelism of a operator like this:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-int parallelism = parameters.get("mapParallelism", 2);
-DataSet<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer()).setParallelism(parallelism);
-{% endhighlight %}
-
-Since the `ParameterTool` is serializable, you can pass it to the functions itself:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-DataSet<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer(parameters));
-{% endhighlight %}
-
-and then use it inside the function for getting values from the command line.
-
-#### Register the parameters globally
-
-Parameters registered as global job parameters in the `ExecutionConfig` can be accessed as configuration values from the JobManager web interface and in all functions defined by the user.
-
-Register the parameters globally:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-
-// set up the execution environment
-final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
-env.getConfig().setGlobalJobParameters(parameters);
-{% endhighlight %}
-
-Access them in any rich user function:
-
-{% highlight java %}
-public static final class Tokenizer extends RichFlatMapFunction<String, Tuple2<String, Integer>> {
-
-    @Override
-    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-	ParameterTool parameters = (ParameterTool)
-	    getRuntimeContext().getExecutionConfig().getGlobalJobParameters();
-	parameters.getRequired("input");
-	// .. do more ..
-{% endhighlight %}
-
-
-## Naming large TupleX types
-
-It is recommended to use POJOs (Plain old Java objects) instead of `TupleX` for data types with many fields.
-Also, POJOs can be used to give large `Tuple`-types a name.
-
-**Example**
-
-Instead of using:
-
-
-{% highlight java %}
-Tuple11<String, String, ..., String> var = new ...;
-{% endhighlight %}
-
-
-It is much easier to create a custom type extending from the large Tuple type.
-
-{% highlight java %}
-CustomType var = new ...;
-
-public static class CustomType extends Tuple11<String, String, ..., String> {
-    // constructor matching super
-}
-{% endhighlight %}
-
-## Using Logback instead of Log4j
-
-**Note: This tutorial is applicable starting from Flink 0.10**
-
-Apache Flink is using [slf4j](http://www.slf4j.org/) as the logging abstraction in the code. Users are advised to use sfl4j as well in their user functions.
-
-Sfl4j is a compile-time logging interface that can use different logging implementations at runtime, such as [log4j](http://logging.apache.org/log4j/2.x/) or [Logback](http://logback.qos.ch/).
-
-Flink is depending on Log4j by default. This page describes how to use Flink with Logback. Users reported that they were also able to set up centralized logging with Graylog using this tutorial.
-
-To get a logger instance in the code, use the following code:
-
-
-{% highlight java %}
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class MyClass implements MapFunction {
-    private static final Logger LOG = LoggerFactory.getLogger(MyClass.class);
-    // ...
-{% endhighlight %}
-
-
-### Use Logback when running Flink out of the IDE / from a Java application
-
-
-In all cases where classes are executed with a classpath created by a dependency manager such as Maven, Flink will pull log4j into the classpath.
-
-Therefore, you will need to exclude log4j from Flink's dependencies. The following description will assume a Maven project created from a [Flink quickstart](./projectsetup/java_api_quickstart.html).
-
-Change your projects `pom.xml` file like this:
-
-{% highlight xml %}
-<dependencies>
-	<!-- Add the two required logback dependencies -->
-	<dependency>
-		<groupId>ch.qos.logback</groupId>
-		<artifactId>logback-core</artifactId>
-		<version>1.1.3</version>
-	</dependency>
-	<dependency>
-		<groupId>ch.qos.logback</groupId>
-		<artifactId>logback-classic</artifactId>
-		<version>1.1.3</version>
-	</dependency>
-
-	<!-- Add the log4j -> sfl4j (-> logback) bridge into the classpath
-	 Hadoop is logging to log4j! -->
-	<dependency>
-		<groupId>org.slf4j</groupId>
-		<artifactId>log4j-over-slf4j</artifactId>
-		<version>1.7.7</version>
-	</dependency>
-
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-java</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-streaming-java{{ site.scala_version_suffix }}</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-clients{{ site.scala_version_suffix }}</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-</dependencies>
-{% endhighlight %}
-
-The following changes were done in the `<dependencies>` section:
-
- * Exclude all `log4j` dependencies from all Flink dependencies: this causes Maven to ignore Flink's transitive dependencies to log4j.
- * Exclude the `slf4j-log4j12` artifact from Flink's dependencies: since we are going to use the slf4j to logback binding, we have to remove the slf4j to log4j binding.
- * Add the Logback dependencies: `logback-core` and `logback-classic`
- * Add dependencies for `log4j-over-slf4j`. `log4j-over-slf4j` is a tool which allows legacy applications which are directly using the Log4j APIs to use the Slf4j interface. Flink depends on Hadoop which is directly using Log4j for logging. Therefore, we need to redirect all logger calls from Log4j to Slf4j which is in turn logging to Logback.
-
-Please note that you need to manually add the exclusions to all new Flink dependencies you are adding to the pom file.
-
-You may also need to check if other (non-Flink) dependencies are pulling in log4j bindings. You can analyze the dependencies of your project with `mvn dependency:tree`.
-
-
-
-### Use Logback when running Flink on a cluster
-
-This tutorial is applicable when running Flink on YARN or as a standalone cluster.
-
-In order to use Logback instead of Log4j with Flink, you need to remove `log4j-1.2.xx.jar` and `sfl4j-log4j12-xxx.jar` from the `lib/` directory.
-
-Next, you need to put the following jar files into the `lib/` folder:
-
- * `logback-classic.jar`
- * `logback-core.jar`
- * `log4j-over-slf4j.jar`: This bridge needs to be present in the classpath for redirecting logging calls from Hadoop (which is using Log4j) to Slf4j.
-
-Note that you need to explicitly set the `lib/` directory when using a per-job YARN cluster.
-
-The command to submit Flink on YARN with a custom logger is: `./bin/flink run -yt $FLINK_HOME/lib <... remaining arguments ...>`
-
-{% top %}
diff --git a/docs/dev/best_practices.zh.md b/docs/dev/best_practices.zh.md
deleted file mode 100644
index b9d16860aa85c..0000000000000
--- a/docs/dev/best_practices.zh.md
+++ /dev/null
@@ -1,298 +0,0 @@
----
-title: "最佳实践"
-nav-parent_id: dev
-nav-pos: 90
----
-<!--
-Licensed to the Apache Software Foundation (ASF) under one
-or more contributor license agreements.  See the NOTICE file
-distributed with this work for additional information
-regarding copyright ownership.  The ASF licenses this file
-to you under the Apache License, Version 2.0 (the
-"License"); you may not use this file except in compliance
-with the License.  You may obtain a copy of the License at
-
-  http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing,
-software distributed under the License is distributed on an
-"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-KIND, either express or implied.  See the License for the
-specific language governing permissions and limitations
-under the License.
--->
-
-This page contains a collection of best practices for Flink programmers on how to solve frequently encountered problems.
-
-
-* This will be replaced by the TOC
-{:toc}
-
-## Parsing command line arguments and passing them around in your Flink application
-
-Almost all Flink applications, both batch and streaming, rely on external configuration parameters.
-They are used to specify input and output sources (like paths or addresses), system parameters (parallelism, runtime configuration), and application specific parameters (typically used within user functions).
-
-Flink provides a simple utility called `ParameterTool` to provide some basic tooling for solving these problems.
-Please note that you don't have to use the `ParameterTool` described here. Other frameworks such as [Commons CLI](https://commons.apache.org/proper/commons-cli/) and
-[argparse4j](http://argparse4j.sourceforge.net/) also work well with Flink.
-
-
-### Getting your configuration values into the `ParameterTool`
-
-The `ParameterTool` provides a set of predefined static methods for reading the configuration. The tool is internally expecting a `Map<String, String>`, so it's very easy to integrate it with your own configuration style.
-
-
-#### From `.properties` files
-
-The following method will read a [Properties](https://docs.oracle.com/javase/tutorial/essential/environment/properties.html) file and provide the key/value pairs:
-{% highlight java %}
-String propertiesFilePath = "/home/sam/flink/myjob.properties";
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFilePath);
-
-File propertiesFile = new File(propertiesFilePath);
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFile);
-
-InputStream propertiesFileInputStream = new FileInputStream(file);
-ParameterTool parameter = ParameterTool.fromPropertiesFile(propertiesFileInputStream);
-{% endhighlight %}
-
-
-#### From the command line arguments
-
-This allows getting arguments like `--input hdfs:///mydata --elements 42` from the command line.
-{% highlight java %}
-public static void main(String[] args) {
-    ParameterTool parameter = ParameterTool.fromArgs(args);
-    // .. regular code ..
-{% endhighlight %}
-
-
-#### From system properties
-
-When starting a JVM, you can pass system properties to it: `-Dinput=hdfs:///mydata`. You can also initialize the `ParameterTool` from these system properties:
-
-{% highlight java %}
-ParameterTool parameter = ParameterTool.fromSystemProperties();
-{% endhighlight %}
-
-
-### Using the parameters in your Flink program
-
-Now that we've got the parameters from somewhere (see above) we can use them in various ways.
-
-**Directly from the `ParameterTool`**
-
-The `ParameterTool` itself has methods for accessing the values.
-{% highlight java %}
-ParameterTool parameters = // ...
-parameter.getRequired("input");
-parameter.get("output", "myDefaultValue");
-parameter.getLong("expectedCount", -1L);
-parameter.getNumberOfParameters()
-// .. there are more methods available.
-{% endhighlight %}
-
-You can use the return values of these methods directly in the `main()` method of the client submitting the application.
-For example, you could set the parallelism of a operator like this:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-int parallelism = parameters.get("mapParallelism", 2);
-DataSet<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer()).setParallelism(parallelism);
-{% endhighlight %}
-
-Since the `ParameterTool` is serializable, you can pass it to the functions itself:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-DataSet<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer(parameters));
-{% endhighlight %}
-
-and then use it inside the function for getting values from the command line.
-
-#### Register the parameters globally
-
-Parameters registered as global job parameters in the `ExecutionConfig` can be accessed as configuration values from the JobManager web interface and in all functions defined by the user.
-
-Register the parameters globally:
-
-{% highlight java %}
-ParameterTool parameters = ParameterTool.fromArgs(args);
-
-// set up the execution environment
-final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
-env.getConfig().setGlobalJobParameters(parameters);
-{% endhighlight %}
-
-Access them in any rich user function:
-
-{% highlight java %}
-public static final class Tokenizer extends RichFlatMapFunction<String, Tuple2<String, Integer>> {
-
-    @Override
-    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
-	ParameterTool parameters = (ParameterTool)
-	    getRuntimeContext().getExecutionConfig().getGlobalJobParameters();
-	parameters.getRequired("input");
-	// .. do more ..
-{% endhighlight %}
-
-
-## Naming large TupleX types
-
-It is recommended to use POJOs (Plain old Java objects) instead of `TupleX` for data types with many fields.
-Also, POJOs can be used to give large `Tuple`-types a name.
-
-**Example**
-
-Instead of using:
-
-
-{% highlight java %}
-Tuple11<String, String, ..., String> var = new ...;
-{% endhighlight %}
-
-
-It is much easier to create a custom type extending from the large Tuple type.
-
-{% highlight java %}
-CustomType var = new ...;
-
-public static class CustomType extends Tuple11<String, String, ..., String> {
-    // constructor matching super
-}
-{% endhighlight %}
-
-## Using Logback instead of Log4j
-
-**Note: This tutorial is applicable starting from Flink 0.10**
-
-Apache Flink is using [slf4j](http://www.slf4j.org/) as the logging abstraction in the code. Users are advised to use sfl4j as well in their user functions.
-
-Sfl4j is a compile-time logging interface that can use different logging implementations at runtime, such as [log4j](http://logging.apache.org/log4j/2.x/) or [Logback](http://logback.qos.ch/).
-
-Flink is depending on Log4j by default. This page describes how to use Flink with Logback. Users reported that they were also able to set up centralized logging with Graylog using this tutorial.
-
-To get a logger instance in the code, use the following code:
-
-
-{% highlight java %}
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class MyClass implements MapFunction {
-    private static final Logger LOG = LoggerFactory.getLogger(MyClass.class);
-    // ...
-{% endhighlight %}
-
-
-### Use Logback when running Flink out of the IDE / from a Java application
-
-
-In all cases where classes are executed with a classpath created by a dependency manager such as Maven, Flink will pull log4j into the classpath.
-
-Therefore, you will need to exclude log4j from Flink's dependencies. The following description will assume a Maven project created from a [Flink quickstart](./projectsetup/java_api_quickstart.html).
-
-Change your projects `pom.xml` file like this:
-
-{% highlight xml %}
-<dependencies>
-	<!-- Add the two required logback dependencies -->
-	<dependency>
-		<groupId>ch.qos.logback</groupId>
-		<artifactId>logback-core</artifactId>
-		<version>1.1.3</version>
-	</dependency>
-	<dependency>
-		<groupId>ch.qos.logback</groupId>
-		<artifactId>logback-classic</artifactId>
-		<version>1.1.3</version>
-	</dependency>
-
-	<!-- Add the log4j -> sfl4j (-> logback) bridge into the classpath
-	 Hadoop is logging to log4j! -->
-	<dependency>
-		<groupId>org.slf4j</groupId>
-		<artifactId>log4j-over-slf4j</artifactId>
-		<version>1.7.7</version>
-	</dependency>
-
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-java</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-streaming-java{{ site.scala_version_suffix }}</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-	<dependency>
-		<groupId>org.apache.flink</groupId>
-		<artifactId>flink-clients{{ site.scala_version_suffix }}</artifactId>
-		<version>{{ site.version }}</version>
-		<exclusions>
-			<exclusion>
-				<groupId>log4j</groupId>
-				<artifactId>*</artifactId>
-			</exclusion>
-			<exclusion>
-				<groupId>org.slf4j</groupId>
-				<artifactId>slf4j-log4j12</artifactId>
-			</exclusion>
-		</exclusions>
-	</dependency>
-</dependencies>
-{% endhighlight %}
-
-The following changes were done in the `<dependencies>` section:
-
- * Exclude all `log4j` dependencies from all Flink dependencies: this causes Maven to ignore Flink's transitive dependencies to log4j.
- * Exclude the `slf4j-log4j12` artifact from Flink's dependencies: since we are going to use the slf4j to logback binding, we have to remove the slf4j to log4j binding.
- * Add the Logback dependencies: `logback-core` and `logback-classic`
- * Add dependencies for `log4j-over-slf4j`. `log4j-over-slf4j` is a tool which allows legacy applications which are directly using the Log4j APIs to use the Slf4j interface. Flink depends on Hadoop which is directly using Log4j for logging. Therefore, we need to redirect all logger calls from Log4j to Slf4j which is in turn logging to Logback.
-
-Please note that you need to manually add the exclusions to all new Flink dependencies you are adding to the pom file.
-
-You may also need to check if other (non-Flink) dependencies are pulling in log4j bindings. You can analyze the dependencies of your project with `mvn dependency:tree`.
-
-
-
-### Use Logback when running Flink on a cluster
-
-This tutorial is applicable when running Flink on YARN or as a standalone cluster.
-
-In order to use Logback instead of Log4j with Flink, you need to remove `log4j-1.2.xx.jar` and `sfl4j-log4j12-xxx.jar` from the `lib/` directory.
-
-Next, you need to put the following jar files into the `lib/` folder:
-
- * `logback-classic.jar`
- * `logback-core.jar`
- * `log4j-over-slf4j.jar`: This bridge needs to be present in the classpath for redirecting logging calls from Hadoop (which is using Log4j) to Slf4j.
-
-Note that you need to explicitly set the `lib/` directory when using a per-job YARN cluster.
-
-The command to submit Flink on YARN with a custom logger is: `./bin/flink run -yt $FLINK_HOME/lib <... remaining arguments ...>`
-
-{% top %}
diff --git a/docs/dev/projectsetup/dependencies.zh.md b/docs/dev/projectsetup/dependencies.zh.md
index 73a2e5c89ef4e..267bc30f14ffe 100644
--- a/docs/dev/projectsetup/dependencies.zh.md
+++ b/docs/dev/projectsetup/dependencies.zh.md
@@ -94,10 +94,8 @@ under the License.
 我们强烈建议保持这些依赖的作用域为 *provided* 。 如果它们的作用域未设置为 *provided* ，则典型的情况是因为包含了 Flink 的核心依赖而导致生成的jar包变得过大。
 最糟糕的情况是添加到应用程序的 Flink 核心依赖项与你自己的一些依赖项版本冲突（通常通过反向类加载来避免）。
 
-**IntelliJ 上的一些注意事项:** 为了可以让 Flink 应用在 IntelliJ IDEA 中运行，这些 Flink 核心依赖的作用域需要设置为 *compile* 而不是 *provided* 。
-否则 IntelliJ 不会添加这些依赖到 classpath，会导致应用运行时抛出 `NoClassDefFountError` 异常。为了避免声明这些依赖的作用域为 *compile* (因为我们不推荐这样做)，
-上文给出的 Java 和 Scala 项目模板使用了一个小技巧：添加了一个 profile，仅当应用程序在 IntelliJ 中运行时该 profile 才会被激活，
-然后将依赖作用域设置为 *compile* ，从而不影响应用 jar 包。
+**IntelliJ 上的一些注意事项:** 为了可以让 Flink 应用在 IntelliJ IDEA 中运行，需要在 Run 配置界面将 `Include dependencies with "Provided" scope` 选项勾选上。
+如果这一选项还不可用（可能是因为你正在使用一个老版本的 IntelliJ IDEA），那么一个简单的解决方案是创建一个测试，并在测试中调用应用程序的 `main()` 方法。
 
 ## 添加连接器以及类库依赖
 
diff --git a/docs/dev/stream/testing.zh.md b/docs/dev/stream/testing.zh.md
index 87d139340ecde..2b019920a0d52 100644
--- a/docs/dev/stream/testing.zh.md
+++ b/docs/dev/stream/testing.zh.md
@@ -578,7 +578,7 @@ A few remarks on integration testing with `MiniClusterWithClientResource`:
 Communicating with operators instantiated by a local Flink mini cluster via static variables is one way around this issue.
 Alternatively, you could write the data to files in a temporary directory with your test sink.
 
-* You can implement a custom *parallel* source function for emitting watermarks if your job uses event timer timers.
+* You can implement a custom *parallel* source function for emitting watermarks if your job uses event time timers.
 
 * It is recommended to always test your pipelines locally with a parallelism > 1 to identify bugs which only surface for the pipelines executed in parallel.
 
diff --git a/docs/dev/table/config.zh.md b/docs/dev/table/config.zh.md
index 1947ab8b54b6b..0ef01a1a6227a 100644
--- a/docs/dev/table/config.zh.md
+++ b/docs/dev/table/config.zh.md
@@ -104,3 +104,7 @@ The following options can be used to tune the performance of the query execution
 The following options can be used to adjust the behavior of the query optimizer to get a better execution plan.
 
 {% include generated/optimizer_config_configuration.html %}
+
+### Python Options
+
+{% include generated/python_configuration.html %}
diff --git a/docs/dev/table/connect.zh.md b/docs/dev/table/connect.zh.md
index e9916ff037ffe..504bb01b30133 100644
--- a/docs/dev/table/connect.zh.md
+++ b/docs/dev/table/connect.zh.md
@@ -852,7 +852,7 @@ CREATE TABLE MyUserTable (
     .start_from_earliest()
     .start_from_latest()
     .start_from_specific_offsets(...)
-    .startFromTimestamp(...)
+    .start_from_timestamp(...)
 
     # optional: output partitioning from Flink's partitions into Kafka's partitions
     .sink_partitioner_fixed()        # each Flink partition ends up in at-most one Kafka partition (default)
@@ -883,7 +883,6 @@ connector:
   specific-offsets: partition:0,offset:42;partition:1,offset:300  # optional: used in case of startup mode with specific offsets
   startup-timestamp-millis: 1578538374471                         # optional: used in case of startup mode with timestamp
 
-
   sink-partitioner: ...    # optional: output partitioning from Flink's partitions into Kafka's partitions
                            # valid are "fixed" (each Flink partition ends up in at most one Kafka partition),
                            # "round-robin" (a Flink partition is distributed to Kafka partitions round-robin)
@@ -1425,28 +1424,28 @@ The CSV format can be used as follows:
 CREATE TABLE MyUserTable (
   ...
 ) WITH (
-  'format.type' = 'csv',                     -- required: specify the schema type
-
-  'format.field-delimiter' = ';',            -- optional: field delimiter character (',' by default)
-
-  'format.line-delimiter' = U&'\000D\000A',  -- optional: line delimiter ("\n" by default, otherwise
-                                             -- "\r" or "\r\n" are allowed), unicode is supported if
-                                             -- the delimiter is an invisible special character,
-                                             -- e.g. U&'\000D' is the unicode representation of carriage return "\r"
-                                             -- e.g. U&'\000A' is the unicode representation of line feed "\n"
-  'format.disable-quote-character' = 'true', -- optional: disabled quote character for enclosing field values (false by default)
-                                             -- if true, format.quote-character can not be set
-  'format.quote-character' = '''',           -- optional: quote character for enclosing field values ('"' by default)
-  'format.allow-comments' = 'true',          -- optional: ignores comment lines that start with "#"
-                                             -- (disabled by default);
-                                             -- if enabled, make sure to also ignore parse errors to allow empty rows
-  'format.ignore-parse-errors' = 'true',     -- optional: skip fields and rows with parse errors instead of failing;
-                                             -- fields are set to null in case of errors
-  'format.array-element-delimiter' = '|',    -- optional: the array element delimiter string for separating
-                                             -- array and row element values (";" by default)
-  'format.escape-character' = '\\',          -- optional: escape character for escaping values (disabled by default)
-  'format.null-literal' = 'n/a'              -- optional: null literal string that is interpreted as a
-                                             -- null value (disabled by default)
+  'format.type' = 'csv',                      -- required: specify the schema type
+
+  'format.field-delimiter' = ';',             -- optional: field delimiter character (',' by default)
+
+  'format.line-delimiter' = U&'\000D\000A',   -- optional: line delimiter ("\n" by default, otherwise
+                                              -- "\r" or "\r\n" are allowed), unicode is supported if
+                                              -- the delimiter is an invisible special character,
+                                              -- e.g. U&'\000D' is the unicode representation of carriage return "\r"
+                                              -- e.g. U&'\000A' is the unicode representation of line feed "\n"
+  'format.disable-quote-character' = 'true',  -- optional: disabled quote character for enclosing field values (false by default)
+                                              -- if true, format.quote-character can not be set
+  'format.quote-character' = '''',            -- optional: quote character for enclosing field values ('"' by default)
+  'format.allow-comments' = 'true',           -- optional: ignores comment lines that start with "#"
+                                              -- (disabled by default);
+                                              -- if enabled, make sure to also ignore parse errors to allow empty rows
+  'format.ignore-parse-errors' = 'true',      -- optional: skip fields and rows with parse errors instead of failing;
+                                              -- fields are set to null in case of errors
+  'format.array-element-delimiter' = '|',     -- optional: the array element delimiter string for separating
+                                              -- array and row element values (";" by default)
+  'format.escape-character' = '\\',           -- optional: escape character for escaping values (disabled by default)
+  'format.null-literal' = 'n/a'               -- optional: null literal string that is interpreted as a
+                                              -- null value (disabled by default)
 )
 {% endhighlight %}
 </div>
diff --git a/docs/dev/table/types.zh.md b/docs/dev/table/types.zh.md
index fbcde406fd232..4dc42af4270b9 100644
--- a/docs/dev/table/types.zh.md
+++ b/docs/dev/table/types.zh.md
@@ -185,7 +185,7 @@ For the *Data Type Representation* column the table omits the prefix `org.apache
 | `OBJECT_ARRAY(...)` | `OBJECT_ARRAY<...>` | `ARRAY(`<br>`DATATYPE.bridgedTo(OBJECT.class))` | |
 | `MULTISET(...)` | | `MULTISET(...)` | |
 | `MAP(..., ...)` | `MAP<...,...>` | `MAP(...)` | |
-| other generic types | | `ANY(...)` | |
+| other generic types | | `RAW(...)` | |
 
 <span class="label label-danger">Attention</span> If there is a problem with the new type system. Users
 can fallback to type information defined in `org.apache.flink.table.api.Types` at any time.
@@ -218,7 +218,7 @@ The following data types are supported:
 | `MULTISET` | |
 | `MAP` | |
 | `ROW` | |
-| `ANY` | |
+| `RAW` | |
 
 Limitations
 -----------
@@ -1197,12 +1197,12 @@ DataTypes.NULL()
 |`java.lang.Object` | X     | X      | *Default*                            |
 |*any class*        |       | (X)    | Any non-primitive type.              |
 
-#### `ANY`
+#### `RAW`
 
 Data type of an arbitrary serialized type. This type is a black box within the table ecosystem
 and is only deserialized at the edges.
 
-The any type is an extension to the SQL standard.
+The raw type is an extension to the SQL standard.
 
 **Declaration**
 
@@ -1210,25 +1210,25 @@ The any type is an extension to the SQL standard.
 
 <div data-lang="SQL" markdown="1">
 {% highlight text %}
-ANY('class', 'snapshot')
+RAW('class', 'snapshot')
 {% endhighlight %}
 </div>
 
 <div data-lang="Java/Scala" markdown="1">
 {% highlight java %}
-DataTypes.ANY(class, serializer)
+DataTypes.RAW(class, serializer)
 
-DataTypes.ANY(typeInfo)
+DataTypes.RAW(typeInfo)
 {% endhighlight %}
 </div>
 
 </div>
 
-The type can be declared using `ANY('class', 'snapshot')` where `class` is the originating class and
+The type can be declared using `RAW('class', 'snapshot')` where `class` is the originating class and
 `snapshot` is the serialized `TypeSerializerSnapshot` in Base64 encoding. Usually, the type string is not
 declared directly but is generated while persisting the type.
 
-In the API, the `ANY` type can be declared either by directly supplying a `Class` + `TypeSerializer` or
+In the API, the `RAW` type can be declared either by directly supplying a `Class` + `TypeSerializer` or
 by passing `TypeInformation` and let the framework extract `Class` + `TypeSerializer` from there.
 
 **Bridging to JVM Types**
diff --git a/docs/getting-started/walkthroughs/python_table_api.zh.md b/docs/getting-started/walkthroughs/python_table_api.zh.md
index 3d3a347d45ac4..0805cbe69bf55 100644
--- a/docs/getting-started/walkthroughs/python_table_api.zh.md
+++ b/docs/getting-started/walkthroughs/python_table_api.zh.md
@@ -64,7 +64,6 @@ t_env = BatchTableEnvironment.create(exec_env, t_config)
 {% highlight python %}
 t_env.connect(FileSystem().path('/tmp/input')) \
     .with_format(OldCsv()
-                 .line_delimiter(' ')
                  .field('word', DataTypes.STRING())) \
     .with_schema(Schema()
                  .field('word', DataTypes.STRING())) \
diff --git a/docs/index.zh.md b/docs/index.zh.md
index 1e02e080165f1..2315024829eab 100644
--- a/docs/index.zh.md
+++ b/docs/index.zh.md
@@ -59,6 +59,7 @@ API 参考列举并解释了 Flink API 的所有功能。
 
 发布日志包含了 Flink 版本之间的重大更新。请在你升级 Flink 之前仔细阅读相应的发布日志。
 
+* [Flink 1.10 的发布日志](release-notes/flink-1.10.html).
 * [Flink 1.9 的发布日志](release-notes/flink-1.9.html)。
 * [Flink 1.8 的发布日志](release-notes/flink-1.8.html)。
 * [Flink 1.7 的发布日志](release-notes/flink-1.7.html)。
diff --git a/docs/monitoring/metrics.zh.md b/docs/monitoring/metrics.zh.md
index ab3b1b5931519..58763c58ebf33 100644
--- a/docs/monitoring/metrics.zh.md
+++ b/docs/monitoring/metrics.zh.md
@@ -563,7 +563,7 @@ reporters will be instantiated on each job and task manager when they are starte
 - `metrics.reporter.<name>.factory.class`: The reporter factory class to use for the reporter named `<name>`.
 - `metrics.reporter.<name>.interval`: The reporter interval to use for the reporter named `<name>`.
 - `metrics.reporter.<name>.scope.delimiter`: The delimiter to use for the identifier (default value use `metrics.scope.delimiter`) for the reporter named `<name>`.
-- `metrics.reporter.<name>.scope.variables.excludes`: (optional) A semicolon (;) separated list of variables that should be ignored by tag-based reporters. 
+- `metrics.reporter.<name>.scope.variables.excludes`: (optional) A semi-colon (;) separate list of variables that should be ignored by tag-based reporters (e.g., Prometheus, InfluxDB).
 - `metrics.reporters`: (optional) A comma-separated include list of reporter names. By default all configured reporters will be used.
 
 All reporters must at least have either the `class` or `factory.class` property. Which property may/should be used depends on the reporter implementation. See the individual reporter configuration sections for more information.
@@ -577,6 +577,7 @@ metrics.reporters: my_jmx_reporter,my_other_reporter
 
 metrics.reporter.my_jmx_reporter.factory.class: org.apache.flink.metrics.jmx.JMXReporterFactory
 metrics.reporter.my_jmx_reporter.port: 9020-9040
+metrics.reporter.my_jmx_reporter.scope.variables.excludes:job_id;task_attempt_num
 
 metrics.reporter.my_other_reporter.class: org.apache.flink.metrics.graphite.GraphiteReporter
 metrics.reporter.my_other_reporter.host: 192.168.1.1
@@ -654,15 +655,7 @@ of your Flink distribution.
 
 Parameters:
 
-- `host` - the InfluxDB server host
-- `port` - (optional) the InfluxDB server port, defaults to `8086`
-- `db` - the InfluxDB database to store metrics
-- `username` - (optional) InfluxDB username used for authentication
-- `password` - (optional) InfluxDB username's password used for authentication
-- `retentionPolicy` - (optional) InfluxDB retention policy, defaults to retention policy defined on the server for the db
-- `consistency` - (optional) InfluxDB consistency level for metrics. Possible values: [ALL, ANY, ONE, QUORUM], default is ONE
-- `connectTimeout` - (optional) the InfluxDB client connect timeout in milliseconds, default is 10000 ms
-- `writeTimeout` - (optional) the InfluxDB client write timeout in milliseconds, default is 10000 ms
+{% include generated/influxdb_reporter_configuration.html %}
 
 Example configuration:
 
@@ -1378,7 +1371,7 @@ Certain RocksDB native metrics are available but disabled by default, you can fi
     <tr>
       <th rowspan="1"><strong>Job (only available on TaskManager)</strong></th>
       <td>[&lt;source_id&gt;.[&lt;source_subtask_index&gt;.]]&lt;operator_id&gt;.&lt;operator_subtask_index&gt;.latency</td>
-      <td>The latency distributions from a given source (subtask) to an operator subtask (in milliseconds), depending on the <a href="{{ site.baseurl }}/zh/ops/config.html#metrics-latency-granularity">latency granularity</a>.</td>
+      <td>The latency distributions from a given source (subtask) to an operator subtask (in milliseconds), depending on the <a href="{{ site.baseurl }}/ops/config.html#metrics-latency-granularity">latency granularity</a>.</td>
       <td>Histogram</td>
     </tr>
     <tr>
diff --git a/docs/ops/deployment/docker.zh.md b/docs/ops/deployment/docker.zh.md
index 3473411caf59f..d82d736d86de9 100644
--- a/docs/ops/deployment/docker.zh.md
+++ b/docs/ops/deployment/docker.zh.md
@@ -1,5 +1,5 @@
 ---
-title:  "Docker 安装"
+title:  "Docker 设置"
 nav-title: Docker
 nav-parent_id: deployment
 nav-pos: 6
diff --git a/docs/ops/deployment/kubernetes.zh.md b/docs/ops/deployment/kubernetes.zh.md
index de3d8850e64b1..071ae29612b1e 100644
--- a/docs/ops/deployment/kubernetes.zh.md
+++ b/docs/ops/deployment/kubernetes.zh.md
@@ -1,5 +1,5 @@
 ---
-title:  "Kubernetes 安装"
+title:  "Kubernetes 设置"
 nav-title: Kubernetes
 nav-parent_id: deployment
 nav-pos: 7
diff --git a/docs/ops/deployment/local.zh.md b/docs/ops/deployment/local.zh.md
index d3e5edc554700..82d85d6dce4bc 100644
--- a/docs/ops/deployment/local.zh.md
+++ b/docs/ops/deployment/local.zh.md
@@ -1,5 +1,5 @@
 ---
-title: "Local Cluster"
+title: "本地集群"
 nav-title: 'Local Cluster'
 nav-parent_id: deployment
 nav-pos: 2
diff --git a/docs/ops/deployment/mesos.zh.md b/docs/ops/deployment/mesos.zh.md
index f39e1603acd16..e00a13d9708e6 100644
--- a/docs/ops/deployment/mesos.zh.md
+++ b/docs/ops/deployment/mesos.zh.md
@@ -1,5 +1,5 @@
 ---
-title:  "Mesos 安装"
+title:  "Mesos 设置"
 nav-title: Mesos
 nav-parent_id: deployment
 nav-pos: 5
@@ -237,7 +237,7 @@ Here is an example configuration for Marathon:
 
     {
         "id": "flink",
-        "cmd": "$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.size=1024m -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.heap.size=1024m -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1",
+        "cmd": "$FLINK_HOME/bin/mesos-appmaster.sh -Djobmanager.heap.size=1024m -Djobmanager.rpc.port=6123 -Drest.port=8081 -Dmesos.resourcemanager.tasks.mem=1024 -Dtaskmanager.memory.process.size=1024m -Dtaskmanager.numberOfTaskSlots=2 -Dparallelism.default=2 -Dmesos.resourcemanager.tasks.cpus=1",
         "cpus": 1.0,
         "mem": 1024
     }
diff --git a/docs/ops/deployment/native_kubernetes.zh.md b/docs/ops/deployment/native_kubernetes.zh.md
index 27155ae22e809..fa572524b8e1a 100644
--- a/docs/ops/deployment/native_kubernetes.zh.md
+++ b/docs/ops/deployment/native_kubernetes.zh.md
@@ -1,5 +1,5 @@
 ---
-title:  "Native Kubernetes 安装"
+title:  "原生 Kubernetes 设置"
 nav-title: Native Kubernetes
 nav-parent_id: deployment
 is_beta: true
diff --git a/docs/ops/deployment/yarn_setup.zh.md b/docs/ops/deployment/yarn_setup.zh.md
index 5e75d91d9adc3..887bd71bda688 100644
--- a/docs/ops/deployment/yarn_setup.zh.md
+++ b/docs/ops/deployment/yarn_setup.zh.md
@@ -1,5 +1,5 @@
 ---
-title:  "YARN Setup"
+title:  "YARN 设置"
 nav-title: YARN
 nav-parent_id: deployment
 nav-pos: 4
diff --git a/docs/ops/filesystems/azure.zh.md b/docs/ops/filesystems/azure.zh.md
index d721be5b8ad8a..4d81b5596b682 100644
--- a/docs/ops/filesystems/azure.zh.md
+++ b/docs/ops/filesystems/azure.zh.md
@@ -62,17 +62,24 @@ cp ./opt/flink-azure-fs-hadoop-{{ site.version }}.jar ./plugins/azure-fs-hadoop/
 
 `flink-azure-fs-hadoop` registers default FileSystem wrappers for URIs with the *wasb://* and *wasbs://* (SSL encrypted access) scheme.
 
-#### Configurations setup
-After setting up the Azure Blob Storage FileSystem wrapper, you need to configure credentials to make sure that Flink is allowed to access Azure Blob Storage.
+### Credentials Configuration
 
-To allow for easy adoption, you can use the same configuration keys in `flink-conf.yaml` as in Hadoop's `core-site.xml`
+Hadoop's Azure Filesystem supports configuration of credentials via the Hadoop configuration as
+outlined in the [Hadoop Azure Blob Storage documentation](https://hadoop.apache.org/docs/current/hadoop-azure/index.html#Configuring_Credentials).
+For convenience Flink forwards all Flink configurations with a key prefix of `fs.azure` to the
+Hadoop configuration of the filesystem. Consequentially, the azure blob storage key can be configured
+in `flink-conf.yaml` via:
 
-You can see the configuration keys in the [Hadoop Azure Blob Storage documentation](https://hadoop.apache.org/docs/current/hadoop-azure/index.html#Configuring_Credentials).
+{% highlight yaml %}
+fs.azure.account.key.<account_name>.blob.core.windows.net: <azure_storage_key>
+{% endhighlight %}
 
-There are some required configurations that must be added to `flink-conf.yaml`:
+Alternatively, the the filesystem can be configured to read the Azure Blob Storage key from an
+environment variable `AZURE_STORAGE_KEY` by setting the following configuration keys in
+`flink-conf.yaml`.
 
 {% highlight yaml %}
-fs.azure.account.key.youraccount.blob.core.windows.net: Azure Blob Storage access key
+fs.azure.account.keyprovider.<account_name>.blob.core.windows.net: org.apache.flink.fs.azurefs.EnvironmentVariableKeyProvider
 {% endhighlight %}
 
 {% top %}
diff --git a/docs/ops/filesystems/oss.zh.md b/docs/ops/filesystems/oss.zh.md
index f310d20ae5e5b..a93754662b0b3 100644
--- a/docs/ops/filesystems/oss.zh.md
+++ b/docs/ops/filesystems/oss.zh.md
@@ -77,4 +77,13 @@ fs.oss.accessKeyId: Aliyun access key ID
 fs.oss.accessKeySecret: Aliyun access key secret
 {% endhighlight %}
 
+An alternative `CredentialsProvider` can also be configured in the `flink-conf.yaml`, e.g.
+{% highlight yaml %}
+# Read Credentials from OSS_ACCESS_KEY_ID and OSS_ACCESS_KEY_SECRET
+fs.oss.credentials.provider: com.aliyun.oss.common.auth.EnvironmentVariableCredentialsProvider
+{% endhighlight %}
+Other credential providers can be found under https://github.com/aliyun/aliyun-oss-java-sdk/tree/master/src/main/java/com/aliyun/oss/common/auth.
+
+
+
 {% top %}
diff --git a/docs/ops/python_shell.zh.md b/docs/ops/python_shell.zh.md
index 0f8a7ee513731..67881247bf993 100644
--- a/docs/ops/python_shell.zh.md
+++ b/docs/ops/python_shell.zh.md
@@ -152,8 +152,7 @@ bin/pyflink-shell.sh remote <hostname> <portnumber>
 
 ### Yarn Python Shell cluster
 
-Python Shell可以运行在YARN集群之上。YARN的container的数量可以通过参数`-n <arg>`进行
-指定。Python shell在Yarn上部署一个新的Flink集群，并进行连接。除了指定container数量，你也
+Python Shell可以运行在YARN集群之上。Python shell在Yarn上部署一个新的Flink集群，并进行连接。除了指定container数量，你也
 可以指定JobManager的内存，YARN应用的名字等参数。
 例如，在一个部署了两个TaskManager的Yarn集群上运行Python Shell:
 
diff --git a/docs/ops/security-ssl.zh.md b/docs/ops/security-ssl.zh.md
index 39a4ff7502135..ae1a36a8ba269 100644
--- a/docs/ops/security-ssl.zh.md
+++ b/docs/ops/security-ssl.zh.md
@@ -1,5 +1,5 @@
 ---
-title: "SSL 配置"
+title: "SSL 设置"
 nav-parent_id: ops
 nav-pos: 11
 ---
@@ -52,15 +52,24 @@ Internal connectivity includes:
 All internal connections are SSL authenticated and encrypted. The connections use **mutual authentication**, meaning both server
 and client side of each connection need to present the certificate to each other. The certificate acts effectively as a shared
 secret when a dedicated CA is used to exclusively sign an internal cert.
+The certificate for internal communication is not needed by any other party to interact with Flink, and can be simply
+added to the container images, or attached to the YARN deployment.
 
-It is highly recommended to generate a dedicated certificate (self-signed) for a Flink deployment. The certificate for internal communication
-is not needed by any other party to interact with Flink, and can be simply added to the container images, or attached to the YARN deployment.
+  - The easiest way to realize this setup is by generating a dedicated public/private key pair and self-signed certificate
+    for the Flink deployment. The key- and truststore are identical and contains only that key pair / certificate.
+    An example is [shown below](#example-ssl-setup-standalone-and-kubernetes).
 
-An environment where operators are constrained to use firm wide Internal CA and can not generate self-signed certificate, specify the SHA1 certificate fingerprint to protect the cluster allowing only specific certificate to trusted by the cluster.
+  - In an environment where operators are constrained to use firm-wide Internal CA (cannot generate self-signed certificates),
+    the recommendation is to still have a dedicated key pair / certificate for the Flink deployment, signed by that CA.
+    However, the TrustStore must then also contain the CA's public certificate tho accept the deployment's certificate
+    during the SSL handshake (requirement in JDK TrustStore implementation).
 
-*Note: Because internal connections are mutually authenticated with shared certificates, Flink can skip hostname verification. This makes container-based setups easier.*
+    **NOTE:** Because of that, it is critical that you specify the fingerprint of the deployment certificate
+    (`security.ssl.internal.cert.fingerprint`), when it is not self-signed, to pin that certificate as the only trusted
+    certificate and prevent the TrustStore from trusting all certificates signed by that CA.
 
-*IMPORTANT: Do not use certificate issued by public CA without pinning the fingerprint of the certificate.*
+*Note: Because internal connections are mutually authenticated with shared certificates, Flink can skip hostname verification.
+This makes container-based setups easier.*
 
 ### External / REST Connectivity
 
@@ -108,9 +117,9 @@ need to be set up such that the truststore trusts the keystore's certificate.
 
 #### Internal Connectivity
 
-Because internal communication is mutually authenticated, keystore and truststore typically contain the same dedicated certificate.
-The certificate can use wild card hostnames or addresses, because the certificate is expected to be a shared secret and host
-names are not verified. It is even possible to use the same file (the keystore) also as the truststore.
+Because internal communication is mutually authenticated between server and client side, keystore and truststore typically refer to a dedicated
+certificate that acts as a shared secret. In such a setup, the certificate can use wild card hostnames or addresses.
+WHen using self-signed certificates, it is even possible to use the same file as keystore and truststore.
 
 {% highlight yaml %}
 security.ssl.internal.keystore: /path/to/file.keystore
@@ -120,7 +129,8 @@ security.ssl.internal.truststore: /path/to/file.truststore
 security.ssl.internal.truststore-password: truststore_password
 {% endhighlight %}
 
-When using certificate from public CA, Use certificate pinning to allow only specific internal certificate to establish the connectivity.
+When using a certificate that is not self-signed, but signed by a CA, you need to use certificate pinning to allow only a
+a specific certificate to be trusted when establishing the connectivity.
 
 {% highlight yaml %}
 security.ssl.internal.cert.fingerprint: 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00
diff --git a/docs/ops/upgrading.zh.md b/docs/ops/upgrading.zh.md
index 4aee9bc54e89d..349ecb0129133 100644
--- a/docs/ops/upgrading.zh.md
+++ b/docs/ops/upgrading.zh.md
@@ -159,12 +159,12 @@ Besides operator uids, there are currently two *hard* preconditions for job migr
 `semi-asynchronous` mode. In case your old job was using this mode, you can still change your job to use
 `fully-asynchronous` mode before taking the savepoint that is used as the basis for the migration.
 
-2. Another **important** precondition is that all the savepoint data must be accessible from the new installation 
-under the same (absolute) path. 
-This also includes access to any additional files that are referenced from inside the 
-savepoint file (the output from state backend snapshots), including, but not limited to additional referenced 
-savepoints from modifications with the [State Processor API]({{ site.baseurl }}/dev/libs/state_processor_api.html). 
-Any savepoint data is currently referenced by absolute paths inside the meta data file and thus a savepoint is 
+2. Another **important** precondition is that all the savepoint data must be accessible from the new installation
+under the same (absolute) path.
+This also includes access to any additional files that are referenced from inside the
+savepoint file (the output from state backend snapshots), including, but not limited to additional referenced
+savepoints from modifications with the [State Processor API]({{ site.baseurl }}/dev/libs/state_processor_api.html).
+Any savepoint data is currently referenced by absolute paths inside the meta data file and thus a savepoint is
 not relocatable via typical filesystem operations.
 
 ### STEP 1: Take a savepoint in the old Flink version.
@@ -216,6 +216,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
       <th class="text-center">1.7.x</th>
       <th class="text-center">1.8.x</th>
       <th class="text-center">1.9.x</th>
+      <th class="text-center">1.10.x</th>
       <th class="text-center">Limitations</th>
     </tr>
   </thead>
@@ -231,6 +232,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center"></td>
           <td class="text-center"></td>
           <td class="text-center"></td>
+          <td class="text-center"></td>
           <td class="text-left">The maximum parallelism of a job that was migrated from Flink 1.1.x to 1.2.x+ is
           currently fixed as the parallelism of the job. This means that the parallelism can not be increased after
           migration. This limitation might be removed in a future bugfix release.</td>
@@ -246,6 +248,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left">
           When migrating from Flink 1.2.x to Flink 1.3.x+, changing parallelism at the same
           time is not supported. Users have to first take a savepoint after migrating to Flink 1.3.x+, and then change
@@ -267,6 +270,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left">Migrating from Flink 1.3.0 to Flink 1.4.[0,1] will fail if the savepoint contains Scala case classes. Users have to directly migrate to 1.4.2+ instead.</td>
     </tr>
     <tr>
@@ -280,6 +284,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left"></td>
     </tr>
     <tr>
@@ -293,6 +298,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left">There is a known issue with resuming broadcast state created with 1.5.x in versions
           1.6.x up to 1.6.2, and 1.7.0: <a href="https://issues.apache.org/jira/browse/FLINK-11087">FLINK-11087</a>. Users
           upgrading to 1.6.x or 1.7.x series need to directly migrate to minor versions higher than 1.6.2 and 1.7.0,
@@ -309,6 +315,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left"></td>
     </tr>
     <tr>
@@ -322,6 +329,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center">O</td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left"></td>
     </tr>
     <tr>
@@ -335,6 +343,7 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center"></td>
           <td class="text-center">O</td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
           <td class="text-left"></td>
     </tr>
     <tr>
@@ -348,6 +357,21 @@ Savepoints are compatible across Flink versions as indicated by the table below:
           <td class="text-center"></td>
           <td class="text-center"></td>
           <td class="text-center">O</td>
+          <td class="text-center">O</td>
+          <td class="text-left"></td>
+    </tr>
+    <tr>
+          <td class="text-center"><strong>1.10.x</strong></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center"></td>
+          <td class="text-center">O</td>
           <td class="text-left"></td>
     </tr>
   </tbody>
diff --git a/flink-core/src/test/java/org/apache/flink/testutils/migration/MigrationVersion.java b/flink-core/src/test/java/org/apache/flink/testutils/migration/MigrationVersion.java
index 6fe393998eba4..4122c5decc4da 100644
--- a/flink-core/src/test/java/org/apache/flink/testutils/migration/MigrationVersion.java
+++ b/flink-core/src/test/java/org/apache/flink/testutils/migration/MigrationVersion.java
@@ -33,7 +33,8 @@ public enum MigrationVersion {
 	v1_6("1.6"),
 	v1_7("1.7"),
 	v1_8("1.8"),
-	v1_9("1.9");
+	v1_9("1.9"),
+	v1_10("1.10");
 
 	private String versionStr;
 
diff --git a/flink-end-to-end-tests/run-nightly-tests.sh b/flink-end-to-end-tests/run-nightly-tests.sh
index aa51819d4c38c..5907b42fb9b45 100755
--- a/flink-end-to-end-tests/run-nightly-tests.sh
+++ b/flink-end-to-end-tests/run-nightly-tests.sh
@@ -88,8 +88,11 @@ run_test "Resuming Externalized Checkpoint after terminal failure (rocks, increm
 # Docker
 ################################################################################
 
-run_test "Running Kerberized YARN on Docker test (default input)" "$END_TO_END_DIR/test-scripts/test_yarn_kerberos_docker.sh"
-run_test "Running Kerberized YARN on Docker test (custom fs plugin)" "$END_TO_END_DIR/test-scripts/test_yarn_kerberos_docker.sh dummy-fs"
+# Ignore these tests on Azure: In these tests, the TaskManagers are not starting on YARN, probably due to memory constraints.
+if [ -z "$TF_BUILD" ] ; then
+	run_test "Running Kerberized YARN on Docker test (default input)" "$END_TO_END_DIR/test-scripts/test_yarn_kerberos_docker.sh"
+	run_test "Running Kerberized YARN on Docker test (custom fs plugin)" "$END_TO_END_DIR/test-scripts/test_yarn_kerberos_docker.sh dummy-fs"
+fi
 
 ################################################################################
 # High Availability
diff --git a/flink-end-to-end-tests/test-scripts/common_kubernetes.sh b/flink-end-to-end-tests/test-scripts/common_kubernetes.sh
index b634e1e196c4c..bcfbb36234d6c 100755
--- a/flink-end-to-end-tests/test-scripts/common_kubernetes.sh
+++ b/flink-end-to-end-tests/test-scripts/common_kubernetes.sh
@@ -30,12 +30,14 @@ RESULT_HASH="e682ec6622b5e83f2eb614617d5ab2cf"
 function setup_kubernetes_for_linux {
     # Download kubectl, which is a requirement for using minikube.
     if ! [ -x "$(command -v kubectl)" ]; then
+        echo "Installing kubectl ..."
         local version=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)
         curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubectl && \
             chmod +x kubectl && sudo mv kubectl /usr/local/bin/
     fi
     # Download minikube.
     if ! [ -x "$(command -v minikube)" ]; then
+        echo "Installing minikube ..."
         curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && \
             chmod +x minikube && sudo mv minikube /usr/local/bin/
     fi
@@ -48,9 +50,10 @@ function check_kubernetes_status {
 
 function start_kubernetes_if_not_running {
     if ! check_kubernetes_status; then
+        echo "Starting minikube ..."
         start_command="minikube start"
         # We need sudo permission to set vm-driver to none in linux os.
-        [[ "${OS_TYPE}" = "linux" ]] && start_command="sudo ${start_command} --vm-driver=none"
+        [[ "${OS_TYPE}" = "linux" ]] && start_command="sudo CHANGE_MINIKUBE_NONE_USER=true ${start_command} --vm-driver=none"
         ${start_command}
         # Fix the kubectl context, as it's often stale.
         minikube update-context
@@ -70,6 +73,7 @@ function start_kubernetes {
 }
 
 function stop_kubernetes {
+    echo "Stopping minikube ..."
     stop_command="minikube stop"
     [[ "${OS_TYPE}" = "linux" ]] && stop_command="sudo ${stop_command}"
     if ! retry_times ${MINIKUBE_START_RETRIES} ${MINIKUBE_START_BACKOFF} "${stop_command}"; then
diff --git a/flink-end-to-end-tests/test-scripts/test-runner-common.sh b/flink-end-to-end-tests/test-scripts/test-runner-common.sh
index 6a0840fd596a7..ecddbd9fe8ff3 100644
--- a/flink-end-to-end-tests/test-scripts/test-runner-common.sh
+++ b/flink-end-to-end-tests/test-scripts/test-runner-common.sh
@@ -19,7 +19,7 @@
 
 source "${END_TO_END_DIR}"/test-scripts/common.sh
 
-FLINK_VERSION=$(mvn --file ${END_TO_END_DIR}/pom.xml org.apache.maven.plugins:maven-help-plugin:3.1.0:evaluate -Dexpression=project.version -q -DforceStdout)
+export FLINK_VERSION=$(mvn --file ${END_TO_END_DIR}/pom.xml org.apache.maven.plugins:maven-help-plugin:3.1.0:evaluate -Dexpression=project.version -q -DforceStdout)
 
 #######################################
 # Prints the given description, runs the given test and prints how long the execution took.
diff --git a/flink-end-to-end-tests/test-scripts/test_mesos_multiple_submissions.sh b/flink-end-to-end-tests/test-scripts/test_mesos_multiple_submissions.sh
index dea2980596b14..fbfcd31fadf89 100755
--- a/flink-end-to-end-tests/test-scripts/test_mesos_multiple_submissions.sh
+++ b/flink-end-to-end-tests/test-scripts/test_mesos_multiple_submissions.sh
@@ -29,7 +29,7 @@ TEST_PROGRAM_JAR=$END_TO_END_DIR/flink-cli-test/target/PeriodicStreamingJob.jar
 
 function submit_job {
     local output_path=$1
-    docker exec -it mesos-master bash -c "${FLINK_DIR}/bin/flink run -d -p 1 ${TEST_PROGRAM_JAR} --durationInSecond ${DURATION} --outputPath ${output_path}" \
+    docker exec mesos-master bash -c "${FLINK_DIR}/bin/flink run -d -p 1 ${TEST_PROGRAM_JAR} --durationInSecond ${DURATION} --outputPath ${output_path}" \
         | grep "Job has been submitted with JobID" | sed 's/.* //g' | tr -d '\r'
 }
 
diff --git a/flink-end-to-end-tests/test-scripts/test_mesos_wordcount.sh b/flink-end-to-end-tests/test-scripts/test_mesos_wordcount.sh
index d2f5c40d2b86e..94db94eda532b 100755
--- a/flink-end-to-end-tests/test-scripts/test_mesos_wordcount.sh
+++ b/flink-end-to-end-tests/test-scripts/test_mesos_wordcount.sh
@@ -31,6 +31,6 @@ mkdir -p "${TEST_DATA_DIR}"
 
 start_flink_cluster_with_mesos
 
-docker exec -it mesos-master nohup bash -c "${FLINK_DIR}/bin/flink run -p 1 ${TEST_PROGRAM_JAR} ${INPUT_ARGS} --output ${OUTPUT_LOCATION}"
+docker exec mesos-master nohup bash -c "${FLINK_DIR}/bin/flink run -p 1 ${TEST_PROGRAM_JAR} ${INPUT_ARGS} --output ${OUTPUT_LOCATION}"
 
 check_result_hash "Mesos WordCount test" "${OUTPUT_LOCATION}" "${RESULT_HASH}"
diff --git a/flink-python/pyflink/fn_execution/coder_impl.py b/flink-python/pyflink/fn_execution/coder_impl.py
index 8f9414f45e1eb..9362cc423d090 100644
--- a/flink-python/pyflink/fn_execution/coder_impl.py
+++ b/flink-python/pyflink/fn_execution/coder_impl.py
@@ -24,22 +24,22 @@
 from pyflink.table import Row
 
 
-class RowCoderImpl(StreamCoderImpl):
+class FlattenRowCoderImpl(StreamCoderImpl):
 
     def __init__(self, field_coders):
         self._field_coders = field_coders
+        self._filed_count = len(field_coders)
 
     def encode_to_stream(self, value, out_stream, nested):
         self.write_null_mask(value, out_stream)
-        for i in range(len(self._field_coders)):
+        for i in range(self._filed_count):
             if value[i] is not None:
                 self._field_coders[i].encode_to_stream(value[i], out_stream, nested)
 
     def decode_from_stream(self, in_stream, nested):
-        null_mask = self.read_null_mask(len(self._field_coders), in_stream)
-        assert len(null_mask) == len(self._field_coders)
-        return Row(*[None if null_mask[idx] else self._field_coders[idx].decode_from_stream(
-            in_stream, nested) for idx in range(0, len(null_mask))])
+        null_mask = self.read_null_mask(self._filed_count, in_stream)
+        return [None if null_mask[idx] else self._field_coders[idx].decode_from_stream(
+            in_stream, nested) for idx in range(0, self._filed_count)]
 
     @staticmethod
     def write_null_mask(value, out_stream):
@@ -77,6 +77,18 @@ def read_null_mask(field_count, in_stream):
             field_pos += num_pos
         return null_mask
 
+    def __repr__(self):
+        return 'FlattenRowCoderImpl[%s]' % ', '.join(str(c) for c in self._field_coders)
+
+
+class RowCoderImpl(FlattenRowCoderImpl):
+
+    def __init__(self, field_coders):
+        super(RowCoderImpl, self).__init__(field_coders)
+
+    def decode_from_stream(self, in_stream, nested):
+        return Row(*super(RowCoderImpl, self).decode_from_stream(in_stream, nested))
+
     def __repr__(self):
         return 'RowCoderImpl[%s]' % ', '.join(str(c) for c in self._field_coders)
 
diff --git a/flink-python/pyflink/fn_execution/coders.py b/flink-python/pyflink/fn_execution/coders.py
index 4d4bc341a4c08..6da1abd47cd68 100644
--- a/flink-python/pyflink/fn_execution/coders.py
+++ b/flink-python/pyflink/fn_execution/coders.py
@@ -23,6 +23,7 @@
 import decimal
 from apache_beam.coders import Coder
 from apache_beam.coders.coders import FastCoder
+from apache_beam.typehints import typehints
 
 from pyflink.fn_execution import coder_impl
 from pyflink.fn_execution import flink_fn_execution_pb2
@@ -31,31 +32,36 @@
 FLINK_SCHEMA_CODER_URN = "flink:coder:schema:v1"
 
 
-__all__ = ['RowCoder', 'BigIntCoder', 'TinyIntCoder', 'BooleanCoder',
+__all__ = ['FlattenRowCoder', 'RowCoder', 'BigIntCoder', 'TinyIntCoder', 'BooleanCoder',
            'SmallIntCoder', 'IntCoder', 'FloatCoder', 'DoubleCoder',
            'BinaryCoder', 'CharCoder', 'DateCoder', 'TimeCoder',
            'TimestampCoder', 'ArrayCoder', 'MapCoder', 'DecimalCoder']
 
 
-class RowCoder(FastCoder):
+class FlattenRowCoder(FastCoder):
     """
-    Coder for Row.
+    Coder for Row. The decoded result will be flattened as a list of column values of a row instead
+    of a row object.
     """
 
     def __init__(self, field_coders):
         self._field_coders = field_coders
 
     def _create_impl(self):
-        return coder_impl.RowCoderImpl([c.get_impl() for c in self._field_coders])
+        return coder_impl.FlattenRowCoderImpl([c.get_impl() for c in self._field_coders])
 
     def is_deterministic(self):
         return all(c.is_deterministic() for c in self._field_coders)
 
     def to_type_hint(self):
-        return Row
+        return typehints.List
+
+    @Coder.register_urn(FLINK_SCHEMA_CODER_URN, flink_fn_execution_pb2.Schema)
+    def _pickle_from_runner_api_parameter(schema_proto, unused_components, unused_context):
+        return FlattenRowCoder([from_proto(f.type) for f in schema_proto.fields])
 
     def __repr__(self):
-        return 'RowCoder[%s]' % ', '.join(str(c) for c in self._field_coders)
+        return 'FlattenRowCoder[%s]' % ', '.join(str(c) for c in self._field_coders)
 
     def __eq__(self, other):
         return (self.__class__ == other.__class__
@@ -70,6 +76,24 @@ def __hash__(self):
         return hash(self._field_coders)
 
 
+class RowCoder(FlattenRowCoder):
+    """
+    Coder for Row.
+    """
+
+    def __init__(self, field_coders):
+        super(RowCoder, self).__init__(field_coders)
+
+    def _create_impl(self):
+        return coder_impl.RowCoderImpl([c.get_impl() for c in self._field_coders])
+
+    def to_type_hint(self):
+        return Row
+
+    def __repr__(self):
+        return 'RowCoder[%s]' % ', '.join(str(c) for c in self._field_coders)
+
+
 class CollectionCoder(FastCoder):
     """
     Base coder for collection.
@@ -317,11 +341,6 @@ def to_type_hint(self):
         return datetime.datetime
 
 
-@Coder.register_urn(FLINK_SCHEMA_CODER_URN, flink_fn_execution_pb2.Schema)
-def _pickle_from_runner_api_parameter(schema_proto, unused_components, unused_context):
-    return RowCoder([from_proto(f.type) for f in schema_proto.fields])
-
-
 type_name = flink_fn_execution_pb2.Schema.TypeName
 _type_name_mappings = {
     type_name.TINYINT: TinyIntCoder(),
diff --git a/flink-python/pyflink/fn_execution/operations.py b/flink-python/pyflink/fn_execution/operations.py
index 733cecb6a9858..a9bdc13ef6d01 100644
--- a/flink-python/pyflink/fn_execution/operations.py
+++ b/flink-python/pyflink/fn_execution/operations.py
@@ -27,7 +27,6 @@
 
 from pyflink.fn_execution import flink_fn_execution_pb2
 from pyflink.serializers import PickleSerializer
-from pyflink.table import Row
 
 SCALAR_FUNCTION_URN = "flink:transform:scalar_function:v1"
 
@@ -222,9 +221,8 @@ def close(self):
     def process(self, windowed_value):
         results = [invoker.invoke_eval(windowed_value.value) for invoker in
                    self.scalar_function_invokers]
-        result = Row(*results)
         # send the execution results back
-        self.output_processor.process_outputs(windowed_value, [result])
+        self.output_processor.process_outputs(windowed_value, [results])
 
 
 class ScalarFunctionOperation(Operation):
diff --git a/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
index 9118261bfc0a1..e3eb68d2f55f9 100644
--- a/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
@@ -20,7 +20,6 @@
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.python.env.PythonEnvironmentManager;
@@ -61,7 +60,7 @@
 	/**
 	 * The Python function execution result receiver.
 	 */
-	private final FnDataReceiver<byte[]> resultReceiver;
+	protected final FnDataReceiver<byte[]> resultReceiver;
 
 	/**
 	 * The Python execution environment manager.
@@ -100,22 +99,17 @@
 	/**
 	 * The receiver which forwards the input elements to a remote environment for processing.
 	 */
-	private transient FnDataReceiver<WindowedValue<?>> mainInputReceiver;
-
-	/**
-	 * The TypeSerializer for input elements.
-	 */
-	private transient TypeSerializer<IN> inputTypeSerializer;
+	protected transient FnDataReceiver<WindowedValue<?>> mainInputReceiver;
 
 	/**
 	 * Reusable OutputStream used to holding the serialized input elements.
 	 */
-	private transient ByteArrayOutputStreamWithPos baos;
+	protected transient ByteArrayOutputStreamWithPos baos;
 
 	/**
 	 * OutputStream Wrapper.
 	 */
-	private transient DataOutputViewStreamWrapper baosWrapper;
+	protected transient DataOutputViewStreamWrapper baosWrapper;
 
 	/**
 	 * Python libraries and shell script extracted from resource of flink-python jar.
@@ -138,7 +132,6 @@ public AbstractPythonFunctionRunner(
 	public void open() throws Exception {
 		baos = new ByteArrayOutputStreamWithPos();
 		baosWrapper = new DataOutputViewStreamWrapper(baos);
-		inputTypeSerializer = getInputTypeSerializer();
 
 		// The creation of stageBundleFactory depends on the initialized environment manager.
 		environmentManager.open();
@@ -189,19 +182,8 @@ public void close() throws Exception {
 
 	@Override
 	public void startBundle() {
-		OutputReceiverFactory receiverFactory =
-			new OutputReceiverFactory() {
-
-				// the input value type is always byte array
-				@SuppressWarnings("unchecked")
-				@Override
-				public FnDataReceiver<WindowedValue<byte[]>> create(String pCollectionId) {
-					return input -> resultReceiver.accept(input.getValue());
-				}
-			};
-
 		try {
-			remoteBundle = stageBundleFactory.getBundle(receiverFactory, stateRequestHandler, progressHandler);
+			remoteBundle = stageBundleFactory.getBundle(createOutputReceiverFactory(), stateRequestHandler, progressHandler);
 			mainInputReceiver =
 				Preconditions.checkNotNull(
 					remoteBundle.getInputReceivers().get(MAIN_INPUT_ID),
@@ -212,7 +194,7 @@ public FnDataReceiver<WindowedValue<byte[]>> create(String pCollectionId) {
 	}
 
 	@Override
-	public void finishBundle() {
+	public void finishBundle() throws Exception {
 		try {
 			remoteBundle.close();
 		} catch (Throwable t) {
@@ -222,17 +204,6 @@ public void finishBundle() {
 		}
 	}
 
-	@Override
-	public void processElement(IN element) {
-		try {
-			baos.reset();
-			inputTypeSerializer.serialize(element, baosWrapper);
-			mainInputReceiver.accept(WindowedValue.valueInGlobalWindow(baos.toByteArray()));
-		} catch (Throwable t) {
-			throw new RuntimeException("Failed to process element when sending data to Python SDK harness.", t);
-		}
-	}
-
 	@VisibleForTesting
 	public JobBundleFactory createJobBundleFactory(Struct pipelineOptions) throws Exception {
 		return DefaultJobBundleFactory.create(
@@ -254,8 +225,5 @@ protected RunnerApi.Environment createPythonExecutionEnvironment() throws Except
 	 */
 	public abstract ExecutableStage createExecutableStage() throws Exception;
 
-	/**
-	 * Returns the TypeSerializer for input elements.
-	 */
-	public abstract TypeSerializer<IN> getInputTypeSerializer();
+	public abstract OutputReceiverFactory createOutputReceiverFactory();
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/functions/python/PythonScalarFunctionFlatMap.java b/flink-python/src/main/java/org/apache/flink/table/runtime/functions/python/PythonScalarFunctionFlatMap.java
index f53220daf5044..97855093a16b5 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/functions/python/PythonScalarFunctionFlatMap.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/functions/python/PythonScalarFunctionFlatMap.java
@@ -37,7 +37,7 @@
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner;
+import org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.utils.LegacyTypeInfoDataTypeConverter;
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractStatelessFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractStatelessFunctionOperator.java
index 84217cd385b95..3670ac7a34834 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractStatelessFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractStatelessFunctionOperator.java
@@ -192,7 +192,7 @@ public void processElement(IN element) throws Exception {
 	/**
 	 * The collector is used to convert a {@link Row} to a {@link CRow}.
 	 */
-	protected static class StreamRecordCRowWrappingCollector implements Collector<Row> {
+	public static class StreamRecordCRowWrappingCollector implements Collector<Row> {
 
 		private final Collector<StreamRecord<CRow>> out;
 		private final CRow reuseCRow = new CRow();
@@ -202,7 +202,7 @@ protected static class StreamRecordCRowWrappingCollector implements Collector<Ro
 		 */
 		private final StreamRecord<CRow> reuseStreamRecord = new StreamRecord<>(reuseCRow);
 
-		StreamRecordCRowWrappingCollector(Collector<StreamRecord<CRow>> out) {
+		public StreamRecordCRowWrappingCollector(Collector<StreamRecord<CRow>> out) {
 			this.out = out;
 		}
 
@@ -225,7 +225,7 @@ public void close() {
 	/**
 	 * The collector is used to convert a {@link BaseRow} to a {@link StreamRecord}.
 	 */
-	protected static class StreamRecordBaseRowWrappingCollector implements Collector<BaseRow> {
+	public static class StreamRecordBaseRowWrappingCollector implements Collector<BaseRow> {
 
 		private final Collector<StreamRecord<BaseRow>> out;
 
@@ -234,7 +234,7 @@ protected static class StreamRecordBaseRowWrappingCollector implements Collector
 		 */
 		private final StreamRecord<BaseRow> reuseStreamRecord = new StreamRecord<>(null);
 
-		StreamRecordBaseRowWrappingCollector(Collector<StreamRecord<BaseRow>> out) {
+		public StreamRecordBaseRowWrappingCollector(Collector<StreamRecord<BaseRow>> out) {
 			this.out = out;
 		}
 
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractBaseRowPythonScalarFunctionOperator.java
similarity index 68%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractBaseRowPythonScalarFunctionOperator.java
index 75c6bf01486cd..02d06daadd718 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractBaseRowPythonScalarFunctionOperator.java
@@ -16,30 +16,21 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
-import org.apache.flink.python.PythonFunctionRunner;
-import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.api.TableConfig;
 import org.apache.flink.table.dataformat.BaseRow;
 import org.apache.flink.table.dataformat.BinaryRow;
-import org.apache.flink.table.dataformat.JoinedRow;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.planner.codegen.CodeGeneratorContext;
 import org.apache.flink.table.planner.codegen.ProjectionCodeGenerator;
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.Projection;
-import org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner;
-import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 
-import org.apache.beam.sdk.fn.data.FnDataReceiver;
-
-import java.io.IOException;
 import java.util.Arrays;
 import java.util.stream.Collectors;
 
@@ -47,7 +38,7 @@
  * The Python {@link ScalarFunction} operator for the blink planner.
  */
 @Internal
-public class BaseRowPythonScalarFunctionOperator
+public abstract class AbstractBaseRowPythonScalarFunctionOperator
 		extends AbstractPythonScalarFunctionOperator<BaseRow, BaseRow, BaseRow> {
 
 	private static final long serialVersionUID = 1L;
@@ -55,12 +46,7 @@
 	/**
 	 * The collector used to collect records.
 	 */
-	private transient StreamRecordBaseRowWrappingCollector baseRowWrapper;
-
-	/**
-	 * The JoinedRow reused holding the execution result.
-	 */
-	private transient JoinedRow reuseJoinedRow;
+	protected transient StreamRecordBaseRowWrappingCollector baseRowWrapper;
 
 	/**
 	 * The Projection which projects the forwarded fields from the input row.
@@ -72,12 +58,7 @@
 	 */
 	private transient Projection<BaseRow, BinaryRow> udfInputProjection;
 
-	/**
-	 * The TypeSerializer for udf execution results.
-	 */
-	private transient TypeSerializer<BaseRow> udfOutputTypeSerializer;
-
-	public BaseRowPythonScalarFunctionOperator(
+	public AbstractBaseRowPythonScalarFunctionOperator(
 		Configuration config,
 		PythonFunctionInfo[] scalarFunctions,
 		RowType inputType,
@@ -92,11 +73,9 @@ public BaseRowPythonScalarFunctionOperator(
 	public void open() throws Exception {
 		super.open();
 		baseRowWrapper = new StreamRecordBaseRowWrappingCollector(output);
-		reuseJoinedRow = new JoinedRow();
 
 		udfInputProjection = createUdfInputProjection();
 		forwardedFieldProjection = createForwardedFieldProjection();
-		udfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionOutputType);
 	}
 
 	@Override
@@ -112,32 +91,6 @@ public BaseRow getFunctionInput(BaseRow element) {
 		return udfInputProjection.apply(element);
 	}
 
-	@Override
-	@SuppressWarnings("ConstantConditions")
-	public void emitResults() throws IOException {
-		byte[] rawUdfResult;
-		while ((rawUdfResult = userDefinedFunctionResultQueue.poll()) != null) {
-			BaseRow input = forwardedInputQueue.poll();
-			reuseJoinedRow.setHeader(input.getHeader());
-			bais.setBuffer(rawUdfResult, 0, rawUdfResult.length);
-			BaseRow udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);
-			baseRowWrapper.collect(reuseJoinedRow.replace(input, udfResult));
-		}
-	}
-
-	@Override
-	public PythonFunctionRunner<BaseRow> createPythonFunctionRunner(
-			FnDataReceiver<byte[]> resultReceiver,
-			PythonEnvironmentManager pythonEnvironmentManager) {
-		return new BaseRowPythonScalarFunctionRunner(
-			getRuntimeContext().getTaskName(),
-			resultReceiver,
-			scalarFunctions,
-			pythonEnvironmentManager,
-			userDefinedFunctionInputType,
-			userDefinedFunctionOutputType);
-	}
-
 	private Projection<BaseRow, BinaryRow> createUdfInputProjection() {
 		final GeneratedProjection generatedProjection = ProjectionCodeGenerator.generateProjection(
 			CodeGeneratorContext.apply(new TableConfig()),
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonScalarFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractPythonScalarFunctionOperator.java
similarity index 95%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonScalarFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractPythonScalarFunctionOperator.java
index 29801550c213c..c280516cdef72 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonScalarFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractPythonScalarFunctionOperator.java
@@ -16,13 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
 
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractRowPythonScalarFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractRowPythonScalarFunctionOperator.java
new file mode 100644
index 0000000000000..11803144621e5
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/AbstractRowPythonScalarFunctionOperator.java
@@ -0,0 +1,93 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.python.scalar;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.typeutils.RowTypeInfo;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.table.functions.ScalarFunction;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.types.CRow;
+import org.apache.flink.table.runtime.types.CRowTypeInfo;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.utils.TypeConversions;
+import org.apache.flink.types.Row;
+
+import java.util.Arrays;
+
+/**
+ * Base Python {@link ScalarFunction} operator for the legacy planner.
+ */
+@Internal
+public abstract class AbstractRowPythonScalarFunctionOperator extends AbstractPythonScalarFunctionOperator<CRow, CRow, Row> {
+
+	private static final long serialVersionUID = 1L;
+
+	/**
+	 * The collector used to collect records.
+	 */
+	protected transient StreamRecordCRowWrappingCollector cRowWrapper;
+
+	/**
+	 * The type serializer for the forwarded fields.
+	 */
+	private transient TypeSerializer<CRow> forwardedInputSerializer;
+
+	public AbstractRowPythonScalarFunctionOperator(
+		Configuration config,
+		PythonFunctionInfo[] scalarFunctions,
+		RowType inputType,
+		RowType outputType,
+		int[] udfInputOffsets,
+		int[] forwardedFields) {
+		super(config, scalarFunctions, inputType, outputType, udfInputOffsets, forwardedFields);
+	}
+
+	@Override
+	@SuppressWarnings("unchecked")
+	public void open() throws Exception {
+		super.open();
+		this.cRowWrapper = new StreamRecordCRowWrappingCollector(output);
+
+		CRowTypeInfo forwardedInputTypeInfo = new CRowTypeInfo(new RowTypeInfo(
+			Arrays.stream(forwardedFields)
+				.mapToObj(i -> inputType.getFields().get(i))
+				.map(RowType.RowField::getType)
+				.map(TypeConversions::fromLogicalToDataType)
+				.map(TypeConversions::fromDataTypeToLegacyInfo)
+				.toArray(TypeInformation[]::new)));
+		forwardedInputSerializer = forwardedInputTypeInfo.createSerializer(getExecutionConfig());
+	}
+
+	@Override
+	public void bufferInput(CRow input) {
+		CRow forwardedFieldsRow = new CRow(Row.project(input.row(), forwardedFields), input.change());
+		if (getExecutionConfig().isObjectReuseEnabled()) {
+			forwardedFieldsRow = forwardedInputSerializer.copy(forwardedFieldsRow);
+		}
+		forwardedInputQueue.add(forwardedFieldsRow);
+	}
+
+	@Override
+	public Row getFunctionInput(CRow element) {
+		return Row.project(element.row(), userDefinedFunctionInputOffsets);
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperator.java
new file mode 100644
index 0000000000000..188ba9c4d4613
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperator.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.operators.python.scalar;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.configuration.Configuration;
+import org.apache.flink.python.PythonFunctionRunner;
+import org.apache.flink.python.env.PythonEnvironmentManager;
+import org.apache.flink.table.dataformat.BaseRow;
+import org.apache.flink.table.dataformat.JoinedRow;
+import org.apache.flink.table.functions.ScalarFunction;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.scalar.BaseRowPythonScalarFunctionRunner;
+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
+import org.apache.flink.table.types.logical.RowType;
+
+import org.apache.beam.sdk.fn.data.FnDataReceiver;
+
+import java.io.IOException;
+
+/**
+ * The Python {@link ScalarFunction} operator for the blink planner.
+ */
+@Internal
+public class BaseRowPythonScalarFunctionOperator extends AbstractBaseRowPythonScalarFunctionOperator {
+
+	private static final long serialVersionUID = 1L;
+
+	/**
+	 * The JoinedRow reused holding the execution result.
+	 */
+	private transient JoinedRow reuseJoinedRow;
+
+	/**
+	 * The TypeSerializer for udf execution results.
+	 */
+	private transient TypeSerializer<BaseRow> udfOutputTypeSerializer;
+
+	public BaseRowPythonScalarFunctionOperator(
+		Configuration config,
+		PythonFunctionInfo[] scalarFunctions,
+		RowType inputType,
+		RowType outputType,
+		int[] udfInputOffsets,
+		int[] forwardedFields) {
+		super(config, scalarFunctions, inputType, outputType, udfInputOffsets, forwardedFields);
+	}
+
+	@Override
+	@SuppressWarnings("unchecked")
+	public void open() throws Exception {
+		super.open();
+		reuseJoinedRow = new JoinedRow();
+		udfOutputTypeSerializer = PythonTypeUtils.toBlinkTypeSerializer(userDefinedFunctionOutputType);
+	}
+
+	@Override
+	@SuppressWarnings("ConstantConditions")
+	public void emitResults() throws IOException {
+		byte[] rawUdfResult;
+		while ((rawUdfResult = userDefinedFunctionResultQueue.poll()) != null) {
+			BaseRow input = forwardedInputQueue.poll();
+			reuseJoinedRow.setHeader(input.getHeader());
+			bais.setBuffer(rawUdfResult, 0, rawUdfResult.length);
+			BaseRow udfResult = udfOutputTypeSerializer.deserialize(baisWrapper);
+			baseRowWrapper.collect(reuseJoinedRow.replace(input, udfResult));
+		}
+	}
+
+	@Override
+	public PythonFunctionRunner<BaseRow> createPythonFunctionRunner(
+			FnDataReceiver<byte[]> resultReceiver,
+			PythonEnvironmentManager pythonEnvironmentManager) {
+		return new BaseRowPythonScalarFunctionRunner(
+			getRuntimeContext().getTaskName(),
+			resultReceiver,
+			scalarFunctions,
+			pythonEnvironmentManager,
+			userDefinedFunctionInputType,
+			userDefinedFunctionOutputType);
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperator.java
similarity index 64%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperator.java
index a6cdfd00d35d0..672882e21008c 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperator.java
@@ -16,48 +16,33 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.annotation.Internal;
-import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
-import org.apache.flink.api.java.typeutils.RowTypeInfo;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.python.PythonFunctionRunner;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner;
+import org.apache.flink.table.runtime.runners.python.scalar.PythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.types.CRow;
-import org.apache.flink.table.runtime.types.CRowTypeInfo;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.utils.TypeConversions;
 import org.apache.flink.types.Row;
 
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 
 import java.io.IOException;
-import java.util.Arrays;
 
 /**
  * The Python {@link ScalarFunction} operator for the legacy planner.
  */
 @Internal
-public class PythonScalarFunctionOperator extends AbstractPythonScalarFunctionOperator<CRow, CRow, Row> {
+public class PythonScalarFunctionOperator extends AbstractRowPythonScalarFunctionOperator {
 
 	private static final long serialVersionUID = 1L;
 
-	/**
-	 * The collector used to collect records.
-	 */
-	private transient StreamRecordCRowWrappingCollector cRowWrapper;
-
-	/**
-	 * The type serializer for the forwarded fields.
-	 */
-	private transient TypeSerializer<CRow> forwardedInputSerializer;
-
 	/**
 	 * The TypeSerializer for udf execution results.
 	 */
@@ -77,33 +62,9 @@ public PythonScalarFunctionOperator(
 	@SuppressWarnings("unchecked")
 	public void open() throws Exception {
 		super.open();
-		this.cRowWrapper = new StreamRecordCRowWrappingCollector(output);
-
-		CRowTypeInfo forwardedInputTypeInfo = new CRowTypeInfo(new RowTypeInfo(
-			Arrays.stream(forwardedFields)
-				.mapToObj(i -> inputType.getFields().get(i))
-				.map(RowType.RowField::getType)
-				.map(TypeConversions::fromLogicalToDataType)
-				.map(TypeConversions::fromDataTypeToLegacyInfo)
-				.toArray(TypeInformation[]::new)));
-		forwardedInputSerializer = forwardedInputTypeInfo.createSerializer(getExecutionConfig());
 		udfOutputTypeSerializer = PythonTypeUtils.toFlinkTypeSerializer(userDefinedFunctionOutputType);
 	}
 
-	@Override
-	public void bufferInput(CRow input) {
-		CRow forwardedFieldsRow = new CRow(Row.project(input.row(), forwardedFields), input.change());
-		if (getExecutionConfig().isObjectReuseEnabled()) {
-			forwardedFieldsRow = forwardedInputSerializer.copy(forwardedFieldsRow);
-		}
-		forwardedInputQueue.add(forwardedFieldsRow);
-	}
-
-	@Override
-	public Row getFunctionInput(CRow element) {
-		return Row.project(element.row(), userDefinedFunctionInputOffsets);
-	}
-
 	@Override
 	@SuppressWarnings("ConstantConditions")
 	public void emitResults() throws IOException {
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonTableFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/AbstractPythonTableFunctionOperator.java
similarity index 95%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonTableFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/AbstractPythonTableFunctionOperator.java
index ab4d8e55a9ade..f8eb0389aa812 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/AbstractPythonTableFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/AbstractPythonTableFunctionOperator.java
@@ -16,13 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.table.functions.TableFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.operators.python.AbstractStatelessFunctionOperator;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
 
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperator.java
similarity index 97%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperator.java
index 524f33b4fc309..d138df37c563e 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperator.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
@@ -34,7 +34,7 @@
 import org.apache.flink.table.planner.codegen.ProjectionCodeGenerator;
 import org.apache.flink.table.runtime.generated.GeneratedProjection;
 import org.apache.flink.table.runtime.generated.Projection;
-import org.apache.flink.table.runtime.runners.python.BaseRowPythonTableFunctionRunner;
+import org.apache.flink.table.runtime.runners.python.table.BaseRowPythonTableFunctionRunner;
 import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperator.java b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperator.java
similarity index 97%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperator.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperator.java
index 9d7d63a291a1e..a4c525993b453 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperator.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperator.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
@@ -26,7 +26,7 @@
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.TableFunction;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner;
+import org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunner;
 import org.apache.flink.table.runtime.types.CRow;
 import org.apache.flink.table.runtime.types.CRowTypeInfo;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonStatelessFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonStatelessFunctionRunner.java
index a66983f4bd665..006d421affe4a 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonStatelessFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonStatelessFunctionRunner.java
@@ -152,7 +152,7 @@ public ExecutableStage createExecutableStage() throws Exception {
 			components, createPythonExecutionEnvironment(), input, sideInputs, userStates, timers, transforms, outputs, createValueOnlyWireCoderSetting());
 	}
 
-	FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {
+	public FlinkFnApi.UserDefinedFunction getUserDefinedFunctionProto(PythonFunctionInfo pythonFunctionInfo) {
 		FlinkFnApi.UserDefinedFunction.Builder builder = FlinkFnApi.UserDefinedFunction.newBuilder();
 		builder.setPayload(ByteString.copyFrom(pythonFunctionInfo.getPythonFunction().getSerializedPythonFunction()));
 		for (Object input : pythonFunctionInfo.getInputs()) {
@@ -201,12 +201,12 @@ public RowType getOutputType() {
 	/**
 	 * Gets the proto representation of the input coder.
 	 */
-	abstract RunnerApi.Coder getInputCoderProto();
+	public abstract RunnerApi.Coder getInputCoderProto();
 
 	/**
 	 * Gets the proto representation of the output coder.
 	 */
-	abstract RunnerApi.Coder getOutputCoderProto();
+	public abstract RunnerApi.Coder getOutputCoderProto();
 
 	/**
 	 * Gets the proto representation of the window coder.
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractGeneralPythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractGeneralPythonScalarFunctionRunner.java
new file mode 100644
index 0000000000000..b953ae75d1eba
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractGeneralPythonScalarFunctionRunner.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.runners.python.scalar;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.python.PythonFunctionRunner;
+import org.apache.flink.python.env.PythonEnvironmentManager;
+import org.apache.flink.table.functions.ScalarFunction;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.types.logical.RowType;
+
+import org.apache.beam.runners.fnexecution.control.OutputReceiverFactory;
+import org.apache.beam.sdk.fn.data.FnDataReceiver;
+import org.apache.beam.sdk.util.WindowedValue;
+
+/**
+ * Abstract {@link PythonFunctionRunner} used to execute Python {@link ScalarFunction}s.
+ *
+ * @param <IN> Type of the input elements.
+ */
+@Internal
+public abstract class AbstractGeneralPythonScalarFunctionRunner<IN> extends AbstractPythonScalarFunctionRunner<IN> {
+
+	private static final String SCHEMA_CODER_URN = "flink:coder:schema:v1";
+
+	/**
+	 * The TypeSerializer for input elements.
+	 */
+	private transient TypeSerializer<IN> inputTypeSerializer;
+
+	AbstractGeneralPythonScalarFunctionRunner(
+		String taskName,
+		FnDataReceiver<byte[]> resultReceiver,
+		PythonFunctionInfo[] scalarFunctions,
+		PythonEnvironmentManager environmentManager,
+		RowType inputType,
+		RowType outputType) {
+		super(taskName, resultReceiver, scalarFunctions, environmentManager, inputType, outputType);
+	}
+
+	@Override
+	public void open() throws Exception {
+		super.open();
+		inputTypeSerializer = getInputTypeSerializer();
+	}
+
+	@Override
+	public void processElement(IN element) {
+		try {
+			baos.reset();
+			inputTypeSerializer.serialize(element, baosWrapper);
+			// TODO: support to use ValueOnlyWindowedValueCoder for better performance.
+			// Currently, FullWindowedValueCoder has to be used in Beam's portability framework.
+			mainInputReceiver.accept(WindowedValue.valueInGlobalWindow(baos.toByteArray()));
+		} catch (Throwable t) {
+			throw new RuntimeException("Failed to process element when sending data to Python SDK harness.", t);
+		}
+	}
+
+	@Override
+	public OutputReceiverFactory createOutputReceiverFactory() {
+		return new OutputReceiverFactory() {
+
+			// the input value type is always byte array
+			@SuppressWarnings("unchecked")
+			@Override
+			public FnDataReceiver<WindowedValue<byte[]>> create(String pCollectionId) {
+				return input -> resultReceiver.accept(input.getValue());
+			}
+		};
+	}
+
+	@Override
+	public String getInputOutputCoderUrn() {
+		return SCHEMA_CODER_URN;
+	}
+
+	/**
+	 * Returns the TypeSerializer for input elements.
+	 */
+	public abstract TypeSerializer<IN> getInputTypeSerializer();
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunner.java
similarity index 89%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunner.java
index 9a56a4c95fa54..88d9785ce745e 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
@@ -25,6 +25,7 @@
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
@@ -41,7 +42,6 @@
 @Internal
 public abstract class AbstractPythonScalarFunctionRunner<IN> extends AbstractPythonStatelessFunctionRunner<IN> {
 
-	private static final String SCHEMA_CODER_URN = "flink:coder:schema:v1";
 	private static final String SCALAR_FUNCTION_URN = "flink:transform:scalar_function:v1";
 
 	private final PythonFunctionInfo[] scalarFunctions;
@@ -73,7 +73,7 @@ public FlinkFnApi.UserDefinedFunctions getUserDefinedFunctionsProto() {
 	 * Gets the proto representation of the input coder.
 	 */
 	@Override
-	RunnerApi.Coder getInputCoderProto() {
+	public RunnerApi.Coder getInputCoderProto() {
 		return getRowCoderProto(getInputType());
 	}
 
@@ -81,7 +81,7 @@ RunnerApi.Coder getInputCoderProto() {
 	 * Gets the proto representation of the output coder.
 	 */
 	@Override
-	RunnerApi.Coder getOutputCoderProto() {
+	public RunnerApi.Coder getOutputCoderProto() {
 		return getRowCoderProto(getOutputType());
 	}
 
@@ -89,7 +89,7 @@ private RunnerApi.Coder getRowCoderProto(RowType rowType) {
 		return RunnerApi.Coder.newBuilder()
 			.setSpec(
 				RunnerApi.FunctionSpec.newBuilder()
-					.setUrn(SCHEMA_CODER_URN)
+					.setUrn(getInputOutputCoderUrn())
 					.setPayload(org.apache.beam.vendor.grpc.v1p21p0.com.google.protobuf.ByteString.copyFrom(
 						toProtoType(rowType).getRowSchema().toByteArray()))
 					.build())
@@ -99,4 +99,9 @@ private RunnerApi.Coder getRowCoderProto(RowType rowType) {
 	private FlinkFnApi.Schema.FieldType toProtoType(LogicalType logicalType) {
 		return logicalType.accept(new PythonTypeUtils.LogicalTypeToProtoTypeConverter());
 	}
+
+	/**
+	 * Returns the URN of the input/output coder.
+	 */
+	public abstract String getInputOutputCoderUrn();
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunner.java
similarity index 92%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunner.java
index 64bac82894c49..758ac648da58f 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.python.PythonFunctionRunner;
@@ -35,7 +35,7 @@
  * It takes {@link BaseRow} as the input and outputs a byte array.
  */
 @Internal
-public class BaseRowPythonScalarFunctionRunner extends AbstractPythonScalarFunctionRunner<BaseRow> {
+public class BaseRowPythonScalarFunctionRunner extends AbstractGeneralPythonScalarFunctionRunner<BaseRow> {
 
 	public BaseRowPythonScalarFunctionRunner(
 		String taskName,
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunner.java
similarity index 92%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunner.java
index abe4dee78fd16..ca7d80a1d6a28 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
@@ -35,7 +35,7 @@
  * It takes {@link Row} as the input and outputs a byte array.
  */
 @Internal
-public class PythonScalarFunctionRunner extends AbstractPythonScalarFunctionRunner<Row> {
+public class PythonScalarFunctionRunner extends AbstractGeneralPythonScalarFunctionRunner<Row> {
 
 	public PythonScalarFunctionRunner(
 		String taskName,
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonTableFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunner.java
similarity index 73%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonTableFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunner.java
index e992bc2ea0ecd..cb959792f59ee 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonTableFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunner.java
@@ -16,22 +16,26 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
 import org.apache.flink.python.PythonFunctionRunner;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.functions.TableFunction;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.AbstractPythonStatelessFunctionRunner;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.beam.model.pipeline.v1.RunnerApi;
+import org.apache.beam.runners.fnexecution.control.OutputReceiverFactory;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
+import org.apache.beam.sdk.util.WindowedValue;
 
 /**
  * Abstract {@link PythonFunctionRunner} used to execute Python {@link TableFunction}.
@@ -46,6 +50,11 @@
 
 	private final PythonFunctionInfo tableFunction;
 
+	/**
+	 * The TypeSerializer for input elements.
+	 */
+	private transient TypeSerializer<IN> inputTypeSerializer;
+
 	public AbstractPythonTableFunctionRunner(
 		String taskName,
 		FnDataReceiver<byte[]> resultReceiver,
@@ -57,6 +66,12 @@ public AbstractPythonTableFunctionRunner(
 		this.tableFunction = Preconditions.checkNotNull(tableFunction);
 	}
 
+	@Override
+	public void open() throws Exception {
+		super.open();
+		inputTypeSerializer = getInputTypeSerializer();
+	}
+
 	/**
 	 * Gets the proto representation of the Python user-defined functions to be executed.
 	 */
@@ -71,7 +86,7 @@ public FlinkFnApi.UserDefinedFunctions getUserDefinedFunctionsProto() {
 	 * Gets the proto representation of the input coder.
 	 */
 	@Override
-	RunnerApi.Coder getInputCoderProto() {
+	public RunnerApi.Coder getInputCoderProto() {
 		return getTableCoderProto(getInputType());
 	}
 
@@ -79,10 +94,39 @@ RunnerApi.Coder getInputCoderProto() {
 	 * Gets the proto representation of the output coder.
 	 */
 	@Override
-	RunnerApi.Coder getOutputCoderProto() {
+	public RunnerApi.Coder getOutputCoderProto() {
 		return getTableCoderProto(getOutputType());
 	}
 
+	@Override
+	public void processElement(IN element) {
+		try {
+			baos.reset();
+			inputTypeSerializer.serialize(element, baosWrapper);
+			mainInputReceiver.accept(WindowedValue.valueInGlobalWindow(baos.toByteArray()));
+		} catch (Throwable t) {
+			throw new RuntimeException("Failed to process element when sending data to Python SDK harness.", t);
+		}
+	}
+
+	@Override
+	public OutputReceiverFactory createOutputReceiverFactory() {
+		return new OutputReceiverFactory() {
+
+			// the input value type is always byte array
+			@SuppressWarnings("unchecked")
+			@Override
+			public FnDataReceiver<WindowedValue<byte[]>> create(String pCollectionId) {
+				return input -> resultReceiver.accept(input.getValue());
+			}
+		};
+	}
+
+	/**
+	 * Returns the TypeSerializer for input elements.
+	 */
+	public abstract TypeSerializer<IN> getInputTypeSerializer();
+
 	private RunnerApi.Coder getTableCoderProto(RowType rowType) {
 		return RunnerApi.Coder.newBuilder()
 			.setSpec(
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonTableFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunner.java
similarity index 97%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonTableFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunner.java
index aa36628486f61..866d1826fa47d 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonTableFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.python.PythonFunctionRunner;
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonTableFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunner.java
similarity index 97%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonTableFunctionRunner.java
rename to flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunner.java
index 2aac44f5e1e8a..c691ca3ee5857 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonTableFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.runners.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.api.common.typeutils.TypeSerializer;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperatorTest.java
similarity index 98%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperatorTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperatorTest.java
index 981ef851b8955..7c64e34f02fba 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/BaseRowPythonScalarFunctionOperatorTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeinfo.Types;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonFunctionRunner.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonFunctionRunner.java
similarity index 97%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonFunctionRunner.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonFunctionRunner.java
index e19b7b0e3f604..dae1a927ad894 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonFunctionRunner.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonFunctionRunner.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonTableFunctionRunner.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonTableFunctionRunner.java
similarity index 92%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonTableFunctionRunner.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonTableFunctionRunner.java
index 093bbce00a29c..9d05345b057ad 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PassThroughPythonTableFunctionRunner.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PassThroughPythonTableFunctionRunner.java
@@ -16,13 +16,13 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;
 import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.python.PythonFunctionRunner;
-import org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner;
+import org.apache.flink.table.runtime.runners.python.table.PythonTableFunctionRunner;
 import org.apache.flink.util.Preconditions;
 
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
@@ -48,7 +48,7 @@
 	 */
 	private transient DataOutputViewStreamWrapper baosWrapper;
 
-	PassThroughPythonTableFunctionRunner(FnDataReceiver<byte[]> resultReceiver) {
+	public PassThroughPythonTableFunctionRunner(FnDataReceiver<byte[]> resultReceiver) {
 		this.resultReceiver = Preconditions.checkNotNull(resultReceiver);
 		bundleStarted = false;
 		bufferedElements = new ArrayList<>();
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
similarity index 98%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
index 29759c719baec..fa7be161fc28f 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTestBase.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTestBase.java
similarity index 98%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTestBase.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTestBase.java
index 2aaff6db04df7..19128f7cd3446 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonScalarFunctionOperatorTestBase.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/scalar/PythonScalarFunctionOperatorTestBase.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.scalar;
 
 import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
 import org.apache.flink.api.java.tuple.Tuple2;
@@ -31,9 +31,9 @@
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
 import org.apache.flink.table.api.Table;
 import org.apache.flink.table.api.java.StreamTableEnvironment;
-import org.apache.flink.table.functions.python.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.planner.runtime.utils.JavaUserDefinedScalarFunctions.PythonScalarFunction;
+import org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.VarCharType;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperatorTest.java
similarity index 96%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperatorTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperatorTest.java
index de2c6a3444b25..c4ad63fe7f4a0 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/BaseRowPythonTableFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/BaseRowPythonTableFunctionOperatorTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.api.common.typeinfo.TypeInformation;
 import org.apache.flink.api.common.typeinfo.Types;
@@ -27,6 +27,7 @@
 import org.apache.flink.table.dataformat.BaseRow;
 import org.apache.flink.table.dataformat.util.BaseRowUtil;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonTableFunctionRunner;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.runtime.util.BaseRowHarnessAssertor;
 import org.apache.flink.table.types.logical.RowType;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTest.java
similarity index 95%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTest.java
index 4293e9d2b6136..d2401f650c6a9 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.Configuration;
@@ -24,6 +24,7 @@
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.streaming.util.TestHarnessUtil;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.operators.python.scalar.PassThroughPythonTableFunctionRunner;
 import org.apache.flink.table.runtime.types.CRow;
 import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTestBase.java b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTestBase.java
similarity index 98%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTestBase.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTestBase.java
index b038ac4787e04..1ef5825bea9ab 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/PythonTableFunctionOperatorTestBase.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/operators/python/table/PythonTableFunctionOperatorTestBase.java
@@ -16,14 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.operators.python;
+package org.apache.flink.table.runtime.operators.python.table;
 
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.python.PythonOptions;
 import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
 import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;
-import org.apache.flink.table.functions.python.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.table.types.logical.VarCharType;
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
similarity index 86%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonScalarFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
index 347b28d7649be..ed63725932ac2 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/AbstractPythonScalarFunctionRunnerTest.java
@@ -16,9 +16,11 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
-import org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner;
+import org.apache.flink.table.functions.python.PythonEnv;
+import org.apache.flink.table.functions.python.PythonFunction;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 
@@ -32,7 +34,7 @@
  */
 public abstract class AbstractPythonScalarFunctionRunnerTest<IN>  {
 
-	AbstractPythonScalarFunctionRunner<IN> createSingleUDFRunner() throws Exception {
+	AbstractGeneralPythonScalarFunctionRunner<IN> createSingleUDFRunner() throws Exception {
 		PythonFunctionInfo[] pythonFunctionInfos = new PythonFunctionInfo[] {
 			new PythonFunctionInfo(
 				DummyPythonFunction.INSTANCE,
@@ -43,7 +45,7 @@ AbstractPythonScalarFunctionRunner<IN> createSingleUDFRunner() throws Exception
 		return createPythonScalarFunctionRunner(pythonFunctionInfos, rowType, rowType);
 	}
 
-	AbstractPythonScalarFunctionRunner<IN> createMultipleUDFRunner() throws Exception {
+	AbstractGeneralPythonScalarFunctionRunner<IN> createMultipleUDFRunner() throws Exception {
 		PythonFunctionInfo[] pythonFunctionInfos = new PythonFunctionInfo[] {
 			new PythonFunctionInfo(
 				DummyPythonFunction.INSTANCE,
@@ -63,7 +65,7 @@ AbstractPythonScalarFunctionRunner<IN> createMultipleUDFRunner() throws Exceptio
 		return createPythonScalarFunctionRunner(pythonFunctionInfos, inputType, outputType);
 	}
 
-	AbstractPythonScalarFunctionRunner<IN> createChainedUDFRunner() throws Exception {
+	AbstractGeneralPythonScalarFunctionRunner<IN> createChainedUDFRunner() throws Exception {
 		PythonFunctionInfo[] pythonFunctionInfos = new PythonFunctionInfo[] {
 			new PythonFunctionInfo(
 				DummyPythonFunction.INSTANCE,
@@ -101,7 +103,7 @@ AbstractPythonScalarFunctionRunner<IN> createChainedUDFRunner() throws Exception
 		return createPythonScalarFunctionRunner(pythonFunctionInfos, inputType, outputType);
 	}
 
-	public abstract AbstractPythonScalarFunctionRunner<IN> createPythonScalarFunctionRunner(
+	public abstract AbstractGeneralPythonScalarFunctionRunner<IN> createPythonScalarFunctionRunner(
 		PythonFunctionInfo[] pythonFunctionInfos, RowType inputType, RowType outputType) throws Exception;
 
 	/**
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunnerTest.java
similarity index 85%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunnerTest.java
index e97a163760dcb..4fca7dd054552 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/BaseRowPythonScalarFunctionRunnerTest.java
@@ -16,15 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
 import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.dataformat.BaseRow;
-import org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner;
-import org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
 import org.apache.flink.table.types.logical.RowType;
 
@@ -44,7 +43,7 @@ public class BaseRowPythonScalarFunctionRunnerTest extends AbstractPythonScalarF
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForSingleUDF() throws Exception {
-		final AbstractPythonScalarFunctionRunner<BaseRow> runner = createSingleUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<BaseRow> runner = createSingleUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -55,7 +54,7 @@ public void testInputOutputDataTypeConstructedProperlyForSingleUDF() throws Exce
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() throws Exception {
-		final AbstractPythonScalarFunctionRunner<BaseRow> runner = createMultipleUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<BaseRow> runner = createMultipleUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -66,7 +65,7 @@ public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() throws E
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForChainedUDFs() throws Exception {
-		final AbstractPythonScalarFunctionRunner<BaseRow> runner = createChainedUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<BaseRow> runner = createChainedUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -76,7 +75,7 @@ public void testInputOutputDataTypeConstructedProperlyForChainedUDFs() throws Ex
 	}
 
 	@Override
-	public AbstractPythonScalarFunctionRunner<BaseRow> createPythonScalarFunctionRunner(
+	public AbstractGeneralPythonScalarFunctionRunner<BaseRow> createPythonScalarFunctionRunner(
 		final PythonFunctionInfo[] pythonFunctionInfos,
 		RowType inputType,
 		RowType outputType) {
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
similarity index 93%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
index 51fd1050dbe35..1594b712b5425 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/scalar/PythonScalarFunctionRunnerTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.scalar;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
@@ -25,8 +25,7 @@
 import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
 import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
-import org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner;
-import org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
@@ -69,7 +68,7 @@ public class PythonScalarFunctionRunnerTest extends AbstractPythonScalarFunction
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForSingleUDF() throws Exception {
-		final AbstractPythonScalarFunctionRunner<Row> runner = createSingleUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<Row> runner = createSingleUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -80,7 +79,7 @@ public void testInputOutputDataTypeConstructedProperlyForSingleUDF() throws Exce
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() throws Exception {
-		final AbstractPythonScalarFunctionRunner<Row> runner = createMultipleUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<Row> runner = createMultipleUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -91,7 +90,7 @@ public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() throws E
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForChainedUDFs() throws Exception {
-		final AbstractPythonScalarFunctionRunner<Row> runner = createChainedUDFRunner();
+		final AbstractGeneralPythonScalarFunctionRunner<Row> runner = createChainedUDFRunner();
 
 		// check input TypeSerializer
 		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
@@ -165,7 +164,7 @@ public void testUDFProtoConstructedProperlyForChainedUDFs() throws Exception {
 	public void testPythonScalarFunctionRunner() throws Exception {
 		JobBundleFactory jobBundleFactorySpy = spy(JobBundleFactory.class);
 		FnDataReceiver<byte[]> resultReceiverSpy = spy(FnDataReceiver.class);
-		final AbstractPythonScalarFunctionRunner<Row> runner =
+		final AbstractGeneralPythonScalarFunctionRunner<Row> runner =
 			createUDFRunner(jobBundleFactorySpy, resultReceiverSpy);
 
 		StageBundleFactory stageBundleFactorySpy = spy(StageBundleFactory.class);
@@ -203,7 +202,7 @@ public void testPythonScalarFunctionRunner() throws Exception {
 	}
 
 	@Override
-	public AbstractPythonScalarFunctionRunner<Row> createPythonScalarFunctionRunner(
+	public AbstractGeneralPythonScalarFunctionRunner<Row> createPythonScalarFunctionRunner(
 		final PythonFunctionInfo[] pythonFunctionInfos,
 		RowType inputType,
 		RowType outputType) {
@@ -226,7 +225,7 @@ public AbstractPythonScalarFunctionRunner<Row> createPythonScalarFunctionRunner(
 			outputType);
 	}
 
-	private AbstractPythonScalarFunctionRunner<Row> createUDFRunner(
+	private AbstractGeneralPythonScalarFunctionRunner<Row> createUDFRunner(
 		JobBundleFactory jobBundleFactory, FnDataReceiver<byte[]> receiver) {
 		PythonFunctionInfo[] pythonFunctionInfos = new PythonFunctionInfo[] {
 			new PythonFunctionInfo(
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonTableFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunnerTest.java
similarity index 88%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonTableFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunnerTest.java
index f22d4108ede55..7ec8acaa19317 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/AbstractPythonTableFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/AbstractPythonTableFunctionRunnerTest.java
@@ -16,9 +16,10 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
-import org.apache.flink.table.runtime.runners.python.AbstractPythonTableFunctionRunner;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonTableFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunnerTest.java
similarity index 92%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonTableFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunnerTest.java
index 8b6043fbbba3b..a93511f4a100a 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonTableFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/BaseRowPythonTableFunctionRunnerTest.java
@@ -16,15 +16,14 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
 import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
 import org.apache.flink.table.dataformat.BaseRow;
-import org.apache.flink.table.runtime.runners.python.AbstractPythonTableFunctionRunner;
-import org.apache.flink.table.runtime.runners.python.BaseRowPythonTableFunctionRunner;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
 import org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializer;
 import org.apache.flink.table.types.logical.RowType;
 
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTableFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
similarity index 95%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTableFunctionRunnerTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
index 4a76c3af44d9e..2c8383cc4fc90 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTableFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/runners/python/table/PythonTableFunctionRunnerTest.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.runners.python.table;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
@@ -24,8 +24,8 @@
 import org.apache.flink.python.env.ProcessPythonEnvironmentManager;
 import org.apache.flink.python.env.PythonDependencyInfo;
 import org.apache.flink.python.env.PythonEnvironmentManager;
-import org.apache.flink.table.runtime.runners.python.AbstractPythonTableFunctionRunner;
-import org.apache.flink.table.runtime.runners.python.PythonTableFunctionRunner;
+import org.apache.flink.table.functions.python.PythonFunctionInfo;
+import org.apache.flink.table.runtime.runners.python.scalar.AbstractPythonScalarFunctionRunnerTest;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtilsTest.java
similarity index 95%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtilsTest.java
index 53ae2881d4e04..b8e9aa1ce6078 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtilsTest.java
@@ -16,14 +16,12 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.functions.python;
+package org.apache.flink.table.runtime.typeutils;
 
 import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
 import org.apache.flink.table.catalog.UnresolvedIdentifier;
-import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
-import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.ArrayType;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.DateType;
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
index a2bca081f756a..96775b2513986 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
@@ -22,6 +22,7 @@
 import org.apache.flink.api.common.time.Time;
 import org.apache.flink.api.dag.Pipeline;
 import org.apache.flink.api.java.ExecutionEnvironment;
+import org.apache.flink.client.ClientUtils;
 import org.apache.flink.client.cli.CliArgsException;
 import org.apache.flink.client.cli.CustomCommandLine;
 import org.apache.flink.client.cli.ExecutionConfigAccessor;
@@ -32,7 +33,6 @@
 import org.apache.flink.client.deployment.ClusterSpecification;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.plugin.TemporaryClassLoaderContext;
-import org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders;
 import org.apache.flink.streaming.api.TimeCharacteristic;
 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
 import org.apache.flink.table.api.BatchQueryConfig;
@@ -96,6 +96,7 @@
 
 import java.lang.reflect.Method;
 import java.net.URL;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -151,9 +152,11 @@ private ExecutionContext(
 		this.flinkConfig = flinkConfig;
 
 		// create class loader
-		classLoader = FlinkUserCodeClassLoaders.parentFirst(
-				dependencies.toArray(new URL[dependencies.size()]),
-				this.getClass().getClassLoader());
+		classLoader = ClientUtils.buildUserCodeClassLoader(
+			dependencies,
+			Collections.emptyList(),
+			this.getClass().getClassLoader(),
+			flinkConfig);
 
 		// Initialize the TableEnvironment.
 		initializeTableEnvironment(sessionState);
diff --git a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSinkFactory.java b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSinkFactory.java
index 86c7e82e455d6..c721424613756 100644
--- a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSinkFactory.java
+++ b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSinkFactory.java
@@ -19,6 +19,7 @@
 package org.apache.flink.table.factories;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.sinks.StreamTableSink;
 import org.apache.flink.table.sinks.TableSink;
 
@@ -38,14 +39,24 @@
 	 *
 	 * @param properties normalized properties describing a table sink.
 	 * @return the configured table sink.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSink(Context)} instead.
 	 */
-	StreamTableSink<T> createStreamTableSink(Map<String, String> properties);
+	@Deprecated
+	default StreamTableSink<T> createStreamTableSink(Map<String, String> properties) {
+		return null;
+	}
 
 	/**
 	 * Only create stream table sink.
 	 */
 	@Override
 	default TableSink<T> createTableSink(Map<String, String> properties) {
-		return createStreamTableSink(properties);
+		StreamTableSink<T> sink = createStreamTableSink(properties);
+		if (sink == null) {
+			throw new ValidationException(
+					"Please override 'createTableSink(Context)' method.");
+		}
+		return sink;
 	}
 }
diff --git a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSourceFactory.java b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSourceFactory.java
index b007d7bc8655d..3e392edf64e38 100644
--- a/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSourceFactory.java
+++ b/flink-table/flink-table-api-java-bridge/src/main/java/org/apache/flink/table/factories/StreamTableSourceFactory.java
@@ -19,6 +19,7 @@
 package org.apache.flink.table.factories;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.table.api.ValidationException;
 import org.apache.flink.table.sources.StreamTableSource;
 import org.apache.flink.table.sources.TableSource;
 
@@ -38,14 +39,24 @@
 	 *
 	 * @param properties normalized properties describing a stream table source.
 	 * @return the configured stream table source.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSource(Context)} instead.
 	 */
-	StreamTableSource<T> createStreamTableSource(Map<String, String> properties);
+	@Deprecated
+	default StreamTableSource<T> createStreamTableSource(Map<String, String> properties) {
+		return null;
+	}
 
 	/**
 	 * Only create a stream table source.
 	 */
 	@Override
 	default TableSource<T> createTableSource(Map<String, String> properties) {
-		return createStreamTableSource(properties);
+		StreamTableSource<T> source = createStreamTableSource(properties);
+		if (source == null) {
+			throw new ValidationException(
+					"Please override 'createTableSource(Context)' method.");
+		}
+		return source;
 	}
 }
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactory.java
index 05d0f345cc586..c92ec9f3432b8 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactory.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactory.java
@@ -19,7 +19,9 @@
 package org.apache.flink.table.factories;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.catalog.ObjectPath;
 import org.apache.flink.table.sinks.TableSink;
 
@@ -39,8 +41,13 @@
 	 *
 	 * @param properties normalized properties describing a table sink.
 	 * @return the configured table sink.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSink(Context)} instead.
 	 */
-	TableSink<T> createTableSink(Map<String, String> properties);
+	@Deprecated
+	default TableSink<T> createTableSink(Map<String, String> properties) {
+		return null;
+	}
 
 	/**
 	 * Creates and configures a {@link TableSink} based on the given {@link CatalogTable} instance.
@@ -48,9 +55,49 @@
 	 * @param tablePath path of the given {@link CatalogTable}
 	 * @param table {@link CatalogTable} instance.
 	 * @return the configured table sink.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSink(Context)} instead.
 	 */
+	@Deprecated
 	default TableSink<T> createTableSink(ObjectPath tablePath, CatalogTable table) {
 		return createTableSink(table.toProperties());
 	}
 
+	/**
+	 * Creates and configures a {@link TableSink} based on the given
+	 {@link Context}.
+	 *
+	 * @param context context of this table sink.
+	 * @return the configured table sink.
+	 */
+	default TableSink<T> createTableSink(Context context) {
+		return createTableSink(
+				context.getObjectIdentifier().toObjectPath(),
+				context.getTable());
+	}
+
+	/**
+	 * Context of table sink creation. Contains table information and
+	 environment information.
+	 */
+	interface Context {
+
+		/**
+		 * @return full identifier of the given {@link CatalogTable}.
+		 */
+		ObjectIdentifier getObjectIdentifier();
+
+		/**
+		 * @return table {@link CatalogTable} instance.
+		 */
+		CatalogTable getTable();
+
+		/**
+		 * @return readable config of this table environment. The configuration gives the ability
+		 * to access {@code TableConfig#getConfiguration()} which holds the current
+		 * {@code TableEnvironment} session configurations.
+		 */
+		ReadableConfig getConfiguration();
+	}
+
 }
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactoryContextImpl.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactoryContextImpl.java
new file mode 100644
index 0000000000000..e69d353aab572
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSinkFactoryContextImpl.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.factories;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.configuration.ReadableConfig;
+import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.ObjectIdentifier;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Implementation of {@link TableSinkFactory.Context}.
+ */
+@Internal
+public class TableSinkFactoryContextImpl implements TableSinkFactory.Context {
+
+	private final ObjectIdentifier identifier;
+	private final CatalogTable table;
+	private final ReadableConfig config;
+
+	public TableSinkFactoryContextImpl(
+			ObjectIdentifier identifier,
+			CatalogTable table,
+			ReadableConfig config) {
+		this.identifier = checkNotNull(identifier);
+		this.table = checkNotNull(table);
+		this.config = checkNotNull(config);
+	}
+
+	@Override
+	public ObjectIdentifier getObjectIdentifier() {
+		return identifier;
+	}
+
+	@Override
+	public CatalogTable getTable() {
+		return table;
+	}
+
+	@Override
+	public ReadableConfig getConfiguration() {
+		return config;
+	}
+}
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactory.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactory.java
index c0f97d9d067a7..8cc5bdcd8c8f1 100644
--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactory.java
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactory.java
@@ -19,7 +19,9 @@
 package org.apache.flink.table.factories;
 
 import org.apache.flink.annotation.PublicEvolving;
+import org.apache.flink.configuration.ReadableConfig;
 import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.ObjectIdentifier;
 import org.apache.flink.table.catalog.ObjectPath;
 import org.apache.flink.table.sources.TableSource;
 
@@ -39,8 +41,13 @@
 	 *
 	 * @param properties normalized properties describing a table source.
 	 * @return the configured table source.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSource(Context)} instead.
 	 */
-	TableSource<T> createTableSource(Map<String, String> properties);
+	@Deprecated
+	default TableSource<T> createTableSource(Map<String, String> properties) {
+		return null;
+	}
 
 	/**
 	 * Creates and configures a {@link TableSource} based on the given {@link CatalogTable} instance.
@@ -48,9 +55,49 @@
 	 * @param tablePath path of the given {@link CatalogTable}
 	 * @param table {@link CatalogTable} instance.
 	 * @return the configured table source.
+	 * @deprecated {@link Context} contains more information, and already contains table schema too.
+	 * Please use {@link #createTableSource(Context)} instead.
 	 */
+	@Deprecated
 	default TableSource<T> createTableSource(ObjectPath tablePath, CatalogTable table) {
 		return createTableSource(table.toProperties());
 	}
 
+	/**
+	 * Creates and configures a {@link TableSource} based on the given
+	 {@link Context}.
+	 *
+	 * @param context context of this table source.
+	 * @return the configured table source.
+	 */
+	default TableSource<T> createTableSource(Context context) {
+		return createTableSource(
+				context.getObjectIdentifier().toObjectPath(),
+				context.getTable());
+	}
+
+	/**
+	 * Context of table source creation. Contains table information and
+	 environment information.
+	 */
+	interface Context {
+
+		/**
+		 * @return full identifier of the given {@link CatalogTable}.
+		 */
+		ObjectIdentifier getObjectIdentifier();
+
+		/**
+		 * @return table {@link CatalogTable} instance.
+		 */
+		CatalogTable getTable();
+
+		/**
+		 * @return readable config of this table environment. The configuration gives the ability
+		 * to access {@code TableConfig#getConfiguration()} which holds the current
+		 * {@code TableEnvironment} session configurations.
+		 */
+		ReadableConfig getConfiguration();
+	}
+
 }
diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactoryContextImpl.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactoryContextImpl.java
new file mode 100644
index 0000000000000..5fcab42adaf7c
--- /dev/null
+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/TableSourceFactoryContextImpl.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.factories;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.configuration.ReadableConfig;
+import org.apache.flink.table.catalog.CatalogTable;
+import org.apache.flink.table.catalog.ObjectIdentifier;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Implementation of {@link TableSourceFactory.Context}.
+ */
+@Internal
+public class TableSourceFactoryContextImpl implements TableSourceFactory.Context {
+
+	private final ObjectIdentifier identifier;
+	private final CatalogTable table;
+	private final ReadableConfig config;
+
+	public TableSourceFactoryContextImpl(
+			ObjectIdentifier identifier,
+			CatalogTable table,
+			ReadableConfig config) {
+		this.identifier = checkNotNull(identifier);
+		this.table = checkNotNull(table);
+		this.config = checkNotNull(config);
+	}
+
+	@Override
+	public ObjectIdentifier getObjectIdentifier() {
+		return identifier;
+	}
+
+	@Override
+	public CatalogTable getTable() {
+		return table;
+	}
+
+	@Override
+	public ReadableConfig getConfiguration() {
+		return config;
+	}
+}
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala
index ebd8e7d90c2f8..c7083fdff819a 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/codegen/ExpressionReducer.scala
@@ -138,8 +138,8 @@ class ExpressionReducer(
                SqlTypeName.MULTISET =>
             reducedValues.add(unreduced)
           case SqlTypeName.VARCHAR | SqlTypeName.CHAR =>
-            val escapeVarchar = StringEscapeUtils
-              .escapeJava(safeToString(reduced.getField(reducedIdx).asInstanceOf[BinaryString]))
+            val escapeVarchar = safeToString(
+              reduced.getField(reducedIdx).asInstanceOf[BinaryString])
             reducedValues.add(maySkipNullLiteralReduce(rexBuilder, escapeVarchar, unreduced))
             reducedIdx += 1
           case SqlTypeName.VARBINARY | SqlTypeName.BINARY =>
diff --git a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
index 21d0553c7ff3b..b0f9c536e3f88 100644
--- a/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
+++ b/flink-table/flink-table-planner-blink/src/main/scala/org/apache/flink/table/planner/plan/nodes/common/CommonPythonCalc.scala
@@ -170,5 +170,5 @@ trait CommonPythonCalc {
 
 object CommonPythonCalc {
   val PYTHON_SCALAR_FUNCTION_OPERATOR_NAME =
-    "org.apache.flink.table.runtime.operators.python.BaseRowPythonScalarFunctionOperator"
+    "org.apache.flink.table.runtime.operators.python.scalar.BaseRowPythonScalarFunctionOperator"
 }
diff --git a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java
index f64fc85c215c4..2c67306c6da9a 100644
--- a/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java
+++ b/flink-table/flink-table-planner-blink/src/test/java/org/apache/flink/table/planner/runtime/stream/sql/FunctionITCase.java
@@ -45,6 +45,7 @@
 
 import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.Matchers.containsInAnyOrder;
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
@@ -588,11 +589,11 @@ public void testRowTableFunction() throws Exception {
 
 	@Test
 	public void testDynamicTableFunction() throws Exception {
-		final List<Row> sinkData = Arrays.asList(
+		final Row[] sinkData = new Row[]{
 			Row.of("Test is a string"),
 			Row.of("42"),
 			Row.of((String) null)
-		);
+		};
 
 		TestCollectionTableFactory.reset();
 
@@ -608,7 +609,7 @@ public void testDynamicTableFunction() throws Exception {
 			"SELECT CAST(T3.i AS STRING) FROM TABLE(DynamicTableFunction(CAST(NULL AS INT))) AS T3(i)");
 		tEnv().execute("Test Job");
 
-		assertThat(TestCollectionTableFactory.getResult(), equalTo(sinkData));
+		assertThat(TestCollectionTableFactory.getResult(), containsInAnyOrder(sinkData));
 	}
 
 	@Test
diff --git a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/CalcTest.xml b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/CalcTest.xml
index dd55968f65e09..32f4260d9ce69 100644
--- a/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/CalcTest.xml
+++ b/flink-table/flink-table-planner-blink/src/test/resources/org/apache/flink/table/planner/plan/stream/sql/CalcTest.xml
@@ -66,6 +66,24 @@ LogicalProject(a=[$0], b=[$1], c=[$2])
       <![CDATA[
 Calc(select=[a, b, c], where=[OR(<(a, 10), >(a, 20))])
 +- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
+]]>
+    </Resource>
+  </TestCase>
+  <TestCase name="testFilterOnNonAsciiLiteral">
+    <Resource name="sql">
+      <![CDATA[SELECT a, b, c, c || TRIM(' 世界 ') FROM MyTable WHERE c = '你好']]>
+    </Resource>
+    <Resource name="planBefore">
+      <![CDATA[
+LogicalProject(a=[$0], b=[$1], c=[$2], EXPR$3=[||($2, TRIM(FLAG(BOTH), _UTF-16LE' ', _UTF-16LE' 世界 '))])
++- LogicalFilter(condition=[=($2, _UTF-16LE'你好')])
+   +- LogicalTableScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]])
+]]>
+    </Resource>
+    <Resource name="planAfter">
+      <![CDATA[
+Calc(select=[a, b, CAST(_UTF-16LE'你好':VARCHAR(2147483647) CHARACTER SET "UTF-16LE") AS c, CAST(_UTF-16LE'你好世界':VARCHAR(2147483647) CHARACTER SET "UTF-16LE") AS EXPR$3], where=[=(c, _UTF-16LE'你好':VARCHAR(2147483647) CHARACTER SET "UTF-16LE")])
++- TableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
 ]]>
     </Resource>
   </TestCase>
diff --git a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/CalcTest.scala b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/CalcTest.scala
index 6a3a825b610c5..3c7f408228cc6 100644
--- a/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/CalcTest.scala
+++ b/flink-table/flink-table-planner-blink/src/test/scala/org/apache/flink/table/planner/plan/stream/sql/CalcTest.scala
@@ -79,6 +79,12 @@ class CalcTest extends TableTestBase {
     util.verifyPlan(sql)
   }
 
+  @Test
+  def testFilterOnNonAsciiLiteral(): Unit = {
+    val sql = s"SELECT a, b, c, c || TRIM(' 世界 ') FROM MyTable WHERE c = '你好'"
+    util.verifyPlan(sql)
+  }
+
   @Test
   def testNotIn(): Unit = {
     val sql = s"SELECT * FROM MyTable WHERE b NOT IN (1, 3, 4, 5, 6) OR c = 'xx'"
diff --git a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamPythonCalc.scala b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamPythonCalc.scala
index 3b0ed3909a987..7eebebc986ca6 100644
--- a/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamPythonCalc.scala
+++ b/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/plan/nodes/datastream/DataStreamPythonCalc.scala
@@ -130,5 +130,5 @@ class DataStreamPythonCalc(
 
 object DataStreamPythonCalc {
   val PYTHON_SCALAR_FUNCTION_OPERATOR_NAME =
-    "org.apache.flink.table.runtime.operators.python.PythonScalarFunctionOperator"
+    "org.apache.flink.table.runtime.operators.python.scalar.PythonScalarFunctionOperator"
 }
diff --git a/tools/azure-pipelines/build-apache-repo.yml b/tools/azure-pipelines/build-apache-repo.yml
new file mode 100644
index 0000000000000..87919f3157f01
--- /dev/null
+++ b/tools/azure-pipelines/build-apache-repo.yml
@@ -0,0 +1,50 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+#
+# This file defines the Flink build for the "apache/flink" repository, including
+# the following:
+#  - PR builds
+#  - custom triggered e2e tests
+#  - nightly builds
+
+resources:
+  containers:
+  # Container with Maven 3.2.5, SSL to have the same environment everywhere.
+  - container: flink-build-container
+    image: rmetzger/flink-ci:ubuntu-jdk8-amd64-2a765ab
+
+variables:
+  MAVEN_CACHE_FOLDER: $(Pipeline.Workspace)/.m2/repository
+  MAVEN_OPTS: '-Dmaven.repo.local=$(MAVEN_CACHE_FOLDER)'
+  CACHE_KEY: maven | $(Agent.OS) | **/pom.xml, !**/target/**
+  CACHE_FALLBACK_KEY: maven | $(Agent.OS)
+  CACHE_FLINK_DIR: $(Pipeline.Workspace)/flink_cache
+
+stages:
+  # CI / PR triggered stage:
+  - stage: ci_build
+    displayName: "CI Build (custom builders)"
+    condition: not(eq(variables['Build.Reason'], in('Schedule', 'Manual')))
+    jobs:
+      - template: jobs-template.yml
+        parameters:
+          stage_name: ci_build
+          test_pool_definition:
+            name: Default
+          e2e_pool_definition:
+            vmImage: 'ubuntu-16.04'
+          environment: PROFILE="-Dhadoop.version=2.8.3 -Dinclude_hadoop_aws -Dscala-2.11"
diff --git a/tools/azure-pipelines/google-mirror-settings.xml b/tools/azure-pipelines/google-mirror-settings.xml
new file mode 100644
index 0000000000000..49a3b7133e9a0
--- /dev/null
+++ b/tools/azure-pipelines/google-mirror-settings.xml
@@ -0,0 +1,28 @@
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<settings>
+  <mirrors>
+    <mirror>
+      <id>google-maven-central</id>
+      <name>GCS Maven Central mirror</name>
+      <url>https://maven-central-eu.storage-download.googleapis.com/maven2/</url>
+      <mirrorOf>central</mirrorOf>
+    </mirror>
+  </mirrors>
+</settings>
diff --git a/tools/azure-pipelines/jobs-template.yml b/tools/azure-pipelines/jobs-template.yml
new file mode 100644
index 0000000000000..6a65d6ff36134
--- /dev/null
+++ b/tools/azure-pipelines/jobs-template.yml
@@ -0,0 +1,161 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+parameters:
+  test_pool_definition: # defines the hardware pool for compilation and unit test execution.
+  e2e_pool_definion: # defines the hardware pool for end-to-end test execution
+  stage_name: # defines a unique identifier for all jobs in a stage (in case the jobs are added multiple times to a stage)
+  environment: # defines environment variables for downstream scripts
+
+jobs:
+- job: compile_${{parameters.stage_name}}
+  condition: not(eq(variables['MODE'], 'e2e'))
+  pool: ${{parameters.test_pool_definition}}
+  container: flink-build-container
+  timeoutInMinutes: 240
+  cancelTimeoutInMinutes: 1
+  workspace:
+    clean: all # this cleans the entire workspace directory before running a new job
+    # It is necessary because the custom build machines are reused for tests.
+    # See also https://docs.microsoft.com/en-us/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml#workspace 
+
+  steps:
+  # The cache task is persisting the .m2 directory between builds, so that
+  # we do not have to re-download all dependencies from maven central for 
+  # each build. The hope is that downloading the cache is faster than
+  # all dependencies individually.
+  # In this configuration, we use a hash over all committed (not generated) .pom files 
+  # as a key for the build cache (CACHE_KEY). If we have a cache miss on the hash
+  # (usually because a pom file has changed), we'll fall back to a key without
+  # the pom files (CACHE_FALLBACK_KEY).
+  # Offical documentation of the Cache task: https://docs.microsoft.com/en-us/azure/devops/pipelines/caching/?view=azure-devops
+  - task: Cache@2
+    inputs:
+      key: $(CACHE_KEY)
+      restoreKeys: $(CACHE_FALLBACK_KEY)
+      path: $(MAVEN_CACHE_FOLDER)
+    continueOnError: true # continue the build even if the cache fails.
+    displayName: Cache Maven local repo
+
+  # Compile
+  - script: STAGE=compile ${{parameters.environment}} ./tools/azure_controller.sh compile
+    displayName: Build
+
+  # upload artifacts for next stage
+  - task: PublishPipelineArtifact@1
+    inputs:
+      path: $(CACHE_FLINK_DIR)
+      artifact: FlinkCompileCacheDir-${{parameters.stage_name}}
+
+- job: test_${{parameters.stage_name}}
+  dependsOn: compile_${{parameters.stage_name}}
+  condition: not(eq(variables['MODE'], 'e2e'))
+  pool: ${{parameters.test_pool_definition}}
+  container: flink-build-container
+  timeoutInMinutes: 240
+  cancelTimeoutInMinutes: 1
+  workspace:
+    clean: all
+  strategy:
+    matrix:
+      core:
+        module: core
+      python:
+        module: python
+      libraries:
+        module: libraries
+      blink_planner:
+        module: blink_planner
+      connectors:
+        module: connectors
+      kafka_gelly:
+        module: kafka/gelly
+      tests:
+        module: tests
+      legacy_scheduler_core:
+        module: legacy_scheduler_core
+      legacy_scheduler_tests:
+        module: legacy_scheduler_tests
+      misc:
+        module: misc
+  steps:
+
+  # download artifacts
+  - task: DownloadPipelineArtifact@2
+    inputs:
+      path: $(CACHE_FLINK_DIR)
+      artifact: FlinkCompileCacheDir-${{parameters.stage_name}}
+
+  # recreate "build-target" symlink for python tests
+  - script: |
+      ln -snf $(CACHE_FLINK_DIR)/flink-dist/target/flink-*-SNAPSHOT-bin/flink-*-SNAPSHOT $(CACHE_FLINK_DIR)/build-target
+    displayName: Recreate 'build-target' symlink
+  # Test
+  - script: STAGE=test ${{parameters.environment}} ./tools/azure_controller.sh $(module)
+    displayName: Test - $(module)
+
+  - task: PublishTestResults@2
+    inputs:
+      testResultsFormat: 'JUnit'
+
+- job: precommit_${{parameters.stage_name}}
+  dependsOn: compile_${{parameters.stage_name}}
+  # We are not running this job on a container, but in a VM.
+  pool: ${{parameters.e2e_pool_definition}}
+  timeoutInMinutes: 240
+  cancelTimeoutInMinutes: 1
+  workspace:
+    clean: all
+  steps:
+    - task: Cache@2
+      inputs:
+        key: $(CACHE_KEY)
+        restoreKeys: $(CACHE_FALLBACK_KEY)
+        path: $(MAVEN_CACHE_FOLDER)
+      displayName: Cache Maven local repo
+    
+    # download artifacts
+    - task: DownloadPipelineArtifact@2
+      inputs:
+        path: $(CACHE_FLINK_DIR)
+        artifact: FlinkCompileCacheDir-${{parameters.stage_name}}
+    - script: ./tools/travis/setup_maven.sh
+    - script: ./tools/azure-pipelines/prepare_precommit.sh
+      displayName: prepare and build Flink
+    - script: FLINK_DIR=build-target ./flink-end-to-end-tests/run-pre-commit-tests.sh
+      displayName: Test - precommit 
+
+- job: e2e_${{parameters.stage_name}}
+  condition: eq(variables['MODE'], 'e2e')
+  # We are not running this job on a container, but in a VM.
+  pool: ${{parameters.e2e_pool_definition}}
+  timeoutInMinutes: 240
+  cancelTimeoutInMinutes: 1
+  workspace:
+    clean: all
+  steps:
+    - task: Cache@2
+      inputs:
+        key: $(CACHE_KEY)
+        restoreKeys: $(CACHE_FALLBACK_KEY)
+        path: $(MAVEN_CACHE_FOLDER)
+      displayName: Cache Maven local repo
+    - script: ./tools/travis/setup_maven.sh
+    - script: ./tools/azure-pipelines/setup_kubernetes.sh
+    - script: M2_HOME=/home/vsts/maven_cache/apache-maven-3.2.5/ PATH=/home/vsts/maven_cache/apache-maven-3.2.5/bin:$PATH PROFILE="-Dinclude-hadoop -Dhadoop.version=2.8.3 -De2e-metrics -Dmaven.wagon.http.pool=false" STAGE=compile ./tools/azure_controller.sh compile
+      displayName: Build
+    - script: FLINK_DIR=`pwd`/build-target flink-end-to-end-tests/run-nightly-tests.sh
+      displayName: Run nightly e2e tests
+
diff --git a/tools/azure-pipelines/prepare_precommit.sh b/tools/azure-pipelines/prepare_precommit.sh
new file mode 100755
index 0000000000000..71a231cc90a48
--- /dev/null
+++ b/tools/azure-pipelines/prepare_precommit.sh
@@ -0,0 +1,39 @@
+#!/usr/bin/env bash
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+echo "Merging cache"
+cp -RT "$CACHE_FLINK_DIR" "."
+echo "Adjusting timestamps"
+# adjust timestamps to prevent recompilation
+find . -type f -name '*.java' | xargs touch
+find . -type f -name '*.scala' | xargs touch
+# wait a bit for better odds of different timestamps
+sleep 5
+find . -type f -name '*.class' | xargs touch
+find . -type f -name '*.timestamp' | xargs touch
+
+
+export M2_HOME=/home/vsts/maven_cache/apache-maven-3.2.5/ 
+export PATH=/home/vsts/maven_cache/apache-maven-3.2.5/bin:$PATH
+mvn -version
+mvn install --settings ./tools/azure-pipelines/google-mirror-settings.xml -DskipTests -Drat.skip
+
+
+chmod -R +x build-target
+chmod -R +x flink-end-to-end-tests
diff --git a/tools/azure-pipelines/setup_kubernetes.sh b/tools/azure-pipelines/setup_kubernetes.sh
new file mode 100755
index 0000000000000..19eb50ae16d0f
--- /dev/null
+++ b/tools/azure-pipelines/setup_kubernetes.sh
@@ -0,0 +1,26 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+echo "Replace moby by docker"
+docker version
+sudo apt-get remove -y moby-engine
+curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
+sudo add-apt-repository \
+   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
+   $(lsb_release -cs) \
+   stable"
+sudo apt-get update
+sudo apt-get install -y docker-ce docker-ce-cli containerd.io
+docker version
diff --git a/tools/azure_controller.sh b/tools/azure_controller.sh
new file mode 100755
index 0000000000000..2682fcb18b6b1
--- /dev/null
+++ b/tools/azure_controller.sh
@@ -0,0 +1,190 @@
+#!/usr/bin/env bash
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+echo $M2_HOME
+echo $PATH
+echo $MAVEN_OPTS
+
+mvn -version
+echo "Commit: $(git rev-parse HEAD)"
+
+
+
+HERE="`dirname \"$0\"`"             # relative
+HERE="`( cd \"$HERE\" && pwd )`"    # absolutized and normalized
+if [ -z "$HERE" ] ; then
+    # error; for some reason, the path is not accessible
+    # to the script (e.g. permissions re-evaled after suid)
+    exit 1  # fail
+fi
+
+source "${HERE}/travis/stage.sh"
+source "${HERE}/travis/shade.sh"
+
+print_system_info() {
+    echo "CPU information"
+    lscpu
+
+    echo "Memory information"
+    cat /proc/meminfo
+
+    echo "Disk information"
+    df -hH
+
+    echo "Running build as"
+    whoami
+}
+
+print_system_info
+
+
+STAGE=$1
+echo "Current stage: \"$STAGE\""
+
+EXIT_CODE=0
+
+# Set up a custom Maven settings file, configuring an Google-hosted maven central
+# mirror. We use a different mirror because the official maven central mirrors
+# often lead to connection timeouts (probably due to rate-limiting)
+
+# adding -Dmaven.wagon.http.pool=false (see https://developercommunity.visualstudio.com/content/problem/851041/microsoft-hosted-agents-run-into-maven-central-tim.html)
+MVN="mvn clean install --settings ./tools/azure-pipelines/google-mirror-settings.xml $MAVEN_OPTS -nsu -Dflink.convergence.phase=install -Pcheck-convergence -Dflink.forkCount=2 -Dflink.forkCountTestPackage=2 -Dmaven.wagon.http.pool=false -Dmaven.javadoc.skip=true -B -U -DskipTests $PROFILE"
+
+# Run actual compile&test steps
+if [ $STAGE == "$STAGE_COMPILE" ]; then
+    # run mvn clean install:
+    $MVN
+    EXIT_CODE=$?
+
+    if [ $EXIT_CODE == 0 ]; then
+        echo "\n\n==============================================================================\n"
+        echo "Checking scala suffixes\n"
+        echo "==============================================================================\n"
+
+        ./tools/verify_scala_suffixes.sh "${PROFILE}"
+        EXIT_CODE=$?
+    else
+        echo "\n==============================================================================\n"
+        echo "Previous build failure detected, skipping scala-suffixes check.\n"
+        echo "==============================================================================\n"
+    fi
+    
+    if [ $EXIT_CODE == 0 ]; then
+        check_shaded_artifacts
+        EXIT_CODE=$(($EXIT_CODE+$?))
+        check_shaded_artifacts_s3_fs hadoop
+        EXIT_CODE=$(($EXIT_CODE+$?))
+        check_shaded_artifacts_s3_fs presto
+        EXIT_CODE=$(($EXIT_CODE+$?))
+        check_shaded_artifacts_connector_elasticsearch 2
+        EXIT_CODE=$(($EXIT_CODE+$?))
+        check_shaded_artifacts_connector_elasticsearch 5
+        EXIT_CODE=$(($EXIT_CODE+$?))
+        check_shaded_artifacts_connector_elasticsearch 6
+        EXIT_CODE=$(($EXIT_CODE+$?))
+    else
+        echo "=============================================================================="
+        echo "Previous build failure detected, skipping shaded dependency check."
+        echo "=============================================================================="
+    fi
+
+    if [ $EXIT_CODE == 0 ]; then
+        echo "Creating cache build directory $CACHE_FLINK_DIR"
+    
+        cp -r . "$CACHE_FLINK_DIR"
+
+        function minimizeCachedFiles() {
+            # reduces the size of the cached directory to speed up
+            # the packing&upload / download&unpacking process
+            # by removing files not required for subsequent stages
+    
+            # jars are re-built in subsequent stages, so no need to cache them (cannot be avoided)
+            find "$CACHE_FLINK_DIR" -maxdepth 8 -type f -name '*.jar' \
+            ! -path "$CACHE_FLINK_DIR/flink-formats/flink-csv/target/flink-csv*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-formats/flink-json/target/flink-json*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-formats/flink-avro/target/flink-avro*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-runtime/target/flink-runtime*tests.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-streaming-java/target/flink-streaming-java*tests.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-dist/target/flink-*-bin/flink-*/lib/flink-dist*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-dist/target/flink-*-bin/flink-*/lib/flink-table_*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-dist/target/flink-*-bin/flink-*/lib/flink-table-blink*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-dist/target/flink-*-bin/flink-*/opt/flink-python*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-connectors/flink-connector-elasticsearch-base/target/flink-*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-connectors/flink-connector-kafka-base/target/flink-*.jar" \
+            ! -path "$CACHE_FLINK_DIR/flink-table/flink-table-planner/target/flink-table-planner*tests.jar" | xargs rm -rf
+    
+            # .git directory
+            # not deleting this can cause build stability issues
+            # merging the cached version sometimes fails
+            rm -rf "$CACHE_FLINK_DIR/.git"
+
+            # AZ Pipelines has a problem with links.
+            rm "$CACHE_FLINK_DIR/build-target"
+        }
+    
+        echo "Minimizing cache"
+        minimizeCachedFiles
+    else
+        echo "=============================================================================="
+        echo "Previous build failure detected, skipping cache setup."
+        echo "=============================================================================="
+    fi
+elif [ $STAGE != "$STAGE_CLEANUP" ]; then
+    if ! [ -e $CACHE_FLINK_DIR ]; then
+        echo "Cached flink dir $CACHE_FLINK_DIR does not exist. Exiting build."
+        exit 1
+    fi
+    # merged compiled flink into local clone
+    # this prevents the cache from being re-uploaded
+    echo "Merging cache"
+    cp -RT "$CACHE_FLINK_DIR" "."
+
+    echo "Adjusting timestamps"
+    # adjust timestamps to prevent recompilation
+    find . -type f -name '*.java' | xargs touch
+    find . -type f -name '*.scala' | xargs touch
+    # wait a bit for better odds of different timestamps
+    sleep 5
+    find . -type f -name '*.class' | xargs touch
+    find . -type f -name '*.timestamp' | xargs touch
+
+    if [ $STAGE == $STAGE_PYTHON ]; then
+        echo "=============================================================================="
+        echo "Python stage found. Re-compiling (this is required on Azure for the python tests to pass)"
+        echo "=============================================================================="
+        # run mvn install (w/o "clean"):
+        PY_MVN="${MVN// clean/}"
+        PY_MVN="$PY_MVN -Drat.skip=true"
+        ${PY_MVN}
+        echo "Done compiling ... "
+    fi
+
+
+    TEST="$STAGE" "./tools/travis_watchdog.sh" 300
+    EXIT_CODE=$?
+elif [ $STAGE == "$STAGE_CLEANUP" ]; then
+    echo "Cleaning up $CACHE_BUILD_DIR"
+    rm -rf "$CACHE_BUILD_DIR"
+else
+    echo "Invalid Stage specified: $STAGE"
+    exit 1
+fi
+
+# Exit code for Azure build success/failure
+exit $EXIT_CODE
diff --git a/tools/travis_watchdog.sh b/tools/travis_watchdog.sh
index ad847fbf23b74..e96934c7a8791 100755
--- a/tools/travis_watchdog.sh
+++ b/tools/travis_watchdog.sh
@@ -95,6 +95,11 @@ UPLOAD_SECRET_KEY=$ARTIFACTS_AWS_SECRET_KEY
 
 ARTIFACTS_FILE=${TRAVIS_JOB_NUMBER}.tar.gz
 
+if [ ! -z "$TF_BUILD" ] ; then
+	# set proper artifacts file name on Azure Pipelines
+	ARTIFACTS_FILE=${BUILD_BUILDNUMBER}.tar.gz
+fi
+
 if [ $TEST == $STAGE_PYTHON ]; then
 	CMD=$PYTHON_TEST
 	CMD_PID=$PYTHON_PID
@@ -166,7 +171,7 @@ print_stacktraces () {
 put_yarn_logs_to_artifacts() {
 	# Make sure to be in project root
 	cd $HERE/../
-	for file in `find ./flink-yarn-tests/target/flink-yarn-tests* -type f -name '*.log'`; do
+	for file in `find ./flink-yarn-tests/target -type f -name '*.log'`; do
 		TARGET_FILE=`echo "$file" | grep -Eo "container_[0-9_]+/(.*).log"`
 		TARGET_DIR=`dirname	 "$TARGET_FILE"`
 		mkdir -p "$ARTIFACTS_DIR/yarn-tests/$TARGET_DIR"
@@ -273,18 +278,22 @@ cd ../../
 # only run end-to-end tests in misc because we only have flink-dist here
 case $TEST in
     (misc)
-        if [ $EXIT_CODE == 0 ]; then
-            echo "\n\n==============================================================================\n"
-            echo "Running bash end-to-end tests\n"
-            echo "==============================================================================\n"
-
-            FLINK_DIR=build-target flink-end-to-end-tests/run-pre-commit-tests.sh
-
-            EXIT_CODE=$?
-        else
-            echo "\n==============================================================================\n"
-            echo "Previous build failure detected, skipping bash end-to-end tests.\n"
-            echo "==============================================================================\n"
+        # If we are not on Azure (we are on Travis) run precommit tests in misc stage.
+        # On Azure, we run them in a separate job
+        if [ -z "$TF_BUILD" ] ; then
+            if [ $EXIT_CODE == 0 ]; then
+                echo "\n\n==============================================================================\n"
+                echo "Running bash end-to-end tests\n"
+                echo "==============================================================================\n"
+
+                FLINK_DIR=build-target flink-end-to-end-tests/run-pre-commit-tests.sh
+
+                EXIT_CODE=$?
+            else
+                echo "\n==============================================================================\n"
+                echo "Previous build failure detected, skipping bash end-to-end tests.\n"
+                echo "==============================================================================\n"
+            fi
         fi
         if [ $EXIT_CODE == 0 ]; then
             echo "\n\n==============================================================================\n"
