diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java
index b712b97e6df3a..8b80373400a7c 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliClient.java
@@ -296,6 +296,12 @@ private void callCommand(SqlCommandCall cmdCall) {
 			case INSERT_INTO:
 				callInsertInto(cmdCall);
 				break;
+			case CREATE_TABLE:
+				callCreateTable(cmdCall);
+				break;
+			case DROP_TABLE:
+				callDropTable(cmdCall);
+				break;
 			case CREATE_VIEW:
 				callCreateView(cmdCall);
 				break;
@@ -537,6 +543,25 @@ private boolean callInsertInto(SqlCommandCall cmdCall) {
 		return true;
 	}
 
+	private void callCreateTable(SqlCommandCall cmdCall) {
+		try {
+			executor.createTable(sessionId, cmdCall.operands[0]);
+			printInfo(CliStrings.MESSAGE_TABLE_CREATED);
+		} catch (SqlExecutionException e) {
+			printExecutionException(e);
+			return;
+		}
+	}
+
+	private void callDropTable(SqlCommandCall cmdCall) {
+		try {
+			executor.dropTable(sessionId, cmdCall.operands[0]);
+			printInfo(CliStrings.MESSAGE_TABLE_REMOVED);
+		} catch (SqlExecutionException e) {
+			printExecutionException(e);
+		}
+	}
+
 	private void callCreateView(SqlCommandCall cmdCall) {
 		final String name = cmdCall.operands[0];
 		final String query = cmdCall.operands[1];
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java
index 6429e1ddc443b..a04068f269ace 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/CliStrings.java
@@ -42,6 +42,8 @@ private CliStrings() {
 	public static final AttributedString MESSAGE_HELP = new AttributedStringBuilder()
 		.append("The following commands are available:\n\n")
 		.append(formatCommand(SqlCommand.CLEAR, "Clears the current terminal."))
+		.append(formatCommand(SqlCommand.CREATE_TABLE, "Create table under current catalog and database."))
+		.append(formatCommand(SqlCommand.DROP_TABLE, "Drop table with optional catalog and database. Syntax: 'DROP TABLE [IF EXISTS] <name>;'"))
 		.append(formatCommand(SqlCommand.CREATE_VIEW, "Creates a virtual table from a SQL query. Syntax: 'CREATE VIEW <name> AS <query>;'"))
 		.append(formatCommand(SqlCommand.DESCRIBE, "Describes the schema of a table with the given name."))
 		.append(formatCommand(SqlCommand.DROP_VIEW, "Deletes a previously created virtual table. Syntax: 'DROP VIEW <name>;'"))
@@ -137,6 +139,10 @@ private CliStrings() {
 
 	public static final String MESSAGE_UNSUPPORTED_SQL = "Unsupported SQL statement.";
 
+	public static final String MESSAGE_TABLE_CREATED = "Table has been created.";
+
+	public static final String MESSAGE_TABLE_REMOVED = "Table has been removed.";
+
 	public static final String MESSAGE_ALTER_TABLE_SUCCEEDED = "Alter table succeeded!";
 
 	public static final String MESSAGE_ALTER_TABLE_FAILED = "Alter table failed!";
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/SqlCommandParser.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/SqlCommandParser.java
index a185553f22ed9..da1fd94ac39d5 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/SqlCommandParser.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/cli/SqlCommandParser.java
@@ -127,6 +127,10 @@ enum SqlCommand {
 			"(INSERT\\s+INTO.*)",
 			SINGLE_OPERAND),
 
+		CREATE_TABLE("(CREATE\\s+TABLE\\s+.*)", SINGLE_OPERAND),
+
+		DROP_TABLE("(DROP\\s+TABLE\\s+.*)", SINGLE_OPERAND),
+
 		CREATE_VIEW(
 			"CREATE\\s+VIEW\\s+(\\S+)\\s+AS\\s+(.*)",
 			(operands) -> {
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
index 33cdf3cbfac53..b6fd68d42de85 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/Executor.java
@@ -113,6 +113,16 @@ public interface Executor {
 	 */
 	List<String> listDatabases(String sessionId) throws SqlExecutionException;
 
+	/**
+	 * Create a table with a DDL statement.
+	 */
+	void createTable(String sessionId, String ddl) throws SqlExecutionException;
+
+	/**
+	 * Drop a table with a DDL statement.
+	 */
+	void dropTable(String sessionId, String ddl) throws SqlExecutionException;
+
 	/**
 	 * Lists all tables in the current database of the current catalog.
 	 */
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
index 6a073a80b201d..e9cf0e6b76000 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/ExecutionContext.java
@@ -31,7 +31,6 @@
 import org.apache.flink.client.deployment.ClusterClientServiceLoader;
 import org.apache.flink.client.deployment.ClusterDescriptor;
 import org.apache.flink.client.deployment.ClusterSpecification;
-import org.apache.flink.client.deployment.DefaultClusterClientServiceLoader;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.core.plugin.TemporaryClassLoaderContext;
 import org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders;
@@ -48,6 +47,7 @@
 import org.apache.flink.table.api.TableException;
 import org.apache.flink.table.api.java.BatchTableEnvironment;
 import org.apache.flink.table.api.java.StreamTableEnvironment;
+import org.apache.flink.table.api.java.internal.BatchTableEnvironmentImpl;
 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;
 import org.apache.flink.table.catalog.Catalog;
 import org.apache.flink.table.catalog.CatalogManager;
@@ -92,6 +92,8 @@
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.Options;
 
+import javax.annotation.Nullable;
+
 import java.lang.reflect.Method;
 import java.net.URL;
 import java.util.HashMap;
@@ -129,26 +131,13 @@
 	private StreamExecutionEnvironment streamExecEnv;
 	private Executor executor;
 
-	public ExecutionContext(
-			Environment environment,
-			SessionContext originalSessionContext,
-			List<URL> dependencies,
-			Configuration flinkConfig,
-			Options commandLineOptions,
-			List<CustomCommandLine> availableCommandLines) throws FlinkException {
-		this(
-				environment,
-				originalSessionContext,
-				dependencies,
-				flinkConfig,
-				new DefaultClusterClientServiceLoader(),
-				commandLineOptions,
-				availableCommandLines);
-	}
+	// Members that should be reused in the same session.
+	private SessionState sessionState;
 
-	public ExecutionContext(
+	private ExecutionContext(
 			Environment environment,
 			SessionContext originalSessionContext,
+			@Nullable SessionState sessionState,
 			List<URL> dependencies,
 			Configuration flinkConfig,
 			ClusterClientServiceLoader clusterClientServiceLoader,
@@ -165,7 +154,7 @@ public ExecutionContext(
 				this.getClass().getClassLoader());
 
 		// Initialize the TableEnvironment.
-		initializeTableEnvironment();
+		initializeTableEnvironment(sessionState);
 
 		// convert deployment options into command line options that describe a cluster
 		final ClusterClientServiceLoader serviceLoader = checkNotNull(clusterClientServiceLoader);
@@ -219,6 +208,10 @@ public Map<String, Catalog> getCatalogs() {
 		return catalogs;
 	}
 
+	public SessionState getSessionState() {
+		return this.sessionState;
+	}
+
 	/**
 	 * Executes the given supplier using the execution context's classloader as thread classloader.
 	 */
@@ -275,6 +268,19 @@ public JobGraph createJobGraph(String name) {
 		return jobGraph;
 	}
 
+	/** Returns a builder for this {@link ExecutionContext}. */
+	public static Builder builder(
+			Environment defaultEnv,
+			SessionContext sessionContext,
+			List<URL> dependencies,
+			Configuration configuration,
+			ClusterClientServiceLoader serviceLoader,
+			Options commandLineOptions,
+			List<CustomCommandLine> commandLines) {
+		return new Builder(defaultEnv, sessionContext, dependencies, configuration,
+				serviceLoader, commandLineOptions, commandLines);
+	}
+
 	//------------------------------------------------------------------------------------------------------------------
 	// Non-public methods
 	//------------------------------------------------------------------------------------------------------------------
@@ -346,16 +352,13 @@ private static TableSink<?> createTableSink(ExecutionEntry execution, Map<String
 	private static TableEnvironment createStreamTableEnvironment(
 			StreamExecutionEnvironment env,
 			EnvironmentSettings settings,
-			Executor executor) {
+			Executor executor,
+			CatalogManager catalogManager,
+			ModuleManager moduleManager,
+			FunctionCatalog functionCatalog) {
 
 		final TableConfig config = TableConfig.getDefault();
 
-		final CatalogManager catalogManager = new CatalogManager(
-			settings.getBuiltInCatalogName(),
-			new GenericInMemoryCatalog(settings.getBuiltInCatalogName(), settings.getBuiltInDatabaseName()));
-		final ModuleManager moduleManager = new ModuleManager();
-		final FunctionCatalog functionCatalog = new FunctionCatalog(catalogManager, moduleManager);
-
 		final Map<String, String> plannerProperties = settings.toPlannerProperties();
 		final Planner planner = ComponentFactoryService.find(PlannerFactory.class, plannerProperties)
 			.create(plannerProperties, executor, config, functionCatalog, catalogManager);
@@ -368,8 +371,7 @@ private static TableEnvironment createStreamTableEnvironment(
 			env,
 			planner,
 			executor,
-			settings.isStreamingMode()
-		);
+			settings.isStreamingMode());
 	}
 
 	private static Executor lookupExecutor(
@@ -391,55 +393,113 @@ private static Executor lookupExecutor(
 		}
 	}
 
-	private void initializeTableEnvironment() {
-		//--------------------------------------------------------------------------------------------------------------
-		// Step.1 Create environments
-		//--------------------------------------------------------------------------------------------------------------
+	private void initializeTableEnvironment(@Nullable SessionState sessionState) {
+		final EnvironmentSettings settings = environment.getExecution().getEnvironmentSettings();
+		final boolean noInheritedState = sessionState == null;
+		if (noInheritedState) {
+			//--------------------------------------------------------------------------------------------------------------
+			// Step.1 Create environments
+			//--------------------------------------------------------------------------------------------------------------
+			// Step 1.1 Initialize the CatalogManager if required.
+			final CatalogManager catalogManager = new CatalogManager(
+					settings.getBuiltInCatalogName(),
+					new GenericInMemoryCatalog(
+							settings.getBuiltInCatalogName(),
+							settings.getBuiltInDatabaseName()));
+			// Step 1.2 Initialize the ModuleManager if required.
+			final ModuleManager moduleManager = new ModuleManager();
+			// Step 1.3 Initialize the FunctionCatalog if required.
+			final FunctionCatalog functionCatalog = new FunctionCatalog(catalogManager, moduleManager);
+			// Step 1.4 Set up session state.
+			this.sessionState = SessionState.of(catalogManager, moduleManager, functionCatalog);
+
+			// Must initialize the table environment before actually the
+			createTableEnvironment(settings, catalogManager, moduleManager, functionCatalog);
+
+			//--------------------------------------------------------------------------------------------------------------
+			// Step.2 Create modules and load them into the TableEnvironment.
+			//--------------------------------------------------------------------------------------------------------------
+			// No need to register the modules info if already inherit from the same session.
+			Map<String, Module> modules = new LinkedHashMap<>();
+			environment.getModules().forEach((name, entry) ->
+					modules.put(name, createModule(entry.asMap(), classLoader))
+			);
+			if (!modules.isEmpty()) {
+				// unload core module first to respect whatever users configure
+				tableEnv.unloadModule(CoreModuleDescriptorValidator.MODULE_TYPE_CORE);
+				modules.forEach(tableEnv::loadModule);
+			}
+
+			//--------------------------------------------------------------------------------------------------------------
+			// Step.3 create user-defined functions and temporal tables then register them.
+			//--------------------------------------------------------------------------------------------------------------
+			// No need to register the functions if already inherit from the same session.
+			registerFunctions();
+
+			//--------------------------------------------------------------------------------------------------------------
+			// Step.4 Create catalogs and register them.
+			//--------------------------------------------------------------------------------------------------------------
+			// No need to register the catalogs if already inherit from the same session.
+			initializeCatalogs();
+		} else {
+			// Set up session state.
+			this.sessionState = sessionState;
+			createTableEnvironment(
+					settings,
+					sessionState.catalogManager,
+					sessionState.moduleManager,
+					sessionState.functionCatalog);
+		}
+	}
+
+	private void createTableEnvironment(
+			EnvironmentSettings settings,
+			CatalogManager catalogManager,
+			ModuleManager moduleManager,
+			FunctionCatalog functionCatalog) {
 		if (environment.getExecution().isStreamingPlanner()) {
 			streamExecEnv = createStreamExecutionEnvironment();
 			execEnv = null;
 
-			final EnvironmentSettings settings = environment.getExecution().getEnvironmentSettings();
 			final Map<String, String> executorProperties = settings.toExecutorProperties();
 			executor = lookupExecutor(executorProperties, streamExecEnv);
-			tableEnv = createStreamTableEnvironment(streamExecEnv, settings, executor);
+			tableEnv = createStreamTableEnvironment(
+					streamExecEnv,
+					settings,
+					executor,
+					catalogManager,
+					moduleManager,
+					functionCatalog);
 		} else if (environment.getExecution().isBatchPlanner()) {
 			streamExecEnv = null;
 			execEnv = createExecutionEnvironment();
 			executor = null;
-			tableEnv = BatchTableEnvironment.create(execEnv);
+			tableEnv = new BatchTableEnvironmentImpl(
+					execEnv,
+					TableConfig.getDefault(),
+					catalogManager,
+					moduleManager);
 		} else {
 			throw new SqlExecutionException("Unsupported execution type specified.");
 		}
 		// set table configuration
 		environment.getConfiguration().asMap().forEach((k, v) ->
-			tableEnv.getConfig().getConfiguration().setString(k, v));
-
-		//--------------------------------------------------------------------------------------------------------------
-		// Step.2 Create modules and load them into the TableEnvironment.
-		//--------------------------------------------------------------------------------------------------------------
-		Map<String, Module> modules = new LinkedHashMap<>();
-		environment.getModules().forEach((name, entry) ->
-			modules.put(name, createModule(entry.asMap(), classLoader))
-		);
-		if (!modules.isEmpty()) {
-			// unload core module first to respect whatever users configure
-			tableEnv.unloadModule(CoreModuleDescriptorValidator.MODULE_TYPE_CORE);
-			modules.forEach(tableEnv::loadModule);
-		}
+				tableEnv.getConfig().getConfiguration().setString(k, v));
+	}
 
+	private void initializeCatalogs() {
 		//--------------------------------------------------------------------------------------------------------------
-		// Step.3 Create catalogs and register them.
+		// Step.1 Create catalogs and register them.
 		//--------------------------------------------------------------------------------------------------------------
 		Map<String, Catalog> catalogs = new LinkedHashMap<>();
 		environment.getCatalogs().forEach((name, entry) ->
-			catalogs.put(name, createCatalog(name, entry.asMap(), classLoader))
+				catalogs.put(name, createCatalog(name, entry.asMap(), classLoader))
 		);
 		// register catalogs
 		catalogs.forEach(tableEnv::registerCatalog);
 
 		//--------------------------------------------------------------------------------------------------------------
-		// Step.4 create table sources & sinks, and register them.
+		// Step.2 create table sources & sinks, and register them.
 		//--------------------------------------------------------------------------------------------------------------
 		Map<String, TableSource<?>> tableSources = new HashMap<>();
 		Map<String, TableSink<?>> tableSinks = new HashMap<>();
@@ -457,17 +517,17 @@ private void initializeTableEnvironment() {
 		tableSinks.forEach(tableEnv::registerTableSink);
 
 		//--------------------------------------------------------------------------------------------------------------
-		// Step.5 create user-defined functions and register them.
+		// Step.4 Register temporal tables.
 		//--------------------------------------------------------------------------------------------------------------
-		Map<String, FunctionDefinition> functions = new LinkedHashMap<>();
-		environment.getFunctions().forEach((name, entry) -> {
-			final UserDefinedFunction function = FunctionService.createFunction(entry.getDescriptor(), classLoader, false);
-			functions.put(name, function);
+		environment.getTables().forEach((name, entry) -> {
+			if (entry instanceof TemporalTableEntry) {
+				final TemporalTableEntry temporalTableEntry = (TemporalTableEntry) entry;
+				registerTemporalTable(temporalTableEntry);
+			}
 		});
-		registerFunctions(functions);
 
 		//--------------------------------------------------------------------------------------------------------------
-		// Step.5 Register views and temporal tables in specified order.
+		// Step.4 Register views in specified order.
 		//--------------------------------------------------------------------------------------------------------------
 		environment.getTables().forEach((name, entry) -> {
 			// if registering a view fails at this point,
@@ -475,14 +535,11 @@ private void initializeTableEnvironment() {
 			if (entry instanceof ViewEntry) {
 				final ViewEntry viewEntry = (ViewEntry) entry;
 				registerView(viewEntry);
-			} else if (entry instanceof TemporalTableEntry) {
-				final TemporalTableEntry temporalTableEntry = (TemporalTableEntry) entry;
-				registerTemporalTable(temporalTableEntry);
 			}
 		});
 
 		//--------------------------------------------------------------------------------------------------------------
-		// Step.6 Set current catalog and database.
+		// Step.5 Set current catalog and database.
 		//--------------------------------------------------------------------------------------------------------------
 		// Switch to the current catalog.
 		Optional<String> catalog = environment.getExecution().getCurrentCatalog();
@@ -512,6 +569,15 @@ private StreamExecutionEnvironment createStreamExecutionEnvironment() {
 		return env;
 	}
 
+	private void registerFunctions() {
+		Map<String, FunctionDefinition> functions = new LinkedHashMap<>();
+		environment.getFunctions().forEach((name, entry) -> {
+			final UserDefinedFunction function = FunctionService.createFunction(entry.getDescriptor(), classLoader, false);
+			functions.put(name, function);
+		});
+		registerFunctions(functions);
+	}
+
 	private void registerFunctions(Map<String, FunctionDefinition> functions) {
 		if (tableEnv instanceof StreamTableEnvironment) {
 			StreamTableEnvironment streamTableEnvironment = (StreamTableEnvironment) tableEnv;
@@ -584,4 +650,93 @@ private Pipeline createPipeline(String name) {
 			return execEnv.createProgramPlan(name);
 		}
 	}
+
+	//~ Inner Class -------------------------------------------------------------------------------
+
+	/** Builder for {@link ExecutionContext}. */
+	public static class Builder {
+		// Required members.
+		private final SessionContext sessionContext;
+		private final List<URL> dependencies;
+		private final Configuration configuration;
+		private final ClusterClientServiceLoader serviceLoader;
+		private final Options commandLineOptions;
+		private final List<CustomCommandLine> commandLines;
+
+		private Environment defaultEnv;
+		private Environment currentEnv;
+
+		// Optional members.
+		@Nullable
+		private SessionState sessionState;
+
+		private Builder(
+				Environment defaultEnv,
+				@Nullable SessionContext sessionContext,
+				List<URL> dependencies,
+				Configuration configuration,
+				ClusterClientServiceLoader serviceLoader,
+				Options commandLineOptions,
+				List<CustomCommandLine> commandLines) {
+			this.defaultEnv = defaultEnv;
+			this.sessionContext = sessionContext;
+			this.dependencies = dependencies;
+			this.configuration = configuration;
+			this.serviceLoader = serviceLoader;
+			this.commandLineOptions = commandLineOptions;
+			this.commandLines = commandLines;
+		}
+
+		public Builder env(Environment environment) {
+			this.currentEnv = environment;
+			return this;
+		}
+
+		public Builder sessionState(SessionState sessionState) {
+			this.sessionState = sessionState;
+			return this;
+		}
+
+		public ExecutionContext<?> build() {
+			try {
+				return new ExecutionContext<>(
+						this.currentEnv == null
+								? Environment.merge(defaultEnv, sessionContext.getSessionEnv())
+								: this.currentEnv,
+						this.sessionContext,
+						this.sessionState,
+						this.dependencies,
+						this.configuration,
+						this.serviceLoader,
+						this.commandLineOptions,
+						this.commandLines);
+			} catch (Throwable t) {
+				// catch everything such that a configuration does not crash the executor
+				throw new SqlExecutionException("Could not create execution context.", t);
+			}
+		}
+	}
+
+	/** Represents the state that should be reused in one session. **/
+	public static class SessionState {
+		public final CatalogManager catalogManager;
+		public final ModuleManager moduleManager;
+		public final FunctionCatalog functionCatalog;
+
+		private SessionState(
+				CatalogManager catalogManager,
+				ModuleManager moduleManager,
+				FunctionCatalog functionCatalog) {
+			this.catalogManager = catalogManager;
+			this.moduleManager = moduleManager;
+			this.functionCatalog = functionCatalog;
+		}
+
+		public static SessionState of(
+				CatalogManager catalogManager,
+				ModuleManager moduleManager,
+				FunctionCatalog functionCatalog) {
+			return new SessionState(catalogManager, moduleManager, functionCatalog);
+		}
+	}
 }
diff --git a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
index b9312ea938f85..8537bf7cee758 100644
--- a/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
+++ b/flink-table/flink-sql-client/src/main/java/org/apache/flink/table/client/gateway/local/LocalExecutor.java
@@ -198,39 +198,27 @@ public void start() {
 		// nothing to do yet
 	}
 
-	/**
-	 * Create a new {@link ExecutionContext} by merging the default environment the the environment in session context.
-	 */
-	private ExecutionContext<?> createExecutionContext(SessionContext sessionContext) {
-		Environment mergedEnv = Environment.merge(defaultEnvironment, sessionContext.getSessionEnv());
-		return createExecutionContext(mergedEnv, sessionContext);
-	}
-
-	/**
-	 * Create a new {@link ExecutionContext} by using the given environment.
-	 */
-	private ExecutionContext<?> createExecutionContext(Environment environment, SessionContext sessionContext) {
-		try {
-			return new ExecutionContext<>(
-					environment,
-					sessionContext,
-					dependencies,
-					flinkConfig,
-					clusterClientServiceLoader,
-					commandLineOptions,
-					commandLines);
-		} catch (Throwable t) {
-			// catch everything such that a configuration does not crash the executor
-			throw new SqlExecutionException("Could not create execution context.", t);
-		}
+	/** Returns ExecutionContext.Builder with given {@link SessionContext} session context. */
+	private ExecutionContext.Builder createExecutionContextBuilder(SessionContext sessionContext) {
+		return ExecutionContext.builder(
+				defaultEnvironment,
+				sessionContext,
+				this.dependencies,
+				this.flinkConfig,
+				this.clusterClientServiceLoader,
+				this.commandLineOptions,
+				this.commandLines);
 	}
 
 	@Override
 	public String openSession(SessionContext sessionContext) throws SqlExecutionException {
 		String sessionId = sessionContext.getSessionId();
-		ExecutionContext previousContext = this.contextMap.putIfAbsent(sessionId, createExecutionContext(sessionContext));
-		if (previousContext != null) {
+		if (this.contextMap.containsKey(sessionId)) {
 			throw new SqlExecutionException("Found another session with the same session identifier: " + sessionId);
+		} else {
+			this.contextMap.put(
+					sessionId,
+					createExecutionContextBuilder(sessionContext).build());
 		}
 		return sessionId;
 	}
@@ -273,7 +261,13 @@ public Map<String, String> getSessionProperties(String sessionId) throws SqlExec
 	public void resetSessionProperties(String sessionId) throws SqlExecutionException {
 		ExecutionContext<?> context = getExecutionContext(sessionId);
 		// Renew the ExecutionContext by merging the default environment with original session context.
-		this.contextMap.put(sessionId, createExecutionContext(context.getOriginalSessionContext()));
+		// Book keep all the session states of current ExecutionContext then
+		// re-register them into the new one.
+		ExecutionContext<?> newContext = createExecutionContextBuilder(
+				context.getOriginalSessionContext())
+				.sessionState(context.getSessionState())
+				.build();
+		this.contextMap.put(sessionId, newContext);
 	}
 
 	@Override
@@ -282,7 +276,14 @@ public void setSessionProperty(String sessionId, String key, String value) throw
 		Environment env = context.getEnvironment();
 		Environment newEnv = Environment.enrich(env, ImmutableMap.of(key, value), ImmutableMap.of());
 		// Renew the ExecutionContext by new environment.
-		this.contextMap.put(sessionId, createExecutionContext(newEnv, context.getOriginalSessionContext()));
+		// Book keep all the session states of current ExecutionContext then
+		// re-register them into the new one.
+		ExecutionContext<?> newContext = createExecutionContextBuilder(
+				context.getOriginalSessionContext())
+				.env(newEnv)
+				.sessionState(context.getSessionState())
+				.build();
+		this.contextMap.put(sessionId, newContext);
 	}
 
 	@Override
@@ -307,7 +308,10 @@ public void removeView(String sessionId, String name) throws SqlExecutionExcepti
 		Environment newEnv = env.clone();
 		if (newEnv.getTables().remove(name) != null) {
 			// Renew the ExecutionContext.
-			this.contextMap.put(sessionId, createExecutionContext(newEnv, context.getOriginalSessionContext()));
+			this.contextMap.put(
+					sessionId,
+					createExecutionContextBuilder(context.getOriginalSessionContext())
+							.env(newEnv).build());
 		}
 	}
 
@@ -337,6 +341,28 @@ public List<String> listDatabases(String sessionId) throws SqlExecutionException
 		return context.wrapClassLoader(() -> Arrays.asList(tableEnv.listDatabases()));
 	}
 
+	@Override
+	public void createTable(String sessionId, String ddl) throws SqlExecutionException {
+		final ExecutionContext<?> context = getExecutionContext(sessionId);
+		final TableEnvironment tEnv = context.getTableEnvironment();
+		try {
+			tEnv.sqlUpdate(ddl);
+		} catch (Exception e) {
+			throw new SqlExecutionException("Could not create a table from statement: " + ddl, e);
+		}
+	}
+
+	@Override
+	public void dropTable(String sessionId, String ddl) throws SqlExecutionException {
+		final ExecutionContext<?> context = getExecutionContext(sessionId);
+		final TableEnvironment tEnv = context.getTableEnvironment();
+		try {
+			tEnv.sqlUpdate(ddl);
+		} catch (Exception e) {
+			throw new SqlExecutionException("Could not drop table from statement: " + ddl, e);
+		}
+	}
+
 	@Override
 	public List<String> listTables(String sessionId) throws SqlExecutionException {
 		final ExecutionContext<?> context = getExecutionContext(sessionId);
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliClientTest.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliClientTest.java
index 9cf9bbea9dbc2..d4fc533bc955d 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliClientTest.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliClientTest.java
@@ -271,6 +271,16 @@ public List<String> listDatabases(String sessionId) throws SqlExecutionException
 			return null;
 		}
 
+		@Override
+		public void createTable(String sessionId, String ddl) throws SqlExecutionException {
+
+		}
+
+		@Override
+		public void dropTable(String sessionId, String ddl) throws SqlExecutionException {
+
+		}
+
 		@Override
 		public List<String> listTables(String sessionId) throws SqlExecutionException {
 			return null;
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliResultViewTest.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliResultViewTest.java
index c4114e780316a..29ac7a1bb038d 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliResultViewTest.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/CliResultViewTest.java
@@ -173,6 +173,16 @@ public List<String> listDatabases(String sessionId) throws SqlExecutionException
 			return null;
 		}
 
+		@Override
+		public void createTable(String sessionId, String ddl) throws SqlExecutionException {
+
+		}
+
+		@Override
+		public void dropTable(String sessionId, String ddl) throws SqlExecutionException {
+
+		}
+
 		@Override
 		public List<String> listTables(String sessionId) throws SqlExecutionException {
 			return null;
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java
index 476f0b0d7f9f3..b5bad48a35dee 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/cli/SqlCommandParserTest.java
@@ -88,6 +88,36 @@ public void testCommands() {
 		testValidSqlCommand("alter table cat1.db1.tb1 set ('k1'='v1', 'k2'='v2')",
 				new SqlCommandCall(SqlCommand.ALTER_TABLE,
 						new String[]{"alter table cat1.db1.tb1 set ('k1'='v1', 'k2'='v2')"}));
+		// Test create table.
+		testInvalidSqlCommand("CREATE tables");
+		testInvalidSqlCommand("CREATE   tables");
+		testValidSqlCommand("create Table hello", new SqlCommandCall(SqlCommand.CREATE_TABLE, new String[]{"create Table hello"}));
+		testValidSqlCommand("create Table hello(a int)", new SqlCommandCall(SqlCommand.CREATE_TABLE, new String[]{"create Table hello(a int)"}));
+		testValidSqlCommand("  CREATE TABLE hello(a int)", new SqlCommandCall(SqlCommand.CREATE_TABLE, new String[]{"CREATE TABLE hello(a int)"}));
+		testValidSqlCommand("CREATE TABLE T(\n"
+						+ "  a int,\n"
+						+ "  b varchar(20),\n"
+						+ "  c as my_udf(b),\n"
+						+ "  watermark for b as my_udf(b, 1) - INTERVAL '5' second\n"
+						+ ") WITH (\n"
+						+ "  'k1' = 'v1',\n"
+						+ "  'k2' = 'v2')\n",
+				new SqlCommandCall(SqlCommand.CREATE_TABLE, new String[] {"CREATE TABLE T(\n"
+						+ "  a int,\n"
+						+ "  b varchar(20),\n"
+						+ "  c as my_udf(b),\n"
+						+ "  watermark for b as my_udf(b, 1) - INTERVAL '5' second\n"
+						+ ") WITH (\n"
+						+ "  'k1' = 'v1',\n"
+						+ "  'k2' = 'v2')"}));
+		// Test drop table.
+		testInvalidSqlCommand("DROP table");
+		testInvalidSqlCommand("DROP tables");
+		testInvalidSqlCommand("DROP   tables");
+		testValidSqlCommand("DROP TABLE t1", new SqlCommandCall(SqlCommand.DROP_TABLE, new String[]{"DROP TABLE t1"}));
+		testValidSqlCommand("DROP TABLE IF EXISTS t1", new SqlCommandCall(SqlCommand.DROP_TABLE, new String[]{"DROP TABLE IF EXISTS t1"}));
+		testValidSqlCommand("DROP TABLE IF EXISTS catalog1.db1.t1", new SqlCommandCall(SqlCommand.DROP_TABLE, new String[]{"DROP TABLE IF EXISTS catalog1.db1.t1"}));
+		testValidSqlCommand("DROP TABLE IF EXISTS db1.t1", new SqlCommandCall(SqlCommand.DROP_TABLE, new String[]{"DROP TABLE IF EXISTS db1.t1"}));
 	}
 
 	private void testInvalidSqlCommand(String stmt) {
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/ExecutionContextTest.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/ExecutionContextTest.java
index 156f1adcab3e3..19517bb96e0bb 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/ExecutionContextTest.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/ExecutionContextTest.java
@@ -21,6 +21,7 @@
 import org.apache.flink.api.common.ExecutionConfig;
 import org.apache.flink.api.common.restartstrategy.RestartStrategies;
 import org.apache.flink.client.cli.DefaultCLI;
+import org.apache.flink.client.deployment.DefaultClusterClientServiceLoader;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.table.api.TableEnvironment;
 import org.apache.flink.table.api.config.ExecutionConfigOptions;
@@ -243,18 +244,21 @@ public void testConfiguration() throws Exception {
 				OptimizerConfigOptions.TABLE_OPTIMIZER_BROADCAST_JOIN_THRESHOLD));
 	}
 
+	@SuppressWarnings("unchecked")
 	private <T> ExecutionContext<T> createExecutionContext(String file, Map<String, String> replaceVars) throws Exception {
 		final Environment env = EnvironmentFileUtil.parseModified(
 			file,
 			replaceVars);
 		final Configuration flinkConfig = new Configuration();
-		return new ExecutionContext<>(
-			env,
-			new SessionContext("test-session", new Environment()),
-			Collections.emptyList(),
-			flinkConfig,
-			new Options(),
-			Collections.singletonList(new DefaultCLI(flinkConfig)));
+		return (ExecutionContext<T>) ExecutionContext.builder(
+				env,
+				new SessionContext("test-session", new Environment()),
+				Collections.emptyList(),
+				flinkConfig,
+				new DefaultClusterClientServiceLoader(),
+				new Options(),
+				Collections.singletonList(new DefaultCLI(flinkConfig)))
+				.build();
 	}
 
 	private <T> ExecutionContext<T> createDefaultExecutionContext() throws Exception {
diff --git a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
index 91af6e959271c..631017094e29c 100644
--- a/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
+++ b/flink-table/flink-sql-client/src/test/java/org/apache/flink/table/client/gateway/local/LocalExecutorITCase.java
@@ -54,6 +54,7 @@
 import org.junit.Assume;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
+import org.junit.Ignore;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.ExpectedException;
@@ -615,7 +616,7 @@ public void testUseCatalogAndUseDatabase() throws Exception {
 		assertEquals("test-session", sessionId);
 
 		try {
-			assertEquals(Arrays.asList("mydatabase"), executor.listDatabases(sessionId));
+			assertEquals(Collections.singletonList("mydatabase"), executor.listDatabases(sessionId));
 
 			executor.useCatalog(sessionId, "hivecatalog");
 
@@ -628,7 +629,7 @@ public void testUseCatalogAndUseDatabase() throws Exception {
 
 			executor.useDatabase(sessionId, DependencyTest.TestHiveCatalogFactory.ADDITIONAL_TEST_DATABASE);
 
-			assertEquals(Arrays.asList(DependencyTest.TestHiveCatalogFactory.TEST_TABLE), executor.listTables(sessionId));
+			assertEquals(Collections.singletonList(DependencyTest.TestHiveCatalogFactory.TEST_TABLE), executor.listTables(sessionId));
 		} finally {
 			executor.closeSession(sessionId);
 		}
@@ -691,6 +692,221 @@ public void testParameterizedTypes() throws Exception {
 		executor.closeSession(sessionId);
 	}
 
+	@Test
+	public void testCreateTable() throws Exception {
+		final Executor executor = createDefaultExecutor(clusterClient);
+		final SessionContext session = new SessionContext("test-session", new Environment());
+		String sessionId = executor.openSession(session);
+		final String ddlTemplate = "create table %s(\n" +
+				"  a int,\n" +
+				"  b bigint,\n" +
+				"  c varchar\n" +
+				") with (\n" +
+				"  'connector.type'='filesystem',\n" +
+				"  'format.type'='csv',\n" +
+				"  'connector.path'='xxx'\n" +
+				")\n";
+		try {
+			// Test create table with simple name.
+			executor.useCatalog(sessionId, "catalog1");
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+
+			// Test create table with full qualified name.
+			executor.useCatalog(sessionId, "catalog1");
+			executor.createTable(sessionId, String.format(ddlTemplate, "`simple-catalog`.`default_database`.MyTable3"));
+			executor.createTable(sessionId, String.format(ddlTemplate, "`simple-catalog`.`default_database`.MyTable4"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+			executor.useCatalog(sessionId, "simple-catalog");
+			assertEquals(Arrays.asList("MyTable3", "MyTable4", "test-table"), executor.listTables(sessionId));
+
+			// Test create table with db and table name.
+			executor.useCatalog(sessionId, "catalog1");
+			executor.createTable(sessionId, String.format(ddlTemplate, "`default`.MyTable5"));
+			executor.createTable(sessionId, String.format(ddlTemplate, "`default`.MyTable6"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2", "MyTable5", "MyTable6"), executor.listTables(sessionId));
+		} finally {
+			executor.closeSession(sessionId);
+		}
+	}
+
+	@Test @Ignore // TODO: reopen when FLINK-15075 was fixed.
+	public void testCreateTableWithComputedColumn() throws Exception {
+		Assume.assumeTrue(planner.equals("blink"));
+		final Map<String, String> replaceVars = new HashMap<>();
+		replaceVars.put("$VAR_PLANNER", planner);
+		replaceVars.put("$VAR_SOURCE_PATH1", "file:///fakePath1");
+		replaceVars.put("$VAR_SOURCE_PATH2", "file:///fakePath2");
+		replaceVars.put("$VAR_EXECUTION_TYPE", "batch");
+		replaceVars.put("$VAR_UPDATE_MODE", "update-mode: append");
+		replaceVars.put("$VAR_MAX_ROWS", "100");
+		replaceVars.put("$VAR_RESULT_MODE", "table");
+		final Executor executor = createModifiedExecutor(clusterClient, replaceVars);
+		final String ddlTemplate = "create table %s(\n" +
+				"  a int,\n" +
+				"  b bigint,\n" +
+				"  c as a + 1\n" +
+				") with (\n" +
+				"  'connector.type'='filesystem',\n" +
+				"  'format.type'='csv',\n" +
+				"  'connector.path'='xxx'\n" +
+				")\n";
+		final SessionContext session = new SessionContext("test-session", new Environment());
+		String sessionId = executor.openSession(session);
+		try {
+			executor.useCatalog(sessionId, "catalog1");
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+		} finally {
+			executor.closeSession(sessionId);
+		}
+	}
+
+	@Test @Ignore // TODO: reopen when FLINK-15075 was fixed.
+	public void testCreateTableWithWatermark() throws Exception {
+		final Map<String, String> replaceVars = new HashMap<>();
+		replaceVars.put("$VAR_PLANNER", planner);
+		replaceVars.put("$VAR_SOURCE_PATH1", "file:///fakePath1");
+		replaceVars.put("$VAR_SOURCE_PATH2", "file:///fakePath2");
+		replaceVars.put("$VAR_EXECUTION_TYPE", "batch");
+		replaceVars.put("$VAR_UPDATE_MODE", "update-mode: append");
+		replaceVars.put("$VAR_MAX_ROWS", "100");
+		replaceVars.put("$VAR_RESULT_MODE", "table");
+		final Executor executor = createModifiedExecutor(clusterClient, replaceVars);
+		final String ddlTemplate = "create table %s(\n" +
+				"  a int,\n" +
+				"  b timestamp(3),\n" +
+				"  watermark for b as b - INTERVAL '5' second\n" +
+				") with (\n" +
+				"  'connector.type'='filesystem',\n" +
+				"  'format.type'='csv',\n" +
+				"  'connector.path'='xxx'\n" +
+				")\n";
+		final SessionContext session = new SessionContext("test-session", new Environment());
+		String sessionId = executor.openSession(session);
+		try {
+			executor.useCatalog(sessionId, "catalog1");
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+		} finally {
+			executor.closeSession(sessionId);
+		}
+	}
+
+	@Test
+	public void testCreateTableWithMultiSession() throws Exception {
+		final Executor executor = createDefaultExecutor(clusterClient);
+		final SessionContext session = new SessionContext("test-session", new Environment());
+		String sessionId = executor.openSession(session);
+		try {
+			executor.useCatalog(sessionId, "catalog1");
+			executor.setSessionProperty(sessionId, "execution.type", "batch");
+			final String ddlTemplate = "create table %s(\n" +
+					"  a int,\n" +
+					"  b bigint,\n" +
+					"  c varchar\n" +
+					") with (\n" +
+					"  'connector.type'='filesystem',\n" +
+					"  'format.type'='csv',\n" +
+					"  'connector.path'='xxx',\n" +
+					"  'update-mode'='append'\n" +
+					")\n";
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			// Change the session property to trigger `new ExecutionContext`.
+			executor.setSessionProperty(sessionId, "execution.restart-strategy.failure-rate-interval", "12345");
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+
+			// Reset the session properties.
+			executor.resetSessionProperties(sessionId);
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable3"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2", "MyTable3"), executor.listTables(sessionId));
+		} finally {
+			executor.closeSession(sessionId);
+		}
+	}
+
+	@Test
+	public void testDropTable() throws Exception {
+		final Executor executor = createDefaultExecutor(clusterClient);
+		final SessionContext session = new SessionContext("test-session", new Environment());
+		String sessionId = executor.openSession(session);
+		try {
+			executor.useCatalog(sessionId, "catalog1");
+			executor.setSessionProperty(sessionId, "execution.type", "batch");
+			final String ddlTemplate = "create table %s(\n" +
+					"  a int,\n" +
+					"  b bigint,\n" +
+					"  c varchar\n" +
+					") with (\n" +
+					"  'connector.type'='filesystem',\n" +
+					"  'format.type'='csv',\n" +
+					"  'connector.path'='xxx',\n" +
+					"  'update-mode'='append'\n" +
+					")\n";
+			// Test drop table.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE MyTable1");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+
+			// Test drop table if exists.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE IF EXISTS MyTable1");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+
+			// Test drop table with full qualified name.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE catalog1.`default`.MyTable1");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+
+			// Test drop table with db and table name.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE `default`.MyTable1");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+
+			// Test drop table that does not exist.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE IF EXISTS catalog2.`default`.MyTable1");
+			assertEquals(Collections.singletonList("MyTable1"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE `default`.MyTable1");
+
+			// Test drop table with properties changed.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			// Change the session property to trigger `new ExecutionContext`.
+			executor.setSessionProperty(sessionId, "execution.restart-strategy.failure-rate-interval", "12345");
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE MyTable1");
+			executor.dropTable(sessionId, "DROP TABLE MyTable2");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+
+			// Test drop table with properties reset.
+			// Reset the session properties.
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable1"));
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable2"));
+			executor.resetSessionProperties(sessionId);
+			executor.createTable(sessionId, String.format(ddlTemplate, "MyTable3"));
+			assertEquals(Arrays.asList("MyTable1", "MyTable2", "MyTable3"), executor.listTables(sessionId));
+			executor.dropTable(sessionId, "DROP TABLE MyTable1");
+			executor.dropTable(sessionId, "DROP TABLE MyTable2");
+			executor.dropTable(sessionId, "DROP TABLE MyTable3");
+			assertEquals(Collections.emptyList(), executor.listTables(sessionId));
+		} finally {
+			executor.closeSession(sessionId);
+		}
+	}
+
 	private void executeStreamQueryTable(
 			Map<String, String> replaceVars,
 			String query,
