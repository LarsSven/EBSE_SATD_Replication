diff --git a/flink-python/pom.xml b/flink-python/pom.xml
index ed16daa09e3a8..3c515efee3559 100644
--- a/flink-python/pom.xml
+++ b/flink-python/pom.xml
@@ -147,6 +147,11 @@ under the License.
 			<scope>test</scope>
 		</dependency>
 
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-test-utils-junit</artifactId>
+		</dependency>
+
 	</dependencies>
 
 	<dependencyManagement>
diff --git a/flink-python/pyflink/fn_execution/coder_impl.py b/flink-python/pyflink/fn_execution/coder_impl.py
index 6e7b21c99c83f..712308505f29c 100644
--- a/flink-python/pyflink/fn_execution/coder_impl.py
+++ b/flink-python/pyflink/fn_execution/coder_impl.py
@@ -16,6 +16,8 @@
 # limitations under the License.
 ################################################################################
 
+import datetime
+import struct
 from apache_beam.coders.coder_impl import StreamCoderImpl
 
 
@@ -27,7 +29,8 @@ def __init__(self, field_coders):
     def encode_to_stream(self, value, out_stream, nested):
         self.write_null_mask(value, out_stream)
         for i in range(len(self._field_coders)):
-            self._field_coders[i].encode_to_stream(value[i], out_stream, nested)
+            if value[i] is not None:
+                self._field_coders[i].encode_to_stream(value[i], out_stream, nested)
 
     def decode_from_stream(self, in_stream, nested):
         from pyflink.table import Row
@@ -74,3 +77,103 @@ def read_null_mask(field_count, in_stream):
 
     def __repr__(self):
         return 'RowCoderImpl[%s]' % ', '.join(str(c) for c in self._field_coders)
+
+
+class BigIntCoderImpl(StreamCoderImpl):
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_int64(value)
+
+    def decode_from_stream(self, in_stream, nested):
+        return in_stream.read_bigendian_int64()
+
+
+class TinyIntCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_byte(value)
+
+    def decode_from_stream(self, in_stream, nested):
+        return int(in_stream.read_byte())
+
+
+class SmallIntImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write(struct.pack('>h', value))
+
+    def decode_from_stream(self, in_stream, nested):
+        return struct.unpack('>h', in_stream.read(2))[0]
+
+
+class IntCoderImpl(StreamCoderImpl):
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_int32(value)
+
+    def decode_from_stream(self, in_stream, nested):
+        return in_stream.read_bigendian_int32()
+
+
+class BooleanCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_byte(value)
+
+    def decode_from_stream(self, in_stream, nested):
+        return not not in_stream.read_byte()
+
+
+class FloatCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write(struct.pack('>f', value))
+
+    def decode_from_stream(self, in_stream, nested):
+        return struct.unpack('>f', in_stream.read(4))[0]
+
+
+class DoubleCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_double(value)
+
+    def decode_from_stream(self, in_stream, nested):
+        return in_stream.read_bigendian_double()
+
+
+class BinaryCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_int32(len(value))
+        out_stream.write(value, False)
+
+    def decode_from_stream(self, in_stream, nested):
+        size = in_stream.read_bigendian_int32()
+        return in_stream.read(size)
+
+
+class CharCoderImpl(StreamCoderImpl):
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_int32(len(value))
+        out_stream.write(value.encode("utf-8"), False)
+
+    def decode_from_stream(self, in_stream, nested):
+        size = in_stream.read_bigendian_int32()
+        return in_stream.read(size).decode("utf-8")
+
+
+class DateCoderImpl(StreamCoderImpl):
+    EPOCH_ORDINAL = datetime.datetime(1970, 1, 1).toordinal()
+
+    def encode_to_stream(self, value, out_stream, nested):
+        out_stream.write_bigendian_int32(self.date_to_internal(value))
+
+    def decode_from_stream(self, in_stream, nested):
+        value = in_stream.read_bigendian_int32()
+        return self.internal_to_date(value)
+
+    def date_to_internal(self, d):
+        return d.toordinal() - self.EPOCH_ORDINAL
+
+    def internal_to_date(self, v):
+        return datetime.date.fromordinal(v + self.EPOCH_ORDINAL)
diff --git a/flink-python/pyflink/fn_execution/coders.py b/flink-python/pyflink/fn_execution/coders.py
index 219aa1fdcbccf..f98561a3bb635 100644
--- a/flink-python/pyflink/fn_execution/coders.py
+++ b/flink-python/pyflink/fn_execution/coders.py
@@ -16,7 +16,11 @@
 # limitations under the License.
 ################################################################################
 
-from apache_beam.coders import Coder, VarIntCoder
+from abc import ABC
+
+
+import datetime
+from apache_beam.coders import Coder
 from apache_beam.coders.coders import FastCoder
 
 from pyflink.fn_execution import coder_impl
@@ -25,7 +29,9 @@
 FLINK_SCHEMA_CODER_URN = "flink:coder:schema:v1"
 
 
-__all__ = ['RowCoder']
+__all__ = ['RowCoder', 'BigIntCoder', 'TinyIntCoder', 'BooleanCoder',
+           'SmallIntCoder', 'IntCoder', 'FloatCoder', 'DoubleCoder',
+           'BinaryCoder', 'CharCoder', 'DateCoder']
 
 
 class RowCoder(FastCoder):
@@ -62,11 +68,156 @@ def __hash__(self):
         return hash(self._field_coders)
 
 
+class DeterministicCoder(FastCoder, ABC):
+    """
+    Base Coder for all deterministic Coders.
+    """
+
+    def is_deterministic(self):
+        return True
+
+
+class BigIntCoder(DeterministicCoder):
+    """
+    Coder for 8 bytes long.
+    """
+
+    def _create_impl(self):
+        return coder_impl.BigIntCoderImpl()
+
+    def to_type_hint(self):
+        return int
+
+
+class TinyIntCoder(DeterministicCoder):
+    """
+    Coder for Byte.
+    """
+
+    def _create_impl(self):
+        return coder_impl.TinyIntCoderImpl()
+
+    def to_type_hint(self):
+        return int
+
+
+class BooleanCoder(DeterministicCoder):
+    """
+    Coder for Boolean.
+    """
+
+    def _create_impl(self):
+        return coder_impl.BooleanCoderImpl()
+
+    def to_type_hint(self):
+        return bool
+
+
+class SmallIntCoder(DeterministicCoder):
+    """
+    Coder for Short.
+    """
+
+    def _create_impl(self):
+        return coder_impl.SmallIntImpl()
+
+    def to_type_hint(self):
+        return int
+
+
+class IntCoder(DeterministicCoder):
+    """
+    Coder for 4 bytes int.
+    """
+
+    def _create_impl(self):
+        return coder_impl.IntCoderImpl()
+
+    def to_type_hint(self):
+        return int
+
+
+class FloatCoder(DeterministicCoder):
+    """
+    Coder for Float.
+    """
+
+    def _create_impl(self):
+        return coder_impl.FloatCoderImpl()
+
+    def to_type_hint(self):
+        return float
+
+
+class DoubleCoder(DeterministicCoder):
+    """
+    Coder for Double.
+    """
+
+    def _create_impl(self):
+        return coder_impl.DoubleCoderImpl()
+
+    def to_type_hint(self):
+        return float
+
+
+class BinaryCoder(DeterministicCoder):
+    """
+    Coder for Byte Array.
+    """
+
+    def _create_impl(self):
+        return coder_impl.BinaryCoderImpl()
+
+    def to_type_hint(self):
+        return bytes
+
+
+class CharCoder(DeterministicCoder):
+    """
+    Coder for Character String.
+    """
+    def _create_impl(self):
+        return coder_impl.CharCoderImpl()
+
+    def to_type_hint(self):
+        return str
+
+
+class DateCoder(DeterministicCoder):
+    """
+    Coder for Date
+    """
+
+    def _create_impl(self):
+        return coder_impl.DateCoderImpl()
+
+    def to_type_hint(self):
+        return datetime.date
+
+
 @Coder.register_urn(FLINK_SCHEMA_CODER_URN, flink_fn_execution_pb2.Schema)
 def _pickle_from_runner_api_parameter(schema_proto, unused_components, unused_context):
     return RowCoder([from_proto(f.type) for f in schema_proto.fields])
 
 
+type_name = flink_fn_execution_pb2.Schema.TypeName
+_type_name_mappings = {
+    type_name.TINYINT: TinyIntCoder(),
+    type_name.SMALLINT: SmallIntCoder(),
+    type_name.INT: IntCoder(),
+    type_name.BIGINT: BigIntCoder(),
+    type_name.BOOLEAN: BooleanCoder(),
+    type_name.FLOAT: FloatCoder(),
+    type_name.DOUBLE: DoubleCoder(),
+    type_name.BINARY: BinaryCoder(),
+    type_name.VARBINARY: BinaryCoder(),
+    type_name.CHAR: CharCoder(),
+    type_name.VARCHAR: CharCoder(),
+    type_name.DATE: DateCoder(),
+}
+
+
 def from_proto(field_type):
     """
     Creates the corresponding :class:`Coder` given the protocol representation of the field type.
@@ -74,9 +225,11 @@ def from_proto(field_type):
     :param field_type: the protocol representation of the field type
     :return: :class:`Coder`
     """
-    if field_type.type_name == flink_fn_execution_pb2.Schema.TypeName.BIGINT:
-        return VarIntCoder()
-    elif field_type.type_name == flink_fn_execution_pb2.Schema.TypeName.ROW:
+    field_type_name = field_type.type_name
+    coder = _type_name_mappings.get(field_type_name)
+    if coder is not None:
+        return coder
+    if field_type_name == type_name.ROW:
         return RowCoder([from_proto(f.type) for f in field_type.row_schema.fields])
     else:
         raise ValueError("field_type %s is not supported." % field_type)
diff --git a/flink-python/pyflink/fn_execution/tests/coders_test_common.py b/flink-python/pyflink/fn_execution/tests/coders_test_common.py
new file mode 100644
index 0000000000000..6af26bf30921a
--- /dev/null
+++ b/flink-python/pyflink/fn_execution/tests/coders_test_common.py
@@ -0,0 +1,86 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+"""Tests common to all coder implementations."""
+import logging
+import unittest
+
+from pyflink.fn_execution.coders import BigIntCoder, TinyIntCoder, BooleanCoder, \
+    SmallIntCoder, IntCoder, FloatCoder, DoubleCoder, BinaryCoder, CharCoder, DateCoder
+
+
+class CodersTest(unittest.TestCase):
+
+    def check_coder(self, coder, *values):
+        for v in values:
+            if isinstance(v, float):
+                from pyflink.table.tests.test_udf import float_equal
+                assert float_equal(v, coder.decode(coder.encode(v)), 1e-6)
+            else:
+                self.assertEqual(v, coder.decode(coder.encode(v)))
+
+    # decide whether two floats are equal
+    @staticmethod
+    def float_equal(a, b, rel_tol=1e-09, abs_tol=0.0):
+        return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
+
+    def test_bigint_coder(self):
+        coder = BigIntCoder()
+        self.check_coder(coder, 1, 100, -100, -1000)
+
+    def test_tinyint_coder(self):
+        coder = TinyIntCoder()
+        self.check_coder(coder, 1, 10, 127)
+
+    def test_boolean_coder(self):
+        coder = BooleanCoder()
+        self.check_coder(coder, True, False)
+
+    def test_smallint_coder(self):
+        coder = SmallIntCoder()
+        self.check_coder(coder, 32767, -32768, 0)
+
+    def test_int_coder(self):
+        coder = IntCoder()
+        self.check_coder(coder, -2147483648, 2147483647)
+
+    def test_float_coder(self):
+        coder = FloatCoder()
+        self.check_coder(coder, 1.02, 1.32)
+
+    def test_double_coder(self):
+        coder = DoubleCoder()
+        self.check_coder(coder, -12.02, 1.98932)
+
+    def test_binary_coder(self):
+        coder = BinaryCoder()
+        self.check_coder(coder, b'pyflink')
+
+    def test_char_coder(self):
+        coder = CharCoder()
+        self.check_coder(coder, 'flink')
+
+    def test_date_coder(self):
+        import datetime
+        coder = DateCoder()
+        self.check_coder(coder, datetime.date(2019, 9, 10))
+
+
+if __name__ == '__main__':
+    logging.getLogger().setLevel(logging.INFO)
+    unittest.main()
diff --git a/flink-python/pyflink/table/tests/test_udf.py b/flink-python/pyflink/table/tests/test_udf.py
index 12f4e8c966422..a433c904f4fa3 100644
--- a/flink-python/pyflink/table/tests/test_udf.py
+++ b/flink-python/pyflink/table/tests/test_udf.py
@@ -120,9 +120,6 @@ def udf_with_constant_params(p, null_param, tinyint_param, smallint_param, int_p
                                      bigint_param, decimal_param, float_param, double_param,
                                      boolean_param, str_param,
                                      date_param, time_param, timestamp_param):
-            # decide whether two floats are equal
-            def float_equal(a, b, rel_tol=1e-09, abs_tol=0.0):
-                return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
 
             from decimal import Decimal
             import datetime
@@ -329,6 +326,140 @@ def test_udf_without_arguments(self):
         actual = source_sink_utils.results()
         self.assert_equals(actual, ["1,2", "1,2", "1,2"])
 
+    def test_all_data_types(self):
+        def boolean_func(bool_param):
+            assert isinstance(bool_param, bool), 'bool_param of wrong type %s !' \
+                                                 % type(bool_param)
+            return bool_param
+
+        def tinyint_func(tinyint_param):
+            assert isinstance(tinyint_param, int), 'tinyint_param of wrong type %s !' \
+                                                   % type(tinyint_param)
+            return tinyint_param
+
+        def smallint_func(smallint_param):
+            assert isinstance(smallint_param, int), 'smallint_param of wrong type %s !' \
+                                                    % type(smallint_param)
+            assert smallint_param == 32767, 'smallint_param of wrong value %s' % smallint_param
+            return smallint_param
+
+        def int_func(int_param):
+            assert isinstance(int_param, int), 'int_param of wrong type %s !' \
+                                               % type(int_param)
+            assert int_param == -2147483648, 'int_param of wrong value %s' % int_param
+            return int_param
+
+        def bigint_func(bigint_param):
+            assert isinstance(bigint_param, int), 'bigint_param of wrong type %s !' \
+                                                  % type(bigint_param)
+            return bigint_param
+
+        def bigint_func_none(bigint_param):
+            assert bigint_param is None, 'bigint_param %s should be None!' % bigint_param
+            return bigint_param
+
+        def float_func(float_param):
+            assert isinstance(float_param, float) and float_equal(float_param, 1.23, 1e-6), \
+                'float_param is wrong value %s !' % float_param
+            return float_param
+
+        def double_func(double_param):
+            assert isinstance(double_param, float) and float_equal(double_param, 1.98932, 1e-7), \
+                'double_param is wrong value %s !' % double_param
+            return double_param
+
+        def bytes_func(bytes_param):
+            assert bytes_param == b'flink', \
+                'bytes_param is wrong value %s !' % bytes_param
+            return bytes_param
+
+        def str_func(str_param):
+            assert str_param == 'pyflink', \
+                'str_param is wrong value %s !' % str_param
+            return str_param
+
+        def date_func(date_param):
+            from datetime import date
+            assert date_param == date(year=2014, month=9, day=13), \
+                'date_param is wrong value %s !' % date_param
+            return date_param
+
+        self.t_env.register_function(
+            "boolean_func", udf(boolean_func, [DataTypes.BOOLEAN()], DataTypes.BOOLEAN()))
+
+        self.t_env.register_function(
+            "tinyint_func", udf(tinyint_func, [DataTypes.TINYINT()], DataTypes.TINYINT()))
+
+        self.t_env.register_function(
+            "smallint_func", udf(smallint_func, [DataTypes.SMALLINT()], DataTypes.SMALLINT()))
+
+        self.t_env.register_function(
+            "int_func", udf(int_func, [DataTypes.INT()], DataTypes.INT()))
+
+        self.t_env.register_function(
+            "bigint_func", udf(bigint_func, [DataTypes.BIGINT()], DataTypes.BIGINT()))
+
+        self.t_env.register_function(
+            "bigint_func_none", udf(bigint_func_none, [DataTypes.BIGINT()], DataTypes.BIGINT()))
+
+        self.t_env.register_function(
+            "float_func", udf(float_func, [DataTypes.FLOAT()], DataTypes.FLOAT()))
+
+        self.t_env.register_function(
+            "double_func", udf(double_func, [DataTypes.DOUBLE()], DataTypes.DOUBLE()))
+
+        self.t_env.register_function(
+            "bytes_func", udf(bytes_func, [DataTypes.BYTES()], DataTypes.BYTES()))
+
+        self.t_env.register_function(
+            "str_func", udf(str_func, [DataTypes.STRING()], DataTypes.STRING()))
+
+        self.t_env.register_function(
+            "date_func", udf(date_func, [DataTypes.DATE()], DataTypes.DATE()))
+
+        table_sink = source_sink_utils.TestAppendSink(
+            ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k'],
+            [DataTypes.BIGINT(), DataTypes.BIGINT(), DataTypes.TINYINT(),
+             DataTypes.BOOLEAN(), DataTypes.SMALLINT(), DataTypes.INT(),
+             DataTypes.FLOAT(), DataTypes.DOUBLE(), DataTypes.BYTES(),
+             DataTypes.STRING(), DataTypes.DATE()])
+        self.t_env.register_table_sink("Results", table_sink)
+
+        import datetime
+        t = self.t_env.from_elements(
+            [(1, None, 1, True, 32767, -2147483648, 1.23, 1.98932,
+              bytearray(b'flink'), 'pyflink', datetime.date(2014, 9, 13))],
+            DataTypes.ROW(
+                [DataTypes.FIELD("a", DataTypes.BIGINT()),
+                 DataTypes.FIELD("b", DataTypes.BIGINT()),
+                 DataTypes.FIELD("c", DataTypes.TINYINT()),
+                 DataTypes.FIELD("d", DataTypes.BOOLEAN()),
+                 DataTypes.FIELD("e", DataTypes.SMALLINT()),
+                 DataTypes.FIELD("f", DataTypes.INT()),
+                 DataTypes.FIELD("g", DataTypes.FLOAT()),
+                 DataTypes.FIELD("h", DataTypes.DOUBLE()),
+                 DataTypes.FIELD("i", DataTypes.BYTES()),
+                 DataTypes.FIELD("j", DataTypes.STRING()),
+                 DataTypes.FIELD("k", DataTypes.DATE())]))
+
+        t.select("bigint_func(a), bigint_func_none(b),"
+                 "tinyint_func(c), boolean_func(d),"
+                 "smallint_func(e),int_func(f),"
+                 "float_func(g),double_func(h),"
+                 "bytes_func(i),str_func(j),"
+                 "date_func(k)") \
+            .insert_into("Results")
+        self.t_env.execute("test")
+        actual = source_sink_utils.results()
+        self.assert_equals(actual,
+                           ["1,null,1,true,32767,-2147483648,1.23,1.98932,"
+                            "[102, 108, 105, 110, 107],pyflink,2014-09-13"])
+
+
+# decide whether two floats are equal
+def float_equal(a, b, rel_tol=1e-09, abs_tol=0.0):
+    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
+
 
 class PyFlinkStreamUserDefinedFunctionTests(UserDefinedFunctionTests,
                                             PyFlinkStreamTableTestCase):
diff --git a/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
index 53e6ef94fb167..9cd343dfa739d 100644
--- a/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/python/AbstractPythonFunctionRunner.java
@@ -20,9 +20,12 @@
 
 import org.apache.flink.annotation.Internal;
 import org.apache.flink.annotation.VisibleForTesting;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.core.memory.ByteArrayInputStreamWithPos;
 import org.apache.flink.core.memory.ByteArrayOutputStreamWithPos;
+import org.apache.flink.core.memory.DataInputViewStreamWrapper;
+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.util.Preconditions;
 import org.apache.flink.util.StringUtils;
@@ -39,7 +42,6 @@
 import org.apache.beam.runners.fnexecution.control.StageBundleFactory;
 import org.apache.beam.runners.fnexecution.provisioning.JobInfo;
 import org.apache.beam.runners.fnexecution.state.StateRequestHandler;
-import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 import org.apache.beam.sdk.options.PipelineOptionsFactory;
 import org.apache.beam.sdk.options.PortablePipelineOptions;
@@ -122,25 +124,35 @@
 	private transient FnDataReceiver<WindowedValue<?>> mainInputReceiver;
 
 	/**
-	 * The coder for input elements.
+	 * The TypeSerializer for input elements.
 	 */
-	private transient Coder<IN> inputCoder;
+	private transient TypeSerializer<IN> inputTypeSerializer;
 
 	/**
-	 * The coder for execution results.
+	 * The TypeSerializer for execution results.
 	 */
-	private transient Coder<OUT> outputCoder;
+	private transient TypeSerializer<OUT> outputTypeSerializer;
 
 	/**
 	 * Reusable InputStream used to holding the execution results to be deserialized.
 	 */
 	private transient ByteArrayInputStreamWithPos bais;
 
+	/**
+	 * InputStream Wrapper.
+	 */
+	private transient DataInputViewStreamWrapper baisWrapper;
+
 	/**
 	 * Reusable OutputStream used to holding the serialized input elements.
 	 */
 	private transient ByteArrayOutputStreamWithPos baos;
 
+	/**
+	 * OutputStream Wrapper.
+	 */
+	private transient DataOutputViewStreamWrapper baosWrapper;
+
 	public AbstractPythonFunctionRunner(
 		String taskName,
 		FnDataReceiver<OUT> resultReceiver,
@@ -157,9 +169,11 @@ public AbstractPythonFunctionRunner(
 	@Override
 	public void open() throws Exception {
 		bais = new ByteArrayInputStreamWithPos();
+		baisWrapper = new DataInputViewStreamWrapper(bais);
 		baos = new ByteArrayOutputStreamWithPos();
-		inputCoder = getInputCoder();
-		outputCoder = getOutputCoder();
+		baosWrapper = new DataOutputViewStreamWrapper(baos);
+		inputTypeSerializer = getInputTypeSerializer();
+		outputTypeSerializer = getOutputTypeSerializer();
 
 		PortablePipelineOptions portableOptions =
 			PipelineOptionsFactory.as(PortablePipelineOptions.class);
@@ -202,7 +216,7 @@ public void startBundle() {
 				public FnDataReceiver<WindowedValue<byte[]>> create(String pCollectionId) {
 					return input -> {
 						bais.setBuffer(input.getValue(), 0, input.getValue().length);
-						resultReceiver.accept(outputCoder.decode(bais));
+						resultReceiver.accept(outputTypeSerializer.deserialize(baisWrapper));
 					};
 				}
 			};
@@ -233,7 +247,7 @@ public void finishBundle() {
 	public void processElement(IN element) {
 		try {
 			baos.reset();
-			inputCoder.encode(element, baos);
+			inputTypeSerializer.serialize(element, baosWrapper);
 			// TODO: support to use ValueOnlyWindowedValueCoder for better performance.
 			// Currently, FullWindowedValueCoder has to be used in Beam's portability framework.
 			mainInputReceiver.accept(WindowedValue.valueInGlobalWindow(baos.toByteArray()));
@@ -302,12 +316,12 @@ protected RunnerApi.Environment createPythonExecutionEnvironment() {
 	public abstract ExecutableStage createExecutableStage();
 
 	/**
-	 * Returns the coder for input elements.
+	 * Returns the TypeSerializer for input elements.
 	 */
-	public abstract Coder<IN> getInputCoder();
+	public abstract TypeSerializer<IN> getInputTypeSerializer();
 
 	/**
-	 * Returns the coder for execution results.
+	 * Returns the TypeSerializer for execution results.
 	 */
-	public abstract Coder<OUT> getOutputCoder();
+	public abstract TypeSerializer<OUT> getOutputTypeSerializer();
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java
index 69c178b0ad5d9..43c0c535e7a85 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/AbstractPythonScalarFunctionRunner.java
@@ -26,7 +26,7 @@
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.typeutils.BeamTypeUtils;
+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.util.Preconditions;
 
@@ -213,7 +213,7 @@ private RunnerApi.Coder getRowCoderProto(RowType rowType) {
 				RunnerApi.FunctionSpec.newBuilder()
 					.setUrn(SCHEMA_CODER_URN)
 					.setPayload(org.apache.beam.vendor.grpc.v1p21p0.com.google.protobuf.ByteString.copyFrom(
-						BeamTypeUtils.toProtoType(rowType).getRowSchema().toByteArray()))
+						PythonTypeUtils.toProtoType(rowType).getRowSchema().toByteArray()))
 					.build())
 			.build();
 	}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java
index 4bb5fc806c034..9c550d2c248c1 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/BaseRowPythonScalarFunctionRunner.java
@@ -24,10 +24,10 @@
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.typeutils.BeamTypeUtils;
+import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 
-import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 
 /**
@@ -50,13 +50,13 @@ public BaseRowPythonScalarFunctionRunner(
 
 	@Override
 	@SuppressWarnings("unchecked")
-	public Coder<BaseRow> getInputCoder() {
-		return (Coder<BaseRow>) BeamTypeUtils.toBlinkCoder(getInputType());
+	public BaseRowSerializer getInputTypeSerializer() {
+		return (BaseRowSerializer) PythonTypeUtils.toBlinkTypeSerializer(getInputType());
 	}
 
 	@Override
 	@SuppressWarnings("unchecked")
-	public Coder<BaseRow> getOutputCoder() {
-		return (Coder<BaseRow>) BeamTypeUtils.toBlinkCoder(getOutputType());
+	public BaseRowSerializer getOutputTypeSerializer() {
+		return (BaseRowSerializer) PythonTypeUtils.toBlinkTypeSerializer(getOutputType());
 	}
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java
index 8edba3a00f235..469ee71521303 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/runners/python/PythonScalarFunctionRunner.java
@@ -19,15 +19,15 @@
 package org.apache.flink.table.runtime.runners.python;
 
 import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
 import org.apache.flink.python.PythonFunctionRunner;
 import org.apache.flink.table.functions.ScalarFunction;
 import org.apache.flink.table.functions.python.PythonEnv;
 import org.apache.flink.table.functions.python.PythonFunctionInfo;
-import org.apache.flink.table.runtime.typeutils.BeamTypeUtils;
+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
 
-import org.apache.beam.sdk.coders.Coder;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 
 /**
@@ -50,13 +50,13 @@ public PythonScalarFunctionRunner(
 
 	@Override
 	@SuppressWarnings("unchecked")
-	public Coder<Row> getInputCoder() {
-		return (Coder<Row>) BeamTypeUtils.toCoder(getInputType());
+	public RowSerializer getInputTypeSerializer() {
+		return (RowSerializer) PythonTypeUtils.toFlinkTypeSerializer(getInputType());
 	}
 
 	@Override
 	@SuppressWarnings("unchecked")
-	public Coder<Row> getOutputCoder() {
-		return (Coder<Row>) BeamTypeUtils.toCoder(getOutputType());
+	public RowSerializer getOutputTypeSerializer() {
+		return (RowSerializer) PythonTypeUtils.toFlinkTypeSerializer(getOutputType());
 	}
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/BeamTypeUtils.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/BeamTypeUtils.java
deleted file mode 100644
index 710529cc775dc..0000000000000
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/BeamTypeUtils.java
+++ /dev/null
@@ -1,408 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.fnexecution.v1.FlinkFnApi;
-import org.apache.flink.table.runtime.typeutils.coders.BaseRowCoder;
-import org.apache.flink.table.runtime.typeutils.coders.RowCoder;
-import org.apache.flink.table.types.logical.AnyType;
-import org.apache.flink.table.types.logical.ArrayType;
-import org.apache.flink.table.types.logical.BigIntType;
-import org.apache.flink.table.types.logical.BinaryType;
-import org.apache.flink.table.types.logical.BooleanType;
-import org.apache.flink.table.types.logical.CharType;
-import org.apache.flink.table.types.logical.DateType;
-import org.apache.flink.table.types.logical.DayTimeIntervalType;
-import org.apache.flink.table.types.logical.DecimalType;
-import org.apache.flink.table.types.logical.DistinctType;
-import org.apache.flink.table.types.logical.DoubleType;
-import org.apache.flink.table.types.logical.FloatType;
-import org.apache.flink.table.types.logical.IntType;
-import org.apache.flink.table.types.logical.LocalZonedTimestampType;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.table.types.logical.LogicalTypeVisitor;
-import org.apache.flink.table.types.logical.MapType;
-import org.apache.flink.table.types.logical.MultisetType;
-import org.apache.flink.table.types.logical.NullType;
-import org.apache.flink.table.types.logical.RowType;
-import org.apache.flink.table.types.logical.SmallIntType;
-import org.apache.flink.table.types.logical.StructuredType;
-import org.apache.flink.table.types.logical.SymbolType;
-import org.apache.flink.table.types.logical.TimeType;
-import org.apache.flink.table.types.logical.TimestampType;
-import org.apache.flink.table.types.logical.TinyIntType;
-import org.apache.flink.table.types.logical.VarBinaryType;
-import org.apache.flink.table.types.logical.VarCharType;
-import org.apache.flink.table.types.logical.YearMonthIntervalType;
-import org.apache.flink.table.types.logical.ZonedTimestampType;
-
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
-
-/**
- * Utilities for converting Flink data types to Beam data types.
- */
-@Internal
-public final class BeamTypeUtils {
-
-	private static final String EMPTY_STRING = "";
-
-	public static Coder toCoder(LogicalType logicalType) {
-		return logicalType.accept(new LogicalTypeToCoderConverter());
-	}
-
-	public static Coder toBlinkCoder(LogicalType logicalType) {
-		return logicalType.accept(new LogicalTypeToBlinkCoderConverter());
-	}
-
-	public static FlinkFnApi.Schema.FieldType toProtoType(LogicalType logicalType) {
-		return logicalType.accept(new LogicalTypeToProtoTypeConverter());
-	}
-
-	private static class LogicalTypeToCoderConverter implements LogicalTypeVisitor<Coder> {
-
-		@Override
-		public Coder visit(CharType charType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(VarCharType varCharType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(BooleanType booleanType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(BinaryType binaryType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(VarBinaryType varBinaryType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(DecimalType decimalType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(TinyIntType tinyIntType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(SmallIntType smallIntType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(IntType intType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(BigIntType bigIntType) {
-			return VarLongCoder.of();
-		}
-
-		@Override
-		public Coder visit(FloatType floatType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(DoubleType doubleType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(DateType dateType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(TimeType timeType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(TimestampType timestampType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(ZonedTimestampType zonedTimestampType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(LocalZonedTimestampType localZonedTimestampType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(YearMonthIntervalType yearMonthIntervalType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(DayTimeIntervalType dayTimeIntervalType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(ArrayType arrayType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(MultisetType multisetType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(MapType mapType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(RowType rowType) {
-			final Coder[] fieldCoders = rowType.getFields()
-				.stream()
-				.map(f -> f.getType().accept(this))
-				.toArray(Coder[]::new);
-			return new RowCoder(fieldCoders);
-		}
-
-		@Override
-		public Coder visit(DistinctType distinctType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(StructuredType structuredType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(NullType nullType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(AnyType<?> anyType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(SymbolType<?> symbolType) {
-			return null;
-		}
-
-		@Override
-		public Coder visit(LogicalType other) {
-			return null;
-		}
-	}
-
-	private static class LogicalTypeToBlinkCoderConverter extends LogicalTypeToCoderConverter {
-
-		@Override
-		public Coder visit(RowType rowType) {
-			final Coder[] fieldCoders = rowType.getFields()
-				.stream()
-				.map(f -> f.getType().accept(this))
-				.toArray(Coder[]::new);
-			return new BaseRowCoder(fieldCoders, rowType.getChildren().toArray(new LogicalType[0]));
-		}
-	}
-
-	private static class LogicalTypeToProtoTypeConverter implements LogicalTypeVisitor<FlinkFnApi.Schema.FieldType> {
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(CharType charType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(VarCharType varCharType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(BooleanType booleanType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(BinaryType binaryType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(VarBinaryType varBinaryType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(DecimalType decimalType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(TinyIntType tinyIntType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(SmallIntType smallIntType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(IntType intType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(BigIntType bigIntType) {
-			return FlinkFnApi.Schema.FieldType.newBuilder()
-				.setTypeName(FlinkFnApi.Schema.TypeName.BIGINT)
-				.setNullable(bigIntType.isNullable())
-				.build();
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(FloatType floatType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(DoubleType doubleType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(DateType dateType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(TimeType timeType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(TimestampType timestampType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(ZonedTimestampType zonedTimestampType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(LocalZonedTimestampType localZonedTimestampType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(YearMonthIntervalType yearMonthIntervalType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(DayTimeIntervalType dayTimeIntervalType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(ArrayType arrayType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(MultisetType multisetType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(MapType mapType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(RowType rowType) {
-			FlinkFnApi.Schema.FieldType.Builder builder =
-				FlinkFnApi.Schema.FieldType.newBuilder()
-					.setTypeName(FlinkFnApi.Schema.TypeName.ROW)
-					.setNullable(rowType.isNullable());
-
-			FlinkFnApi.Schema.Builder schemaBuilder = FlinkFnApi.Schema.newBuilder();
-			for (RowType.RowField field : rowType.getFields()) {
-				schemaBuilder.addFields(
-					FlinkFnApi.Schema.Field.newBuilder()
-						.setName(field.getName())
-						.setDescription(field.getDescription().orElse(EMPTY_STRING))
-						.setType(field.getType().accept(this))
-						.build());
-			}
-			builder.setRowSchema(schemaBuilder.build());
-			return builder.build();
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(DistinctType distinctType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(StructuredType structuredType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(NullType nullType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(AnyType<?> anyType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(SymbolType<?> symbolType) {
-			return null;
-		}
-
-		@Override
-		public FlinkFnApi.Schema.FieldType visit(LogicalType other) {
-			return null;
-		}
-	}
-}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtils.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtils.java
new file mode 100644
index 0000000000000..fbe4db04d1650
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/PythonTypeUtils.java
@@ -0,0 +1,300 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.common.typeutils.base.BooleanSerializer;
+import org.apache.flink.api.common.typeutils.base.ByteSerializer;
+import org.apache.flink.api.common.typeutils.base.DoubleSerializer;
+import org.apache.flink.api.common.typeutils.base.FloatSerializer;
+import org.apache.flink.api.common.typeutils.base.IntSerializer;
+import org.apache.flink.api.common.typeutils.base.LongSerializer;
+import org.apache.flink.api.common.typeutils.base.ShortSerializer;
+import org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer;
+import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
+import org.apache.flink.fnexecution.v1.FlinkFnApi;
+import org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializer;
+import org.apache.flink.table.runtime.typeutils.serializers.python.DateSerializer;
+import org.apache.flink.table.runtime.typeutils.serializers.python.StringSerializer;
+import org.apache.flink.table.types.logical.BigIntType;
+import org.apache.flink.table.types.logical.BinaryType;
+import org.apache.flink.table.types.logical.BooleanType;
+import org.apache.flink.table.types.logical.CharType;
+import org.apache.flink.table.types.logical.DateType;
+import org.apache.flink.table.types.logical.DoubleType;
+import org.apache.flink.table.types.logical.FloatType;
+import org.apache.flink.table.types.logical.IntType;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.SmallIntType;
+import org.apache.flink.table.types.logical.TinyIntType;
+import org.apache.flink.table.types.logical.VarBinaryType;
+import org.apache.flink.table.types.logical.VarCharType;
+import org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor;
+
+/**
+ * Utilities for converting Flink logical types, such as convert it to the related
+ * TypeSerializer or ProtoType.
+ */
+@Internal
+public final class PythonTypeUtils {
+
+	private static final String EMPTY_STRING = "";
+
+	public static TypeSerializer toFlinkTypeSerializer(LogicalType logicalType) {
+		return logicalType.accept(new LogicalTypeToTypeSerializerConverter());
+	}
+
+	public static TypeSerializer toBlinkTypeSerializer(LogicalType logicalType) {
+		return logicalType.accept(new LogicalTypeToBlinkTypeSerializerConverter());
+	}
+
+	public static FlinkFnApi.Schema.FieldType toProtoType(LogicalType logicalType) {
+		return logicalType.accept(new LogicalTypeToProtoTypeConverter());
+	}
+
+	private static class LogicalTypeToTypeSerializerConverter extends LogicalTypeDefaultVisitor<TypeSerializer> {
+		@Override
+		public TypeSerializer visit(BooleanType booleanType) {
+			return BooleanSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(TinyIntType tinyIntType) {
+			return ByteSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(SmallIntType smallIntType) {
+			return ShortSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(IntType intType) {
+			return IntSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(BigIntType bigIntType) {
+			return LongSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(FloatType floatType) {
+			return FloatSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(DoubleType doubleType) {
+			return DoubleSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(BinaryType binaryType) {
+			return BytePrimitiveArraySerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(VarBinaryType varBinaryType) {
+			return BytePrimitiveArraySerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(VarCharType varCharType) {
+			return StringSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(CharType charType) {
+			return StringSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(DateType dateType) {
+			return DateSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(RowType rowType) {
+			final TypeSerializer[] fieldTypeSerializers = rowType.getFields()
+				.stream()
+				.map(f -> f.getType().accept(this))
+				.toArray(TypeSerializer[]::new);
+			return new RowSerializer(fieldTypeSerializers);
+		}
+
+		@Override
+		protected TypeSerializer defaultMethod(LogicalType logicalType) {
+			throw new UnsupportedOperationException(String.format(
+				"Python UDF doesn't support logical type %s currently.", logicalType.asSummaryString()));
+		}
+	}
+
+	private static class LogicalTypeToBlinkTypeSerializerConverter extends LogicalTypeToTypeSerializerConverter {
+
+		@Override
+		public TypeSerializer visit(RowType rowType) {
+			final TypeSerializer[] fieldTypeSerializers = rowType.getFields()
+				.stream()
+				.map(f -> f.getType().accept(this))
+				.toArray(TypeSerializer[]::new);
+			return new BaseRowSerializer(rowType.getChildren().toArray(new LogicalType[0]), fieldTypeSerializers);
+		}
+
+		@Override
+		public TypeSerializer visit(VarCharType varCharType) {
+			return BinaryStringSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(CharType charType) {
+			return BinaryStringSerializer.INSTANCE;
+		}
+
+		@Override
+		public TypeSerializer visit(DateType dateType) {
+			return IntSerializer.INSTANCE;
+		}
+	}
+
+	private static class LogicalTypeToProtoTypeConverter extends LogicalTypeDefaultVisitor<FlinkFnApi.Schema.FieldType> {
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(BooleanType booleanType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.BOOLEAN)
+				.setNullable(booleanType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(TinyIntType tinyIntType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.TINYINT)
+				.setNullable(tinyIntType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(SmallIntType smallIntType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.SMALLINT)
+				.setNullable(smallIntType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(IntType intType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.INT)
+				.setNullable(intType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(BigIntType bigIntType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.BIGINT)
+				.setNullable(bigIntType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(FloatType floatType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.FLOAT)
+				.setNullable(floatType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(DoubleType doubleType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.DOUBLE)
+				.setNullable(doubleType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(BinaryType binaryType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.BINARY)
+				.setNullable(binaryType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(VarBinaryType varBinaryType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.VARBINARY)
+				.setNullable(varBinaryType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(CharType charType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.CHAR)
+				.setNullable(charType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(VarCharType varCharType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.VARCHAR)
+				.setNullable(varCharType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(DateType dateType) {
+			return FlinkFnApi.Schema.FieldType.newBuilder()
+				.setTypeName(FlinkFnApi.Schema.TypeName.DATE)
+				.setNullable(dateType.isNullable())
+				.build();
+		}
+
+		@Override
+		public FlinkFnApi.Schema.FieldType visit(RowType rowType) {
+			FlinkFnApi.Schema.FieldType.Builder builder =
+				FlinkFnApi.Schema.FieldType.newBuilder()
+					.setTypeName(FlinkFnApi.Schema.TypeName.ROW)
+					.setNullable(rowType.isNullable());
+
+			FlinkFnApi.Schema.Builder schemaBuilder = FlinkFnApi.Schema.newBuilder();
+			for (RowType.RowField field : rowType.getFields()) {
+				schemaBuilder.addFields(
+					FlinkFnApi.Schema.Field.newBuilder()
+						.setName(field.getName())
+						.setDescription(field.getDescription().orElse(EMPTY_STRING))
+						.setType(field.getType().accept(this))
+						.build());
+			}
+			builder.setRowSchema(schemaBuilder.build());
+			return builder.build();
+		}
+
+		@Override
+		protected FlinkFnApi.Schema.FieldType defaultMethod(LogicalType logicalType) {
+			throw new UnsupportedOperationException(String.format(
+				"Python UDF doesn't support logical type %s currently.", logicalType.asSummaryString()));
+		}
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoder.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoder.java
deleted file mode 100644
index 9834c50b24c93..0000000000000
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoder.java
+++ /dev/null
@@ -1,146 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils.coders;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.core.memory.DataOutputView;
-import org.apache.flink.table.dataformat.BaseRow;
-import org.apache.flink.table.dataformat.GenericRow;
-import org.apache.flink.table.dataformat.TypeGetterSetters;
-import org.apache.flink.table.types.logical.LogicalType;
-import org.apache.flink.util.Preconditions;
-
-import org.apache.beam.sdk.coders.Coder;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.Collections;
-import java.util.List;
-
-import static org.apache.flink.api.java.typeutils.runtime.NullMaskUtils.readIntoNullMask;
-
-/**
- * A {@link Coder} for {@link BaseRow}. It should be noted that the header will not be encoded.
- */
-@Internal
-public class BaseRowCoder extends Coder<BaseRow> {
-
-	private static final long serialVersionUID = 1L;
-
-	private final Coder<Object>[] fieldCoders;
-	private final LogicalType[] fieldTypes;
-
-	private transient ReusableDataInputView reusableInputStream;
-	private transient ReusableDataOutputView reusableOutputStream;
-
-	private transient boolean[] nullMask;
-
-	@SuppressWarnings("unchecked")
-	public BaseRowCoder(Coder<?>[] fieldCoders, LogicalType[] fieldTypes) {
-		this.fieldCoders = (Coder<Object>[]) Preconditions.checkNotNull(fieldCoders);
-		this.fieldTypes = Preconditions.checkNotNull(fieldTypes);
-		this.reusableInputStream = new ReusableDataInputView();
-		this.reusableOutputStream = new ReusableDataOutputView();
-		this.nullMask = new boolean[fieldCoders.length];
-	}
-
-	public Coder<?>[] getFieldCoders() {
-		return this.fieldCoders;
-	}
-
-	@Override
-	public void encode(BaseRow row, OutputStream outStream) throws IOException {
-		int len = fieldCoders.length;
-
-		if (row.getArity() != len) {
-			throw new RuntimeException("Row arity of input element does not match coders.");
-		}
-
-		// write a null mask
-		reusableOutputStream.reset(outStream);
-		writeNullMask(len, row, reusableOutputStream);
-
-		for (int i = 0; i < row.getArity(); i++) {
-			if (!row.isNullAt(i)) {
-				// TODO: support BaseRow natively in Python, then we can eliminate the redundant serialize/deserialize
-				fieldCoders[i].encode(TypeGetterSetters.get(row, i, fieldTypes[i]), outStream);
-			}
-		}
-	}
-
-	@Override
-	public BaseRow decode(InputStream inStream) throws IOException {
-		int len = fieldCoders.length;
-
-		// read null mask
-		reusableInputStream.resetInputStream(inStream);
-		readIntoNullMask(len, reusableInputStream, nullMask);
-
-		GenericRow row = new GenericRow(fieldCoders.length);
-		for (int i = 0; i < row.getArity(); i++) {
-			if (nullMask[i]) {
-				row.setField(i, null);
-			} else {
-				row.setField(i, fieldCoders[i].decode(inStream));
-			}
-		}
-		return row;
-	}
-
-	@Override
-	public List<? extends Coder<?>> getCoderArguments() {
-		return Collections.emptyList();
-	}
-
-	@Override
-	public void verifyDeterministic() {}
-
-	@Override
-	public boolean consistentWithEquals() {
-		return true;
-	}
-
-	private static void writeNullMask(int len, BaseRow value, DataOutputView target) throws IOException {
-		int b = 0x00;
-		int bytePos = 0;
-
-		int fieldPos = 0;
-		int numPos = 0;
-		while (fieldPos < len) {
-			b = 0x00;
-			// set bits in byte
-			bytePos = 0;
-			numPos = Math.min(8, len - fieldPos);
-			while (bytePos < numPos) {
-				b = b << 1;
-				// set bit if field is null
-				if (value.isNullAt(fieldPos + bytePos)) {
-					b |= 0x01;
-				}
-				bytePos += 1;
-			}
-			fieldPos += numPos;
-			// shift bits if last byte is not completely filled
-			b <<= (8 - bytePos);
-			// write byte
-			target.writeByte(b);
-		}
-	}
-}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataInputView.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataInputView.java
deleted file mode 100644
index d71cf3bf8fcdd..0000000000000
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataInputView.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils.coders;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.core.memory.DataInputView;
-import org.apache.flink.core.memory.DataInputViewStreamWrapper;
-
-import java.io.InputStream;
-
-/**
- * An implementation of {@link DataInputView} that allows the instance
- * to be re-used with another underlying input stream.
- */
-@Internal
-public class ReusableDataInputView extends DataInputViewStreamWrapper {
-
-	public ReusableDataInputView() {
-		super(null);
-	}
-
-	public void resetInputStream(InputStream in) {
-		this.in = in;
-	}
-}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/RowCoder.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/RowCoder.java
deleted file mode 100644
index b72d5f117fcea..0000000000000
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/RowCoder.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils.coders;
-
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.types.Row;
-import org.apache.flink.util.Preconditions;
-
-import org.apache.beam.sdk.coders.Coder;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.Collections;
-import java.util.List;
-
-import static org.apache.flink.api.java.typeutils.runtime.NullMaskUtils.readIntoNullMask;
-import static org.apache.flink.api.java.typeutils.runtime.NullMaskUtils.writeNullMask;
-
-/**
- * A {@link Coder} for {@link Row}.
- */
-@Internal
-public class RowCoder extends Coder<Row> {
-
-	private static final long serialVersionUID = 1L;
-
-	private final Coder<Object>[] fieldCoders;
-
-	private transient ReusableDataInputView reuseInputStream;
-	private transient ReusableDataOutputView reuseOutputStream;
-
-	private transient boolean[] nullMask;
-
-	@SuppressWarnings("unchecked")
-	public RowCoder(Coder<?>[] fieldCoders) {
-		this.fieldCoders = (Coder<Object>[]) Preconditions.checkNotNull(fieldCoders);
-		this.reuseInputStream = new ReusableDataInputView();
-		this.reuseOutputStream = new ReusableDataOutputView();
-		this.nullMask = new boolean[fieldCoders.length];
-	}
-
-	public Coder<?>[] getFieldCoders() {
-		return this.fieldCoders;
-	}
-
-	@Override
-	public void encode(Row row, OutputStream outStream) throws IOException {
-		int len = fieldCoders.length;
-
-		if (row.getArity() != len) {
-			throw new RuntimeException("Row arity of input element does not match coders.");
-		}
-
-		// write a null mask
-		reuseOutputStream.reset(outStream);
-		writeNullMask(len, row, reuseOutputStream);
-
-		for (int i = 0; i < row.getArity(); i++) {
-			Object o = row.getField(i);
-			if (o != null) {
-				fieldCoders[i].encode(o, outStream);
-			}
-		}
-	}
-
-	@Override
-	public Row decode(InputStream inStream) throws IOException {
-		int len = fieldCoders.length;
-
-		// read null mask
-		reuseInputStream.resetInputStream(inStream);
-		readIntoNullMask(len, reuseInputStream, nullMask);
-
-		Row row = new Row(len);
-		for (int i = 0; i < len; i++) {
-			if (nullMask[i]) {
-				row.setField(i, null);
-			}
-			else {
-				row.setField(i, fieldCoders[i].decode(inStream));
-			}
-		}
-		return row;
-	}
-
-	@Override
-	public List<? extends Coder<?>> getCoderArguments() {
-		return Collections.emptyList();
-	}
-
-	@Override
-	public void verifyDeterministic() {}
-
-	@Override
-	public boolean consistentWithEquals() {
-		return true;
-	}
-}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializer.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializer.java
new file mode 100644
index 0000000000000..cb422b52cc196
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializer.java
@@ -0,0 +1,232 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils.serializers.python;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.CompositeTypeSerializerUtil;
+import org.apache.flink.api.common.typeutils.NestedSerializersSnapshotDelegate;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.common.typeutils.TypeSerializerSchemaCompatibility;
+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
+import org.apache.flink.api.java.typeutils.runtime.DataInputViewStream;
+import org.apache.flink.api.java.typeutils.runtime.DataOutputViewStream;
+import org.apache.flink.core.memory.DataInputView;
+import org.apache.flink.core.memory.DataOutputView;
+import org.apache.flink.table.dataformat.BaseRow;
+import org.apache.flink.table.dataformat.GenericRow;
+import org.apache.flink.table.dataformat.TypeGetterSetters;
+import org.apache.flink.table.types.logical.LogicalType;
+import org.apache.flink.util.InstantiationUtil;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import static org.apache.flink.api.java.typeutils.runtime.NullMaskUtils.readIntoNullMask;
+
+/**
+ * A {@link TypeSerializer} for {@link BaseRow}. It should be noted that the header will not be encoded.
+ * Currently Python doesn't support BaseRow natively, so we can't use BaseRowSerializer in blink directly.
+ */
+@Internal
+public class BaseRowSerializer extends org.apache.flink.table.runtime.typeutils.BaseRowSerializer {
+
+	private final LogicalType[] fieldTypes;
+
+	private final TypeSerializer[] fieldSerializers;
+
+	private transient boolean[] nullMask;
+
+	public BaseRowSerializer(LogicalType[] types, TypeSerializer[] fieldSerializers) {
+		super(types, fieldSerializers);
+		this.fieldTypes = types;
+		this.fieldSerializers = fieldSerializers;
+		this.nullMask = new boolean[fieldTypes.length];
+	}
+
+	@Override
+	public void serialize(BaseRow row, DataOutputView target) throws IOException {
+		int len = fieldSerializers.length;
+
+		if (row.getArity() != len) {
+			throw new RuntimeException("Row arity of input element does not match serializers.");
+		}
+
+		// write a null mask
+		writeNullMask(len, row, target);
+
+		for (int i = 0; i < row.getArity(); i++) {
+			if (!row.isNullAt(i)) {
+				// TODO: support BaseRow natively in Python, then we can eliminate the redundant serialize/deserialize
+				fieldSerializers[i].serialize(TypeGetterSetters.get(row, i, fieldTypes[i]), target);
+			}
+		}
+	}
+
+	@Override
+	public BaseRow deserialize(DataInputView source) throws IOException {
+		int len = fieldSerializers.length;
+
+		// read null mask
+		readIntoNullMask(len, source, nullMask);
+
+		GenericRow row = new GenericRow(fieldSerializers.length);
+		for (int i = 0; i < row.getArity(); i++) {
+			if (nullMask[i]) {
+				row.setField(i, null);
+			} else {
+				row.setField(i, fieldSerializers[i].deserialize(source));
+			}
+		}
+		return row;
+	}
+
+	@Override
+	public BaseRow deserialize(BaseRow reuse, DataInputView source) throws IOException {
+		return deserialize(source);
+	}
+
+	@Override
+	public void copy(DataInputView source, DataOutputView target) throws IOException {
+		serialize(deserialize(source), target);
+	}
+
+	private static void writeNullMask(int len, BaseRow value, DataOutputView target) throws IOException {
+		int b = 0x00;
+		int bytePos = 0;
+
+		int fieldPos = 0;
+		int numPos = 0;
+		while (fieldPos < len) {
+			b = 0x00;
+			// set bits in byte
+			bytePos = 0;
+			numPos = Math.min(8, len - fieldPos);
+			while (bytePos < numPos) {
+				b = b << 1;
+				// set bit if field is null
+				if (value.isNullAt(fieldPos + bytePos)) {
+					b |= 0x01;
+				}
+				bytePos += 1;
+			}
+			fieldPos += numPos;
+			// shift bits if last byte is not completely filled
+			b <<= (8 - bytePos);
+			// write byte
+			target.writeByte(b);
+		}
+	}
+
+	@Override
+	public TypeSerializerSnapshot<BaseRow> snapshotConfiguration() {
+		return new BaseRowSerializerSnapshot(fieldTypes, fieldSerializers);
+	}
+
+	/**
+	 * {@link TypeSerializerSnapshot} for {@link BaseRowSerializer}.
+	 */
+	public static final class BaseRowSerializerSnapshot implements TypeSerializerSnapshot<BaseRow> {
+		private static final int CURRENT_VERSION = 3;
+
+		private LogicalType[] previousTypes;
+		private NestedSerializersSnapshotDelegate nestedSerializersSnapshotDelegate;
+
+		@SuppressWarnings("unused")
+		public BaseRowSerializerSnapshot() {
+			// this constructor is used when restoring from a checkpoint/savepoint.
+		}
+
+		BaseRowSerializerSnapshot(LogicalType[] types, TypeSerializer[] serializers) {
+			this.previousTypes = types;
+			this.nestedSerializersSnapshotDelegate = new NestedSerializersSnapshotDelegate(
+				serializers);
+		}
+
+		@Override
+		public int getCurrentVersion() {
+			return CURRENT_VERSION;
+		}
+
+		@Override
+		public void writeSnapshot(DataOutputView out) throws IOException {
+			out.writeInt(previousTypes.length);
+			DataOutputViewStream stream = new DataOutputViewStream(out);
+			for (LogicalType previousType : previousTypes) {
+				InstantiationUtil.serializeObject(stream, previousType);
+			}
+			nestedSerializersSnapshotDelegate.writeNestedSerializerSnapshots(out);
+		}
+
+		@Override
+		public void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader)
+			throws IOException {
+			int length = in.readInt();
+			DataInputViewStream stream = new DataInputViewStream(in);
+			previousTypes = new LogicalType[length];
+			for (int i = 0; i < length; i++) {
+				try {
+					previousTypes[i] = InstantiationUtil.deserializeObject(
+						stream,
+						userCodeClassLoader
+					);
+				} catch (ClassNotFoundException e) {
+					throw new IOException(e);
+				}
+			}
+			this.nestedSerializersSnapshotDelegate = NestedSerializersSnapshotDelegate.readNestedSerializerSnapshots(
+				in,
+				userCodeClassLoader
+			);
+		}
+
+		@Override
+		public BaseRowSerializer restoreSerializer() {
+			return new BaseRowSerializer(
+				previousTypes,
+				nestedSerializersSnapshotDelegate.getRestoredNestedSerializers()
+			);
+		}
+
+		@Override
+		public TypeSerializerSchemaCompatibility<BaseRow> resolveSchemaCompatibility(TypeSerializer<BaseRow> newSerializer) {
+			if (!(newSerializer instanceof BaseRowSerializer)) {
+				return TypeSerializerSchemaCompatibility.incompatible();
+			}
+
+			BaseRowSerializer newRowSerializer = (BaseRowSerializer) newSerializer;
+			if (!Arrays.equals(previousTypes, newRowSerializer.fieldTypes)) {
+				return TypeSerializerSchemaCompatibility.incompatible();
+			}
+
+			CompositeTypeSerializerUtil.IntermediateCompatibilityResult<BaseRow> intermediateResult =
+				CompositeTypeSerializerUtil.constructIntermediateCompatibilityResult(
+					newRowSerializer.fieldSerializers,
+					nestedSerializersSnapshotDelegate.getNestedSerializerSnapshots()
+				);
+
+			if (intermediateResult.isCompatibleWithReconfiguredSerializer()) {
+				BaseRowSerializer reconfiguredCompositeSerializer = restoreSerializer();
+				return TypeSerializerSchemaCompatibility.compatibleWithReconfiguredSerializer(
+					reconfiguredCompositeSerializer);
+			}
+
+			return intermediateResult.getFinalResult();
+		}
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializer.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializer.java
new file mode 100644
index 0000000000000..5c279d95f822f
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializer.java
@@ -0,0 +1,131 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils.serializers.python;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
+import org.apache.flink.core.memory.DataInputView;
+import org.apache.flink.core.memory.DataOutputView;
+
+import java.io.IOException;
+import java.sql.Date;
+import java.util.TimeZone;
+
+/**
+ * Takes int instead of long as the serialized value. It not only reduces the length of
+ * the serialized value, but also makes the serialized value consistent between
+ * the legacy planner and the blink planner.
+ */
+@Internal
+public class DateSerializer extends TypeSerializerSingleton<Date> {
+
+	private static final long serialVersionUID = 1L;
+
+	/**
+	 * The local time zone.
+	 */
+	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();
+
+	private static final long MILLIS_PER_DAY = 86400000L; // = 24 * 60 * 60 * 1000
+
+	public static final DateSerializer INSTANCE = new DateSerializer();
+
+	@Override
+	public boolean isImmutableType() {
+		return false;
+	}
+
+	@Override
+	public Date createInstance() {
+		return new Date(0L);
+	}
+
+	@Override
+	public Date copy(Date from) {
+		if (from == null) {
+			return null;
+		}
+		return new Date(from.getTime());
+	}
+
+	@Override
+	public Date copy(Date from, Date reuse) {
+		if (from == null) {
+			return null;
+		}
+		reuse.setTime(from.getTime());
+		return reuse;
+	}
+
+	@Override
+	public int getLength() {
+		return 4;
+	}
+
+	@Override
+	public void serialize(Date record, DataOutputView target) throws IOException {
+		if (record == null) {
+			throw new IllegalArgumentException("The Date record must not be null.");
+		}
+		target.writeInt(dateToInternal(record));
+	}
+
+	@Override
+	public Date deserialize(DataInputView source) throws IOException {
+		return internalToDate(source.readInt());
+	}
+
+	private int dateToInternal(Date date) {
+		long ts = date.getTime() + LOCAL_TZ.getOffset(date.getTime());
+		return (int) (ts / MILLIS_PER_DAY);
+	}
+
+	private Date internalToDate(int v) {
+		final long t = v * MILLIS_PER_DAY;
+		return new Date(t - LOCAL_TZ.getOffset(t));
+	}
+
+	@Override
+	public Date deserialize(Date reuse, DataInputView source) throws IOException {
+		return deserialize(source);
+	}
+
+	@Override
+	public void copy(DataInputView source, DataOutputView target) throws IOException {
+		serialize(deserialize(source), target);
+	}
+
+	@Override
+	public TypeSerializerSnapshot<Date> snapshotConfiguration() {
+		return new DateSerializerSnapshot();
+	}
+
+	/**
+	 * Serializer configuration snapshot for compatibility and format evolution.
+	 */
+	@SuppressWarnings("WeakerAccess")
+	public static final class DateSerializerSnapshot extends SimpleTypeSerializerSnapshot<Date> {
+
+		public DateSerializerSnapshot() {
+			super(() -> INSTANCE);
+		}
+	}
+}
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializer.java b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializer.java
new file mode 100644
index 0000000000000..0901bdb4bdae7
--- /dev/null
+++ b/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializer.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.table.runtime.typeutils.serializers.python;
+
+import org.apache.flink.annotation.Internal;
+import org.apache.flink.api.common.typeutils.SimpleTypeSerializerSnapshot;
+import org.apache.flink.api.common.typeutils.TypeSerializerSnapshot;
+import org.apache.flink.api.common.typeutils.base.TypeSerializerSingleton;
+import org.apache.flink.core.memory.DataInputView;
+import org.apache.flink.core.memory.DataOutputView;
+import org.apache.flink.table.runtime.util.StringUtf8Utils;
+
+import java.io.IOException;
+
+/**
+ * <p>We create the StringSerializer instead of using the StringSerializer of flink-core module
+ * because the StringSerializer of flink-core module serialize every Char of String in serialize
+ * method and deserialize the Char to build the String. We want to convert String to UTF-8 bytes
+ * to serialize which is compatible with BinaryStringSerializer in blink.</p>
+ *
+ * <p>So we create this StringSerializer (only used in Java and Python data communication in udf).
+ *
+ * <p>StringSerializer for String.
+ */
+@Internal
+public class StringSerializer extends TypeSerializerSingleton<String> {
+
+	private static final long serialVersionUID = 1L;
+
+	public static final StringSerializer INSTANCE = new StringSerializer();
+
+	private static final String EMPTY = "";
+
+	@Override
+	public boolean isImmutableType() {
+		return true;
+	}
+
+	@Override
+	public String createInstance() {
+		return EMPTY;
+	}
+
+	@Override
+	public String copy(String from) {
+		return from;
+	}
+
+	@Override
+	public String copy(String from, String reuse) {
+		return from;
+	}
+
+	@Override
+	public int getLength() {
+		return -1;
+	}
+
+	@Override
+	public void serialize(String record, DataOutputView target) throws IOException {
+		if (record == null) {
+			throw new IllegalArgumentException("The String record must not be null.");
+		}
+		byte[] bytes = StringUtf8Utils.encodeUTF8(record);
+		target.writeInt(bytes.length);
+		target.write(bytes);
+	}
+
+	@Override
+	public String deserialize(DataInputView source) throws IOException {
+		int len = source.readInt();
+		byte[] bytes = new byte[len];
+		source.read(bytes);
+		return StringUtf8Utils.decodeUTF8(bytes, 0, len);
+	}
+
+	@Override
+	public String deserialize(String reuse, DataInputView source) throws IOException {
+		return deserialize(source);
+	}
+
+	@Override
+	public void copy(DataInputView source, DataOutputView target) throws IOException {
+		int len = source.readInt();
+		target.writeInt(len);
+		byte[] bytes = new byte[len];
+		source.read(bytes);
+		target.write(bytes);
+	}
+
+	@Override
+	public TypeSerializerSnapshot<String> snapshotConfiguration() {
+		return new StringSerializerSnapshot();
+	}
+
+	/**
+	 * Serializer configuration snapshot for compatibility and format evolution.
+	 */
+	@SuppressWarnings("WeakerAccess")
+	public static final class StringSerializerSnapshot extends SimpleTypeSerializerSnapshot<String> {
+
+		public StringSerializerSnapshot() {
+			super(() -> INSTANCE);
+		}
+	}
+}
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java
index dacd5092b54f4..87c5e98db5204 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/functions/python/BaseRowPythonScalarFunctionRunnerTest.java
@@ -18,14 +18,13 @@
 
 package org.apache.flink.table.functions.python;
 
+import org.apache.flink.api.common.typeutils.TypeSerializer;
 import org.apache.flink.table.dataformat.BaseRow;
 import org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.runners.python.BaseRowPythonScalarFunctionRunner;
-import org.apache.flink.table.runtime.typeutils.coders.BaseRowCoder;
+import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
 import org.apache.flink.table.types.logical.RowType;
 
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 import org.junit.Test;
 
@@ -42,69 +41,48 @@ public class BaseRowPythonScalarFunctionRunnerTest extends AbstractPythonScalarF
 	public void testInputOutputDataTypeConstructedProperlyForSingleUDF() {
 		final AbstractPythonScalarFunctionRunner<BaseRow, BaseRow> runner = createSingleUDFRunner();
 
-		// check input coder
-		Coder<BaseRow> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof BaseRowCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof BaseRowSerializer);
 
-		Coder<?>[] inputFieldCoders = ((BaseRowCoder) inputCoder).getFieldCoders();
-		assertEquals(1, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
+		assertEquals(1, ((BaseRowSerializer) inputTypeSerializer).getArity());
 
-		// check output coder
-		Coder<BaseRow> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof BaseRowCoder);
-		Coder<?>[] outputFieldCoders = ((BaseRowCoder) outputCoder).getFieldCoders();
-		assertEquals(1, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof BaseRowSerializer);
+		assertEquals(1, ((BaseRowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() {
 		final AbstractPythonScalarFunctionRunner<BaseRow, BaseRow> runner = createMultipleUDFRunner();
 
-		// check input coder
-		Coder<BaseRow> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof BaseRowCoder);
-
-		Coder<?>[] inputFieldCoders = ((BaseRowCoder) inputCoder).getFieldCoders();
-		assertEquals(3, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[2] instanceof VarLongCoder);
-
-		// check output coder
-		Coder<BaseRow> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof BaseRowCoder);
-		Coder<?>[] outputFieldCoders = ((BaseRowCoder) outputCoder).getFieldCoders();
-		assertEquals(2, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[1] instanceof VarLongCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof BaseRowSerializer);
+
+		assertEquals(3, ((BaseRowSerializer) inputTypeSerializer).getArity());
+
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof BaseRowSerializer);
+		assertEquals(2, ((BaseRowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForChainedUDFs() {
 		final AbstractPythonScalarFunctionRunner<BaseRow, BaseRow> runner = createChainedUDFRunner();
 
-		// check input coder
-		Coder<BaseRow> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof BaseRowCoder);
-
-		Coder<?>[] inputFieldCoders = ((BaseRowCoder) inputCoder).getFieldCoders();
-		assertEquals(5, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[2] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[3] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[4] instanceof VarLongCoder);
-
-		// check output coder
-		Coder<BaseRow> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof BaseRowCoder);
-		Coder<?>[] outputFieldCoders = ((BaseRowCoder) outputCoder).getFieldCoders();
-		assertEquals(3, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[2] instanceof VarLongCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof BaseRowSerializer);
+
+		assertEquals(5, ((BaseRowSerializer) inputTypeSerializer).getArity());
+
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof BaseRowSerializer);
+		assertEquals(3, ((BaseRowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Override
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java b/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java
index 212bf2a6e8e96..870dff4320f07 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonScalarFunctionRunnerTest.java
@@ -18,10 +18,12 @@
 
 package org.apache.flink.table.functions.python;
 
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
+import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
 import org.apache.flink.table.runtime.runners.python.AbstractPythonScalarFunctionRunner;
 import org.apache.flink.table.runtime.runners.python.PythonScalarFunctionRunner;
-import org.apache.flink.table.runtime.typeutils.coders.RowCoder;
 import org.apache.flink.table.types.logical.BigIntType;
 import org.apache.flink.table.types.logical.RowType;
 import org.apache.flink.types.Row;
@@ -29,8 +31,6 @@
 import org.apache.beam.runners.fnexecution.control.JobBundleFactory;
 import org.apache.beam.runners.fnexecution.control.RemoteBundle;
 import org.apache.beam.runners.fnexecution.control.StageBundleFactory;
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
 import org.apache.beam.sdk.fn.data.FnDataReceiver;
 import org.apache.beam.sdk.transforms.windowing.BoundedWindow;
 import org.apache.beam.sdk.transforms.windowing.GlobalWindow;
@@ -68,69 +68,48 @@ public class PythonScalarFunctionRunnerTest extends AbstractPythonScalarFunction
 	public void testInputOutputDataTypeConstructedProperlyForSingleUDF() {
 		final AbstractPythonScalarFunctionRunner<Row, Row> runner = createSingleUDFRunner();
 
-		// check input coder
-		Coder<Row> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof RowCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof RowSerializer);
 
-		Coder<?>[] inputFieldCoders = ((RowCoder) inputCoder).getFieldCoders();
-		assertEquals(1, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
+		assertEquals(1, ((RowSerializer) inputTypeSerializer).getArity());
 
-		// check output coder
-		Coder<Row> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof RowCoder);
-		Coder<?>[] outputFieldCoders = ((RowCoder) outputCoder).getFieldCoders();
-		assertEquals(1, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof RowSerializer);
+		assertEquals(1, ((RowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForMultipleUDFs() {
 		final AbstractPythonScalarFunctionRunner<Row, Row> runner = createMultipleUDFRunner();
 
-		// check input coder
-		Coder<Row> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof RowCoder);
-
-		Coder<?>[] inputFieldCoders = ((RowCoder) inputCoder).getFieldCoders();
-		assertEquals(3, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[2] instanceof VarLongCoder);
-
-		// check output coder
-		Coder<Row> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof RowCoder);
-		Coder<?>[] outputFieldCoders = ((RowCoder) outputCoder).getFieldCoders();
-		assertEquals(2, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[1] instanceof VarLongCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof RowSerializer);
+
+		assertEquals(3, ((RowSerializer) inputTypeSerializer).getArity());
+
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof RowSerializer);
+		assertEquals(2, ((RowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Test
 	public void testInputOutputDataTypeConstructedProperlyForChainedUDFs() {
 		final AbstractPythonScalarFunctionRunner<Row, Row> runner = createChainedUDFRunner();
 
-		// check input coder
-		Coder<Row> inputCoder = runner.getInputCoder();
-		assertTrue(inputCoder instanceof RowCoder);
-
-		Coder<?>[] inputFieldCoders = ((RowCoder) inputCoder).getFieldCoders();
-		assertEquals(5, inputFieldCoders.length);
-		assertTrue(inputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[2] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[3] instanceof VarLongCoder);
-		assertTrue(inputFieldCoders[4] instanceof VarLongCoder);
-
-		// check output coder
-		Coder<Row> outputCoder = runner.getOutputCoder();
-		assertTrue(outputCoder instanceof RowCoder);
-		Coder<?>[] outputFieldCoders = ((RowCoder) outputCoder).getFieldCoders();
-		assertEquals(3, outputFieldCoders.length);
-		assertTrue(outputFieldCoders[0] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[1] instanceof VarLongCoder);
-		assertTrue(outputFieldCoders[2] instanceof VarLongCoder);
+		// check input TypeSerializer
+		TypeSerializer inputTypeSerializer = runner.getInputTypeSerializer();
+		assertTrue(inputTypeSerializer instanceof RowSerializer);
+
+		assertEquals(5, ((RowSerializer) inputTypeSerializer).getArity());
+
+		// check output TypeSerializer
+		TypeSerializer outputTypeSerializer = runner.getOutputTypeSerializer();
+		assertTrue(outputTypeSerializer instanceof RowSerializer);
+		assertEquals(3, ((RowSerializer) outputTypeSerializer).getArity());
 	}
 
 	@Test
@@ -221,7 +200,7 @@ public void testPythonScalarFunctionRunner() throws Exception {
 		// verify input element is hand over to input receiver
 		runner.processElement(Row.of(1L));
 		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		runner.getInputCoder().encode(Row.of(1L), baos);
+		runner.getInputTypeSerializer().serialize(Row.of(1L), new DataOutputViewStreamWrapper(baos));
 		verify(windowedValueReceiverSpy, times(1)).accept(argThat(
 			windowedValue ->
 				windowedValue.getWindows().equals(Collections.singletonList(GlobalWindow.INSTANCE)) &&
diff --git a/flink-python/src/test/java/org/apache/flink/table/functions/python/BeamTypeUtilsTest.java b/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java
similarity index 56%
rename from flink-python/src/test/java/org/apache/flink/table/functions/python/BeamTypeUtilsTest.java
rename to flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java
index 0f3f3083e214a..85c43de87c6f2 100644
--- a/flink-python/src/test/java/org/apache/flink/table/functions/python/BeamTypeUtilsTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/functions/python/PythonTypeUtilsTest.java
@@ -18,15 +18,17 @@
 
 package org.apache.flink.table.functions.python;
 
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.typeutils.runtime.RowSerializer;
 import org.apache.flink.fnexecution.v1.FlinkFnApi;
-import org.apache.flink.table.runtime.typeutils.BeamTypeUtils;
-import org.apache.flink.table.runtime.typeutils.coders.BaseRowCoder;
-import org.apache.flink.table.runtime.typeutils.coders.RowCoder;
+import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
+import org.apache.flink.table.runtime.typeutils.PythonTypeUtils;
 import org.apache.flink.table.types.logical.BigIntType;
+import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.table.types.logical.RowType;
+import org.apache.flink.table.types.logical.UnresolvedUserDefinedType;
+import org.apache.flink.util.ExceptionUtils;
 
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
 import org.junit.Test;
 
 import java.util.ArrayList;
@@ -36,34 +38,30 @@
 import static org.junit.Assert.assertTrue;
 
 /**
- * Tests for {@link BeamTypeUtils}.
+ * Tests for {@link PythonTypeUtils}.
  */
-public class BeamTypeUtilsTest {
+public class PythonTypeUtilsTest {
 
 	@Test
-	public void testLogicalTypeToCoder() {
+	public void testLogicalTypeToFlinkTypeSerializer() {
 		List<RowType.RowField> rowFields = new ArrayList<>();
 		rowFields.add(new RowType.RowField("f1", new BigIntType()));
 		RowType rowType = new RowType(rowFields);
-		Coder coder = BeamTypeUtils.toCoder(rowType);
-		assertTrue(coder instanceof RowCoder);
+		TypeSerializer rowSerializer = PythonTypeUtils.toFlinkTypeSerializer(rowType);
+		assertTrue(rowSerializer instanceof RowSerializer);
 
-		Coder<?>[] fieldCoders = ((RowCoder) coder).getFieldCoders();
-		assertEquals(1, fieldCoders.length);
-		assertTrue(fieldCoders[0] instanceof VarLongCoder);
+		assertEquals(1, ((RowSerializer) rowSerializer).getArity());
 	}
 
 	@Test
-	public void testLogicalTypeToBlinkCoder() {
+	public void testLogicalTypeToBlinkTypeSerializer() {
 		List<RowType.RowField> rowFields = new ArrayList<>();
 		rowFields.add(new RowType.RowField("f1", new BigIntType()));
 		RowType rowType = new RowType(rowFields);
-		Coder coder = BeamTypeUtils.toBlinkCoder(rowType);
-		assertTrue(coder instanceof BaseRowCoder);
+		TypeSerializer baseSerializer = PythonTypeUtils.toBlinkTypeSerializer(rowType);
+		assertTrue(baseSerializer instanceof BaseRowSerializer);
 
-		Coder<?>[] fieldCoders = ((BaseRowCoder) coder).getFieldCoders();
-		assertEquals(1, fieldCoders.length);
-		assertTrue(fieldCoders[0] instanceof VarLongCoder);
+		assertEquals(1, ((BaseRowSerializer) baseSerializer).getArity());
 	}
 
 	@Test
@@ -71,10 +69,21 @@ public void testLogicalTypeToProto() {
 		List<RowType.RowField> rowFields = new ArrayList<>();
 		rowFields.add(new RowType.RowField("f1", new BigIntType()));
 		RowType rowType = new RowType(rowFields);
-		FlinkFnApi.Schema.FieldType protoType = BeamTypeUtils.toProtoType(rowType);
+		FlinkFnApi.Schema.FieldType protoType = PythonTypeUtils.toProtoType(rowType);
 		FlinkFnApi.Schema schema = protoType.getRowSchema();
 		assertEquals(1, schema.getFieldsCount());
 		assertEquals("f1", schema.getFields(0).getName());
 		assertEquals(FlinkFnApi.Schema.TypeName.BIGINT, schema.getFields(0).getType().getTypeName());
 	}
+
+	@Test
+	public void testUnsupportedTypeSerializer() {
+		LogicalType logicalType = new UnresolvedUserDefinedType("", "", "");
+		String expectedTestException = "Python UDF doesn't support logical type ``.``.`` currently.";
+		try {
+			PythonTypeUtils.toFlinkTypeSerializer(logicalType);
+		} catch (Exception e) {
+			assertTrue(ExceptionUtils.findThrowableWithMessage(e, expectedTestException).isPresent());
+		}
+	}
 }
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/CoderTestBase.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/CoderTestBase.java
deleted file mode 100644
index 308fe5dd7412d..0000000000000
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/CoderTestBase.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.flink.table.runtime.typeutils.coders;
-
-import org.apache.flink.testutils.CustomEqualityMatcher;
-import org.apache.flink.testutils.DeeplyEqualsChecker;
-
-import org.apache.beam.sdk.coders.Coder;
-import org.junit.Test;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertThat;
-import static org.junit.Assert.fail;
-
-/**
- * Abstract test base for coders.
- */
-public abstract class CoderTestBase<T> {
-
-	private final DeeplyEqualsChecker checker;
-
-	protected CoderTestBase() {
-		this.checker = new DeeplyEqualsChecker();
-	}
-
-	protected CoderTestBase(DeeplyEqualsChecker checker) {
-		this.checker = checker;
-	}
-
-	protected abstract Coder<T> createCoder();
-
-	protected abstract T[] getTestData();
-
-	// --------------------------------------------------------------------------------------------
-
-	@Test
-	public void testEncodeDecode() {
-		try {
-			Coder<T> coder = getCoder();
-			T[] testData = getData();
-
-			ByteArrayOutputStream baos = new ByteArrayOutputStream();
-			for (T value : testData) {
-				coder.encode(value, baos);
-			}
-
-			ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
-			int num = 0;
-			while (bais.available() > 0) {
-				T deserialized = coder.decode(bais);
-
-				deepEquals("Decoded value if wrong.", testData[num], deserialized);
-				num++;
-			}
-
-			assertEquals("Wrong number of elements decoded.", testData.length, num);
-		}
-		catch (Exception e) {
-			System.err.println(e.getMessage());
-			e.printStackTrace();
-			fail("Exception in test: " + e.getMessage());
-		}
-	}
-
-	// --------------------------------------------------------------------------------------------
-
-	private void deepEquals(String message, T should, T is) {
-		assertThat(message, is, CustomEqualityMatcher.deeplyEquals(should).withChecker(checker));
-	}
-
-	private Coder<T> getCoder() {
-		Coder<T> coder = createCoder();
-		if (coder == null) {
-			throw new RuntimeException("Test case corrupt. Returns null as coder.");
-		}
-		return coder;
-	}
-
-	private T[] getData() {
-		T[] data = getTestData();
-		if (data == null) {
-			throw new RuntimeException("Test case corrupt. Returns null as test data.");
-		}
-		return data;
-	}
-}
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoderTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializerTest.java
similarity index 71%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoderTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializerTest.java
index 7b5e281a3a5b1..7d894b56f86a5 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/BaseRowCoderTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/BaseRowSerializerTest.java
@@ -16,9 +16,12 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.typeutils.coders;
+package org.apache.flink.table.runtime.typeutils.serializers.python;
 
 import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.common.typeutils.SerializerTestBase;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.common.typeutils.base.LongSerializer;
 import org.apache.flink.table.dataformat.BaseRow;
 import org.apache.flink.table.dataformat.BinaryRow;
 import org.apache.flink.table.runtime.typeutils.BaseRowSerializer;
@@ -27,21 +30,17 @@
 import org.apache.flink.table.types.logical.LogicalType;
 import org.apache.flink.testutils.DeeplyEqualsChecker;
 
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
-
 import java.util.Objects;
 
 /**
- * Tests for {@link BaseRowCoder}.
+ * Test for {@link org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializer}.
  */
-public class BaseRowCoderTest extends CoderTestBase<BaseRow> {
-
-	public BaseRowCoderTest() {
+public class BaseRowSerializerTest extends SerializerTestBase<BaseRow> {
+	public BaseRowSerializerTest() {
 		super(
 			new DeeplyEqualsChecker()
 				.withCustomCheck(
-				(o1, o2) -> o1 instanceof BaseRow && o2 instanceof BaseRow,
+					(o1, o2) -> o1 instanceof BaseRow && o2 instanceof BaseRow,
 					(o1, o2, checker) -> {
 						LogicalType[] fieldTypes = new LogicalType[] {
 							new BigIntType(),
@@ -58,24 +57,29 @@ public BaseRowCoderTest() {
 	}
 
 	@Override
-	protected Coder<BaseRow> createCoder() {
-		Coder<?>[] fieldCoders = {
-			VarLongCoder.of(),
-			VarLongCoder.of()
+	protected TypeSerializer<BaseRow> createSerializer() {
+		TypeSerializer<?>[] fieldTypeSerializers = {
+			LongSerializer.INSTANCE,
+			LongSerializer.INSTANCE
 		};
 
 		LogicalType[] fieldTypes = {
 			new BigIntType(),
 			new BigIntType()
 		};
-		return new BaseRowCoder(fieldCoders, fieldTypes);
+		return new org.apache.flink.table.runtime.typeutils.serializers.python.BaseRowSerializer(
+			fieldTypes,
+			fieldTypeSerializers);
 	}
 
 	@Override
-	protected BaseRow[] getTestData() {
-		BaseRow row1 = StreamRecordUtils.baserow(null, 1L);
-		BinaryRow row2 = StreamRecordUtils.binaryrow(1L, null);
-		return new BaseRow[]{row1, row2};
+	protected int getLength() {
+		return -1;
+	}
+
+	@Override
+	protected Class<BaseRow> getTypeClass() {
+		return BaseRow.class;
 	}
 
 	private static boolean deepEqualsBaseRow(
@@ -89,4 +93,11 @@ private static boolean deepEqualsBaseRow(
 
 		return Objects.equals(row1, row2);
 	}
+
+	@Override
+	protected BaseRow[] getTestData() {
+		BaseRow row1 = StreamRecordUtils.baserow(null, 1L);
+		BinaryRow row2 = StreamRecordUtils.binaryrow(1L, null);
+		return new BaseRow[]{row1, row2};
+	}
 }
diff --git a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/RowCoderTest.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializerTest.java
similarity index 57%
rename from flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/RowCoderTest.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializerTest.java
index e8b64830b67ef..1a22c72c02300 100644
--- a/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/coders/RowCoderTest.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/DateSerializerTest.java
@@ -16,36 +16,35 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.typeutils.coders;
+package org.apache.flink.table.runtime.typeutils.serializers.python;
 
-import org.apache.flink.types.Row;
+import org.apache.flink.api.common.typeutils.SerializerTestBase;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
 
-import org.apache.beam.sdk.coders.Coder;
-import org.apache.beam.sdk.coders.VarLongCoder;
+import java.sql.Date;
 
 /**
- * Tests for {@link RowCoder}.
+ * Test for {@link DateSerializer}.
  */
-public class RowCoderTest extends CoderTestBase<Row> {
+public class DateSerializerTest extends SerializerTestBase<Date> {
 
 	@Override
-	protected Coder<Row> createCoder() {
-		Coder<?>[] fieldCoders = {
-			VarLongCoder.of(),
-			VarLongCoder.of()};
-		return new RowCoder(fieldCoders);
+	protected TypeSerializer<Date> createSerializer() {
+		return DateSerializer.INSTANCE;
 	}
 
 	@Override
-	protected Row[] getTestData() {
-		Row row1 = new Row(2);
-		row1.setField(0, 1L);
-		row1.setField(1, -1L);
+	protected int getLength() {
+		return 4;
+	}
 
-		Row row2 = new Row(2);
-		row2.setField(0, 2L);
-		row2.setField(1, null);
+	@Override
+	protected Class<Date> getTypeClass() {
+		return Date.class;
+	}
 
-		return new Row[]{row1, row2};
+	@Override
+	protected Date[] getTestData() {
+		return new Date[]{Date.valueOf("2014-09-13")};
 	}
 }
diff --git a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataOutputView.java b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializerTest.java
similarity index 56%
rename from flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataOutputView.java
rename to flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializerTest.java
index 6128044648194..e5b6f8bd1604f 100644
--- a/flink-python/src/main/java/org/apache/flink/table/runtime/typeutils/coders/ReusableDataOutputView.java
+++ b/flink-python/src/test/java/org/apache/flink/table/runtime/typeutils/serializers/python/StringSerializerTest.java
@@ -16,26 +16,33 @@
  * limitations under the License.
  */
 
-package org.apache.flink.table.runtime.typeutils.coders;
+package org.apache.flink.table.runtime.typeutils.serializers.python;
 
-import org.apache.flink.annotation.Internal;
-import org.apache.flink.core.memory.DataOutputView;
-import org.apache.flink.core.memory.DataOutputViewStreamWrapper;
-
-import java.io.OutputStream;
+import org.apache.flink.api.common.typeutils.SerializerTestBase;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
 
 /**
- * An implementation of {@link DataOutputView} that allows the instance
- * to be re-used with another underlying output stream.
+ * Test for {@link StringSerializer}.
  */
-@Internal
-public class ReusableDataOutputView extends DataOutputViewStreamWrapper {
+public class StringSerializerTest extends SerializerTestBase<String> {
+
+	@Override
+	protected TypeSerializer<String> createSerializer() {
+		return StringSerializer.INSTANCE;
+	}
+
+	@Override
+	protected int getLength() {
+		return -1;
+	}
 
-	public ReusableDataOutputView() {
-		super(null);
+	@Override
+	protected Class<String> getTypeClass() {
+		return String.class;
 	}
 
-	public void reset(OutputStream out) {
-		this.out = out;
+	@Override
+	protected String[] getTestData() {
+		return new String[]{"pyflink", "flink"};
 	}
 }
