diff --git a/docs/apis/streaming/connectors/cassandra.md b/docs/apis/streaming/connectors/cassandra.md
new file mode 100644
index 0000000000000..5773b35997946
--- /dev/null
+++ b/docs/apis/streaming/connectors/cassandra.md
@@ -0,0 +1,103 @@
+---
+title: "Apache Cassandra Connector"
+
+# Sub-level navigation
+sub-nav-group: streaming
+sub-nav-parent: connectors
+sub-nav-pos: 1
+sub-nav-title: Cassandra
+---
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+This connector provides a sink that writes data into a [Cassandra](https://cassandra.apache.org/) database.
+
+The Flink Cassandra sink integrates with Flink's checkpointing mechanism to provide
+at-least-once processing semantics. To achieve that, Flink buffers incoming records
+and commits them only when a checkpoint completes.
+
+To use this connector, add the following dependency to your project:
+
+{% highlight xml %}
+<dependency>
+  <groupId>org.apache.flink</groupId>
+  <artifactId>flink-connector-cassandra{{ site.scala_version_suffix }}</artifactId>
+  <version>{{site.version }}</version>
+</dependency>
+{% endhighlight %}
+
+Note that the streaming connectors are currently not part of the binary distribution. See how to link with them for cluster execution [here]({{ site.baseurl}}/apis/cluster_execution.html#linking-with-modules-not-contained-in-the-binary-distribution).
+
+#### Installing Cassandra
+Follow the instructions from the [Cassandra Getting Started page](http://wiki.apache.org/cassandra/GettingStarted).
+
+#### Cassandra Sink
+
+Flink's Cassandra sink is called `CassandraAtLeastOnceSink`.
+
+The constructor accepts the following arguments:
+
+1. The Host address
+2. query to create a new table to write into (optional)
+3. query to insert data the a table
+4. checkpoint committer
+5. input serializer
+
+A checkpoint committer stores additional information about completed checkpoints
+in some resource. You can use a `CassandraCommitter` to store these in a separate
+table in cassandra. Note that this table will NOT be cleaned up by Flink.
+
+The CassandraCommitter constructor accepts the following arguments:
+1. Host address
+2. Keyspace
+3. Table name
+
+The CassandraAtLeastOnceSink is implemented as a custom operator
+instead of a sink, and as such has to be used in a transform() call.
+
+Example:
+
+<div class="codetabs" markdown="1">
+<div data-lang="java" markdown="1">
+{% highlight java %}
+input.transform(
+  "Cassandra Sink",
+  null,
+  new CassandraAtLeastOnceSink<Tuple2<String, Integer>>(
+    "127.0.0.1",
+    "CREATE TABLE example.values (id text PRIMARY KEY, counter int);",
+    "INSERT INTO example.values (id, counter) VALUES (?, ?);",
+    new CassandraCommitter("127.0.0.1", "example", "checkpoints"),
+    input.getType().createSerializer(env.getConfig())));
+{% endhighlight %}
+</div>
+<div data-lang="scala" markdown="1">
+{% highlight scala %}
+input.transform(
+  "Cassandra Sink",
+  null,
+  new CassandraAtLeastOnceSink[(String, Integer)](
+    "127.0.0.1",
+    "CREATE TABLE example.values (id text PRIMARY KEY, counter int);",
+    "INSERT INTO example.values (id, counter) VALUES (?, ?);",
+    new CassandraCommitter("127.0.0.1", "example", "checkpoints"),
+    input.getType().createSerializer(env.getConfig())));
+{% endhighlight %}
+</div>
+</div>
diff --git a/docs/apis/streaming/connectors/index.md b/docs/apis/streaming/connectors/index.md
index 378be6cf700da..409a1016bc073 100644
--- a/docs/apis/streaming/connectors/index.md
+++ b/docs/apis/streaming/connectors/index.md
@@ -35,6 +35,7 @@ Currently these systems are supported:
  * [Hadoop FileSystem](http://hadoop.apache.org) (sink)
  * [RabbitMQ](http://www.rabbitmq.com/) (sink/source)
  * [Twitter Streaming API](https://dev.twitter.com/docs/streaming-apis) (source)
+ * [Apache Cassandra](https://cassandra.apache.org/) (sink)
 
 To run an application using one of these connectors, additional third party
 components are usually required to be installed and launched, e.g. the servers
diff --git a/docs/apis/streaming/fault_tolerance.md b/docs/apis/streaming/fault_tolerance.md
index 4b91c61a92e58..9c30129c8efa5 100644
--- a/docs/apis/streaming/fault_tolerance.md
+++ b/docs/apis/streaming/fault_tolerance.md
@@ -175,6 +175,11 @@ state updates) of Flink coupled with bundled sinks:
         <td>at least once</td>
         <td></td>
     </tr>
+    <tr>
+        <td>Cassandra sink</td>
+        <td>at least once</td>
+        <td></td>
+    </tr>
     <tr>
         <td>File sinks</td>
         <td>at least once</td>
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/ResultPartitionWriter.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/ResultPartitionWriter.java
index 79c21c6a5a601..cfab34d829ef4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/ResultPartitionWriter.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/api/writer/ResultPartitionWriter.java
@@ -37,7 +37,7 @@
  * The {@link ResultPartitionWriter} is the runtime API for producing results. It
  * supports two kinds of data to be sent: buffers and events.
  */
-public final class ResultPartitionWriter implements EventListener<TaskEvent> {
+public class ResultPartitionWriter implements EventListener<TaskEvent> {
 
 	private final ResultPartition partition;
 
diff --git a/flink-streaming-connectors/flink-connector-cassandra/pom.xml b/flink-streaming-connectors/flink-connector-cassandra/pom.xml
new file mode 100644
index 0000000000000..6d9eab026db3c
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/pom.xml
@@ -0,0 +1,138 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+  http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+		 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+	<modelVersion>4.0.0</modelVersion>
+
+	<parent>
+		<groupId>org.apache.flink</groupId>
+		<artifactId>flink-streaming-connectors</artifactId>
+		<version>1.0-SNAPSHOT</version>
+		<relativePath>..</relativePath>
+	</parent>
+
+	<artifactId>flink-connector-cassandra_2.10</artifactId>
+	<name>flink-connector-cassandra</name>
+
+	<packaging>jar</packaging>
+
+	<build>
+		<plugins>
+			<plugin>
+				<groupId>org.apache.maven.plugins</groupId>
+				<artifactId>maven-surefire-plugin</artifactId>
+				<configuration>
+					<reuseForks>true</reuseForks>
+					<forkCount>1</forkCount>
+				</configuration>
+			</plugin>
+		</plugins>
+	</build>
+
+	<!-- Allow users to pass custom connector versions -->
+	<properties>
+		<cassandra.version>2.2.0</cassandra.version>
+		<driver.version>3.0.0</driver.version>
+	</properties>
+
+	<dependencies>
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-streaming-java_2.10</artifactId>
+			<version>${project.version}</version>
+		</dependency>
+		<dependency>
+			<groupId>com.datastax.cassandra</groupId>
+			<artifactId>cassandra-driver-core</artifactId>
+			<version>${driver.version}</version>
+			<exclusions>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>log4j-over-slf4j</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>com.google.guava</groupId>
+					<artifactId>guava</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>ch.qos.logback</groupId>
+					<artifactId>logback-classic</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+		<dependency>
+			<groupId>com.datastax.cassandra</groupId>
+			<artifactId>cassandra-driver-mapping</artifactId>
+			<version>${driver.version}</version>
+			<exclusions>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>log4j-over-slf4j</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>com.google.guava</groupId>
+					<artifactId>guava</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>ch.qos.logback</groupId>
+					<artifactId>logback-classic</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+		<dependency>
+			<groupId>com.google.guava</groupId>
+			<artifactId>guava</artifactId>
+			<version>${guava.version}</version>
+		</dependency>
+
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-runtime_2.10</artifactId>
+			<version>${project.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+		</dependency>
+		<dependency>
+			<groupId>org.apache.flink</groupId>
+			<artifactId>flink-streaming-java_2.10</artifactId>
+			<version>${project.version}</version>
+			<scope>test</scope>
+			<type>test-jar</type>
+		</dependency>
+		<dependency>
+			<groupId>org.apache.cassandra</groupId>
+			<artifactId>cassandra-all</artifactId>
+			<version>${cassandra.version}</version>
+			<scope>test</scope>
+			<exclusions>
+				<exclusion>
+					<groupId>org.slf4j</groupId>
+					<artifactId>log4j-over-slf4j</artifactId>
+				</exclusion>
+				<exclusion>
+					<groupId>ch.qos.logback</groupId>
+					<artifactId>logback-classic</artifactId>
+				</exclusion>
+			</exclusions>
+		</dependency>
+	</dependencies>
+</project>
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSink.java b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSink.java
new file mode 100644
index 0000000000000..3dcdf77a8d23f
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSink.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.cassandra;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.PreparedStatement;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.ResultSetFuture;
+import com.datastax.driver.core.Session;
+import com.google.common.util.concurrent.FutureCallback;
+import com.google.common.util.concurrent.Futures;
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.api.java.tuple.Tuple;
+import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;
+import org.apache.flink.streaming.runtime.operators.GenericAtLeastOnceSink;
+
+/**
+ * Sink that emits its input elements into a Cassandra database. This sink is integrated with the checkpointing
+ * mechanism to provide near at-least-once semantics.
+ * <p/>
+ * Incoming records are stored within a {@link org.apache.flink.runtime.state.AbstractStateBackend}, and only committed if a
+ * checkpoint is completed. Should a job fail, while data is being committed, no exactly once guarantee can be made.
+ *
+ * @param <IN> Type of the elements emitted by this sink
+ */
+public class CassandraAtLeastOnceSink<IN extends Tuple> extends GenericAtLeastOnceSink<IN> {
+	private final String host;
+	private final String insertQuery;
+
+	private transient Cluster cluster;
+	private transient Session session;
+	private transient PreparedStatement preparedStatement;
+
+	private transient Throwable exception = null;
+	private transient FutureCallback<ResultSet> callback;
+
+	public CassandraAtLeastOnceSink(String host, String insertQuery, CheckpointCommitter committer, TypeSerializer<IN> serializer) {
+		super(committer, serializer);
+		if (host == null) {
+			throw new IllegalArgumentException("Host argument must not be null.");
+		}
+		if (insertQuery == null) {
+			throw new IllegalArgumentException("Insert query argument must not be null.");
+		}
+		this.host = host;
+		this.insertQuery = insertQuery;
+	}
+
+	@Override
+	public void close() throws Exception {
+		super.close();
+		try {
+			session.close();
+		} catch (Exception e) {
+			LOG.error("Error while closing session.", e);
+		}
+		try {
+			cluster.close();
+		} catch (Exception e) {
+			LOG.error("Error while closing cluster.", e);
+		}
+	}
+
+	@Override
+	public void open() throws Exception {
+		super.open();
+		this.callback = new FutureCallback<ResultSet>() {
+			@Override
+			public void onSuccess(ResultSet resultSet) {
+			}
+
+			@Override
+			public void onFailure(Throwable throwable) {
+				exception = throwable;
+			}
+		};
+		cluster = Cluster.builder().addContactPoint(host).build();
+		session = cluster.connect();
+		preparedStatement = session.prepare(insertQuery);
+	}
+
+	@Override
+	protected void sendValue(Iterable<IN> values) throws Exception {
+		//verify that no query failed until now
+		if (exception != null) {
+			throw new Exception(exception);
+		}
+		//set values for prepared statement
+		for (IN value : values) {
+			Object[] fields = new Object[value.getArity()];
+			for (int x = 0; x < value.getArity(); x++) {
+				fields[x] = value.getField(x);
+			}
+			//insert values and send to cassandra
+			ResultSetFuture result = session.executeAsync(preparedStatement.bind(fields));
+			//add callback to detect errors
+			Futures.addCallback(result, callback);
+		}
+	}
+}
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraCommitter.java b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraCommitter.java
new file mode 100644
index 0000000000000..ddb65bbe01464
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/CassandraCommitter.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.cassandra;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import org.apache.flink.streaming.runtime.operators.CheckpointCommitter;
+
+/**
+ * CheckpointCommitter that saves information about completed checkpoints within a separate table in a cassandra
+ * database.
+ *
+ * Entries are in the form |operator_id | subtask_id | last_completed_checkpoint|
+ */
+public class CassandraCommitter extends CheckpointCommitter {
+	private final String host;
+	private final String keyspace;
+	private final String table;
+
+	private transient Cluster cluster;
+	private transient Session session;
+
+	public CassandraCommitter(String host, String keyspace, String table) {
+		this.host = host;
+		this.keyspace = keyspace;
+		this.table = table;
+	}
+
+	@Override
+	public void open() throws Exception {
+		cluster = Cluster.builder().addContactPoint(host).build();
+		session = cluster.connect();
+
+		session.execute("CREATE KEYSPACE IF NOT EXISTS " + keyspace + " with replication={'class':'SimpleStrategy', 'replication_factor':3};");
+		session.execute("CREATE TABLE IF NOT EXISTS " + keyspace + "." + table + " (sink_id text, sub_id int, checkpoint_id bigint, PRIMARY KEY (sink_id, sub_id));");
+		session.execute("INSERT INTO " + keyspace + "." + table + " (sink_id, sub_id, checkpoint_id) values ('" + operatorId + "', " + subtaskId + ", " + -1 + ");");
+	}
+
+	@Override
+	public void close() throws Exception {
+		session.executeAsync("DELETE FROM " + keyspace + "." + table + " where sink_id='" + operatorId + "' and sub_id=" + subtaskId + ";");
+		try {
+			session.close();
+		} catch (Exception e) {
+			LOG.error("Error while closing session.", e);
+		}
+		try {
+			cluster.close();
+		} catch (Exception e) {
+			LOG.error("Error while closing cluster.", e);
+		}
+	}
+
+	@Override
+	public void commitCheckpoint(long checkpointID) {
+		SimpleStatement s = new SimpleStatement("UPDATE " + keyspace + "." + table + " set checkpoint_id=" + checkpointID + " where sink_id='" + operatorId + "' and sub_id=" + subtaskId + ";");
+		s.setConsistencyLevel(ConsistencyLevel.ALL);
+		session.execute(s);
+	}
+
+	@Override
+	public boolean isCheckpointCommitted(long checkpointID) {
+		SimpleStatement s = new SimpleStatement("SELECT checkpoint_id FROM " + keyspace + "." + table + " where sink_id='" + operatorId + "' and sub_id=" + subtaskId + ";");
+		s.setConsistencyLevel(ConsistencyLevel.ALL);
+		long lastId = session.execute(s).one().getLong("checkpoint_id");
+		return checkpointID <= lastId;
+	}
+}
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/example/CassandraAtLeastOnceSinkExample.java b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/example/CassandraAtLeastOnceSinkExample.java
new file mode 100644
index 0000000000000..3157fc305a539
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/main/java/org/apache/flink/streaming/connectors/cassandra/example/CassandraAtLeastOnceSinkExample.java
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.cassandra.example;
+
+import org.apache.flink.api.java.tuple.Tuple2;
+import org.apache.flink.runtime.state.filesystem.FsStateBackend;
+import org.apache.flink.streaming.api.checkpoint.Checkpointed;
+import org.apache.flink.streaming.api.datastream.DataStream;
+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
+import org.apache.flink.streaming.api.functions.source.SourceFunction;
+import org.apache.flink.streaming.connectors.cassandra.CassandraAtLeastOnceSink;
+import org.apache.flink.streaming.connectors.cassandra.CassandraCommitter;
+
+import java.util.UUID;
+
+public class CassandraAtLeastOnceSinkExample {
+	public static void main(String[] args) throws Exception {
+
+		class MySource implements SourceFunction<Tuple2<String, Integer>>, Checkpointed<Integer> {
+			private int counter = 0;
+			private boolean stop = false;
+
+			@Override
+			public void run(SourceContext<Tuple2<String, Integer>> ctx) throws Exception {
+				while (!stop) {
+					Thread.sleep(50);
+					ctx.collect(new Tuple2<>("" + UUID.randomUUID(), counter));
+					counter++;
+				}
+			}
+
+			@Override
+			public void cancel() {
+				stop = true;
+			}
+
+			@Override
+			public Integer snapshotState(long checkpointId, long checkpointTimestamp) throws Exception {
+				return counter;
+			}
+
+			@Override
+			public void restoreState(Integer state) throws Exception {
+				this.counter = state;
+			}
+		}
+
+		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
+		env.setParallelism(1);
+		env.enableCheckpointing(1000);
+		env.setNumberOfExecutionRetries(1);
+		env.setStateBackend(new FsStateBackend("file:///" + System.getProperty("java.io.tmpdir") + "/flink/backend"));
+
+		DataStream<Tuple2<String, Integer>> input = env.addSource(new MySource());
+		input.transform(
+			"Cassandra Sink",
+			null,
+			new CassandraAtLeastOnceSink<>(
+				"127.0.0.1",
+				"INSERT INTO example.values (id, counter) VALUES (?, ?);",
+				new CassandraCommitter("127.0.0.1", "example", "checkpoints"),
+				input.getType().createSerializer(env.getConfig())));
+
+		env.execute();
+	}
+}
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSinkTest.java b/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSinkTest.java
new file mode 100644
index 0000000000000..76e79b3defab6
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/test/java/org/apache/flink/streaming/connectors/cassandra/CassandraAtLeastOnceSinkTest.java
@@ -0,0 +1,203 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.connectors.cassandra;
+
+import com.datastax.driver.core.Cluster;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Row;
+import com.datastax.driver.core.Session;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.java.tuple.Tuple3;
+import org.apache.flink.api.java.typeutils.TupleTypeInfo;
+import org.apache.flink.api.java.typeutils.TypeExtractor;
+import org.apache.flink.runtime.testutils.CommonTestUtils;
+import org.apache.flink.streaming.runtime.operators.AtLeastOnceSinkTestBase;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTask;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+
+import java.io.BufferedWriter;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Scanner;
+import java.util.UUID;
+
+public class CassandraAtLeastOnceSinkTest extends AtLeastOnceSinkTestBase<Tuple3<String, Integer, Integer>, CassandraAtLeastOnceSink<Tuple3<String, Integer, Integer>>> {
+	private static File tmpDir;
+
+	private static final boolean EMBEDDED = true;
+	private static EmbeddedCassandraService cassandra;
+	private static Cluster cluster;
+	private static Session session;
+
+	private static final String CREATE_KEYSPACE_QUERY = "CREATE KEYSPACE flink WITH replication= {'class':'SimpleStrategy', 'replication_factor':1};";
+	private static final String DROP_KEYSPACE_QUERY = "DROP KEYSPACE flink;";
+	private static final String CREATE_TABLE_QUERY = "CREATE TABLE flink.test (id text PRIMARY KEY, counter int, batch_id int);";
+	private static final String CLEAR_TABLE_QUERY = "TRUNCATE flink.test;";
+	private static final String INSERT_DATA_QUERY = "INSERT INTO flink.test (id, counter, batch_id) VALUES (?, ?, ?)";
+	private static final String SELECT_DATA_QUERY = "SELECT * FROM flink.test;";
+
+	private static class EmbeddedCassandraService {
+		CassandraDaemon cassandraDaemon;
+
+		public void start() throws IOException {
+			this.cassandraDaemon = new CassandraDaemon();
+			this.cassandraDaemon.init(null);
+			this.cassandraDaemon.start();
+		}
+
+		public void stop() {
+			this.cassandraDaemon.stop();
+		}
+	}
+
+	@BeforeClass
+	public static void startCassandra() throws IOException {
+		//generate temporary files
+		tmpDir = CommonTestUtils.createTempDirectory();
+		ClassLoader classLoader = CassandraAtLeastOnceSink.class.getClassLoader();
+		File file = new File(classLoader.getResource("cassandra.yaml").getFile());
+		File tmp = new File(tmpDir.getAbsolutePath() + File.separator + "cassandra.yaml");
+		tmp.createNewFile();
+		BufferedWriter b = new BufferedWriter(new FileWriter(tmp));
+
+		//copy cassandra.yaml; inject absolute paths into cassandra.yaml
+		Scanner scanner = new Scanner(file);
+		while (scanner.hasNextLine()) {
+			String line = scanner.nextLine();
+			line = line.replace("$PATH", "'" + tmp.getParentFile());
+			b.write(line + "\n");
+			b.flush();
+		}
+		scanner.close();
+
+
+		// Tell cassandra where the configuration files are.
+		// Use the test configuration file.
+		System.setProperty("cassandra.config", "file:" + File.separator + File.separator + File.separator + tmp.getAbsolutePath());
+
+		if (EMBEDDED) {
+			cassandra = new EmbeddedCassandraService();
+			cassandra.start();
+		}
+
+		cluster = Cluster.builder().addContactPoint("127.0.0.1").build();
+		session = cluster.connect();
+
+		session.execute(CREATE_KEYSPACE_QUERY);
+		session.execute(CREATE_TABLE_QUERY);
+	}
+
+	@After
+	public void deleteSchema() throws Exception {
+		session.executeAsync(CLEAR_TABLE_QUERY);
+	}
+
+	@AfterClass
+	public static void closeCassandra() {
+		session.executeAsync(DROP_KEYSPACE_QUERY);
+		session.close();
+		cluster.close();
+		if (EMBEDDED) {
+			cassandra.stop();
+		}
+		tmpDir.delete();
+	}
+
+	@Override
+	protected CassandraAtLeastOnceSink<Tuple3<String, Integer, Integer>> createSink() {
+		return new CassandraAtLeastOnceSink<>(
+			"127.0.0.1",
+			INSERT_DATA_QUERY,
+			new CassandraCommitter("127.0.0.1", "flink", "checkpoints"),
+			TypeExtractor.getForObject(new Tuple3<>("", 0, 0)).createSerializer(new ExecutionConfig()));
+	}
+
+	@Override
+	protected TupleTypeInfo<Tuple3<String, Integer, Integer>> createTypeInfo() {
+		return TupleTypeInfo.getBasicTupleTypeInfo(String.class, Integer.class, Integer.class);
+	}
+
+	@Override
+	protected Tuple3<String, Integer, Integer> generateValue(int counter, int checkpointID) {
+		return new Tuple3<>("" + UUID.randomUUID(), counter, checkpointID);
+	}
+
+	@Override
+	protected void verifyResultsIdealCircumstances(
+		OneInputStreamTaskTestHarness<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> harness,
+		OneInputStreamTask<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> task,
+		CassandraAtLeastOnceSink<Tuple3<String, Integer, Integer>> sink) {
+
+		ResultSet result = session.execute(SELECT_DATA_QUERY);
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Row s : result) {
+			list.remove(new Integer(s.getInt("counter")));
+		}
+		Assert.assertTrue("The following ID's were not found in the ResultSet: " + list.toString(), list.isEmpty());
+	}
+
+	@Override
+	protected void verifyResultsDataPersistenceUponMissedNotify(
+		OneInputStreamTaskTestHarness<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> harness,
+		OneInputStreamTask<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> task,
+		CassandraAtLeastOnceSink<Tuple3<String, Integer, Integer>> sink) {
+
+		ResultSet result = session.execute(SELECT_DATA_QUERY);
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Row s : result) {
+			list.remove(new Integer(s.getInt("counter")));
+		}
+		Assert.assertTrue("The following ID's were not found in the ResultSet: " + list.toString(), list.isEmpty());
+	}
+
+	@Override
+	protected void verifyResultsDataDiscardingUponRestore(
+		OneInputStreamTaskTestHarness<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> harness,
+		OneInputStreamTask<Tuple3<String, Integer, Integer>, Tuple3<String, Integer, Integer>> task,
+		CassandraAtLeastOnceSink<Tuple3<String, Integer, Integer>> sink) {
+
+		ResultSet result = session.execute(SELECT_DATA_QUERY);
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 20; x++) {
+			list.add(x);
+		}
+		for (int x = 41; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Row s : result) {
+			list.remove(new Integer(s.getInt("counter")));
+		}
+		Assert.assertTrue("The following ID's were not found in the ResultSet: " + list.toString(), list.isEmpty());
+	}
+}
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/cassandra.yaml b/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/cassandra.yaml
new file mode 100644
index 0000000000000..6cd7d4e2c734d
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/cassandra.yaml
@@ -0,0 +1,40 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+cluster_name: 'Test Cluster'
+commitlog_sync: 'periodic'
+commitlog_sync_period_in_ms: 10000
+partitioner: 'org.apache.cassandra.dht.RandomPartitioner'
+endpoint_snitch: 'org.apache.cassandra.locator.SimpleSnitch'
+commitlog_directory: $PATH\commit'
+data_file_directories:
+    - $PATH\data'
+saved_caches_directory: $PATH\cache'
+listen_address: '127.0.0.1'
+seed_provider:
+    - class_name: 'org.apache.cassandra.locator.SimpleSeedProvider'
+      parameters:
+          - seeds: '127.0.0.1'
+native_transport_port: 9042
+start_native_transport: true
+
+read_request_timeout_in_ms: 60000
+range_request_timeout_in_ms: 60000
+write_request_timeout_in_ms: 40000
+cas_contention_timeout_in_ms: 3000
+truncate_request_timeout_in_ms: 60000
+request_timeout_in_ms: 60000
diff --git a/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/log4j-test.properties b/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/log4j-test.properties
new file mode 100644
index 0000000000000..6d70ebddbfdfa
--- /dev/null
+++ b/flink-streaming-connectors/flink-connector-cassandra/src/test/resources/log4j-test.properties
@@ -0,0 +1,29 @@
+################################################################################
+#  Licensed to the Apache Software Foundation (ASF) under one
+#  or more contributor license agreements.  See the NOTICE file
+#  distributed with this work for additional information
+#  regarding copyright ownership.  The ASF licenses this file
+#  to you under the Apache License, Version 2.0 (the
+#  "License"); you may not use this file except in compliance
+#  with the License.  You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+# limitations under the License.
+################################################################################
+
+log4j.rootLogger=ERROR, testlogger
+
+log4j.appender.testlogger=org.apache.log4j.ConsoleAppender
+log4j.appender.testlogger.target = System.err
+log4j.appender.testlogger.layout=org.apache.log4j.PatternLayout
+log4j.appender.testlogger.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n
+
+# suppress the irrelevant (wrong) warnings from the netty channel handler
+log4j.logger.org.jboss.netty.channel.DefaultChannelPipeline=ERROR, testlogger
+
+
diff --git a/flink-streaming-connectors/pom.xml b/flink-streaming-connectors/pom.xml
index fceae25f9de80..1249bf24a0b1a 100644
--- a/flink-streaming-connectors/pom.xml
+++ b/flink-streaming-connectors/pom.xml
@@ -44,6 +44,7 @@ under the License.
 		<module>flink-connector-rabbitmq</module>
 		<module>flink-connector-twitter</module>
 		<module>flink-connector-nifi</module>
+		<module>flink-connector-cassandra</module>
 	</modules>
 
 	<!-- See main pom.xml for explanation of profiles -->
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/CheckpointCommitter.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/CheckpointCommitter.java
new file mode 100644
index 0000000000000..94aa0302ef274
--- /dev/null
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/CheckpointCommitter.java
@@ -0,0 +1,97 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.runtime.operators;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.Serializable;
+
+/**
+ * This class is used to save information about which sink operator instance has committed checkpoints to a backend.
+ * <p/>
+ * The current checkpointing mechanism is ill-suited for sinks relying on backends that do not support roll-backs.
+ * When dealing with such a system, while trying to get near exactly-once semantics, one may neither commit data while
+ * creating the snapshot (since another sink instance may fail, leading to a replay on the same data) nor when receiving
+ * a checkpoint-complete notification (since a subsequent failure would leave us with no knowledge as to whether data
+ * was committed or not).
+ * <p/>
+ * A CheckpointCommitter can be used to solve the second problem by saving whether an instance committed all data
+ * belonging to a checkpoint. This data must be stored in a backend that is persistent across retries (which rules
+ * out Flink's state mechanism) and accessible from all machines, like a database or distributed file.
+ * <p/>
+ * There is no mandate as to how the resource is shared; there may be one resource for all Flink jobs, or one for
+ * each job/operator/-instance separately. This implies that the resource must not be cleaned up by the system itself,
+ * and as such should kept as small as possible.
+ */
+public abstract class CheckpointCommitter implements Serializable {
+	protected static final Logger LOG = LoggerFactory.getLogger(CheckpointCommitter.class);
+	protected String operatorId;
+	protected int subtaskId;
+
+	/**
+	 * Internally used to set the operator ID after instantiation.
+	 *
+	 * @param id
+	 * @throws Exception
+	 */
+	public void setOperatorId(String id) throws Exception {
+		this.operatorId = id;
+	}
+
+	/**
+	 * Internally used to set the operator subtask ID after instantiation.
+	 *
+	 * @param id
+	 * @throws Exception
+	 */
+	public void setOperatorSubtaskId(int id) throws Exception {
+		this.subtaskId = id;
+	}
+
+	/**
+	 * Opens/connects to the resource, and possibly creates it beforehand.
+	 *
+	 * @throws Exception
+	 */
+	public abstract void open() throws Exception;
+
+	/**
+	 * Closes the resource/connection to it. The resource should generally still exist after this call.
+	 *
+	 * @throws Exception
+	 */
+	public abstract void close() throws Exception;
+
+	/**
+	 * Mark the given checkpoint as completed in the resource.
+	 *
+	 * @param checkpointID
+	 * @throws Exception
+	 */
+	public abstract void commitCheckpoint(long checkpointID) throws Exception;
+
+	/**
+	 * Checked the resource whether the given checkpoint was committed completely.
+	 *
+	 * @param checkpointID
+	 * @return true if the checkpoint was committed completely, false otherwise
+	 * @throws Exception
+	 */
+	public abstract boolean isCheckpointCommitted(long checkpointID) throws Exception;
+}
diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSink.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSink.java
new file mode 100644
index 0000000000000..ce6ad394e50ff
--- /dev/null
+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSink.java
@@ -0,0 +1,192 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.runtime.operators;
+
+import org.apache.flink.api.common.typeutils.TypeSerializer;
+import org.apache.flink.core.memory.DataInputView;
+import org.apache.flink.runtime.io.disk.InputViewIterator;
+import org.apache.flink.runtime.state.AbstractStateBackend;
+import org.apache.flink.runtime.state.StateHandle;
+import org.apache.flink.runtime.util.NonReusingMutableToRegularIteratorWrapper;
+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;
+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
+import org.apache.flink.streaming.api.watermark.Watermark;
+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.streaming.runtime.tasks.StreamTaskState;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.TreeMap;
+import java.util.UUID;
+
+/**
+ * Generic Sink that emits its input elements into an arbitrary backend. This sink is integrated with the checkpointing
+ * mechanism to provide near at-least-once semantics.
+ * <p/>
+ * Incoming records are stored within a {@link org.apache.flink.runtime.state.AbstractStateBackend}, and only committed if a
+ * checkpoint is completed. Should a job fail, while data is being committed, data will be committed twice.
+ *
+ * @param <IN> Type of the elements emitted by this sink
+ */
+public abstract class GenericAtLeastOnceSink<IN> extends AbstractStreamOperator<IN> implements OneInputStreamOperator<IN, IN> {
+	protected static final Logger LOG = LoggerFactory.getLogger(GenericAtLeastOnceSink.class);
+	private transient AbstractStateBackend.CheckpointStateOutputView out;
+	protected final TypeSerializer<IN> serializer;
+	protected final CheckpointCommitter committer;
+	protected final String id;
+
+	private ExactlyOnceState state = new ExactlyOnceState();
+
+	public GenericAtLeastOnceSink(CheckpointCommitter committer, TypeSerializer<IN> serializer) {
+		if (committer == null) {
+			throw new IllegalArgumentException("CheckpointCommitter argument must not be null.");
+		}
+		this.committer = committer;
+		this.serializer = serializer;
+		this.id = UUID.randomUUID().toString();
+	}
+
+	@Override
+	public void open() throws Exception {
+		committer.setOperatorId(id);
+		committer.setOperatorSubtaskId(getRuntimeContext().getIndexOfThisSubtask());
+		committer.open();
+	}
+
+	public void close() throws Exception {
+		committer.close();
+	}
+
+	/**
+	 * Saves a handle in the state.
+	 *
+	 * @param checkpointId
+	 * @throws IOException
+	 */
+	private void saveHandleInState(final long checkpointId) throws IOException {
+		//only add handle if a new OperatorState was created since the last snapshot
+		if (out != null) {
+			StateHandle<DataInputView> handle = out.closeAndGetHandle();
+			state.pendingHandles.put(checkpointId, handle);
+			out = null;
+		}
+	}
+
+	@Override
+	public StreamTaskState snapshotOperatorState(final long checkpointId, final long timestamp) throws Exception {
+		StreamTaskState taskState = super.snapshotOperatorState(checkpointId, timestamp);
+		saveHandleInState(checkpointId);
+		taskState.setFunctionState(state);
+		return taskState;
+	}
+
+	@Override
+	public void restoreState(StreamTaskState state, long recoveryTimestamp) throws Exception {
+		super.restoreState(state, recoveryTimestamp);
+		this.state = (ExactlyOnceState) state.getFunctionState();
+		out = null;
+	}
+
+	@Override
+	public void notifyOfCompletedCheckpoint(long checkpointId) throws Exception {
+		super.notifyOfCompletedCheckpoint(checkpointId);
+
+		synchronized (state.pendingHandles) {
+			Set<Long> pastCheckpointIds = state.pendingHandles.keySet();
+			Set<Long> checkpointsToRemove = new HashSet<>();
+			for (Long pastCheckpointId : pastCheckpointIds) {
+				if (pastCheckpointId <= checkpointId) {
+					if (!committer.isCheckpointCommitted(pastCheckpointId)) {
+						StateHandle<DataInputView> handle = state.pendingHandles.get(pastCheckpointId);
+						DataInputView in = handle.getState(getUserCodeClassloader());
+						sendValue(new NonReusingMutableToRegularIteratorWrapper<>(new InputViewIterator<>(in, serializer), serializer));
+						committer.commitCheckpoint(pastCheckpointId);
+					}
+					checkpointsToRemove.add(pastCheckpointId);
+				}
+			}
+			for (Long toRemove : checkpointsToRemove) {
+				StateHandle<DataInputView> handle = state.pendingHandles.get(toRemove);
+				state.pendingHandles.remove(toRemove);
+				handle.discardState();
+			}
+		}
+	}
+
+
+	/**
+	 * Write the given element into the backend.
+	 *
+	 * @param value value to be written
+	 * @throws Exception
+	 */
+	protected abstract void sendValue(Iterable<IN> value) throws Exception;
+
+	@Override
+	public void processElement(StreamRecord<IN> element) throws Exception {
+		IN value = element.getValue();
+		//generate initial operator state
+		if (out == null) {
+			out = getStateBackend().createCheckpointStateOutputView(0, 0);
+		}
+		serializer.serialize(value, out);
+	}
+
+	@Override
+	public void processWatermark(Watermark mark) throws Exception {
+		//don't do anything, since we are a sink
+	}
+
+	/**
+	 * This state is used to keep a list of all StateHandles (essentially references to past OperatorStates) that were
+	 * used since the last completed checkpoint.
+	 **/
+	public class ExactlyOnceState implements StateHandle<Serializable> {
+		protected TreeMap<Long, StateHandle<DataInputView>> pendingHandles;
+
+		public ExactlyOnceState() {
+			pendingHandles = new TreeMap<>();
+		}
+
+		@Override
+		public TreeMap<Long, StateHandle<DataInputView>> getState(ClassLoader userCodeClassLoader) throws Exception {
+			return pendingHandles;
+		}
+
+		@Override
+		public void discardState() throws Exception {
+			for (StateHandle<DataInputView> handle : pendingHandles.values()) {
+				handle.discardState();
+			}
+			pendingHandles = new TreeMap<>();
+		}
+
+		@Override
+		public long getStateSize() throws Exception {
+			int stateSize = 0;
+			for (StateHandle<DataInputView> handle : pendingHandles.values()) {
+				stateSize += handle.getStateSize();
+			}
+			return stateSize;
+		}
+	}
+}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/AtLeastOnceSinkTestBase.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/AtLeastOnceSinkTestBase.java
new file mode 100644
index 0000000000000..38b43e6325b4a
--- /dev/null
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/AtLeastOnceSinkTestBase.java
@@ -0,0 +1,216 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.runtime.operators;
+
+import org.apache.flink.api.common.typeinfo.TypeInformation;
+import org.apache.flink.streaming.api.graph.StreamConfig;
+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;
+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTask;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness;
+import org.apache.flink.streaming.runtime.tasks.StreamTaskState;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.ArrayList;
+
+public abstract class AtLeastOnceSinkTestBase<IN, S extends GenericAtLeastOnceSink<IN>> {
+
+	protected class OperatorExposingTask<IN> extends OneInputStreamTask<IN, IN> {
+		public OneInputStreamOperator<IN, IN> getOperator() {
+			return this.headOperator;
+		}
+	}
+
+	protected OperatorExposingTask<IN> createTask() {
+		return new OperatorExposingTask<>();
+	}
+
+	protected abstract S createSink();
+
+	protected abstract TypeInformation<IN> createTypeInfo();
+
+	protected abstract IN generateValue(int counter, int checkpointID);
+
+	protected abstract void verifyResultsIdealCircumstances(
+		OneInputStreamTaskTestHarness<IN, IN> harness, OneInputStreamTask<IN, IN> task, S sink) throws Exception;
+
+	protected abstract void verifyResultsDataPersistenceUponMissedNotify(
+		OneInputStreamTaskTestHarness<IN, IN> harness, OneInputStreamTask<IN, IN> task, S sink) throws Exception;
+
+	protected abstract void verifyResultsDataDiscardingUponRestore(
+		OneInputStreamTaskTestHarness<IN, IN> harness, OneInputStreamTask<IN, IN> task, S sink) throws Exception;
+
+	@Test
+	public void testIdealCircumstances() throws Exception {
+		OperatorExposingTask<IN> task = createTask();
+		TypeInformation<IN> info = createTypeInfo();
+		OneInputStreamTaskTestHarness<IN, IN> testHarness = new OneInputStreamTaskTestHarness<>(task, 1, 1, info, info);
+		StreamConfig streamConfig = testHarness.getStreamConfig();
+		streamConfig.setStreamOperator(createSink());
+
+		int elementCounter = 1;
+
+		testHarness.invoke();
+		testHarness.waitForTaskRunning();
+
+		ArrayList<StreamTaskState> states = new ArrayList<>();
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 0)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 1)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 2)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		testHarness.endInput();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		testHarness.waitForTaskCompletion();
+
+		verifyResultsIdealCircumstances(testHarness, task, (S) task.getOperator());
+	}
+
+	@Test
+	public void testDataPersistenceUponMissedNotify() throws Exception {
+		S sink = createSink();
+		OperatorExposingTask<IN> task = createTask();
+		TypeInformation<IN> info = createTypeInfo();
+		OneInputStreamTaskTestHarness<IN, IN> testHarness = new OneInputStreamTaskTestHarness<>(task, 1, 1, info, info);
+		StreamConfig streamConfig = testHarness.getStreamConfig();
+		streamConfig.setStreamOperator(sink);
+
+		int elementCounter = 1;
+
+		testHarness.invoke();
+		testHarness.waitForTaskRunning();
+
+		ArrayList<StreamTaskState> states = new ArrayList<>();
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 0)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 1)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 2)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		testHarness.endInput();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		testHarness.waitForTaskCompletion();
+
+		verifyResultsDataPersistenceUponMissedNotify(testHarness, task, (S) task.getOperator());
+	}
+
+	@Test
+	public void testDataDiscardingUponRestore() throws Exception {
+		S sink = createSink();
+		OperatorExposingTask<IN> task = createTask();
+		TypeInformation<IN> info = createTypeInfo();
+		OneInputStreamTaskTestHarness<IN, IN> testHarness = new OneInputStreamTaskTestHarness<>(task, 1, 1, info, info);
+		StreamConfig streamConfig = testHarness.getStreamConfig();
+		streamConfig.setStreamOperator(sink);
+
+		int elementCounter = 1;
+
+		testHarness.invoke();
+		testHarness.waitForTaskRunning();
+
+		ArrayList<StreamTaskState> states = new ArrayList<>();
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 0)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 1)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		task.getOperator().restoreState(states.get(states.size() - 1), 0);
+
+		for (int x = 0; x < 20; x++) {
+			testHarness.processElement(new StreamRecord<>(generateValue(elementCounter, 2)));
+			elementCounter++;
+		}
+		testHarness.waitForInputProcessing();
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		task.notifyCheckpointComplete(states.size() - 1);
+
+		testHarness.endInput();
+
+		states.add(copyTaskState(task.getOperator().snapshotOperatorState(states.size(), 0)));
+		testHarness.waitForTaskCompletion();
+
+		verifyResultsDataDiscardingUponRestore(testHarness, task, (S) task.getOperator());
+	}
+
+	private StreamTaskState copyTaskState(StreamTaskState toCopy) throws IOException, ClassNotFoundException {
+		ByteArrayOutputStream baos = new ByteArrayOutputStream();
+		ObjectOutputStream oos = new ObjectOutputStream(baos);
+		oos.writeObject(toCopy);
+
+		ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
+		ObjectInputStream ois = new ObjectInputStream(bais);
+		return (StreamTaskState) ois.readObject();
+	}
+}
diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSinkTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSinkTest.java
new file mode 100644
index 0000000000000..8300544e551fe
--- /dev/null
+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/GenericAtLeastOnceSinkTest.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.flink.streaming.runtime.operators;
+
+import org.apache.flink.api.common.ExecutionConfig;
+import org.apache.flink.api.java.tuple.Tuple1;
+import org.apache.flink.api.java.typeutils.TupleTypeInfo;
+import org.apache.flink.api.java.typeutils.TypeExtractor;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTask;
+import org.apache.flink.streaming.runtime.tasks.OneInputStreamTaskTestHarness;
+import org.junit.Assert;
+
+import java.util.ArrayList;
+import java.util.List;
+
+public class GenericAtLeastOnceSinkTest extends AtLeastOnceSinkTestBase<Tuple1<Integer>, GenericAtLeastOnceSinkTest.ListSink> {
+	@Override
+	protected ListSink createSink() {
+		return new ListSink();
+	}
+
+	@Override
+	protected TupleTypeInfo<Tuple1<Integer>> createTypeInfo() {
+		return TupleTypeInfo.getBasicTupleTypeInfo(Integer.class);
+	}
+
+	@Override
+	protected Tuple1<Integer> generateValue(int counter, int checkpointID) {
+		return new Tuple1<>(counter);
+	}
+
+	@Override
+	protected void verifyResultsIdealCircumstances(
+		OneInputStreamTaskTestHarness<Tuple1<Integer>, Tuple1<Integer>> harness,
+		OneInputStreamTask<Tuple1<Integer>, Tuple1<Integer>> task, ListSink sink) {
+
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Integer i : sink.values) {
+			list.remove(i);
+		}
+		Assert.assertTrue("The following ID's where not found in the result list: " + list.toString(), list.isEmpty());
+		Assert.assertTrue("The sink emitted to many values: " + (sink.values.size() - 60), sink.values.size() == 60);
+	}
+
+	@Override
+	protected void verifyResultsDataPersistenceUponMissedNotify(
+		OneInputStreamTaskTestHarness<Tuple1<Integer>, Tuple1<Integer>> harness,
+		OneInputStreamTask<Tuple1<Integer>, Tuple1<Integer>> task, ListSink sink) {
+
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Integer i : sink.values) {
+			list.remove(i);
+		}
+		Assert.assertTrue("The following ID's where not found in the result list: " + list.toString(), list.isEmpty());
+		Assert.assertTrue("The sink emitted to many values: " + (sink.values.size() - 60), sink.values.size() == 60);
+	}
+
+	@Override
+	protected void verifyResultsDataDiscardingUponRestore(
+		OneInputStreamTaskTestHarness<Tuple1<Integer>, Tuple1<Integer>> harness,
+		OneInputStreamTask<Tuple1<Integer>, Tuple1<Integer>> task, ListSink sink) {
+
+		ArrayList<Integer> list = new ArrayList<>();
+		for (int x = 1; x <= 20; x++) {
+			list.add(x);
+		}
+		for (int x = 41; x <= 60; x++) {
+			list.add(x);
+		}
+
+		for (Integer i : sink.values) {
+			list.remove(i);
+		}
+		Assert.assertTrue("The following ID's where not found in the result list: " + list.toString(), list.isEmpty());
+		Assert.assertTrue("The sink emitted to many values: " + (sink.values.size() - 40), sink.values.size() == 40);
+	}
+
+	/**
+	 * Simple sink that stores all records in a public list.
+	 */
+	public static class ListSink extends GenericAtLeastOnceSink<Tuple1<Integer>> {
+		public List<Integer> values = new ArrayList<>();
+
+		public ListSink() {
+			super(new SimpleCommitter(), TypeExtractor.getForObject(new Tuple1<>(1)).createSerializer(new ExecutionConfig()));
+		}
+
+		@Override
+		protected void sendValue(Iterable<Tuple1<Integer>> values) throws Exception {
+			for (Tuple1<Integer> value : values) {
+				this.values.add(value.f0);
+			}
+		}
+	}
+
+	public static class SimpleCommitter extends CheckpointCommitter {
+		private List<Long> checkpoints;
+
+		@Override
+		public void open() throws Exception {
+			checkpoints = new ArrayList<>();
+		}
+
+		@Override
+		public void close() throws Exception {
+			checkpoints.clear();
+			checkpoints = null;
+		}
+
+		@Override
+		public void commitCheckpoint(long checkpointID) {
+			checkpoints.add(checkpointID);
+		}
+
+		@Override
+		public boolean isCheckpointCommitted(long checkpointID) {
+			return checkpoints.contains(checkpointID);
+		}
+	}
+}
