diff --git a/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java b/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java
index 956943aced677..26d4fbe6f14aa 100644
--- a/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java
+++ b/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java
@@ -41,7 +41,6 @@ public final class ConfigConstants {
 	@Deprecated
 	public static final String DEFAULT_PARALLELISM_KEY_OLD = "parallelization.degree.default";
 
-
 	/**
 	 * Config parameter for the number of re-tries for failed tasks. Setting this
 	 * value to 0 effectively disables fault tolerance.
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
index a3c82c23aa4e2..eae696c0327d4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/Execution.java
@@ -46,8 +46,8 @@
 import org.apache.flink.runtime.messages.Messages;
 import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
 import org.apache.flink.runtime.state.StateHandle;
-import org.apache.flink.runtime.util.SerializedValue;
 import org.apache.flink.runtime.taskmanager.Task;
+import org.apache.flink.runtime.util.SerializedValue;
 import org.apache.flink.util.ExceptionUtils;
 import org.slf4j.Logger;
 import scala.concurrent.Future;
@@ -71,7 +71,6 @@
 import static org.apache.flink.runtime.execution.ExecutionState.FINISHED;
 import static org.apache.flink.runtime.execution.ExecutionState.RUNNING;
 import static org.apache.flink.runtime.execution.ExecutionState.SCHEDULED;
-
 import static org.apache.flink.runtime.messages.TaskMessages.CancelTask;
 import static org.apache.flink.runtime.messages.TaskMessages.FailIntermediateResultPartitions;
 import static org.apache.flink.runtime.messages.TaskMessages.SubmitTask;
@@ -434,7 +433,7 @@ else if (current == CREATED || current == SCHEDULED) {
 	}
 
 	void scheduleOrUpdateConsumers(List<List<ExecutionEdge>> allConsumers) {
-		if (allConsumers.size() != 1) {
+		if (allConsumers.size() > 1) {
 			fail(new IllegalStateException("Currently, only a single consumer group per partition is supported."));
 		}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
index 2ad3a552970f9..d44cb6ada7e07 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionVertex.java
@@ -219,6 +219,10 @@ public ExecutionGraph getExecutionGraph() {
 		return this.jobVertex.getGraph();
 	}
 
+	public Map<IntermediateResultPartitionID, IntermediateResultPartition> getProducedPartitions() {
+		return resultPartitions;
+	}
+
 	// --------------------------------------------------------------------------------------------
 	//  Graph building
 	// --------------------------------------------------------------------------------------------
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
index 259ea5556507e..079a7f4f651df 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/NetworkEnvironment.java
@@ -31,17 +31,21 @@
 import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;
 import org.apache.flink.runtime.io.network.netty.NettyConfig;
 import org.apache.flink.runtime.io.network.netty.NettyConnectionManager;
-import org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.io.network.partition.ResultPartition;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;
 import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.messages.JobManagerMessages.RequestPartitionState;
 import org.apache.flink.runtime.taskmanager.NetworkEnvironmentConfiguration;
 import org.apache.flink.runtime.taskmanager.Task;
 import org.apache.flink.runtime.taskmanager.TaskManager;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import scala.Option;
+import scala.Tuple2;
 import scala.concurrent.Future;
 import scala.concurrent.duration.FiniteDuration;
 
@@ -80,8 +84,9 @@ public class NetworkEnvironment {
 
 	private ResultPartitionConsumableNotifier partitionConsumableNotifier;
 
-	private boolean isShutdown;
+	private PartitionStateChecker partitionStateChecker;
 
+	private boolean isShutdown;
 
 	/**
 	 * Initializes all network I/O components.
@@ -130,6 +135,14 @@ public ResultPartitionConsumableNotifier getPartitionConsumableNotifier() {
 		return partitionConsumableNotifier;
 	}
 
+	public PartitionStateChecker getPartitionStateChecker() {
+		return partitionStateChecker;
+	}
+
+	public Tuple2<Integer, Integer> getPartitionRequestInitialAndMaxBackoff() {
+		return configuration.partitionRequestInitialAndMaxBackoff();
+	}
+
 	// --------------------------------------------------------------------------------------------
 	//  Association / Disassociation with JobManager / TaskManager
 	// --------------------------------------------------------------------------------------------
@@ -171,6 +184,9 @@ public void associateWithTaskManagerAndJobManager(ActorRef jobManagerRef, ActorR
 				this.partitionConsumableNotifier = new JobManagerResultPartitionConsumableNotifier(
 													jobManagerRef, taskManagerRef, new Timeout(jobManagerTimeout));
 
+				this.partitionStateChecker = new JobManagerPartitionStateChecker(
+						jobManagerRef, taskManagerRef);
+
 				// -----  Network connections  -----
 				final Option<NettyConfig> nettyConfig = configuration.nettyConfig();
 				connectionManager = nettyConfig.isDefined() ? new NettyConnectionManager(nettyConfig.get())
@@ -225,6 +241,8 @@ public void disassociate() throws IOException {
 
 			partitionConsumableNotifier = null;
 
+			partitionStateChecker = null;
+
 			if (taskEventDispatcher != null) {
 				taskEventDispatcher.clearAll();
 				taskEventDispatcher = null;
@@ -235,8 +253,6 @@ public void disassociate() throws IOException {
 		}
 	}
 
-
-
 	// --------------------------------------------------------------------------------------------
 	//  Task operations
 	// --------------------------------------------------------------------------------------------
@@ -404,9 +420,9 @@ private static class JobManagerResultPartitionConsumableNotifier implements Resu
 
 		private final Timeout jobManagerMessageTimeout;
 
-		public JobManagerResultPartitionConsumableNotifier(ActorRef jobManager,
-															ActorRef taskManager,
-															Timeout jobManagerMessageTimeout) {
+		public JobManagerResultPartitionConsumableNotifier(
+				ActorRef jobManager, ActorRef taskManager, Timeout jobManagerMessageTimeout) {
+
 			this.jobManager = jobManager;
 			this.taskManager = taskManager;
 			this.jobManagerMessageTimeout = jobManagerMessageTimeout;
@@ -435,4 +451,29 @@ public void onFailure(Throwable failure) {
 			}, AkkaUtils.globalExecutionContext());
 		}
 	}
+
+	private static class JobManagerPartitionStateChecker implements PartitionStateChecker {
+
+		private final ActorRef jobManager;
+
+		private final ActorRef taskManager;
+
+		public JobManagerPartitionStateChecker(ActorRef jobManager, ActorRef taskManager) {
+			this.jobManager = jobManager;
+			this.taskManager = taskManager;
+		}
+
+		@Override
+		public void triggerPartitionStateCheck(
+				JobID jobId,
+				ExecutionAttemptID executionAttemptID,
+				IntermediateDataSetID resultId,
+				ResultPartitionID partitionId) {
+
+			RequestPartitionState msg = new RequestPartitionState(
+					jobId, partitionId, executionAttemptID, resultId);
+
+			jobManager.tell(msg, taskManager);
+		}
+	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyConfig.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyConfig.java
index 55057fecad5f0..9d4f0785427ae 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyConfig.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyConfig.java
@@ -47,7 +47,7 @@ public class NettyConfig {
 
 	// ------------------------------------------------------------------------
 
-	static enum TransportType {
+	enum TransportType {
 		NIO, EPOLL, AUTO
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java
index bef2740183248..0606f4bbf337f 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java
@@ -394,7 +394,7 @@ public void readFrom(ByteBuf buffer) {
 
 		@Override
 		public String toString() {
-			return String.format("PartitionRequest(%s)", partitionId);
+			return String.format("PartitionRequest(%s:%d)", partitionId, queueIndex);
 		}
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClient.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClient.java
index b5f89e0ac0393..6d7725fdccd71 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClient.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClient.java
@@ -26,8 +26,11 @@
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel;
 import org.apache.flink.runtime.util.AtomicDisposableReferenceCounter;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.io.IOException;
+import java.util.concurrent.TimeUnit;
 
 import static com.google.common.base.Preconditions.checkNotNull;
 import static org.apache.flink.runtime.io.network.netty.NettyMessage.PartitionRequest;
@@ -41,6 +44,8 @@
  */
 public class PartitionRequestClient {
 
+	private static final Logger LOG = LoggerFactory.getLogger(PartitionRequestClient.class);
+
 	private final Channel tcpChannel;
 
 	private final PartitionRequestClientHandler partitionRequestHandler;
@@ -79,21 +84,41 @@ boolean incrementReferenceCounter() {
 	 * The request goes to the remote producer, for which this partition
 	 * request client instance has been created.
 	 */
-	public void requestSubpartition(final ResultPartitionID partitionId, int requestedQueueIndex, final RemoteInputChannel inputChannel) throws IOException {
+	public void requestSubpartition(
+			final ResultPartitionID partitionId,
+			final int subpartitionIndex,
+			final RemoteInputChannel inputChannel,
+			int delayMs) throws IOException {
+
+		LOG.debug("Requesting subpartition {} of partition {} with {} ms delay.",
+				subpartitionIndex, partitionId, delayMs);
+
 		partitionRequestHandler.addInputChannel(inputChannel);
 
-		tcpChannel.writeAndFlush(new PartitionRequest(partitionId, requestedQueueIndex, inputChannel.getInputChannelId()))
-				.addListener(
-						new ChannelFutureListener() {
-							@Override
-							public void operationComplete(ChannelFuture future) throws Exception {
-								if (!future.isSuccess()) {
-									partitionRequestHandler.removeInputChannel(inputChannel);
-									inputChannel.onError(future.cause());
-								}
-							}
-						}
-				);
+		final PartitionRequest request = new PartitionRequest(
+				partitionId, subpartitionIndex, inputChannel.getInputChannelId());
+
+		final ChannelFutureListener listener = new ChannelFutureListener() {
+			@Override
+			public void operationComplete(ChannelFuture future) throws Exception {
+				if (!future.isSuccess()) {
+					partitionRequestHandler.removeInputChannel(inputChannel);
+					inputChannel.onError(future.cause());
+				}
+			}
+		};
+
+		if (delayMs == 0) {
+			tcpChannel.writeAndFlush(request).addListener(listener);
+		}
+		else {
+			tcpChannel.eventLoop().schedule(new Runnable() {
+				@Override
+				public void run() {
+					tcpChannel.writeAndFlush(request).addListener(listener);
+				}
+			}, delayMs, TimeUnit.MILLISECONDS);
+		}
 	}
 
 	/**
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java
index 382f385ec1ffb..c3e65b122fa45 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandler.java
@@ -24,6 +24,7 @@
 import org.apache.flink.runtime.io.network.api.serialization.EventSerializer;
 import org.apache.flink.runtime.io.network.buffer.Buffer;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
+import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;
 import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;
 import org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel;
 import org.apache.flink.runtime.util.event.EventListener;
@@ -67,7 +68,9 @@ class PartitionRequestClientHandler extends ChannelInboundHandlerAdapter {
 	void addInputChannel(RemoteInputChannel listener) {
 		checkState(!channelError.get(), "There has been an error in the channel.");
 
-		inputChannels.put(listener.getInputChannelId(), listener);
+		if (!inputChannels.containsKey(listener.getInputChannelId())) {
+			inputChannels.put(listener.getInputChannelId(), listener);
+		}
 	}
 
 	void removeInputChannel(RemoteInputChannel listener) {
@@ -166,6 +169,7 @@ private boolean decodeMsg(Object msg) throws Throwable {
 		else if (msgClazz == NettyMessage.ErrorResponse.class) {
 			NettyMessage.ErrorResponse error = (NettyMessage.ErrorResponse) msg;
 
+
 			if (error.isFatalError()) {
 				notifyAllChannelsOfErrorAndClose(error.error);
 			}
@@ -173,7 +177,12 @@ else if (msgClazz == NettyMessage.ErrorResponse.class) {
 				RemoteInputChannel inputChannel = inputChannels.get(error.receiverId);
 
 				if (inputChannel != null) {
-					inputChannel.onError(error.error);
+					if (error.error.getClass() == PartitionNotFoundException.class) {
+						inputChannel.onFailedPartitionRequest();
+					}
+					else {
+						inputChannel.onError(error.error);
+					}
 				}
 			}
 		}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java
index 6f4becd7d04b5..7fa37e8fefc93 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionRequestServerHandler.java
@@ -23,6 +23,7 @@
 import org.apache.flink.runtime.io.network.TaskEventDispatcher;
 import org.apache.flink.runtime.io.network.buffer.BufferPool;
 import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;
+import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionProvider;
 import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;
 import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;
@@ -32,6 +33,9 @@
 import static org.apache.flink.runtime.io.network.netty.NettyMessage.PartitionRequest;
 import static org.apache.flink.runtime.io.network.netty.NettyMessage.TaskEventRequest;
 
+/**
+ * Channel handler to initiate data transfers and dispatch backwards flowing task events.
+ */
 class PartitionRequestServerHandler extends SimpleChannelInboundHandler<NettyMessage> {
 
 	private static final Logger LOG = LoggerFactory.getLogger(PartitionRequestServerHandler.class);
@@ -85,19 +89,19 @@ protected void channelRead0(ChannelHandlerContext ctx, NettyMessage msg) throws
 			if (msgClazz == PartitionRequest.class) {
 				PartitionRequest request = (PartitionRequest) msg;
 
-				LOG.debug("Read channel on {}: {}.",ctx.channel().localAddress(), request);
+				LOG.debug("Read channel on {}: {}.", ctx.channel().localAddress(), request);
 
-				ResultSubpartitionView queueIterator =
-						partitionProvider.createSubpartitionView(
-								request.partitionId,
-								request.queueIndex,
-								bufferPool);
+				try {
+					ResultSubpartitionView subpartition =
+							partitionProvider.createSubpartitionView(
+									request.partitionId,
+									request.queueIndex,
+									bufferPool);
 
-				if (queueIterator != null) {
-					outboundQueue.enqueue(queueIterator, request.receiverId);
+					outboundQueue.enqueue(subpartition, request.receiverId);
 				}
-				else {
-					respondWithError(ctx, new IllegalArgumentException("Partition not found"), request.receiverId);
+				catch (PartitionNotFoundException notFound) {
+					respondWithError(ctx, notFound, request.receiverId);
 				}
 			}
 			// ----------------------------------------------------------------
@@ -124,6 +128,8 @@ private void respondWithError(ChannelHandlerContext ctx, Throwable error) {
 	}
 
 	private void respondWithError(ChannelHandlerContext ctx, Throwable error, InputChannelID sourceId) {
+		LOG.debug("Responding with error {}.", error);
+
 		ctx.writeAndFlush(new NettyMessage.ErrorResponse(error, sourceId));
 	}
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionStateChecker.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionStateChecker.java
new file mode 100644
index 0000000000000..ecbcdaa7c30e2
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/PartitionStateChecker.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.io.network.netty;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+
+public interface PartitionStateChecker {
+
+	void triggerPartitionStateCheck(
+			JobID jobId,
+			ExecutionAttemptID executionId,
+			IntermediateDataSetID resultId,
+			ResultPartitionID partitionId);
+
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PartitionNotFoundException.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PartitionNotFoundException.java
new file mode 100644
index 0000000000000..74798625416d8
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PartitionNotFoundException.java
@@ -0,0 +1,41 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.io.network.partition;
+
+import java.io.IOException;
+
+public class PartitionNotFoundException extends IOException {
+
+	private static final long serialVersionUID = 0L;
+
+	private final ResultPartitionID partitionId;
+
+	public PartitionNotFoundException(ResultPartitionID partitionId) {
+		this.partitionId = partitionId;
+	}
+
+	public ResultPartitionID getPartitionId() {
+		return partitionId;
+	}
+
+	@Override
+	public String getMessage() {
+		return "Partition " + partitionId + " not found.";
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java
index 4c8174a10fb32..931790a03eefb 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/PipelinedSubpartition.java
@@ -170,13 +170,14 @@ public int releaseMemory() {
 	public PipelinedSubpartitionView createReadView(BufferProvider bufferProvider) {
 		synchronized (buffers) {
 			if (readView != null) {
-				throw new IllegalStateException("Subpartition is being or already has been " +
+				throw new IllegalStateException("Subpartition " + index + " of "
+						+ parent.getPartitionId() + " is being or already has been " +
 						"consumed, but pipelined subpartitions can only be consumed once.");
 			}
 
 			readView = new PipelinedSubpartitionView(this);
 
-			LOG.debug("Created {}.", readView);
+			LOG.debug("Created read view for subpartition {} of partition {}.", index, parent.getPartitionId());
 
 			return readView;
 		}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java
index df1f254c60292..bd9499e2b4a9a 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartition.java
@@ -38,6 +38,7 @@
 import java.util.concurrent.atomic.AtomicInteger;
 
 import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkElementIndex;
 import static com.google.common.base.Preconditions.checkNotNull;
 import static com.google.common.base.Preconditions.checkState;
 
@@ -311,6 +312,8 @@ public ResultSubpartitionView createSubpartitionView(int index, BufferProvider b
 		checkState(refCnt != -1, "Partition released.");
 		checkState(refCnt > 0, "Partition not pinned.");
 
+		checkElementIndex(index, subpartitions.length, "Subpartition not found.");
+
 		return subpartitions[index].createReadView(bufferProvider);
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java
index a666208b383a3..5f25a4cbf2bf9 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/ResultPartitionManager.java
@@ -73,10 +73,10 @@ public ResultSubpartitionView createSubpartitionView(
 					partitionId.getPartitionId());
 
 			if (partition == null) {
-				throw new IOException("Unknown partition " + partitionId + ".");
+				throw new PartitionNotFoundException(partitionId);
 			}
 
-			LOG.debug("Requested partition {}.", partition);
+			LOG.debug("Requesting subpartition {} of {}.", subpartitionIndex, partition);
 
 			return partition.createSubpartitionView(subpartitionIndex, bufferProvider);
 		}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/InputGate.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/InputGate.java
index 43cdd298b418c..af089fcb0190b 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/InputGate.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/InputGate.java
@@ -25,16 +25,16 @@
 
 public interface InputGate {
 
-	public int getNumberOfInputChannels();
+	int getNumberOfInputChannels();
 
-	public boolean isFinished();
+	boolean isFinished();
 
-	public void requestPartitions() throws IOException, InterruptedException;
+	void requestPartitions() throws IOException, InterruptedException;
 
-	public BufferOrEvent getNextBufferOrEvent() throws IOException, InterruptedException;
+	BufferOrEvent getNextBufferOrEvent() throws IOException, InterruptedException;
 
-	public void sendTaskEvent(TaskEvent event) throws IOException;
+	void sendTaskEvent(TaskEvent event) throws IOException;
 
-	public void registerListener(EventListener<InputGate> listener);
+	void registerListener(EventListener<InputGate> listener);
 
 }
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java
index df653a46e669e..449b1cf115131 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java
@@ -24,15 +24,19 @@
 import org.apache.flink.runtime.io.network.buffer.Buffer;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
 import org.apache.flink.runtime.io.network.netty.PartitionRequestClient;
+import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import scala.Tuple2;
 
 import java.io.IOException;
 import java.util.ArrayDeque;
 import java.util.Queue;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicReference;
 
+import static com.google.common.base.Preconditions.checkArgument;
 import static com.google.common.base.Preconditions.checkNotNull;
 import static com.google.common.base.Preconditions.checkState;
 
@@ -52,6 +56,12 @@ public class RemoteInputChannel extends InputChannel {
 	/** The connection manager to use connect to the remote partition provider. */
 	private final ConnectionManager connectionManager;
 
+	/**
+	 * An asynchronous error notification. Set by either the network I/O thread or the thread
+	 * failing a partition request.
+	 */
+	private final AtomicReference<Throwable> error = new AtomicReference<Throwable>();
+
 	/**
 	 * The received buffers. Received buffers are enqueued by the network I/O thread and the queue
 	 * is consumed by the receiving task thread.
@@ -65,7 +75,7 @@ public class RemoteInputChannel extends InputChannel {
 	private final AtomicBoolean isReleased = new AtomicBoolean();
 
 	/** Client to establish a (possibly shared) TCP connection and request the partition. */
-	private PartitionRequestClient partitionRequestClient;
+	private volatile PartitionRequestClient partitionRequestClient;
 
 	/**
 	 * The next expected sequence number for the next buffer. This is modified by the network
@@ -73,10 +83,11 @@ public class RemoteInputChannel extends InputChannel {
 	 */
 	private int expectedSequenceNumber = 0;
 
-	/**
-	 * An error possibly set by the network I/O thread.
-	 */
-	private volatile Throwable error;
+	/** The current backoff time (in ms) for partition requests. */
+	private int nextRequestBackoffMs;
+
+	/** The maximum backoff time (in ms) after which a request fails */
+	private final int maxRequestBackoffMs;
 
 	RemoteInputChannel(
 			SingleInputGate inputGate,
@@ -85,27 +96,67 @@ public class RemoteInputChannel extends InputChannel {
 			ConnectionID connectionId,
 			ConnectionManager connectionManager) {
 
+		this(inputGate, channelIndex, partitionId, connectionId, connectionManager,
+				new Tuple2<Integer, Integer>(0, 0));
+	}
+
+	RemoteInputChannel(
+			SingleInputGate inputGate,
+			int channelIndex,
+			ResultPartitionID partitionId,
+			ConnectionID connectionId,
+			ConnectionManager connectionManager,
+			Tuple2<Integer, Integer> initialAndMaxBackoff) {
+
 		super(inputGate, channelIndex, partitionId);
 
 		this.connectionId = checkNotNull(connectionId);
 		this.connectionManager = checkNotNull(connectionManager);
+
+		checkArgument(initialAndMaxBackoff._1() <= initialAndMaxBackoff._2());
+
+		this.nextRequestBackoffMs = initialAndMaxBackoff._1();
+		this.maxRequestBackoffMs = initialAndMaxBackoff._2();
 	}
 
 	// ------------------------------------------------------------------------
 	// Consume
 	// ------------------------------------------------------------------------
 
+	/**
+	 * Requests a remote subpartition.
+	 */
 	@Override
 	void requestSubpartition(int subpartitionIndex) throws IOException, InterruptedException {
 		if (partitionRequestClient == null) {
-			LOG.debug("{}: Requesting REMOTE subpartition {} of partition {}.",
-					this, subpartitionIndex, partitionId);
-
 			// Create a client and request the partition
 			partitionRequestClient = connectionManager
 					.createPartitionRequestClient(connectionId);
 
-			partitionRequestClient.requestSubpartition(partitionId, subpartitionIndex, this);
+			partitionRequestClient.requestSubpartition(partitionId, subpartitionIndex, this, 0);
+		}
+	}
+
+	/**
+	 * Retriggers a remote subpartition request.
+	 */
+	void retriggerSubpartitionRequest(int subpartitionIndex) throws IOException, InterruptedException {
+		checkState(partitionRequestClient != null, "Missing initial subpartition request.");
+
+		// Disabled
+		if (nextRequestBackoffMs == 0) {
+			failPartitionRequest();
+		}
+		else if (nextRequestBackoffMs <= maxRequestBackoffMs) {
+			partitionRequestClient.requestSubpartition(partitionId, subpartitionIndex, this, nextRequestBackoffMs);
+
+			// Exponential backoff
+			nextRequestBackoffMs = nextRequestBackoffMs < maxRequestBackoffMs
+					? Math.min(nextRequestBackoffMs * 2, maxRequestBackoffMs)
+					: maxRequestBackoffMs + 1; // Fail the next request
+		}
+		else {
+			failPartitionRequest();
 		}
 	}
 
@@ -178,6 +229,10 @@ void releaseAllResources() throws IOException {
 		}
 	}
 
+	public void failPartitionRequest() {
+		onError(new PartitionNotFoundException(partitionId));
+	}
+
 	@Override
 	public String toString() {
 		return "RemoteInputChannel [" + partitionId + " at " + connectionId + "]";
@@ -245,23 +300,30 @@ public void onEmptyBuffer(int sequenceNumber) {
 		}
 	}
 
-	public void onError(Throwable cause) {
-		if (error == null) {
-			error = cause;
+	public void onFailedPartitionRequest() {
+		inputGate.triggerPartitionStateCheck(partitionId);
+	}
 
+	public void onError(Throwable cause) {
+		if (error.compareAndSet(null, cause)) {
 			// Notify the input gate to trigger querying of this channel
 			notifyAvailableBuffer();
 		}
 	}
 
 	/**
-	 * Checks whether this channel got notified by the network I/O thread about an error.
+	 * Checks whether this channel got notified about an error.
 	 */
 	private void checkError() throws IOException {
-		final Throwable t = error;
+		final Throwable t = error.get();
 
 		if (t != null) {
-			throw new IOException(t);
+			if (t instanceof IOException) {
+				throw (IOException) t;
+			}
+			else {
+				throw new IOException(t);
+			}
 		}
 	}
 
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java
index acda1d82c9e1b..b4a084538dde4 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGate.java
@@ -19,17 +19,20 @@
 package org.apache.flink.runtime.io.network.partition.consumer;
 
 import com.google.common.collect.Maps;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.ResultPartitionLocation;
 import org.apache.flink.runtime.event.task.AbstractEvent;
 import org.apache.flink.runtime.event.task.TaskEvent;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.runtime.io.network.NetworkEnvironment;
 import org.apache.flink.runtime.io.network.api.EndOfPartitionEvent;
 import org.apache.flink.runtime.io.network.api.serialization.EventSerializer;
 import org.apache.flink.runtime.io.network.buffer.Buffer;
 import org.apache.flink.runtime.io.network.buffer.BufferPool;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.jobgraph.DistributionPattern;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
@@ -103,6 +106,12 @@ public class SingleInputGate implements InputGate {
 	/** The name of the owning task, for logging purposes. */
 	private final String owningTaskName;
 
+	/** The job ID of the owning task. */
+	private final JobID jobId;
+
+	/** The execution attempt ID of the owning task. */
+	private final ExecutionAttemptID executionId;
+
 	/**
 	 * The ID of the consumed intermediate result. Each input gate consumes partitions of the
 	 * intermediate result specified by this ID. This ID also identifies the input gate at the
@@ -130,6 +139,9 @@ public class SingleInputGate implements InputGate {
 
 	private final BitSet channelsWithEndOfPartitionEvents;
 
+	/** The partition state checker to use for failed partition requests. */
+	private final PartitionStateChecker partitionStateChecker;
+
 	/**
 	 * Buffer pool for incoming buffers. Incoming data from remote channels is copied to buffers
 	 * from this pool.
@@ -153,11 +165,17 @@ public class SingleInputGate implements InputGate {
 
 	public SingleInputGate(
 			String owningTaskName,
+			JobID jobId,
+			ExecutionAttemptID executionId,
 			IntermediateDataSetID consumedResultId,
 			int consumedSubpartitionIndex,
-			int numberOfInputChannels) {
+			int numberOfInputChannels,
+			PartitionStateChecker partitionStateChecker) {
 
 		this.owningTaskName = checkNotNull(owningTaskName);
+		this.jobId = checkNotNull(jobId);
+		this.executionId = checkNotNull(executionId);
+
 		this.consumedResultId = checkNotNull(consumedResultId);
 
 		checkArgument(consumedSubpartitionIndex >= 0);
@@ -168,6 +186,8 @@ public SingleInputGate(
 
 		this.inputChannels = Maps.newHashMapWithExpectedSize(numberOfInputChannels);
 		this.channelsWithEndOfPartitionEvents = new BitSet(numberOfInputChannels);
+
+		this.partitionStateChecker = checkNotNull(partitionStateChecker);
 	}
 
 	// ------------------------------------------------------------------------
@@ -260,6 +280,30 @@ else if (partitionLocation.isRemote()) {
 		}
 	}
 
+	/**
+	 * Retriggers a partition request.
+	 */
+	public void retriggerPartitionRequest(IntermediateResultPartitionID partitionId) throws IOException, InterruptedException {
+		synchronized (requestLock) {
+			if (!isReleased) {
+				final InputChannel ch = inputChannels.get(partitionId);
+
+				checkNotNull(ch, "Unknown input channel with ID " + partitionId);
+
+				if (ch.getClass() != RemoteInputChannel.class) {
+					throw new IllegalArgumentException("Channel identified by " + partitionId
+							+ " is not a remote channel.");
+				}
+
+				final RemoteInputChannel rch = (RemoteInputChannel) ch;
+
+				LOG.debug("Retriggering partition request {}:{}.", ch.partitionId, consumedSubpartitionIndex);
+
+				rch.retriggerSubpartitionRequest(consumedSubpartitionIndex);
+			}
+		}
+	}
+
 	public void releaseAllResources() throws IOException {
 		synchronized (requestLock) {
 			if (!isReleased) {
@@ -303,14 +347,14 @@ public boolean isFinished() {
 
 	@Override
 	public void requestPartitions() throws IOException, InterruptedException {
-		if (!requestedPartitionsFlag) {
-			// Sanity check
-			if (numberOfInputChannels != inputChannels.size()) {
-				throw new IllegalStateException("Bug in input gate setup logic: mismatch between" +
-						"number of total input channels and the currently set number of input " +
-						"channels.");
-			}
+		// Sanity check
+		if (numberOfInputChannels != inputChannels.size()) {
+			throw new IllegalStateException("Bug in input gate setup logic: mismatch between" +
+					"number of total input channels and the currently set number of input " +
+					"channels.");
+		}
 
+		if (!requestedPartitionsFlag) {
 			synchronized (requestLock) {
 				for (InputChannel inputChannel : inputChannels.values()) {
 					inputChannel.requestSubpartition(consumedSubpartitionIndex);
@@ -403,6 +447,14 @@ public void onAvailableBuffer(InputChannel channel) {
 		}
 	}
 
+	void triggerPartitionStateCheck(ResultPartitionID partitionId) {
+		partitionStateChecker.triggerPartitionStateCheck(
+				jobId,
+				executionId,
+				consumedResultId,
+				partitionId);
+	}
+
 	// ------------------------------------------------------------------------
 
 	/**
@@ -410,6 +462,8 @@ public void onAvailableBuffer(InputChannel channel) {
 	 */
 	public static SingleInputGate create(
 			String owningTaskName,
+			JobID jobId,
+			ExecutionAttemptID executionId,
 			InputGateDeploymentDescriptor igdd,
 			NetworkEnvironment networkEnvironment) {
 
@@ -421,7 +475,7 @@ public static SingleInputGate create(
 		final InputChannelDeploymentDescriptor[] icdd = checkNotNull(igdd.getInputChannelDeploymentDescriptors());
 
 		final SingleInputGate inputGate = new SingleInputGate(
-				owningTaskName, consumedResultId, consumedSubpartitionIndex, icdd.length);
+				owningTaskName, jobId, executionId, consumedResultId, consumedSubpartitionIndex, icdd.length, networkEnvironment.getPartitionStateChecker());
 
 		// Create the input channels. There is one input channel for each consumed partition.
 		final InputChannel[] inputChannels = new InputChannel[icdd.length];
@@ -439,13 +493,17 @@ public static SingleInputGate create(
 			else if (partitionLocation.isRemote()) {
 				inputChannels[i] = new RemoteInputChannel(inputGate, i, partitionId,
 						partitionLocation.getConnectionId(),
-						networkEnvironment.getConnectionManager());
+						networkEnvironment.getConnectionManager(),
+						networkEnvironment.getPartitionRequestInitialAndMaxBackoff()
+				);
 			}
 			else if (partitionLocation.isUnknown()) {
 				inputChannels[i] = new UnknownInputChannel(inputGate, i, partitionId,
 						networkEnvironment.getPartitionManager(),
 						networkEnvironment.getTaskEventDispatcher(),
-						networkEnvironment.getConnectionManager());
+						networkEnvironment.getConnectionManager(),
+						networkEnvironment.getPartitionRequestInitialAndMaxBackoff()
+				);
 			}
 			else {
 				throw new IllegalStateException("Unexpected partition location.");
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/UnknownInputChannel.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/UnknownInputChannel.java
index 4bde292624d86..0aa7ea3e76712 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/UnknownInputChannel.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/UnknownInputChannel.java
@@ -26,6 +26,7 @@
 import org.apache.flink.runtime.io.network.buffer.Buffer;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;
+import scala.Tuple2;
 
 import java.io.IOException;
 
@@ -43,19 +44,24 @@ public class UnknownInputChannel extends InputChannel {
 
 	private final ConnectionManager connectionManager;
 
+	/** Initial and maximum backoff (in ms) after failed partition requests. */
+	private final Tuple2<Integer, Integer> partitionRequestInitialAndMaxBackoff;
+
 	public UnknownInputChannel(
 			SingleInputGate gate,
 			int channelIndex,
 			ResultPartitionID partitionId,
 			ResultPartitionManager partitionManager,
 			TaskEventDispatcher taskEventDispatcher,
-			ConnectionManager connectionManager) {
+			ConnectionManager connectionManager,
+			Tuple2<Integer, Integer> partitionRequestInitialAndMaxBackoff) {
 
 		super(gate, channelIndex, partitionId);
 
-		this.partitionManager = partitionManager;
-		this.taskEventDispatcher = taskEventDispatcher;
-		this.connectionManager = connectionManager;
+		this.partitionManager = checkNotNull(partitionManager);
+		this.taskEventDispatcher = checkNotNull(taskEventDispatcher);
+		this.connectionManager = checkNotNull(connectionManager);
+		this.partitionRequestInitialAndMaxBackoff = checkNotNull(partitionRequestInitialAndMaxBackoff);
 	}
 
 	@Override
@@ -106,7 +112,7 @@ public String toString() {
 	// ------------------------------------------------------------------------
 
 	public RemoteInputChannel toRemoteInputChannel(ConnectionID producerAddress) {
-		return new RemoteInputChannel(inputGate, channelIndex, partitionId, checkNotNull(producerAddress), connectionManager);
+		return new RemoteInputChannel(inputGate, channelIndex, partitionId, checkNotNull(producerAddress), connectionManager, partitionRequestInitialAndMaxBackoff);
 	}
 
 	public LocalInputChannel toLocalInputChannel() {
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
index 8f613fc41b04f..51ce91f3821c7 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java
@@ -20,7 +20,6 @@
 
 import akka.actor.ActorRef;
 import akka.util.Timeout;
-
 import org.apache.flink.api.common.JobID;
 import org.apache.flink.api.common.cache.DistributedCache;
 import org.apache.flink.configuration.Configuration;
@@ -43,23 +42,24 @@
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.jobgraph.tasks.CheckpointCommittingOperator;
 import org.apache.flink.runtime.jobgraph.tasks.CheckpointedOperator;
 import org.apache.flink.runtime.jobgraph.tasks.OperatorStateCarrier;
 import org.apache.flink.runtime.memorymanager.MemoryManager;
+import org.apache.flink.runtime.messages.TaskManagerMessages.FatalError;
 import org.apache.flink.runtime.messages.TaskMessages;
 import org.apache.flink.runtime.messages.TaskMessages.TaskInFinalState;
-import org.apache.flink.runtime.messages.TaskManagerMessages.FatalError;
 import org.apache.flink.runtime.state.StateHandle;
 import org.apache.flink.runtime.state.StateUtils;
 import org.apache.flink.runtime.util.SerializedValue;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import scala.concurrent.duration.FiniteDuration;
 
+import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -181,7 +181,6 @@ public class Task implements Runnable {
 
 	/** The thread that executes the task */
 	private final Thread executingThread;
-	
 
 	// ------------------------------------------------------------------------
 	//  Fields that control the task execution. All these fields are volatile
@@ -204,7 +203,6 @@ public class Task implements Runnable {
 	 * initialization, to be memory friendly */
 	private volatile SerializedValue<StateHandle<?>> operatorState;
 
-	
 	/**
 	 * <p><b>IMPORTANT:</b> This constructor may not start any work that would need to 
 	 * be undone in the case of a failing task deployment.</p>
@@ -287,7 +285,7 @@ public Task(TaskDeploymentDescriptor tdd,
 
 		for (int i = 0; i < this.inputGates.length; i++) {
 			SingleInputGate gate = SingleInputGate.create(
-					taskNameWithSubtasksAndId, consumedPartitions.get(i), networkEnvironment);
+					taskNameWithSubtasksAndId, jobId, executionId, consumedPartitions.get(i), networkEnvironment);
 
 			this.inputGates[i] = gate;
 			inputGatesById.put(gate.getConsumedResultId(), gate);
@@ -401,6 +399,7 @@ public void startTaskThread() {
 	/**
 	 * The core work method that bootstraps the task and executes it code
 	 */
+	@Override
 	public void run() {
 
 		// ----------------------------
@@ -913,12 +912,52 @@ public void run() {
 			LOG.debug("Ignoring checkpoint commit notification for non-running task.");
 		}
 	}
+
+	// ------------------------------------------------------------------------
+
+	/**
+	 * Answer to a partition state check issued after a failed partition request.
+	 */
+	public void onPartitionStateUpdate(
+			IntermediateDataSetID resultId,
+			IntermediateResultPartitionID partitionId,
+			ExecutionState partitionState) throws IOException, InterruptedException {
+
+		if (executionState == ExecutionState.RUNNING) {
+			final SingleInputGate inputGate = inputGatesById.get(resultId);
+
+			if (inputGate != null) {
+				if (partitionState == ExecutionState.RUNNING) {
+					// Retrigger the partition request
+					inputGate.retriggerPartitionRequest(partitionId);
+				}
+				else if (partitionState == ExecutionState.CANCELED
+						|| partitionState == ExecutionState.CANCELING
+						|| partitionState == ExecutionState.FAILED) {
+
+					cancelExecution();
+				}
+				else {
+					failExternally(new IllegalStateException("Received unexpected partition state "
+							+ partitionState + " for partition request. This is a bug."));
+				}
+			}
+			else {
+				failExternally(new IllegalStateException("Received partition state for " +
+						"unknown input gate " + resultId + ". This is a bug."));
+			}
+		}
+		else {
+			LOG.debug("Ignoring partition state notification for not running task.");
+		}
+	}
 	
 	private void executeAsyncCallRunnable(Runnable runnable, String callName) {
 		Thread thread = new Thread(runnable, callName);
 		thread.setDaemon(true);
 		thread.start();
 	}
+
 	// ------------------------------------------------------------------------
 	//  Utilities
 	// ------------------------------------------------------------------------
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
index 4745fb6fb5ea0..0c71938aeba86 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/jobmanager/JobManager.scala
@@ -30,12 +30,14 @@ import org.apache.flink.core.io.InputSplitAssigner
 import org.apache.flink.runtime.accumulators.StringifiedAccumulatorResult
 import org.apache.flink.runtime.blob.BlobServer
 import org.apache.flink.runtime.client._
+import org.apache.flink.runtime.execution.ExecutionState
 import org.apache.flink.runtime.executiongraph.{ExecutionJobVertex, ExecutionGraph}
 import org.apache.flink.runtime.jobmanager.web.WebInfoServer
 import org.apache.flink.runtime.messages.ArchiveMessages.ArchiveExecutionGraph
 import org.apache.flink.runtime.messages.ExecutionGraphMessages.JobStatusChanged
 import org.apache.flink.runtime.messages.Messages.{Disconnect, Acknowledge}
-import org.apache.flink.runtime.messages.TaskMessages.UpdateTaskExecutionState
+import org.apache.flink.runtime.messages.TaskMessages
+import org.apache.flink.runtime.messages.TaskMessages.{FailTask, PartitionState, UpdateTaskExecutionState}
 import org.apache.flink.runtime.messages.accumulators._
 import org.apache.flink.runtime.messages.checkpoint.{AcknowledgeCheckpoint, AbstractCheckpointMessage}
 import org.apache.flink.runtime.process.ProcessReaper
@@ -280,7 +282,7 @@ class JobManager(protected val flinkConfiguration: Configuration,
 
     case checkpointMessage : AbstractCheckpointMessage =>
       handleCheckpointMessage(checkpointMessage)
-      
+
     case JobStatusChanged(jobID, newJobStatus, timeStamp, error) =>
       currentJobs.get(jobID) match {
         case Some((executionGraph, jobInfo)) => executionGraph.getJobName
@@ -338,6 +340,23 @@ class JobManager(protected val flinkConfiguration: Configuration,
             s"$jobId to schedule or update consumers."))
       }
 
+    case RequestPartitionState(jobId, partitionId, taskExecutionId, taskResultId) =>
+      val state = currentJobs.get(jobId) match {
+        case Some((executionGraph, _)) =>
+          val execution = executionGraph.getRegisteredExecutions.get(partitionId.getProducerId)
+
+          if (execution != null) execution.getState else null
+        case None =>
+          // Nothing to do. This is not an error, because the request is received when a sending
+          // task fails during a remote partition request.
+          log.debug(s"Cannot find execution graph for job $jobId.")
+
+          null
+      }
+
+      sender ! PartitionState(
+        taskExecutionId, taskResultId, partitionId.getPartitionId, state)
+
     case RequestJobStatus(jobID) =>
       currentJobs.get(jobID) match {
         case Some((executionGraph,_)) => sender ! CurrentJobStatus(jobID, executionGraph.getState)
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
index 03e837db1e70a..c9a2878834122 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/JobManagerMessages.scala
@@ -23,7 +23,7 @@ import org.apache.flink.runtime.client.{SerializedJobExecutionResult, JobStatusM
 import org.apache.flink.runtime.executiongraph.{ExecutionAttemptID, ExecutionGraph}
 import org.apache.flink.runtime.instance.{InstanceID, Instance}
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID
-import org.apache.flink.runtime.jobgraph.{JobGraph, JobStatus, JobVertexID}
+import org.apache.flink.runtime.jobgraph.{IntermediateDataSetID, JobGraph, JobStatus, JobVertexID}
 
 import scala.collection.JavaConverters._
 
@@ -70,6 +70,21 @@ object JobManagerMessages {
    */
   case class NextInputSplit(splitData: Array[Byte])
 
+  /**
+   * Requests the current state of the partition.
+   *
+   * The state of a partition is currently bound to the state of the producing execution.
+   * 
+   * @param jobId The job ID of the job, which produces the partition.
+   * @param partitionId The partition ID of the partition to request the state of.
+   * @param taskExecutionId The execution attempt ID of the task requesting the partition state.
+   * @param taskResultId The input gate ID of the task requesting the partition state.
+   */
+  case class RequestPartitionState(jobId: JobID,
+                                   partitionId: ResultPartitionID,
+                                   taskExecutionId: ExecutionAttemptID,
+                                   taskResultId: IntermediateDataSetID)
+
   /**
    * Notifies the [[org.apache.flink.runtime.jobmanager.JobManager]] about available data for a
    * produced partition.
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala
index b1a08caff991a..9373576dcb5c9 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/messages/TaskMessages.scala
@@ -18,14 +18,16 @@
 
 package org.apache.flink.runtime.messages
 
-import org.apache.flink.runtime.deployment.{TaskDeploymentDescriptor, InputChannelDeploymentDescriptor}
+import org.apache.flink.runtime.deployment.{InputChannelDeploymentDescriptor, TaskDeploymentDescriptor}
+import org.apache.flink.runtime.execution.ExecutionState
 import org.apache.flink.runtime.executiongraph.ExecutionAttemptID
-import org.apache.flink.runtime.jobgraph.IntermediateDataSetID
+import org.apache.flink.runtime.jobgraph.{IntermediateDataSetID, IntermediateResultPartitionID}
+import org.apache.flink.runtime.messages.JobManagerMessages.RequestPartitionState
 import org.apache.flink.runtime.taskmanager.TaskExecutionState
 
 /**
  * A set of messages that control the deployment and the state of Tasks executed
- * on the TaskManager
+ * on the TaskManager.
  */
 object TaskMessages {
 
@@ -80,6 +82,15 @@ object TaskMessages {
   //  Updates to Intermediate Results
   // --------------------------------------------------------------------------
 
+  /**
+   * Answer to a [[RequestPartitionState]] with the state of the respective partition.
+   */
+  case class PartitionState(
+    taskExecutionId: ExecutionAttemptID,
+    taskResultId: IntermediateDataSetID,
+    partitionId: IntermediateResultPartitionID,
+    state: ExecutionState) extends TaskMessage
+
   /**
    * Base class for messages that update the information about location of input partitions
    */
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/NetworkEnvironmentConfiguration.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/NetworkEnvironmentConfiguration.scala
index f99aac0e6cafc..51ca90da39dba 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/NetworkEnvironmentConfiguration.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/NetworkEnvironmentConfiguration.scala
@@ -21,7 +21,9 @@ package org.apache.flink.runtime.taskmanager
 import org.apache.flink.runtime.io.disk.iomanager.IOManager.IOMode
 import org.apache.flink.runtime.io.network.netty.NettyConfig
 
-case class NetworkEnvironmentConfiguration(numNetworkBuffers: Int,
-                                           networkBufferSize: Int,
-                                           ioMode: IOMode,
-                                           nettyConfig: Option[NettyConfig] = None)
+case class NetworkEnvironmentConfiguration(
+  numNetworkBuffers: Int,
+  networkBufferSize: Int,
+  ioMode: IOMode,
+  nettyConfig: Option[NettyConfig] = None,
+  partitionRequestInitialAndMaxBackoff: Tuple2[Integer, Integer] = (50, 3000))
diff --git a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
index 6a896242f8fad..65ce7dc27a845 100644
--- a/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
+++ b/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala
@@ -20,10 +20,9 @@ package org.apache.flink.runtime.taskmanager
 
 import java.io.{File, IOException}
 import java.net.{InetAddress, InetSocketAddress}
-import java.util
 import java.util.concurrent.TimeUnit
 import java.lang.reflect.Method
-import java.lang.management.{GarbageCollectorMXBean, ManagementFactory, MemoryMXBean}
+import java.lang.management.ManagementFactory
 
 import akka.actor._
 import akka.pattern.ask
@@ -393,6 +392,14 @@ extends Actor with ActorLogMessages with ActorSynchronousLogging {
           sender ! new TaskOperationResult(executionID, false,
               "No task with that execution ID was found.")
         }
+
+      case PartitionState(taskExecutionId, taskResultId, partitionId, state) =>
+        Option(runningTasks.get(taskExecutionId)) match {
+          case Some(task) =>
+            task.onPartitionStateUpdate(taskResultId, partitionId, state)
+          case None =>
+            log.debug(s"Cannot find task $taskExecutionId to respond with partition state.")
+        }
     }
   }
 
@@ -1560,8 +1567,8 @@ object TaskManager {
 
     val ioMode : IOMode = if (syncOrAsync == "async") IOMode.ASYNC else IOMode.SYNC
 
-    val networkConfig = NetworkEnvironmentConfiguration(numNetworkBuffers, pageSize,
-                                                        ioMode, nettyConfig)
+    val networkConfig = NetworkEnvironmentConfiguration(
+      numNetworkBuffers, pageSize, ioMode, nettyConfig)
 
     // ----> timeouts, library caching, profiling
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java
index bb95f4b7b2412..7739dea990405 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/NetworkEnvironmentTest.java
@@ -30,6 +30,7 @@
 import org.junit.Test;
 import org.mockito.Mockito;
 import scala.Some;
+import scala.Tuple2;
 import scala.concurrent.duration.FiniteDuration;
 
 import java.net.InetAddress;
@@ -54,7 +55,8 @@ public void testAssociateDisassociate() {
 		try {
 			NettyConfig nettyConf = new NettyConfig(InetAddress.getLocalHost(), port, BUFFER_SIZE, new Configuration());
 			NetworkEnvironmentConfiguration config = new NetworkEnvironmentConfiguration(
-					NUM_BUFFERS, BUFFER_SIZE, IOManager.IOMode.SYNC, new Some<NettyConfig>(nettyConf));
+					NUM_BUFFERS, BUFFER_SIZE, IOManager.IOMode.SYNC, new Some<NettyConfig>(nettyConf),
+					new Tuple2<Integer, Integer>(0, 0));
 
 			NetworkEnvironment env = new NetworkEnvironment(new FiniteDuration(30, TimeUnit.SECONDS), config);
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java
index c0e2bcb91717b..3632d6ce27566 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/netty/PartitionRequestClientHandlerTest.java
@@ -24,6 +24,9 @@
 import org.apache.flink.runtime.io.network.buffer.Buffer;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
 import org.apache.flink.runtime.io.network.netty.NettyMessage.BufferResponse;
+import org.apache.flink.runtime.io.network.netty.NettyMessage.ErrorResponse;
+import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.consumer.InputChannelID;
 import org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel;
 import org.apache.flink.runtime.io.network.util.TestBufferFactory;
@@ -35,6 +38,7 @@
 import static org.mockito.Matchers.any;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.never;
+import static org.mockito.Mockito.times;
 import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
@@ -96,7 +100,6 @@ public void testReceiveEmptyBuffer() throws Exception {
 				emptyBuffer, 0, inputChannel.getInputChannelId());
 
 		final PartitionRequestClientHandler client = new PartitionRequestClientHandler();
-
 		client.addInputChannel(inputChannel);
 
 		// Read the empty buffer
@@ -106,6 +109,34 @@ public void testReceiveEmptyBuffer() throws Exception {
 		verify(inputChannel, never()).onError(any(Throwable.class));
 	}
 
+	/**
+	 * Verifies that {@link RemoteInputChannel#onFailedPartitionRequest()} is called when a
+	 * {@link PartitionNotFoundException} is received.
+	 */
+	@Test
+	public void testReceivePartitionNotFoundException() throws Exception {
+		// Minimal mock of a remote input channel
+		final BufferProvider bufferProvider = mock(BufferProvider.class);
+		when(bufferProvider.requestBuffer()).thenReturn(TestBufferFactory.createBuffer());
+
+		final RemoteInputChannel inputChannel = mock(RemoteInputChannel.class);
+		when(inputChannel.getInputChannelId()).thenReturn(new InputChannelID());
+		when(inputChannel.getBufferProvider()).thenReturn(bufferProvider);
+
+		final ErrorResponse partitionNotFound = new ErrorResponse(
+				new PartitionNotFoundException(new ResultPartitionID()),
+				inputChannel.getInputChannelId());
+
+		final PartitionRequestClientHandler client = new PartitionRequestClientHandler();
+		client.addInputChannel(inputChannel);
+
+		client.channelRead(mock(ChannelHandlerContext.class), partitionNotFound);
+
+		verify(inputChannel, times(1)).onFailedPartitionRequest();
+	}
+
+	// ---------------------------------------------------------------------------------------------
+
 	/**
 	 * Returns a deserialized buffer message as it would be received during runtime.
 	 */
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java
index fdf41f04e802f..9bef8863d5596 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/LocalInputChannelTest.java
@@ -21,11 +21,13 @@
 import com.google.common.collect.Lists;
 
 import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.runtime.io.disk.iomanager.IOManager;
 import org.apache.flink.runtime.io.network.TaskEventDispatcher;
 import org.apache.flink.runtime.io.network.buffer.BufferPool;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
 import org.apache.flink.runtime.io.network.buffer.NetworkBufferPool;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.io.network.partition.ResultPartition;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
@@ -224,9 +226,12 @@ public TestLocalInputChannelConsumer(
 
 			this.inputGate = new SingleInputGate(
 					"Test Name",
+					new JobID(),
+					new ExecutionAttemptID(),
 					new IntermediateDataSetID(),
 					subpartitionIndex,
-					numberOfInputChannels);
+					numberOfInputChannels,
+					mock(PartitionStateChecker.class));
 
 			// Set buffer pool
 			inputGate.setBufferPool(bufferPool);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java
index 76fae5b453cb5..22a11f9bb4ba5 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannelTest.java
@@ -25,6 +25,7 @@
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.util.TestBufferFactory;
 import org.junit.Test;
+import scala.Tuple2;
 
 import java.io.IOException;
 import java.util.List;
@@ -129,20 +130,154 @@ public Void call() throws Exception {
 		}
 	}
 
+	@Test(expected = IllegalStateException.class)
+	public void testRetriggerWithoutPartitionRequest() throws Exception {
+		Tuple2<Integer, Integer> backoff = new Tuple2<Integer, Integer>(500, 3000);
+		PartitionRequestClient connClient = mock(PartitionRequestClient.class);
+		SingleInputGate inputGate = mock(SingleInputGate.class);
+
+		RemoteInputChannel ch = createRemoteInputChannel(inputGate, connClient, backoff);
+
+		ch.retriggerSubpartitionRequest(0);
+	}
+
+	@Test
+	public void testPartitionRequestExponentialBackoff() throws Exception {
+		// Config
+		Tuple2<Integer, Integer> backoff = new Tuple2<Integer, Integer>(500, 3000);
+
+		// Start with initial backoff, then keep doubling, and cap at max.
+		int[] expectedDelays = {backoff._1(), 1000, 2000, backoff._2()};
+
+		// Setup
+		PartitionRequestClient connClient = mock(PartitionRequestClient.class);
+		SingleInputGate inputGate = mock(SingleInputGate.class);
+
+		RemoteInputChannel ch = createRemoteInputChannel(inputGate, connClient, backoff);
+
+		// Initial request
+		ch.requestSubpartition(0);
+		verify(connClient).requestSubpartition(eq(ch.partitionId), eq(0), eq(ch), eq(0));
+
+		// Request subpartition and verify that the actual requests are delayed.
+		for (int expected : expectedDelays) {
+			ch.retriggerSubpartitionRequest(0);
+
+			verify(connClient).requestSubpartition(eq(ch.partitionId), eq(0), eq(ch), eq(expected));
+		}
+
+		// Exception after backoff is greater than the maximum backoff.
+		try {
+			ch.retriggerSubpartitionRequest(0);
+			ch.getNextBuffer();
+			fail("Did not throw expected exception.");
+		}
+		catch (Exception expected) {
+		}
+	}
+
+	@Test
+	public void testPartitionRequestSingleBackoff() throws Exception {
+		// Config
+		Tuple2<Integer, Integer> backoff = new Tuple2<Integer, Integer>(500, 500);
+
+		// Setup
+		PartitionRequestClient connClient = mock(PartitionRequestClient.class);
+		SingleInputGate inputGate = mock(SingleInputGate.class);
+
+		RemoteInputChannel ch = createRemoteInputChannel(inputGate, connClient, backoff);
+
+		// No delay for first request
+		ch.requestSubpartition(0);
+		verify(connClient).requestSubpartition(eq(ch.partitionId), eq(0), eq(ch), eq(0));
+
+		// Initial delay for second request
+		ch.retriggerSubpartitionRequest(0);
+		verify(connClient).requestSubpartition(eq(ch.partitionId), eq(0), eq(ch), eq(backoff._1()));
+
+		// Exception after backoff is greater than the maximum backoff.
+		try {
+			ch.retriggerSubpartitionRequest(0);
+			ch.getNextBuffer();
+			fail("Did not throw expected exception.");
+		}
+		catch (Exception expected) {
+		}
+	}
+
+	@Test
+	public void testPartitionRequestNoBackoff() throws Exception {
+		// Config
+		Tuple2<Integer, Integer> backoff = new Tuple2<Integer, Integer>(0, 0);
+
+		// Setup
+		PartitionRequestClient connClient = mock(PartitionRequestClient.class);
+		SingleInputGate inputGate = mock(SingleInputGate.class);
+
+		RemoteInputChannel ch = createRemoteInputChannel(inputGate, connClient, backoff);
+
+		// No delay for first request
+		ch.requestSubpartition(0);
+		verify(connClient).requestSubpartition(eq(ch.partitionId), eq(0), eq(ch), eq(0));
+
+		// Exception, because backoff is disabled.
+		try {
+			ch.retriggerSubpartitionRequest(0);
+			ch.getNextBuffer();
+			fail("Did not throw expected exception.");
+		}
+		catch (Exception expected) {
+		}
+	}
+
+	@Test
+	public void testOnFailedPartitionRequest() throws Exception {
+		final ConnectionManager connectionManager = mock(ConnectionManager.class);
+		when(connectionManager.createPartitionRequestClient(any(ConnectionID.class)))
+				.thenReturn(mock(PartitionRequestClient.class));
+
+		final ResultPartitionID partitionId = new ResultPartitionID();
+
+		final SingleInputGate inputGate = mock(SingleInputGate.class);
+
+		final RemoteInputChannel ch = new RemoteInputChannel(
+				inputGate,
+				0,
+				partitionId,
+				mock(ConnectionID.class),
+				connectionManager
+		);
+
+		ch.onFailedPartitionRequest();
+
+		verify(inputGate).triggerPartitionStateCheck(eq(partitionId));
+	}
+
 	// ---------------------------------------------------------------------------------------------
 
 	private RemoteInputChannel createRemoteInputChannel(SingleInputGate inputGate)
 			throws IOException, InterruptedException {
 
+		return createRemoteInputChannel(
+				inputGate, mock(PartitionRequestClient.class), new Tuple2<Integer, Integer>(0, 0));
+	}
+
+	private RemoteInputChannel createRemoteInputChannel(
+			SingleInputGate inputGate,
+			PartitionRequestClient partitionRequestClient,
+			Tuple2<Integer, Integer> initialAndMaxRequestBackoff)
+			throws IOException, InterruptedException {
+
 		final ConnectionManager connectionManager = mock(ConnectionManager.class);
 		when(connectionManager.createPartitionRequestClient(any(ConnectionID.class)))
-				.thenReturn(mock(PartitionRequestClient.class));
+				.thenReturn(partitionRequestClient);
 
 		return new RemoteInputChannel(
 				inputGate,
 				0,
 				new ResultPartitionID(),
 				mock(ConnectionID.class),
-				connectionManager);
+				connectionManager,
+				initialAndMaxRequestBackoff);
 	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java
index 9a7ffe53f0b89..24543994ac78c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/SingleInputGateTest.java
@@ -18,6 +18,7 @@
 
 package org.apache.flink.runtime.io.network.partition.consumer;
 
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.core.memory.MemorySegment;
 import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.ResultPartitionLocation;
@@ -29,6 +30,7 @@
 import org.apache.flink.runtime.io.network.buffer.BufferPool;
 import org.apache.flink.runtime.io.network.buffer.BufferProvider;
 import org.apache.flink.runtime.io.network.buffer.BufferRecycler;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;
 import org.apache.flink.runtime.io.network.partition.ResultSubpartitionView;
@@ -36,6 +38,7 @@
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 import org.junit.Test;
+import scala.Tuple2;
 
 import java.io.IOException;
 
@@ -59,7 +62,7 @@ public class SingleInputGateTest {
 	public void testBasicGetNextLogic() throws Exception {
 		// Setup
 		final SingleInputGate inputGate = new SingleInputGate(
-				"Test Task Name", new IntermediateDataSetID(), 0, 2);
+				"Test Task Name", new JobID(), new ExecutionAttemptID(), new IntermediateDataSetID(), 0, 2, mock(PartitionStateChecker.class));
 
 		final TestInputChannel[] inputChannels = new TestInputChannel[]{
 				new TestInputChannel(inputGate, 0),
@@ -105,7 +108,7 @@ public void testBackwardsEventWithUninitializedChannel() throws Exception {
 		// Setup reader with one local and one unknown input channel
 		final IntermediateDataSetID resultId = new IntermediateDataSetID();
 
-		final SingleInputGate inputGate = new SingleInputGate("Test Task Name", resultId, 0, 2);
+		final SingleInputGate inputGate = new SingleInputGate("Test Task Name", new JobID(), new ExecutionAttemptID(), resultId, 0, 2, mock(PartitionStateChecker.class));
 		final BufferPool bufferPool = mock(BufferPool.class);
 		when(bufferPool.getNumberOfRequiredMemorySegments()).thenReturn(2);
 
@@ -119,7 +122,7 @@ public void testBackwardsEventWithUninitializedChannel() throws Exception {
 		// Unknown
 		ResultPartitionID unknownPartitionId = new ResultPartitionID(new IntermediateResultPartitionID(), new ExecutionAttemptID());
 
-		InputChannel unknown = new UnknownInputChannel(inputGate, 1, unknownPartitionId, partitionManager, taskEventDispatcher, mock(ConnectionManager.class));
+		InputChannel unknown = new UnknownInputChannel(inputGate, 1, unknownPartitionId, partitionManager, taskEventDispatcher, mock(ConnectionManager.class), new Tuple2<Integer, Integer>(0, 0));
 
 		// Set channels
 		inputGate.setInputChannel(localPartitionId.getPartitionId(), local);
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java
index 2dafaa26919ea..c282519cc869c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/TestSingleInputGate.java
@@ -18,7 +18,10 @@
 
 package org.apache.flink.runtime.io.network.partition.consumer;
 
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.runtime.io.network.buffer.Buffer;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
 
@@ -29,6 +32,7 @@
 
 import static com.google.common.base.Preconditions.checkArgument;
 import static com.google.common.base.Preconditions.checkElementIndex;
+import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.spy;
 
 /**
@@ -48,7 +52,7 @@ public TestSingleInputGate(int numberOfInputChannels, boolean initialize) {
 		checkArgument(numberOfInputChannels >= 1);
 
 		this.inputGate = spy(new SingleInputGate(
-				"Test Task Name", new IntermediateDataSetID(), 0, numberOfInputChannels));
+				"Test Task Name", new JobID(), new ExecutionAttemptID(), new IntermediateDataSetID(), 0, numberOfInputChannels, mock(PartitionStateChecker.class)));
 
 		this.inputChannels = new TestInputChannel[numberOfInputChannels];
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java
index 050f43a92af6e..d8714d191279b 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/consumer/UnionInputGateTest.java
@@ -18,6 +18,9 @@
 
 package org.apache.flink.runtime.io.network.partition.consumer;
 
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
+import org.apache.flink.runtime.io.network.netty.PartitionStateChecker;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 
 import org.junit.Test;
@@ -25,6 +28,7 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.mockito.Mockito.mock;
 
 public class UnionInputGateTest {
 
@@ -39,8 +43,8 @@ public class UnionInputGateTest {
 	public void testBasicGetNextLogic() throws Exception {
 		// Setup
 		final String testTaskName = "Test Task";
-		final SingleInputGate ig1 = new SingleInputGate(testTaskName, new IntermediateDataSetID(), 0, 3);
-		final SingleInputGate ig2 = new SingleInputGate(testTaskName, new IntermediateDataSetID(), 0, 5);
+		final SingleInputGate ig1 = new SingleInputGate(testTaskName, new JobID(), new ExecutionAttemptID(), new IntermediateDataSetID(), 0, 3, mock(PartitionStateChecker.class));
+		final SingleInputGate ig2 = new SingleInputGate(testTaskName, new JobID(), new ExecutionAttemptID(), new IntermediateDataSetID(), 0, 5, mock(PartitionStateChecker.class));
 
 		final UnionInputGate union = new UnionInputGate(new SingleInputGate[]{ig1, ig2});
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java
index 702d656d0c972..9dd078d0036f0 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmanager/JobManagerTest.java
@@ -15,21 +15,65 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.flink.runtime.jobmanager;
 
+import akka.actor.ActorRef;
+import akka.actor.ActorSystem;
+import akka.actor.Status;
+import akka.testkit.JavaTestKit;
 import com.typesafe.config.Config;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.akka.AkkaUtils;
+import org.apache.flink.runtime.execution.ExecutionState;
+import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
+import org.apache.flink.runtime.executiongraph.ExecutionGraph;
+import org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils;
+import org.apache.flink.runtime.executiongraph.ExecutionVertex;
+import org.apache.flink.runtime.executiongraph.IntermediateResultPartition;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
+import org.apache.flink.runtime.jobgraph.AbstractJobVertex;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.JobGraph;
+import org.apache.flink.runtime.messages.JobManagerMessages;
+import org.apache.flink.runtime.messages.JobManagerMessages.RequestPartitionState;
+import org.apache.flink.runtime.messages.TaskMessages.PartitionState;
+import org.apache.flink.runtime.testingUtils.TestingCluster;
+import org.apache.flink.runtime.testingUtils.TestingJobManagerMessages;
+import org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.ExecutionGraphFound;
+import org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.RequestExecutionGraph;
+import org.apache.flink.runtime.testingUtils.TestingJobManagerMessages.WaitForAllVerticesToBeRunningOrFinished;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
 import org.junit.Test;
 import scala.Some;
 import scala.Tuple2;
 
 import java.net.InetAddress;
 
-import static org.junit.Assert.*;
+import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.PIPELINED;
+import static org.apache.flink.runtime.testingUtils.TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT;
+import static org.apache.flink.runtime.testingUtils.TestingUtils.startTestingCluster;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 public class JobManagerTest {
 
+	private static ActorSystem system;
+
+	@BeforeClass
+	public static void setup() {
+		system = AkkaUtils.createLocalActorSystem(new Configuration());
+	}
+
+	@AfterClass
+	public static void teardown() {
+		JavaTestKit.shutdownActorSystem(system);
+	}
+
 	@Test
 	public void testNullHostnameGoesToLocalhost() {
 		try {
@@ -45,4 +89,113 @@ public void testNullHostnameGoesToLocalhost() {
 			fail(e.getMessage());
 		}
 	}
+
+	/**
+	 * Tests responses to partition state requests.
+	 */
+	@Test
+	public void testRequestPartitionState() throws Exception {
+		new JavaTestKit(system) {{
+			// Setup
+			TestingCluster cluster = null;
+
+			try {
+				cluster = startTestingCluster(2, 1, DEFAULT_AKKA_ASK_TIMEOUT());
+
+				final IntermediateDataSetID rid = new IntermediateDataSetID();
+
+				// Create a task
+				final AbstractJobVertex sender = new AbstractJobVertex("Sender");
+				sender.setParallelism(1);
+				sender.setInvokableClass(Tasks.BlockingNoOpInvokable.class); // just block
+				sender.createAndAddResultDataSet(rid, PIPELINED);
+
+				final JobGraph jobGraph = new JobGraph("Blocking test job", sender);
+				final JobID jid = jobGraph.getJobID();
+
+				final ActorRef jm = cluster.getJobManager();
+
+				// Submit the job and wait for all vertices to be running
+				jm.tell(new JobManagerMessages.SubmitJob(jobGraph, false), getTestActor());
+				expectMsgClass(Status.Success.class);
+
+				jm.tell(new WaitForAllVerticesToBeRunningOrFinished(jobGraph.getJobID()),
+						getTestActor());
+
+				expectMsgClass(TestingJobManagerMessages.AllVerticesRunning.class);
+
+				// This is the mock execution ID of the task requesting the state of the partition
+				final ExecutionAttemptID receiver = new ExecutionAttemptID();
+
+				// Request the execution graph to get the runtime info
+				jm.tell(new RequestExecutionGraph(jid), getTestActor());
+
+				final ExecutionGraph eg = expectMsgClass(ExecutionGraphFound.class)
+						.executionGraph();
+
+				final ExecutionVertex vertex = eg.getJobVertex(sender.getID())
+						.getTaskVertices()[0];
+
+				final IntermediateResultPartition partition = vertex.getProducedPartitions()
+						.values().iterator().next();
+
+				final ResultPartitionID partitionId = new ResultPartitionID(
+						partition.getPartitionId(),
+						vertex.getCurrentExecutionAttempt().getAttemptId());
+
+				// - The test ----------------------------------------------------------------------
+
+				// 1. All execution states
+				RequestPartitionState request = new RequestPartitionState(
+						jid, partitionId, receiver, rid);
+
+				for (ExecutionState state : ExecutionState.values()) {
+					ExecutionGraphTestUtils.setVertexState(vertex, state);
+
+					jm.tell(request, getTestActor());
+
+					PartitionState resp = expectMsgClass(PartitionState.class);
+
+					assertEquals(request.taskExecutionId(), resp.taskExecutionId());
+					assertEquals(request.taskResultId(), resp.taskResultId());
+					assertEquals(request.partitionId().getPartitionId(), resp.partitionId());
+					assertEquals(state, resp.state());
+				}
+
+				// 2. Non-existing execution
+				request = new RequestPartitionState(jid, new ResultPartitionID(), receiver, rid);
+
+				jm.tell(request, getTestActor());
+
+				PartitionState resp = expectMsgClass(PartitionState.class);
+
+				assertEquals(request.taskExecutionId(), resp.taskExecutionId());
+				assertEquals(request.taskResultId(), resp.taskResultId());
+				assertEquals(request.partitionId().getPartitionId(), resp.partitionId());
+				assertNull(resp.state());
+
+				// 3. Non-existing job
+				request = new RequestPartitionState(
+						new JobID(), new ResultPartitionID(), receiver, rid);
+
+				jm.tell(request, getTestActor());
+
+				resp = expectMsgClass(PartitionState.class);
+
+				assertEquals(request.taskExecutionId(), resp.taskExecutionId());
+				assertEquals(request.taskResultId(), resp.taskResultId());
+				assertEquals(request.partitionId().getPartitionId(), resp.partitionId());
+				assertNull(resp.state());
+			}
+			catch (Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+			finally {
+				if (cluster != null) {
+					cluster.shutdown();
+				}
+			}
+		}};
+	}
 }
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java
index 131549b045c78..dca3c58718276 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerComponentsStartupShutdownTest.java
@@ -40,6 +40,7 @@
 import org.apache.flink.runtime.messages.TaskManagerMessages;
 import org.junit.Test;
 import scala.Option;
+import scala.Tuple2;
 import scala.concurrent.duration.FiniteDuration;
 
 import java.net.InetAddress;
@@ -79,7 +80,8 @@ public void testComponentsStartupShutdown() {
 					config);
 
 			final NetworkEnvironmentConfiguration netConf = new NetworkEnvironmentConfiguration(
-					32, BUFFER_SIZE, IOManager.IOMode.SYNC, Option.<NettyConfig>empty());
+					32, BUFFER_SIZE, IOManager.IOMode.SYNC, Option.<NettyConfig>empty(),
+					new Tuple2<Integer, Integer>(0, 0));
 
 			final InstanceConnectionInfo connectionInfo = new InstanceConnectionInfo(InetAddress.getLocalHost(), 10000);
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
index 76ff86fee7b01..a308c811ed19c 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java
@@ -27,49 +27,49 @@
 import akka.pattern.Patterns;
 import akka.testkit.JavaTestKit;
 import akka.util.Timeout;
+import org.apache.flink.api.common.JobID;
 import org.apache.flink.configuration.ConfigConstants;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.blob.BlobKey;
+import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.InputGateDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.ResultPartitionDeploymentDescriptor;
-import org.apache.flink.runtime.deployment.InputChannelDeploymentDescriptor;
 import org.apache.flink.runtime.deployment.ResultPartitionLocation;
 import org.apache.flink.runtime.deployment.TaskDeploymentDescriptor;
 import org.apache.flink.runtime.execution.ExecutionState;
 import org.apache.flink.runtime.executiongraph.ExecutionAttemptID;
 import org.apache.flink.runtime.instance.InstanceID;
+import org.apache.flink.runtime.io.network.ConnectionID;
+import org.apache.flink.runtime.io.network.partition.PartitionNotFoundException;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
 import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
-import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
-import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.jobmanager.Tasks;
-import org.apache.flink.runtime.messages.JobManagerMessages;
 import org.apache.flink.runtime.messages.Messages;
 import org.apache.flink.runtime.messages.RegistrationMessages;
 import org.apache.flink.runtime.messages.TaskManagerMessages;
-import org.apache.flink.runtime.messages.TaskMessages;
 import org.apache.flink.runtime.messages.TaskMessages.CancelTask;
+import org.apache.flink.runtime.messages.TaskMessages.PartitionState;
 import org.apache.flink.runtime.messages.TaskMessages.SubmitTask;
 import org.apache.flink.runtime.messages.TaskMessages.TaskOperationResult;
+import org.apache.flink.runtime.net.NetUtils;
 import org.apache.flink.runtime.testingUtils.TestingTaskManager;
 import org.apache.flink.runtime.testingUtils.TestingTaskManagerMessages;
-
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import scala.Option;
 import scala.concurrent.Await;
 import scala.concurrent.Future;
 import scala.concurrent.duration.FiniteDuration;
 
+import java.net.InetSocketAddress;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashSet;
@@ -78,6 +78,10 @@
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
+import static org.apache.flink.runtime.messages.JobManagerMessages.ConsumerNotificationResult;
+import static org.apache.flink.runtime.messages.JobManagerMessages.RequestPartitionState;
+import static org.apache.flink.runtime.messages.JobManagerMessages.ScheduleOrUpdateConsumers;
+import static org.apache.flink.runtime.messages.TaskMessages.UpdateTaskExecutionState;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.fail;
@@ -160,11 +164,11 @@ protected void run() {
 						} while (System.currentTimeMillis() < deadline);
 
 						// task should have switched to running
-						Object toRunning = new TaskMessages.UpdateTaskExecutionState(
+						Object toRunning = new UpdateTaskExecutionState(
 								new TaskExecutionState(jid, eid, ExecutionState.RUNNING));
 
 						// task should have switched to finished
-						Object toFinished = new TaskMessages.UpdateTaskExecutionState(
+						Object toFinished = new UpdateTaskExecutionState(
 								new TaskExecutionState(jid, eid, ExecutionState.FINISHED));
 						
 						deadline = System.currentTimeMillis() + 10000;
@@ -682,6 +686,91 @@ protected void run() {
 		}};
 	}
 
+	/**
+	 * Tests that repeated {@link PartitionNotFoundException}s fail the receiver.
+	 */
+	@Test
+	public void testPartitionNotFound() throws Exception {
+
+		new JavaTestKit(system){{
+
+			ActorRef jobManager = null;
+			ActorRef taskManager = null;
+
+			try {
+				final IntermediateDataSetID resultId = new IntermediateDataSetID();
+
+				// Create the JM
+				jobManager = system.actorOf(Props.create(
+						new SimplePartitionStateLookupJobManagerCreator(resultId, getTestActor())));
+
+				final int dataPort = NetUtils.getAvailablePort();
+				taskManager = createTaskManager(jobManager, true, false, dataPort);
+
+				// ---------------------------------------------------------------------------------
+
+				final ActorRef tm = taskManager;
+
+				final JobID jid = new JobID();
+				final JobVertexID vid = new JobVertexID();
+				final ExecutionAttemptID eid = new ExecutionAttemptID();
+
+				final ResultPartitionID partitionId = new ResultPartitionID();
+
+				// Remote location (on the same TM though) for the partition
+				final ResultPartitionLocation loc = ResultPartitionLocation
+						.createRemote(new ConnectionID(
+								new InetSocketAddress("localhost", dataPort), 0));
+
+				final InputChannelDeploymentDescriptor[] icdd =
+						new InputChannelDeploymentDescriptor[] {
+								new InputChannelDeploymentDescriptor(partitionId, loc)};
+
+				final InputGateDeploymentDescriptor igdd =
+						new InputGateDeploymentDescriptor(resultId, 0, icdd);
+
+				final TaskDeploymentDescriptor tdd = new TaskDeploymentDescriptor(
+						jid, vid, eid, "Receiver", 0, 1,
+						new Configuration(), new Configuration(),
+						Tasks.AgnosticReceiver.class.getName(),
+						Collections.<ResultPartitionDeploymentDescriptor>emptyList(),
+						Collections.singletonList(igdd),
+						Collections.<BlobKey>emptyList(), 0);
+
+				new Within(d) {
+					@Override
+					protected void run() {
+						// Submit the task
+						tm.tell(new SubmitTask(tdd), getTestActor());
+						expectMsgClass(Messages.getAcknowledge().getClass());
+
+						// Wait to be notified about the final execution state by the mock JM
+						TaskExecutionState msg = expectMsgClass(TaskExecutionState.class);
+
+						// The task should fail after repeated requests
+						assertEquals(msg.getExecutionState(), ExecutionState.FAILED);
+						assertEquals(msg.getError(ClassLoader.getSystemClassLoader()).getClass(),
+								PartitionNotFoundException.class);
+					}
+				};
+			}
+			catch(Exception e) {
+				e.printStackTrace();
+				fail(e.getMessage());
+			}
+			finally {
+				if (taskManager != null) {
+					taskManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+
+				if (jobManager != null) {
+					jobManager.tell(Kill.getInstance(), ActorRef.noSender());
+				}
+			}
+		}};
+	}
+
+
 	// --------------------------------------------------------------------------------------------
 
 	public static class SimpleJobManager extends UntypedActor {
@@ -693,7 +782,7 @@ public void onReceive(Object message) throws Exception {
 				final ActorRef self = getSelf();
 				getSender().tell(new RegistrationMessages.AcknowledgeRegistration(self, iid, 12345), self);
 			}
-			else if(message instanceof TaskMessages.UpdateTaskExecutionState){
+			else if(message instanceof UpdateTaskExecutionState){
 				getSender().tell(true, getSelf());
 			}
 		}
@@ -703,8 +792,8 @@ public static class SimpleLookupJobManager extends SimpleJobManager {
 
 		@Override
 		public void onReceive(Object message) throws Exception {
-			if (message instanceof JobManagerMessages.ScheduleOrUpdateConsumers) {
-				getSender().tell(new JobManagerMessages.ConsumerNotificationResult(true, scala.Option.<Throwable>apply(null)), getSelf());
+			if (message instanceof ScheduleOrUpdateConsumers) {
+				getSender().tell(new ConsumerNotificationResult(true, scala.Option.<Throwable>apply(null)), getSelf());
 			} else {
 				super.onReceive(message);
 			}
@@ -721,9 +810,9 @@ public SimpleLookupFailingUpdateJobManager(Set<ExecutionAttemptID> ids) {
 
 		@Override
 		public void onReceive(Object message) throws Exception{
-			if (message instanceof TaskMessages.UpdateTaskExecutionState) {
-				TaskMessages.UpdateTaskExecutionState updateMsg =
-						(TaskMessages.UpdateTaskExecutionState) message;
+			if (message instanceof UpdateTaskExecutionState) {
+				UpdateTaskExecutionState updateMsg =
+						(UpdateTaskExecutionState) message;
 
 				if(validIDs.contains(updateMsg.taskExecutionState().getID())) {
 					getSender().tell(true, getSelf());
@@ -736,6 +825,40 @@ public void onReceive(Object message) throws Exception{
 		}
 	}
 
+	public static class SimplePartitionStateLookupJobManager extends SimpleJobManager {
+
+		private final ActorRef testActor;
+
+		public SimplePartitionStateLookupJobManager(ActorRef testActor) {
+			this.testActor = testActor;
+		}
+
+		@Override
+		public void onReceive(Object message) throws Exception {
+			if (message instanceof RequestPartitionState) {
+				final RequestPartitionState msg = (RequestPartitionState) message;
+
+				PartitionState resp = new PartitionState(
+						msg.taskExecutionId(),
+						msg.taskResultId(),
+						msg.partitionId().getPartitionId(),
+						ExecutionState.RUNNING);
+
+				getSender().tell(resp, getSelf());
+			}
+			else if (message instanceof UpdateTaskExecutionState) {
+				final TaskExecutionState msg = ((UpdateTaskExecutionState) message)
+						.taskExecutionState();
+
+				if (msg.getExecutionState().isTerminal()) {
+					testActor.tell(msg, self());
+				}
+			} else {
+				super.onReceive(message);
+			}
+		}
+	}
+
 	public static class SimpleLookupJobManagerCreator implements Creator<SimpleLookupJobManager>{
 
 		@Override
@@ -762,11 +885,30 @@ public SimpleLookupFailingUpdateJobManager create() throws Exception {
 		}
 	}
 
+	public static class SimplePartitionStateLookupJobManagerCreator implements Creator<SimplePartitionStateLookupJobManager>{
+
+		private final ActorRef testActor;
+
+		public SimplePartitionStateLookupJobManagerCreator(IntermediateDataSetID dataSetId, ActorRef testActor) {
+			this.testActor = testActor;
+		}
+
+		@Override
+		public SimplePartitionStateLookupJobManager create() throws Exception {
+			return new SimplePartitionStateLookupJobManager(testActor);
+		}
+	}
+
 	public static ActorRef createTaskManager(ActorRef jobManager, boolean waitForRegistration) {
+		return createTaskManager(jobManager, waitForRegistration, true, NetUtils.getAvailablePort());
+	}
+
+	public static ActorRef createTaskManager(ActorRef jobManager, boolean waitForRegistration, boolean useLocalCommunication, int dataPort) {
 		ActorRef taskManager = null;
 		try {
 			Configuration cfg = new Configuration();
 			cfg.setInteger(ConfigConstants.TASK_MANAGER_MEMORY_SIZE_KEY, 10);
+			cfg.setInteger(ConfigConstants.TASK_MANAGER_DATA_PORT_KEY, dataPort);
 
 			Option<String> jobMangerUrl = Option.apply(jobManager.path().toString());
 
@@ -774,7 +916,7 @@ public static ActorRef createTaskManager(ActorRef jobManager, boolean waitForReg
 					cfg, system, "localhost",
 					Option.<String>empty(),
 					jobMangerUrl,
-					true, TestingTaskManager.class);
+					useLocalCommunication, TestingTaskManager.class);
 		}
 		catch (Exception e) {
 			e.printStackTrace();
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
index 4713bae5e37f2..bcc7ffe917a1e 100644
--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskTest.java
@@ -23,6 +23,7 @@
 import akka.actor.Kill;
 import akka.actor.Props;
 
+import com.google.common.collect.Maps;
 import org.apache.flink.configuration.Configuration;
 import org.apache.flink.runtime.akka.AkkaUtils;
 import org.apache.flink.runtime.blob.BlobKey;
@@ -38,7 +39,10 @@
 import org.apache.flink.api.common.JobID;
 import org.apache.flink.runtime.io.network.NetworkEnvironment;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionConsumableNotifier;
+import org.apache.flink.runtime.io.network.partition.ResultPartitionID;
 import org.apache.flink.runtime.io.network.partition.ResultPartitionManager;
+import org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
 import org.apache.flink.runtime.jobgraph.JobVertexID;
 import org.apache.flink.runtime.jobgraph.tasks.AbstractInvokable;
 import org.apache.flink.runtime.memorymanager.MemoryManager;
@@ -51,7 +55,9 @@
 import org.junit.Test;
 import scala.concurrent.duration.FiniteDuration;
 
+import java.lang.reflect.Field;
 import java.util.Collections;
+import java.util.Map;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.TimeUnit;
@@ -64,8 +70,11 @@
 import static org.junit.Assert.fail;
 
 import static org.mockito.Matchers.any;
+import static org.mockito.Matchers.eq;
 import static org.mockito.Mockito.doThrow;
 import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
 import static org.mockito.Mockito.when;
 
 /**
@@ -560,7 +569,7 @@ public void testExecutionFailesAfterCanceling() {
 	}
 
 	@Test
-	public void testExecutionFailesAfterTaskMarkedFailed() {
+	public void testExecutionFailsAfterTaskMarkedFailed() {
 		try {
 			Task task = createTask(InvokableWithExceptionOnTrigger.class);
 			task.registerExecutionListener(listenerActor);
@@ -595,6 +604,78 @@ public void testExecutionFailesAfterTaskMarkedFailed() {
 		}
 	}
 
+	@Test
+	public void testOnPartitionStateUpdate() throws Exception {
+		IntermediateDataSetID resultId = new IntermediateDataSetID();
+		ResultPartitionID partitionId = new ResultPartitionID();
+
+		SingleInputGate inputGate = mock(SingleInputGate.class);
+		when(inputGate.getConsumedResultId()).thenReturn(resultId);
+
+		final Task task = createTask(InvokableBlockingInInvoke.class);
+
+		// Set the mock input gate
+		setInputGate(task, inputGate);
+
+		// Expected task state for each partition state
+		final Map<ExecutionState, ExecutionState> expected = Maps
+				.newHashMapWithExpectedSize(ExecutionState.values().length);
+
+		// Fail the task for unexpected states
+		for (ExecutionState state : ExecutionState.values()) {
+			expected.put(state, ExecutionState.FAILED);
+		}
+
+		expected.put(ExecutionState.RUNNING, ExecutionState.RUNNING);
+
+		expected.put(ExecutionState.CANCELED, ExecutionState.CANCELING);
+		expected.put(ExecutionState.CANCELING, ExecutionState.CANCELING);
+		expected.put(ExecutionState.FAILED, ExecutionState.CANCELING);
+
+		for (ExecutionState state : ExecutionState.values()) {
+			setState(task, ExecutionState.RUNNING);
+
+			task.onPartitionStateUpdate(resultId, partitionId.getPartitionId(), state);
+
+			ExecutionState newTaskState = task.getExecutionState();
+
+			assertEquals(expected.get(state), newTaskState);
+		}
+
+		verify(inputGate, times(1)).retriggerPartitionRequest(eq(partitionId.getPartitionId()));
+	}
+
+	// ------------------------------------------------------------------------
+
+	private void setInputGate(Task task, SingleInputGate inputGate) {
+		try {
+			Field f = Task.class.getDeclaredField("inputGates");
+			f.setAccessible(true);
+			f.set(task, new SingleInputGate[]{inputGate});
+
+			Map<IntermediateDataSetID, SingleInputGate> byId = Maps.newHashMapWithExpectedSize(1);
+			byId.put(inputGate.getConsumedResultId(), inputGate);
+
+			f = Task.class.getDeclaredField("inputGatesById");
+			f.setAccessible(true);
+			f.set(task, byId);
+		}
+		catch (Exception e) {
+			throw new RuntimeException("Modifying the task state failed", e);
+		}
+	}
+
+	private void setState(Task task, ExecutionState state) {
+		try {
+			Field f = Task.class.getDeclaredField("executionState");
+			f.setAccessible(true);
+			f.set(task, state);
+		}
+		catch (Exception e) {
+			throw new RuntimeException("Modifying the task state failed", e);
+		}
+	}
+
 	private Task createTask(Class<? extends AbstractInvokable> invokable) {
 		LibraryCacheManager libCache = mock(LibraryCacheManager.class);
 		when(libCache.getClassLoader(any(JobID.class))).thenReturn(getClass().getClassLoader());
diff --git a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
index 9069573a44782..f3c061ce784bd 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java
@@ -30,20 +30,20 @@
 import org.junit.Test;
 
 public class ClassLoaderITCase {
-	
+
 	private static final String INPUT_SPLITS_PROG_JAR_FILE = "target/customsplit-test-jar.jar";
 
 	private static final String STREAMING_PROG_JAR_FILE = "target/streamingclassloader-test-jar.jar";
 
 	private static final String KMEANS_JAR_PATH = "target/kmeans-test-jar.jar";
-	
+
 	@Test
 	public void testJobWithCustomInputFormat() {
 		try {
 			Configuration config = new Configuration();
 			config.setInteger(ConfigConstants.LOCAL_INSTANCE_MANAGER_NUMBER_TASK_MANAGER, 2);
 			config.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 2);
-			
+
 			ForkableFlinkMiniCluster testCluster = new ForkableFlinkMiniCluster(config, false);
 
 			try {
diff --git a/flink-tests/src/test/java/org/apache/flink/test/exampleJavaPrograms/WordCountITCase.java b/flink-tests/src/test/java/org/apache/flink/test/exampleJavaPrograms/WordCountITCase.java
index 41b84e6c9c3d5..01f1b00ad2c50 100644
--- a/flink-tests/src/test/java/org/apache/flink/test/exampleJavaPrograms/WordCountITCase.java
+++ b/flink-tests/src/test/java/org/apache/flink/test/exampleJavaPrograms/WordCountITCase.java
@@ -27,7 +27,6 @@ public class WordCountITCase extends JavaProgramTestBase {
 	protected String textPath;
 	protected String resultPath;
 
-
 	@Override
 	protected void preSubmit() throws Exception {
 		textPath = createTempFile("text.txt", WordCountData.TEXT);
