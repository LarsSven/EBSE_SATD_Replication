diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertex.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertex.java
new file mode 100644
index 0000000000000..4b13d70c97d5b
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertex.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.runtime.execution.ExecutionState;
+import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.function.Supplier;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Default implementation of {@link SchedulingExecutionVertex}.
+ */
+class DefaultSchedulingExecutionVertex implements SchedulingExecutionVertex {
+
+	private final ExecutionVertexID executionVertexId;
+
+	private final List<SchedulingResultPartition> consumedPartitions;
+
+	private final List<? extends SchedulingResultPartition> producedPartitions;
+
+	private final Supplier<ExecutionState> stateSupplier;
+
+	DefaultSchedulingExecutionVertex(
+			ExecutionVertexID executionVertexId,
+			List<? extends SchedulingResultPartition> producedPartitions,
+			Supplier<ExecutionState> stateSupplier) {
+		this.executionVertexId = checkNotNull(executionVertexId);
+		this.consumedPartitions = new ArrayList<>();
+		this.stateSupplier = checkNotNull(stateSupplier);
+		this.producedPartitions = checkNotNull(producedPartitions);
+	}
+
+	@Override
+	public ExecutionVertexID getId() {
+		return executionVertexId;
+	}
+
+	@Override
+	public ExecutionState getState() {
+		return stateSupplier.get();
+	}
+
+	@Override
+	public Collection<SchedulingResultPartition> getConsumedResultPartitions() {
+		return Collections.unmodifiableCollection(consumedPartitions);
+	}
+
+	@Override
+	public Collection<SchedulingResultPartition> getProducedResultPartitions() {
+		return Collections.unmodifiableCollection(producedPartitions);
+	}
+
+	<X extends SchedulingResultPartition> void addConsumedPartition(X partition) {
+		consumedPartitions.add(partition);
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartition.java
new file mode 100644
index 0000000000000..45a80dcddb0f7
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartition.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.runtime.io.network.partition.ResultPartitionType;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.DONE;
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.EMPTY;
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.PRODUCING;
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Default implementation of {@link SchedulingResultPartition}.
+ */
+class DefaultSchedulingResultPartition implements SchedulingResultPartition {
+
+	private final IntermediateResultPartitionID resultPartitionId;
+
+	private final IntermediateDataSetID intermediateDataSetId;
+
+	private final ResultPartitionType partitionType;
+
+	private SchedulingExecutionVertex producer;
+
+	private final List<SchedulingExecutionVertex> consumers;
+
+	DefaultSchedulingResultPartition(
+			IntermediateResultPartitionID partitionId,
+			IntermediateDataSetID intermediateDataSetId,
+			ResultPartitionType partitionType) {
+		this.resultPartitionId = checkNotNull(partitionId);
+		this.intermediateDataSetId = checkNotNull(intermediateDataSetId);
+		this.partitionType = checkNotNull(partitionType);
+		this.consumers = new ArrayList<>();
+	}
+
+	@Override
+	public IntermediateResultPartitionID getId() {
+		return resultPartitionId;
+	}
+
+	@Override
+	public IntermediateDataSetID getResultId() {
+		return intermediateDataSetId;
+	}
+
+	@Override
+	public ResultPartitionType getPartitionType() {
+		return partitionType;
+	}
+
+	@Override
+	public ResultPartitionState getState() {
+		switch (producer.getState()) {
+			case RUNNING:
+				return PRODUCING;
+			case FINISHED:
+				return DONE;
+			default:
+				return EMPTY;
+		}
+	}
+
+	@Override
+	public SchedulingExecutionVertex getProducer() {
+		return producer;
+	}
+
+	@Override
+	public Collection<SchedulingExecutionVertex> getConsumers() {
+		return Collections.unmodifiableCollection(consumers);
+	}
+
+	void addConsumer(SchedulingExecutionVertex vertex) {
+		consumers.add(checkNotNull(vertex));
+	}
+
+	void setProducer(SchedulingExecutionVertex vertex) {
+		producer = checkNotNull(vertex);
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java
new file mode 100644
index 0000000000000..abf94697fbeee
--- /dev/null
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapter.java
@@ -0,0 +1,150 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.runtime.execution.ExecutionState;
+import org.apache.flink.runtime.executiongraph.ExecutionEdge;
+import org.apache.flink.runtime.executiongraph.ExecutionGraph;
+import org.apache.flink.runtime.executiongraph.ExecutionVertex;
+import org.apache.flink.runtime.executiongraph.IntermediateResultPartition;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingTopology;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Optional;
+import java.util.function.Supplier;
+
+import static org.apache.flink.util.Preconditions.checkNotNull;
+
+/**
+ * Adapter of {@link ExecutionGraph} to {@link SchedulingTopology}.
+ */
+public class ExecutionGraphToSchedulingTopologyAdapter implements SchedulingTopology {
+
+	private final Map<ExecutionVertexID, DefaultSchedulingExecutionVertex> executionVerticesById;
+
+	private final List<SchedulingExecutionVertex> executionVerticesList;
+
+	private final Map<IntermediateResultPartitionID, ? extends SchedulingResultPartition> resultPartitionsById;
+
+	public ExecutionGraphToSchedulingTopologyAdapter(ExecutionGraph graph) {
+		checkNotNull(graph, "execution graph can not be null");
+
+		this.executionVerticesById = new HashMap<>();
+		this.executionVerticesList = new ArrayList<>(graph.getTotalNumberOfVertices());
+		Map<IntermediateResultPartitionID, DefaultSchedulingResultPartition> tmpResultPartitionsById = new HashMap<>();
+		Map<ExecutionVertex, DefaultSchedulingExecutionVertex> executionVertexMap = new HashMap<>();
+
+		for (ExecutionVertex vertex : graph.getAllExecutionVertices()) {
+			List<DefaultSchedulingResultPartition> producedPartitions = generateProducedSchedulingResultPartition(vertex.getProducedPartitions());
+
+			producedPartitions.forEach(partition -> tmpResultPartitionsById.put(partition.getId(), partition));
+
+			DefaultSchedulingExecutionVertex schedulingVertex = generateSchedulingExecutionVertex(vertex, producedPartitions);
+			this.executionVerticesById.put(schedulingVertex.getId(), schedulingVertex);
+			this.executionVerticesList.add(schedulingVertex);
+			executionVertexMap.put(vertex, schedulingVertex);
+		}
+		this.resultPartitionsById = tmpResultPartitionsById;
+
+		connectVerticesToConsumedPartitions(executionVertexMap, tmpResultPartitionsById);
+	}
+
+	@Override
+	public Iterable<SchedulingExecutionVertex> getVertices() {
+		return executionVerticesList;
+	}
+
+	@Override
+	public Optional<SchedulingExecutionVertex> getVertex(ExecutionVertexID executionVertexId) {
+		return Optional.ofNullable(executionVerticesById.get(executionVertexId));
+	}
+
+	@Override
+	public Optional<SchedulingResultPartition> getResultPartition(IntermediateResultPartitionID intermediateResultPartitionId) {
+		return Optional.ofNullable(resultPartitionsById.get(intermediateResultPartitionId));
+	}
+
+	private static List<DefaultSchedulingResultPartition> generateProducedSchedulingResultPartition(
+		Map<IntermediateResultPartitionID, IntermediateResultPartition> producedIntermediatePartitions) {
+
+		List<DefaultSchedulingResultPartition> producedSchedulingPartitions = new ArrayList<>(producedIntermediatePartitions.size());
+
+		producedIntermediatePartitions.values().forEach(
+			irp -> producedSchedulingPartitions.add(
+				new DefaultSchedulingResultPartition(
+					irp.getPartitionId(),
+					irp.getIntermediateResult().getId(),
+					irp.getResultType())));
+
+		return producedSchedulingPartitions;
+	}
+
+	private static DefaultSchedulingExecutionVertex generateSchedulingExecutionVertex(
+		ExecutionVertex vertex,
+		List<DefaultSchedulingResultPartition> producedPartitions) {
+
+		DefaultSchedulingExecutionVertex schedulingVertex = new DefaultSchedulingExecutionVertex(
+			new ExecutionVertexID(vertex.getJobvertexId(), vertex.getParallelSubtaskIndex()),
+			producedPartitions,
+			new ExecutionStateSupplier(vertex));
+
+		producedPartitions.forEach(partition -> partition.setProducer(schedulingVertex));
+
+		return schedulingVertex;
+	}
+
+	private static void connectVerticesToConsumedPartitions(
+		Map<ExecutionVertex, DefaultSchedulingExecutionVertex> executionVertexMap,
+		Map<IntermediateResultPartitionID, DefaultSchedulingResultPartition> resultPartitions) {
+
+		for (Map.Entry<ExecutionVertex, DefaultSchedulingExecutionVertex> mapEntry : executionVertexMap.entrySet()) {
+			final DefaultSchedulingExecutionVertex schedulingVertex = mapEntry.getValue();
+			final ExecutionVertex executionVertex = mapEntry.getKey();
+
+			for (int index = 0; index < executionVertex.getNumberOfInputs(); index++) {
+				for (ExecutionEdge edge : executionVertex.getInputEdges(index)) {
+					DefaultSchedulingResultPartition partition = resultPartitions.get(edge.getSource().getPartitionId());
+					schedulingVertex.addConsumedPartition(partition);
+					partition.addConsumer(schedulingVertex);
+				}
+			}
+		}
+	}
+
+	private static class ExecutionStateSupplier implements Supplier<ExecutionState> {
+
+		private final ExecutionVertex executionVertex;
+
+		ExecutionStateSupplier(ExecutionVertex vertex) {
+			executionVertex = checkNotNull(vertex);
+		}
+
+		@Override
+		public ExecutionState get() {
+			return executionVertex.getExecutionState();
+		}
+	}
+}
diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
index aefc5613d7ffe..86ea8ba015a02 100644
--- a/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/strategy/SchedulingResultPartition.java
@@ -77,7 +77,7 @@ public interface SchedulingResultPartition {
 	 */
 	enum ResultPartitionState {
 		/**
-		 * Producer is not yet running.
+		 * Producer is not yet running or in abnormal state.
 		 */
 		EMPTY,
 
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertexTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertexTest.java
new file mode 100644
index 0000000000000..e9af7c6980c3a
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingExecutionVertexTest.java
@@ -0,0 +1,112 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.runtime.execution.ExecutionState;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+import org.apache.flink.util.TestLogger;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.function.Supplier;
+
+import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.BLOCKING;
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Unit tests for {@link DefaultSchedulingExecutionVertex}.
+ */
+public class DefaultSchedulingExecutionVertexTest extends TestLogger {
+
+	private final TestExecutionStateSupplier stateSupplier = new TestExecutionStateSupplier();
+
+	private DefaultSchedulingExecutionVertex producerVertex;
+
+	private DefaultSchedulingExecutionVertex consumerVertex;
+
+	private IntermediateResultPartitionID intermediateResultPartitionId;
+
+	@Before
+	public void setUp() throws Exception {
+
+		intermediateResultPartitionId = new IntermediateResultPartitionID();
+
+		DefaultSchedulingResultPartition schedulingResultPartition = new DefaultSchedulingResultPartition(
+			intermediateResultPartitionId,
+			new IntermediateDataSetID(),
+			BLOCKING);
+		producerVertex = new DefaultSchedulingExecutionVertex(
+			new ExecutionVertexID(new JobVertexID(), 0),
+			Collections.singletonList(schedulingResultPartition),
+			stateSupplier);
+		schedulingResultPartition.setProducer(producerVertex);
+		consumerVertex = new DefaultSchedulingExecutionVertex(
+			new ExecutionVertexID(new JobVertexID(), 0),
+			Collections.emptyList(),
+			stateSupplier);
+		consumerVertex.addConsumedPartition(schedulingResultPartition);
+	}
+
+	@Test
+	public void testGetExecutionState() {
+		for (ExecutionState state : ExecutionState.values()) {
+			stateSupplier.setExecutionState(state);
+			assertEquals(state, producerVertex.getState());
+		}
+	}
+
+	@Test
+	public void testGetProducedResultPartitions() {
+		IntermediateResultPartitionID partitionIds1 = producerVertex
+			.getProducedResultPartitions().stream().findAny().map(SchedulingResultPartition::getId)
+			.orElseThrow(() -> new IllegalArgumentException("can not find result partition"));
+		assertEquals(partitionIds1, intermediateResultPartitionId);
+	}
+
+	@Test
+	public void testGetConsumedResultPartitions() {
+		IntermediateResultPartitionID partitionIds1 = consumerVertex
+			.getConsumedResultPartitions().stream().findAny().map(SchedulingResultPartition::getId)
+			.orElseThrow(() -> new IllegalArgumentException("can not find result partition"));
+		assertEquals(partitionIds1, intermediateResultPartitionId);
+	}
+
+	/**
+	 * A simple implementation of {@link Supplier} for testing.
+	 */
+	static class TestExecutionStateSupplier implements Supplier<ExecutionState> {
+
+		private ExecutionState executionState;
+
+		void setExecutionState(ExecutionState state) {
+			executionState = state;
+		}
+
+		@Override
+		public ExecutionState get() {
+			return executionState;
+		}
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartitionTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartitionTest.java
new file mode 100644
index 0000000000000..d114b2ea28f07
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/DefaultSchedulingResultPartitionTest.java
@@ -0,0 +1,102 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.runtime.execution.ExecutionState;
+import org.apache.flink.runtime.jobgraph.IntermediateDataSetID;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.jobgraph.JobVertexID;
+import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+import org.apache.flink.util.TestLogger;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.function.Supplier;
+
+import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.BLOCKING;
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.DONE;
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.EMPTY;
+import static org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition.ResultPartitionState.PRODUCING;
+import static org.junit.Assert.assertEquals;
+
+/**
+ * Unit tests for {@link DefaultSchedulingResultPartition}.
+ */
+public class DefaultSchedulingResultPartitionTest extends TestLogger {
+
+	private static final TestExecutionStateSupplier stateProvider = new TestExecutionStateSupplier();
+
+	private final IntermediateResultPartitionID resultPartitionId = new IntermediateResultPartitionID();
+	private final IntermediateDataSetID intermediateResultId = new IntermediateDataSetID();
+
+	private DefaultSchedulingResultPartition resultPartition;
+
+	@Before
+	public void setUp() {
+		resultPartition = new DefaultSchedulingResultPartition(
+			resultPartitionId,
+			intermediateResultId,
+			BLOCKING);
+
+		DefaultSchedulingExecutionVertex producerVertex = new DefaultSchedulingExecutionVertex(
+			new ExecutionVertexID(new JobVertexID(), 0),
+			Collections.singletonList(resultPartition),
+			stateProvider);
+		resultPartition.setProducer(producerVertex);
+	}
+
+	@Test
+	public void testGetPartitionState() {
+		for (ExecutionState state : ExecutionState.values()) {
+			stateProvider.setExecutionState(state);
+			SchedulingResultPartition.ResultPartitionState partitionState = resultPartition.getState();
+			switch (state) {
+				case RUNNING:
+					assertEquals(PRODUCING, partitionState);
+					break;
+				case FINISHED:
+					assertEquals(DONE, partitionState);
+					break;
+				default:
+					assertEquals(EMPTY, partitionState);
+					break;
+			}
+		}
+	}
+
+	/**
+	 * A simple implementation of {@link Supplier} for testing.
+	 */
+	private static class TestExecutionStateSupplier implements Supplier<ExecutionState> {
+
+		private ExecutionState executionState;
+
+		void setExecutionState(ExecutionState state) {
+			executionState = state;
+		}
+
+		@Override
+		public ExecutionState get() {
+			return executionState;
+		}
+	}
+}
diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapterTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapterTest.java
new file mode 100644
index 0000000000000..0861f13d64c23
--- /dev/null
+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/scheduler/adapter/ExecutionGraphToSchedulingTopologyAdapterTest.java
@@ -0,0 +1,187 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.flink.runtime.scheduler.adapter;
+
+import org.apache.flink.api.common.JobID;
+import org.apache.flink.runtime.executiongraph.ExecutionEdge;
+import org.apache.flink.runtime.executiongraph.ExecutionGraph;
+import org.apache.flink.runtime.executiongraph.ExecutionVertex;
+import org.apache.flink.runtime.executiongraph.IntermediateResultPartition;
+import org.apache.flink.runtime.executiongraph.TestRestartStrategy;
+import org.apache.flink.runtime.executiongraph.utils.SimpleAckingTaskManagerGateway;
+import org.apache.flink.runtime.jobgraph.IntermediateResultPartitionID;
+import org.apache.flink.runtime.jobgraph.JobVertex;
+import org.apache.flink.runtime.scheduler.strategy.ExecutionVertexID;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingExecutionVertex;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingResultPartition;
+import org.apache.flink.runtime.scheduler.strategy.SchedulingTopology;
+import org.apache.flink.util.TestLogger;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+
+import static junit.framework.TestCase.assertTrue;
+import static org.apache.flink.api.common.InputDependencyConstraint.ALL;
+import static org.apache.flink.api.common.InputDependencyConstraint.ANY;
+import static org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.createNoOpVertex;
+import static org.apache.flink.runtime.executiongraph.ExecutionGraphTestUtils.createSimpleTestGraph;
+import static org.apache.flink.runtime.io.network.partition.ResultPartitionType.BLOCKING;
+import static org.apache.flink.runtime.jobgraph.DistributionPattern.ALL_TO_ALL;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+
+/**
+ * Unit tests for {@link ExecutionGraphToSchedulingTopologyAdapter}.
+ */
+public class ExecutionGraphToSchedulingTopologyAdapterTest extends TestLogger {
+
+	private final SimpleAckingTaskManagerGateway taskManagerGateway = new SimpleAckingTaskManagerGateway();
+
+	private final TestRestartStrategy triggeredRestartStrategy = TestRestartStrategy.manuallyTriggered();
+
+	private ExecutionGraph executionGraph;
+
+	private ExecutionGraphToSchedulingTopologyAdapter adapter;
+
+	@Before
+	public void setUp() throws Exception {
+		JobVertex[] jobVertices = new JobVertex[2];
+		int parallelism = 3;
+		jobVertices[0] = createNoOpVertex(parallelism);
+		jobVertices[1] = createNoOpVertex(parallelism);
+		jobVertices[1].connectNewDataSetAsInput(jobVertices[0], ALL_TO_ALL, BLOCKING);
+		jobVertices[0].setInputDependencyConstraint(ALL);
+		jobVertices[1].setInputDependencyConstraint(ANY);
+		executionGraph = createSimpleTestGraph(
+			new JobID(),
+			taskManagerGateway,
+			triggeredRestartStrategy,
+			jobVertices);
+		adapter = new ExecutionGraphToSchedulingTopologyAdapter(executionGraph);
+	}
+
+	@Test
+	public void testConstructor() {
+		// implicitly tests order constraint of getVertices()
+		assertGraphEquals(executionGraph, adapter);
+	}
+
+	@Test
+	public void testGetResultPartition() {
+		for (ExecutionVertex vertex : executionGraph.getAllExecutionVertices()) {
+			for (Map.Entry<IntermediateResultPartitionID, IntermediateResultPartition> entry : vertex.getProducedPartitions().entrySet()) {
+				IntermediateResultPartition partition = entry.getValue();
+				SchedulingResultPartition schedulingResultPartition = adapter.getResultPartition(entry.getKey())
+					.orElseThrow(() -> new IllegalArgumentException("can not find partition " + entry.getKey()));
+
+				assertPartitionEquals(partition, schedulingResultPartition);
+			}
+		}
+	}
+
+	private static void assertGraphEquals(
+		ExecutionGraph originalGraph,
+		SchedulingTopology adaptedTopology) {
+
+		Iterator<ExecutionVertex> originalVertices = originalGraph.getAllExecutionVertices().iterator();
+		Iterator<SchedulingExecutionVertex> adaptedVertices = adaptedTopology.getVertices().iterator();
+
+		while (originalVertices.hasNext()) {
+			ExecutionVertex originalVertex = originalVertices.next();
+			SchedulingExecutionVertex adaptedVertex = adaptedVertices.next();
+
+			assertVertexEquals(originalVertex, adaptedVertex);
+
+			List<IntermediateResultPartition> originalConsumedPartitions = IntStream.range(0, originalVertex.getNumberOfInputs())
+				.mapToObj(originalVertex::getInputEdges)
+				.flatMap(Arrays::stream)
+				.map(ExecutionEdge::getSource)
+				.collect(Collectors.toList());
+			Collection<SchedulingResultPartition> adaptedConsumedPartitions = adaptedVertex.getConsumedResultPartitions();
+
+			assertPartitionsEquals(originalConsumedPartitions, adaptedConsumedPartitions);
+
+			Collection<IntermediateResultPartition> originalProducedPartitions = originalVertex.getProducedPartitions().values();
+			Collection<SchedulingResultPartition> adaptedProducedPartitions = adaptedVertex.getProducedResultPartitions();
+
+			assertPartitionsEquals(originalProducedPartitions, adaptedProducedPartitions);
+		}
+
+		assertFalse("Number of adapted vertices exceeds number of original vertices.", adaptedVertices.hasNext());
+	}
+
+	private static void assertPartitionsEquals(
+		Collection<IntermediateResultPartition> originalPartitions,
+		Collection<SchedulingResultPartition> adaptedPartitions) {
+
+		assertEquals(originalPartitions.size(), adaptedPartitions.size());
+
+		for (IntermediateResultPartition originalPartition : originalPartitions) {
+			SchedulingResultPartition adaptedPartition = adaptedPartitions.stream()
+				.filter(adapted -> adapted.getId().equals(originalPartition.getPartitionId()))
+				.findAny()
+				.orElseThrow(() -> new AssertionError("Could not find matching adapted partition for " + originalPartition));
+
+			assertPartitionEquals(originalPartition, adaptedPartition);
+
+			List<ExecutionVertex> originalConsumers = originalPartition.getConsumers().stream()
+				.flatMap(Collection::stream)
+				.map(ExecutionEdge::getTarget)
+				.collect(Collectors.toList());
+			Collection<SchedulingExecutionVertex> adaptedConsumers = adaptedPartition.getConsumers();
+
+			for (ExecutionVertex originalConsumer : originalConsumers) {
+				// it is sufficient to verify that some vertex exists with the correct ID here,
+				// since deep equality is verified later in the main loop
+				// this DOES rely on an implicit assumption that the vertices objects returned by the topology are
+				// identical to those stored in the partition
+				ExecutionVertexID originalId = new ExecutionVertexID(originalConsumer.getJobvertexId(), originalConsumer.getParallelSubtaskIndex());
+				assertTrue(adaptedConsumers.stream().anyMatch(adaptedConsumer -> adaptedConsumer.getId().equals(originalId)));
+			}
+		}
+	}
+
+	private static void assertPartitionEquals(
+		IntermediateResultPartition originalPartition,
+		SchedulingResultPartition adaptedPartition) {
+
+		assertEquals(originalPartition.getPartitionId(), adaptedPartition.getId());
+		assertEquals(originalPartition.getIntermediateResult().getId(), adaptedPartition.getResultId());
+		assertEquals(originalPartition.getResultType(), adaptedPartition.getPartitionType());
+		assertVertexEquals(
+			originalPartition.getProducer(),
+			adaptedPartition.getProducer());
+	}
+
+	private static void assertVertexEquals(
+		ExecutionVertex originalVertex,
+		SchedulingExecutionVertex adaptedVertex) {
+		assertEquals(
+			new ExecutionVertexID(originalVertex.getJobvertexId(), originalVertex.getParallelSubtaskIndex()),
+			adaptedVertex.getId());
+	}
+}
