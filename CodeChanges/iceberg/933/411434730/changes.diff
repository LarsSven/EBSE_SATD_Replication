diff --git a/README.md b/README.md
index 68f8bf48b61..4e998f13f2c 100644
--- a/README.md
+++ b/README.md
@@ -52,7 +52,7 @@ Community discussions happen primarily on the [dev mailing list][dev-list] or on
 
 ### Building
 
-Iceberg is built using Gradle 5.4.1.
+Iceberg is built using Gradle 5.4.1 with Java 1.8.
 
 * To invoke a build and run tests: `./gradlew build`
 * To skip tests: `./gradlew build -x test`
diff --git a/api/src/main/java/org/apache/iceberg/catalog/SupportsNamespaces.java b/api/src/main/java/org/apache/iceberg/catalog/SupportsNamespaces.java
new file mode 100644
index 00000000000..0e3302a5aa8
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/catalog/SupportsNamespaces.java
@@ -0,0 +1,141 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.catalog;
+
+import com.google.common.collect.ImmutableMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import org.apache.iceberg.exceptions.AlreadyExistsException;
+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;
+import org.apache.iceberg.exceptions.NoSuchNamespaceException;
+
+/**
+ * Catalog methods for working with namespaces.
+ * <p>
+ * If an object such as a table, view, or function exists, its parent namespaces must also exist
+ * and must be returned by the discovery methods {@link #listNamespaces()} and
+ * {@link #listNamespaces(Namespace namespace)}.
+ * <p>
+ * Catalog implementations are not required to maintain the existence of namespaces independent of
+ * objects in a namespace. For example, a function catalog that loads functions using reflection
+ * and uses Java packages as namespaces is not required to support the methods to create, alter, or
+ * drop a namespace. Implementations are allowed to discover the existence of objects or namespaces
+ * without throwing {@link NoSuchNamespaceException} when no namespace is found.
+ */
+public interface SupportsNamespaces {
+  /**
+   * Create a namespace in the catalog.
+   *
+   * @param namespace a namespace. {@link Namespace}.
+   * @throws AlreadyExistsException If the namespace already exists
+   * @throws UnsupportedOperationException If create is not a supported operation
+   */
+  default void createNamespace(Namespace namespace) {
+    createNamespace(namespace, ImmutableMap.of());
+  }
+
+  /**
+   * Create a namespace in the catalog.
+   *
+   * @param namespace a multi-part namespace
+   * @param metadata a string Map of properties for the given namespace
+   * @throws AlreadyExistsException If the namespace already exists
+   * @throws UnsupportedOperationException If create is not a supported operation
+   */
+  void createNamespace(Namespace namespace, Map<String, String> metadata);
+
+  /**
+   * List top-level namespaces from the catalog.
+   * <p>
+   * If an object such as a table, view, or function exists, its parent namespaces must also exist
+   * and must be returned by this discovery method. For example, if table a.b.t exists, this method
+   * must return ["a"] in the result array.
+   *
+   * @return an List of namespace {@link Namespace} names
+   */
+  default List<Namespace> listNamespaces() {
+    return listNamespaces(Namespace.empty());
+  }
+
+  /**
+   * List namespaces from the namespace.
+   * <p>
+   * For example, if table a.b.t exists, use 'SELECT NAMESPACE IN a' this method
+   * must return Namepace.of("a","b") {@link Namespace}.
+   *
+   * @return a List of namespace {@link Namespace} names
+   * @throws NoSuchNamespaceException If the namespace does not exist (optional)
+   */
+  List<Namespace> listNamespaces(Namespace namespace) throws NoSuchNamespaceException;
+
+  /**
+   * Load metadata properties for a namespace.
+   *
+   * @param namespace a namespace. {@link Namespace}
+   * @return a string map of properties for the given namespace
+   * @throws NoSuchNamespaceException If the namespace does not exist (optional)
+   */
+  Map<String, String> loadNamespaceMetadata(Namespace namespace) throws NoSuchNamespaceException;
+
+  /**
+   * Drop a namespace. If the namespace exists and was dropped, this will return true.
+   *
+   * @param namespace a namespace. {@link Namespace}
+   * @return true if the namespace was dropped, false otherwise.
+   * @throws NamespaceNotEmptyException If the namespace does not empty
+   */
+  boolean dropNamespace(Namespace namespace) throws NamespaceNotEmptyException;
+
+  /**
+   * Apply a set of metadata to a namespace in the catalog.
+   *
+   * @param namespace a namespace. {@link Namespace}
+   * @param properties a collection of metadata to apply to the namespace
+   * @throws NoSuchNamespaceException If the namespace does not exist (optional)
+   * @throws UnsupportedOperationException If namespace properties are not supported
+   */
+  boolean setProperties(Namespace namespace, Map<String, String> properties) throws NoSuchNamespaceException;
+
+  /**
+   * Remove a set of metadata from a namespace in the catalog.
+   *
+   * @param namespace a namespace. {@link Namespace}
+   * @param properties a collection of metadata to apply to the namespace
+   * @throws NoSuchNamespaceException If the namespace does not exist (optional)
+   * @throws UnsupportedOperationException If namespace properties are not supported
+   */
+  boolean removeProperties(Namespace namespace, Set<String> properties) throws NoSuchNamespaceException;
+
+  /**
+   * Checks whether the Namespace exists.
+   *
+   * @param namespace a namespace. {@link Namespace}
+   * @return true if the Namespace exists, false otherwise
+   */
+  default boolean namespaceExists(Namespace namespace) {
+    try {
+      loadNamespaceMetadata(namespace);
+      return true;
+    } catch (NoSuchNamespaceException e) {
+      return false;
+    }
+  }
+}
diff --git a/api/src/main/java/org/apache/iceberg/events/Listeners.java b/api/src/main/java/org/apache/iceberg/events/Listeners.java
index 3b279c0f1a0..4a70a79d244 100644
--- a/api/src/main/java/org/apache/iceberg/events/Listeners.java
+++ b/api/src/main/java/org/apache/iceberg/events/Listeners.java
@@ -20,11 +20,10 @@
 package org.apache.iceberg.events;
 
 import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-import java.util.Iterator;
-import java.util.List;
 import java.util.Map;
+import java.util.Queue;
+import java.util.concurrent.ConcurrentLinkedQueue;
 
 /**
  * Static registration and notification for listeners.
@@ -33,21 +32,10 @@ public class Listeners {
   private Listeners() {
   }
 
-  private static final Map<Class<?>, List<Listener<?>>> listeners = Maps.newConcurrentMap();
+  private static final Map<Class<?>, Queue<Listener<?>>> listeners = Maps.newConcurrentMap();
 
   public static <E> void register(Listener<E> listener, Class<E> eventType) {
-    List<Listener<?>> list = listeners.get(eventType);
-
-    if (list == null) {
-      synchronized (listeners) {
-        list = listeners.get(eventType);
-        if (list == null) {
-          list = Lists.newArrayList();
-          listeners.put(eventType, list);
-        }
-      }
-    }
-
+    Queue<Listener<?>> list = listeners.computeIfAbsent(eventType, k -> new ConcurrentLinkedQueue<>());
     list.add(listener);
   }
 
@@ -55,11 +43,10 @@ public static <E> void register(Listener<E> listener, Class<E> eventType) {
   public static <E> void notifyAll(E event) {
     Preconditions.checkNotNull(event, "Cannot notify listeners for a null event.");
 
-    List<Listener<?>> list = listeners.get(event.getClass());
+    Queue<Listener<?>> list = listeners.get(event.getClass());
     if (list != null) {
-      Iterator<Listener<?>> iter = list.iterator();
-      while (iter.hasNext()) {
-        Listener<E> listener = (Listener<E>) iter.next();
+      for (Listener<?> value : list) {
+        Listener<E> listener = (Listener<E>) value;
         listener.notify(event);
       }
     }
diff --git a/api/src/main/java/org/apache/iceberg/exceptions/NamespaceNotEmptyException.java b/api/src/main/java/org/apache/iceberg/exceptions/NamespaceNotEmptyException.java
new file mode 100644
index 00000000000..130cd3e4c0c
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/exceptions/NamespaceNotEmptyException.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.exceptions;
+
+/**
+ * Exception raised when attempting to drop a namespace that is not empty.
+ */
+public class NamespaceNotEmptyException extends RuntimeException {
+  public NamespaceNotEmptyException(String message, Object... args) {
+    super(String.format(message, args));
+  }
+
+  public NamespaceNotEmptyException(Throwable cause, String message, Object... args) {
+    super(String.format(message, args), cause);
+  }
+}
diff --git a/api/src/main/java/org/apache/iceberg/exceptions/NoSuchNamespaceException.java b/api/src/main/java/org/apache/iceberg/exceptions/NoSuchNamespaceException.java
new file mode 100644
index 00000000000..f9348f75dd2
--- /dev/null
+++ b/api/src/main/java/org/apache/iceberg/exceptions/NoSuchNamespaceException.java
@@ -0,0 +1,33 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.exceptions;
+
+/**
+ * Exception raised when attempting to load a namespace that does not exist.
+ */
+public class NoSuchNamespaceException extends RuntimeException {
+  public NoSuchNamespaceException(String message, Object... args) {
+    super(String.format(message, args));
+  }
+
+  public NoSuchNamespaceException(Throwable cause, String message, Object... args) {
+    super(String.format(message, args), cause);
+  }
+}
diff --git a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
index ce485fee4c4..d59292f1410 100644
--- a/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
+++ b/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.arrow.vectorized;
 
-import javax.annotation.Nullable;
 import org.apache.arrow.vector.FieldVector;
 import org.apache.parquet.column.ColumnDescriptor;
 import org.apache.parquet.column.Dictionary;
@@ -33,7 +32,6 @@ public class VectorHolder {
   private final FieldVector vector;
   private final boolean isDictionaryEncoded;
 
-  @Nullable
   private final Dictionary dictionary;
   private final NullabilityHolder nullabilityHolder;
 
diff --git a/build.gradle b/build.gradle
index b8cd487b44d..85a0cb5d35c 100644
--- a/build.gradle
+++ b/build.gradle
@@ -94,6 +94,13 @@ subprojects {
     testCompile 'org.slf4j:slf4j-simple'
     testCompile 'org.mockito:mockito-core'
   }
+
+  test {
+    testLogging {
+      events "failed"
+      exceptionFormat "full"
+    }
+  }
 }
 
 apply from: 'baseline.gradle'
@@ -306,9 +313,11 @@ project(':iceberg-arrow') {
     compile("org.apache.arrow:arrow-vector") {
       exclude group: 'io.netty', module: 'netty-buffer'
       exclude group: 'io.netty', module: 'netty-common'
+      exclude group: 'com.google.code.findbugs', module: 'jsr305'
     }
     compile("org.apache.arrow:arrow-memory") {
       exclude group: 'io.netty', module: 'netty-common'
+      exclude group: 'com.google.code.findbugs', module: 'jsr305'
     }
   }
 }
@@ -320,6 +329,7 @@ project(':iceberg-spark') {
     compile project(':iceberg-api')
     compile project(':iceberg-common')
     compile project(':iceberg-core')
+    compile project(':iceberg-data')
     compile project(':iceberg-orc')
     compile project(':iceberg-parquet')
     compile project(':iceberg-arrow')
@@ -330,7 +340,6 @@ project(':iceberg-spark') {
       exclude group: 'org.apache.avro', module: 'avro'
     }
 
-    testCompile project(':iceberg-data')
     testCompile "org.apache.hadoop:hadoop-hdfs::tests"
     testCompile "org.apache.hadoop:hadoop-common::tests"
     testCompile("org.apache.hadoop:hadoop-minicluster") {
@@ -402,6 +411,7 @@ project(':iceberg-spark-runtime') {
       exclude group: 'commons-codec'
       exclude group: 'org.xerial.snappy'
       exclude group: 'javax.xml.bind'
+      exclude group: 'javax.annotation'
     }
   }
 
@@ -419,6 +429,7 @@ project(':iceberg-spark-runtime') {
     from(projectDir) {
       include 'LICENSE'
       include 'NOTICE'
+      include 'DISCLAIMER'
     }
 
     // Relocate dependencies to avoid conflicts
@@ -438,6 +449,7 @@ project(':iceberg-spark-runtime') {
     // relocate Arrow and related deps to shade Iceberg specific version
     relocate 'io.netty.buffer', 'org.apache.iceberg.shaded.io.netty.buffer'
     relocate 'org.apache.arrow', 'org.apache.iceberg.shaded.org.apache.arrow'
+    relocate 'com.carrotsearch', 'org.apache.iceberg.shaded.com.carrotsearch'
 
     archiveName = "iceberg-spark-runtime-${version}.${extension}"
   }
diff --git a/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java b/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
index 7cadf91077a..5e1d3a2bc75 100644
--- a/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
+++ b/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
@@ -39,6 +39,7 @@
 import java.util.stream.Collectors;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.exceptions.ValidationException;
+import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.Tasks;
@@ -56,9 +57,6 @@ public class BaseRewriteManifests extends SnapshotProducer<RewriteManifests> imp
   private static final String REPLACED_MANIFESTS_COUNT = "manifests-replaced";
   private static final String PROCESSED_ENTRY_COUNT = "entries-processed";
 
-  private static final ImmutableSet<ManifestEntry.Status> ALLOWED_ENTRY_STATUSES = ImmutableSet.of(
-      ManifestEntry.Status.EXISTING);
-
   private final TableOperations ops;
   private final Map<Integer, PartitionSpec> specsById;
   private final long manifestTargetSizeBytes;
@@ -154,13 +152,11 @@ public RewriteManifests addManifest(ManifestFile manifest) {
   }
 
   private ManifestFile copyManifest(ManifestFile manifest) {
-    try (ManifestReader reader = ManifestFiles.read(manifest, ops.io(), specsById)) {
-      OutputFile newFile = newManifestOutput();
-      return ManifestFiles.copyManifest(
-          ops.current().formatVersion(), reader, newFile, snapshotId(), summaryBuilder, ALLOWED_ENTRY_STATUSES);
-    } catch (IOException e) {
-      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
-    }
+    TableMetadata current = ops.current();
+    InputFile toCopy = ops.io().newInputFile(manifest.path());
+    OutputFile newFile = newManifestOutput();
+    return ManifestFiles.copyRewriteManifest(
+        current.formatVersion(), toCopy, specsById, newFile, snapshotId(), summaryBuilder);
   }
 
   @Override
diff --git a/core/src/main/java/org/apache/iceberg/DataTableScan.java b/core/src/main/java/org/apache/iceberg/DataTableScan.java
index 10f46d925eb..f3a5d923594 100644
--- a/core/src/main/java/org/apache/iceberg/DataTableScan.java
+++ b/core/src/main/java/org/apache/iceberg/DataTableScan.java
@@ -26,12 +26,8 @@
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.util.ThreadPools;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 public class DataTableScan extends BaseTableScan {
-  private static final Logger LOG = LoggerFactory.getLogger(DataTableScan.class);
-
   static final ImmutableList<String> SCAN_COLUMNS = ImmutableList.of(
       "snapshot_id", "file_path", "file_ordinal", "file_format", "block_size_in_bytes",
       "file_size_in_bytes", "record_count", "partition", "key_metadata"
diff --git a/core/src/main/java/org/apache/iceberg/FastAppend.java b/core/src/main/java/org/apache/iceberg/FastAppend.java
index f0dda9a19b6..9596932af8a 100644
--- a/core/src/main/java/org/apache/iceberg/FastAppend.java
+++ b/core/src/main/java/org/apache/iceberg/FastAppend.java
@@ -28,6 +28,7 @@
 import java.util.Set;
 import org.apache.iceberg.exceptions.CommitFailedException;
 import org.apache.iceberg.exceptions.RuntimeIOException;
+import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.io.OutputFile;
 
 import static org.apache.iceberg.TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED;
@@ -107,13 +108,11 @@ public FastAppend appendManifest(ManifestFile manifest) {
   }
 
   private ManifestFile copyManifest(ManifestFile manifest) {
-    try (ManifestReader reader = ManifestFiles.read(manifest, ops.io(), ops.current().specsById())) {
-      OutputFile newManifestPath = newManifestOutput();
-      return ManifestFiles.copyAppendManifest(
-          ops.current().formatVersion(), reader, newManifestPath, snapshotId(), summaryBuilder);
-    } catch (IOException e) {
-      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
-    }
+    TableMetadata current = ops.current();
+    InputFile toCopy = ops.io().newInputFile(manifest.path());
+    OutputFile newManifestPath = newManifestOutput();
+    return ManifestFiles.copyAppendManifest(
+        current.formatVersion(), toCopy, current.specsById(), newManifestPath, snapshotId(), summaryBuilder);
   }
 
   @Override
diff --git a/core/src/main/java/org/apache/iceberg/GenericManifestEntry.java b/core/src/main/java/org/apache/iceberg/GenericManifestEntry.java
new file mode 100644
index 00000000000..9fc29a62bbf
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/GenericManifestEntry.java
@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg;
+
+import com.google.common.base.MoreObjects;
+import org.apache.avro.generic.IndexedRecord;
+import org.apache.avro.specific.SpecificData;
+import org.apache.iceberg.avro.AvroSchemaUtil;
+import org.apache.iceberg.types.Types;
+
+class GenericManifestEntry implements ManifestEntry, IndexedRecord, SpecificData.SchemaConstructable, StructLike {
+  private final org.apache.avro.Schema schema;
+  private final V1Metadata.IndexedDataFile fileWrapper;
+  private Status status = Status.EXISTING;
+  private Long snapshotId = null;
+  private Long sequenceNumber = null;
+  private DataFile file = null;
+
+  GenericManifestEntry(org.apache.avro.Schema schema) {
+    this.schema = schema;
+    this.fileWrapper = null; // do not use the file wrapper to read
+  }
+
+  GenericManifestEntry(Types.StructType partitionType) {
+    this.schema = AvroSchemaUtil.convert(V1Metadata.entrySchema(partitionType), "manifest_entry");
+    this.fileWrapper = new V1Metadata.IndexedDataFile(schema.getField("data_file").schema());
+  }
+
+  private GenericManifestEntry(GenericManifestEntry toCopy, boolean fullCopy) {
+    this.schema = toCopy.schema;
+    this.fileWrapper = new V1Metadata.IndexedDataFile(schema.getField("data_file").schema());
+    this.status = toCopy.status;
+    this.snapshotId = toCopy.snapshotId;
+    if (fullCopy) {
+      this.file = toCopy.file().copy();
+    } else {
+      this.file = toCopy.file().copyWithoutStats();
+    }
+  }
+
+  ManifestEntry wrapExisting(Long newSnapshotId, Long newSequenceNumber, DataFile newFile) {
+    this.status = Status.EXISTING;
+    this.snapshotId = newSnapshotId;
+    this.sequenceNumber = newSequenceNumber;
+    this.file = newFile;
+    return this;
+  }
+
+  ManifestEntry wrapAppend(Long newSnapshotId, DataFile newFile) {
+    this.status = Status.ADDED;
+    this.snapshotId = newSnapshotId;
+    this.sequenceNumber = null;
+    this.file = newFile;
+    return this;
+  }
+
+  ManifestEntry wrapDelete(Long newSnapshotId, DataFile newFile) {
+    this.status = Status.DELETED;
+    this.snapshotId = newSnapshotId;
+    this.sequenceNumber = null;
+    this.file = newFile;
+    return this;
+  }
+
+  /**
+   * @return the status of the file, whether EXISTING, ADDED, or DELETED
+   */
+  @Override
+  public Status status() {
+    return status;
+  }
+
+  /**
+   * @return id of the snapshot in which the file was added to the table
+   */
+  @Override
+  public Long snapshotId() {
+    return snapshotId;
+  }
+
+  @Override
+  public Long sequenceNumber() {
+    return sequenceNumber;
+  }
+
+  /**
+   * @return a file
+   */
+  @Override
+  public DataFile file() {
+    return file;
+  }
+
+  @Override
+  public ManifestEntry copy() {
+    return new GenericManifestEntry(this, true /* full copy */);
+  }
+
+  @Override
+  public ManifestEntry copyWithoutStats() {
+    return new GenericManifestEntry(this, false /* drop stats */);
+  }
+
+  @Override
+  public void setSnapshotId(long newSnapshotId) {
+    this.snapshotId = newSnapshotId;
+  }
+
+  @Override
+  public void setSequenceNumber(long newSequenceNumber) {
+    this.sequenceNumber = newSequenceNumber;
+  }
+
+  @Override
+  public void put(int i, Object v) {
+    switch (i) {
+      case 0:
+        this.status = Status.values()[(Integer) v];
+        return;
+      case 1:
+        this.snapshotId = (Long) v;
+        return;
+      case 2:
+        this.sequenceNumber = (Long) v;
+        return;
+      case 3:
+        this.file = (DataFile) v;
+        return;
+      default:
+        // ignore the object, it must be from a newer version of the format
+    }
+  }
+
+  @Override
+  public <T> void set(int pos, T value) {
+    put(pos, value);
+  }
+
+  @Override
+  public Object get(int i) {
+    switch (i) {
+      case 0:
+        return status.id();
+      case 1:
+        return snapshotId;
+      case 2:
+        return sequenceNumber;
+      case 3:
+        if (fileWrapper == null || file instanceof GenericDataFile) {
+          return file;
+        } else {
+          return fileWrapper.wrap(file);
+        }
+      default:
+        throw new UnsupportedOperationException("Unknown field ordinal: " + i);
+    }
+  }
+
+  @Override
+  public <T> T get(int pos, Class<T> javaClass) {
+    return javaClass.cast(get(pos));
+  }
+
+  @Override
+  public org.apache.avro.Schema getSchema() {
+    return schema;
+  }
+
+  @Override
+  public int size() {
+    return 4;
+  }
+
+  @Override
+  public String toString() {
+    return MoreObjects.toStringHelper(this)
+        .add("status", status)
+        .add("snapshot_id", snapshotId)
+        .add("sequence_number", sequenceNumber)
+        .add("file", file)
+        .toString();
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/IndexedStructLike.java b/core/src/main/java/org/apache/iceberg/IndexedStructLike.java
new file mode 100644
index 00000000000..cf5d0e6a3e3
--- /dev/null
+++ b/core/src/main/java/org/apache/iceberg/IndexedStructLike.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg;
+
+import org.apache.avro.generic.IndexedRecord;
+
+/**
+ * IndexedRecord implementation to wrap a StructLike for writing to Avro.
+ */
+class IndexedStructLike implements StructLike, IndexedRecord {
+  private final org.apache.avro.Schema avroSchema;
+  private StructLike wrapped = null;
+
+  IndexedStructLike(org.apache.avro.Schema avroSchema) {
+    this.avroSchema = avroSchema;
+  }
+
+  IndexedStructLike wrap(StructLike struct) {
+    this.wrapped = struct;
+    return this;
+  }
+
+  @Override
+  public int size() {
+    return wrapped.size();
+  }
+
+  @Override
+  public <T> T get(int pos, Class<T> javaClass) {
+    return wrapped.get(pos, javaClass);
+  }
+
+  @Override
+  public Object get(int pos) {
+    return get(pos, Object.class);
+  }
+
+  @Override
+  public <T> void set(int pos, T value) {
+    wrapped.set(pos, value);
+  }
+
+  @Override
+  public void put(int pos, Object value) {
+    set(pos, value);
+  }
+
+  @Override
+  public org.apache.avro.Schema getSchema() {
+    return avroSchema;
+  }
+}
diff --git a/core/src/main/java/org/apache/iceberg/InheritableMetadataFactory.java b/core/src/main/java/org/apache/iceberg/InheritableMetadataFactory.java
index 384ea64baa4..043428053fd 100644
--- a/core/src/main/java/org/apache/iceberg/InheritableMetadataFactory.java
+++ b/core/src/main/java/org/apache/iceberg/InheritableMetadataFactory.java
@@ -19,6 +19,8 @@
 
 package org.apache.iceberg;
 
+import com.google.common.base.Preconditions;
+
 class InheritableMetadataFactory {
 
   private static final InheritableMetadata EMPTY = new EmptyInheritableMetadata();
@@ -30,15 +32,22 @@ static InheritableMetadata empty() {
   }
 
   static InheritableMetadata fromManifest(ManifestFile manifest) {
-    return new BaseInheritableMetadata(manifest.snapshotId());
+    Preconditions.checkArgument(manifest.snapshotId() != null,
+        "Cannot read from ManifestFile with null (unassigned) snapshot ID");
+    return new BaseInheritableMetadata(manifest.snapshotId(), manifest.sequenceNumber());
   }
 
-  static class BaseInheritableMetadata implements InheritableMetadata {
+  static InheritableMetadata forCopy(long snapshotId) {
+    return new CopyMetadata(snapshotId);
+  }
 
-    private final Long snapshotId;
+  static class BaseInheritableMetadata implements InheritableMetadata {
+    private final long snapshotId;
+    private final long sequenceNumber;
 
-    private BaseInheritableMetadata(Long snapshotId) {
+    private BaseInheritableMetadata(long snapshotId, long sequenceNumber) {
       this.snapshotId = snapshotId;
+      this.sequenceNumber = sequenceNumber;
     }
 
     @Override
@@ -46,6 +55,23 @@ public ManifestEntry apply(ManifestEntry manifestEntry) {
       if (manifestEntry.snapshotId() == null) {
         manifestEntry.setSnapshotId(snapshotId);
       }
+      if (manifestEntry.sequenceNumber() == null) {
+        manifestEntry.setSequenceNumber(sequenceNumber);
+      }
+      return manifestEntry;
+    }
+  }
+
+  static class CopyMetadata implements InheritableMetadata {
+    private final long snapshotId;
+
+    private CopyMetadata(long snapshotId) {
+      this.snapshotId = snapshotId;
+    }
+
+    @Override
+    public ManifestEntry apply(ManifestEntry manifestEntry) {
+      manifestEntry.setSnapshotId(snapshotId);
       return manifestEntry;
     }
   }
diff --git a/core/src/main/java/org/apache/iceberg/ManifestEntriesTable.java b/core/src/main/java/org/apache/iceberg/ManifestEntriesTable.java
index 56646780854..d2458f80a93 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestEntriesTable.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestEntriesTable.java
@@ -19,12 +19,14 @@
 
 package org.apache.iceberg;
 
+import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Sets;
 import java.util.Collection;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.ResidualEvaluator;
 import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.types.TypeUtil;
 
 /**
@@ -100,11 +102,39 @@ protected long targetSplitSize(TableOperations ops) {
     protected CloseableIterable<FileScanTask> planFiles(
         TableOperations ops, Snapshot snapshot, Expression rowFilter, boolean caseSensitive, boolean colStats) {
       CloseableIterable<ManifestFile> manifests = CloseableIterable.withNoopClose(snapshot.manifests());
+      Schema fileSchema = new Schema(schema().findType("data_file").asStructType().fields());
       String schemaString = SchemaParser.toJson(schema());
       String specString = PartitionSpecParser.toJson(PartitionSpec.unpartitioned());
+      ResidualEvaluator residuals = ResidualEvaluator.unpartitioned(rowFilter);
 
-      return CloseableIterable.transform(manifests, manifest -> new BaseFileScanTask(
-          DataFiles.fromManifest(manifest), schemaString, specString, ResidualEvaluator.unpartitioned(rowFilter)));
+      return CloseableIterable.transform(manifests, manifest ->
+          new ManifestReadTask(ops.io(), manifest, fileSchema, schemaString, specString, residuals));
+    }
+  }
+
+  static class ManifestReadTask extends BaseFileScanTask implements DataTask {
+    private final Schema fileSchema;
+    private final FileIO io;
+    private final ManifestFile manifest;
+
+    ManifestReadTask(FileIO io, ManifestFile manifest, Schema fileSchema, String schemaString,
+                     String specString, ResidualEvaluator residuals) {
+      super(DataFiles.fromManifest(manifest), schemaString, specString, residuals);
+      this.fileSchema = fileSchema;
+      this.io = io;
+      this.manifest = manifest;
+    }
+
+    @Override
+    public CloseableIterable<StructLike> rows() {
+      return CloseableIterable.transform(
+          ManifestFiles.read(manifest, io).project(fileSchema).allEntries(),
+          file -> (GenericManifestEntry) file);
+    }
+
+    @Override
+    public Iterable<FileScanTask> split(long splitSize) {
+      return ImmutableList.of(this); // don't split
     }
   }
 }
diff --git a/core/src/main/java/org/apache/iceberg/ManifestEntry.java b/core/src/main/java/org/apache/iceberg/ManifestEntry.java
index d0f85d6f5fe..bc03850c363 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestEntry.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestEntry.java
@@ -19,22 +19,13 @@
 
 package org.apache.iceberg;
 
-import com.google.common.base.MoreObjects;
-import java.nio.ByteBuffer;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-import org.apache.avro.generic.IndexedRecord;
-import org.apache.avro.specific.SpecificData;
-import org.apache.iceberg.avro.AvroSchemaUtil;
-import org.apache.iceberg.types.Types.IntegerType;
-import org.apache.iceberg.types.Types.LongType;
+import org.apache.iceberg.types.Types;
 import org.apache.iceberg.types.Types.StructType;
 
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 
-class ManifestEntry implements IndexedRecord, SpecificData.SchemaConstructable {
+interface ManifestEntry {
   enum Status {
     EXISTING(0),
     ADDED(1),
@@ -51,326 +42,56 @@ public int id() {
     }
   }
 
-  private final org.apache.avro.Schema schema;
-  private final IndexedDataFile fileWrapper;
-  private Status status = Status.EXISTING;
-  private Long snapshotId = null;
-  private DataFile file = null;
+  // ids for data-file columns are assigned from 1000
+  Types.NestedField STATUS = required(0, "status", Types.IntegerType.get());
+  Types.NestedField SNAPSHOT_ID = optional(1, "snapshot_id", Types.LongType.get());
+  Types.NestedField SEQUENCE_NUMBER = optional(3, "sequence_number", Types.LongType.get());
+  int DATA_FILE_ID = 2;
+  // next ID to assign: 4
 
-  ManifestEntry(org.apache.avro.Schema schema) {
-    this.schema = schema;
-    this.fileWrapper = null; // do not use the file wrapper to read
-  }
-
-  ManifestEntry(StructType partitionType) {
-    this.schema = AvroSchemaUtil.convert(getSchema(partitionType), "manifest_entry");
-    this.fileWrapper = new IndexedDataFile(schema.getField("data_file").schema());
-  }
-
-  private ManifestEntry(ManifestEntry toCopy, boolean fullCopy) {
-    this.schema = toCopy.schema;
-    this.fileWrapper = new IndexedDataFile(schema.getField("data_file").schema());
-    this.status = toCopy.status;
-    this.snapshotId = toCopy.snapshotId;
-    if (fullCopy) {
-      this.file = toCopy.file().copy();
-    } else {
-      this.file = toCopy.file().copyWithoutStats();
-    }
-  }
-
-  ManifestEntry wrapExisting(Long newSnapshotId, DataFile newFile) {
-    this.status = Status.EXISTING;
-    this.snapshotId = newSnapshotId;
-    this.file = newFile;
-    return this;
-  }
-
-  ManifestEntry wrapAppend(Long newSnapshotId, DataFile newFile) {
-    this.status = Status.ADDED;
-    this.snapshotId = newSnapshotId;
-    this.file = newFile;
-    return this;
+  static Schema getSchema(StructType partitionType) {
+    return wrapFileSchema(DataFile.getType(partitionType));
   }
 
-  ManifestEntry wrapDelete(Long newSnapshotId, DataFile newFile) {
-    this.status = Status.DELETED;
-    this.snapshotId = newSnapshotId;
-    this.file = newFile;
-    return this;
+  static Schema wrapFileSchema(StructType fileType) {
+    return new Schema(STATUS, SNAPSHOT_ID, SEQUENCE_NUMBER, required(DATA_FILE_ID, "data_file", fileType));
   }
 
   /**
    * @return the status of the file, whether EXISTING, ADDED, or DELETED
    */
-  public Status status() {
-    return status;
-  }
+  Status status();
 
   /**
    * @return id of the snapshot in which the file was added to the table
    */
-  public Long snapshotId() {
-    return snapshotId;
-  }
+  Long snapshotId();
 
   /**
-   * @return a file
+   * Set the snapshot id for this manifest entry.
+   *
+   * @param snapshotId a long snapshot id
    */
-  public DataFile file() {
-    return file;
-  }
-
-  public ManifestEntry copy() {
-    return new ManifestEntry(this, true /* full copy */);
-  }
-
-  public ManifestEntry copyWithoutStats() {
-    return new ManifestEntry(this, false /* drop stats */);
-  }
-
-  public void setSnapshotId(Long snapshotId) {
-    this.snapshotId = snapshotId;
-  }
-
-  @Override
-  public void put(int i, Object v) {
-    switch (i) {
-      case 0:
-        this.status = Status.values()[(Integer) v];
-        return;
-      case 1:
-        this.snapshotId = (Long) v;
-        return;
-      case 2:
-        this.file = (DataFile) v;
-        return;
-      default:
-        // ignore the object, it must be from a newer version of the format
-    }
-  }
-
-  @Override
-  public Object get(int i) {
-    switch (i) {
-      case 0:
-        return status.id();
-      case 1:
-        return snapshotId;
-      case 2:
-        if (fileWrapper == null || file instanceof GenericDataFile) {
-          return file;
-        } else {
-          return fileWrapper.wrap(file);
-        }
-      default:
-        throw new UnsupportedOperationException("Unknown field ordinal: " + i);
-    }
-  }
-
-  @Override
-  public org.apache.avro.Schema getSchema() {
-    return schema;
-  }
-
-  static Schema getSchema(StructType partitionType) {
-    return wrapFileSchema(DataFile.getType(partitionType));
-  }
-
-  static Schema projectSchema(StructType partitionType, Collection<String> columns) {
-    return wrapFileSchema(
-        new Schema(DataFile.getType(partitionType).fields()).select(columns).asStruct());
-  }
-
-  static Schema wrapFileSchema(StructType fileStruct) {
-    // ids for top-level columns are assigned from 1000
-    return new Schema(
-        required(0, "status", IntegerType.get()),
-        optional(1, "snapshot_id", LongType.get()),
-        required(2, "data_file", fileStruct));
-  }
-
-  @Override
-  public String toString() {
-    return MoreObjects.toStringHelper(this)
-        .add("status", status)
-        .add("snapshot_id", snapshotId)
-        .add("file", file)
-        .toString();
-  }
-
-  private static class IndexedStructLike implements StructLike, IndexedRecord {
-    private final org.apache.avro.Schema avroSchema;
-    private StructLike wrapped = null;
-
-    IndexedStructLike(org.apache.avro.Schema avroSchema) {
-      this.avroSchema = avroSchema;
-    }
-
-    public IndexedStructLike wrap(StructLike struct) {
-      this.wrapped = struct;
-      return this;
-    }
-
-    @Override
-    public int size() {
-      return wrapped.size();
-    }
-
-    @Override
-    public <T> T get(int pos, Class<T> javaClass) {
-      return wrapped.get(pos, javaClass);
-    }
-
-    @Override
-    public Object get(int pos) {
-      return get(pos, Object.class);
-    }
+  void setSnapshotId(long snapshotId);
 
-    @Override
-    public <T> void set(int pos, T value) {
-      wrapped.set(pos, value);
-    }
-
-    @Override
-    public void put(int pos, Object value) {
-      set(pos, value);
-    }
-
-    @Override
-    public org.apache.avro.Schema getSchema() {
-      return avroSchema;
-    }
-  }
-
-  private static class IndexedDataFile implements DataFile, IndexedRecord {
-    private static final long DEFAULT_BLOCK_SIZE = 64 * 1024 * 1024;
-
-    private final org.apache.avro.Schema avroSchema;
-    private final IndexedStructLike partitionWrapper;
-    private DataFile wrapped = null;
-
-    IndexedDataFile(org.apache.avro.Schema avroSchema) {
-      this.avroSchema = avroSchema;
-      this.partitionWrapper = new IndexedStructLike(avroSchema.getField("partition").schema());
-    }
-
-    public IndexedDataFile wrap(DataFile file) {
-      this.wrapped = file;
-      return this;
-    }
-
-    @Override
-    public Object get(int pos) {
-      switch (pos) {
-        case 0:
-          return wrapped.path().toString();
-        case 1:
-          return wrapped.format() != null ? wrapped.format().toString() : null;
-        case 2:
-          return partitionWrapper.wrap(wrapped.partition());
-        case 3:
-          return wrapped.recordCount();
-        case 4:
-          return wrapped.fileSizeInBytes();
-        case 5:
-          return DEFAULT_BLOCK_SIZE;
-        case 6:
-          return wrapped.columnSizes();
-        case 7:
-          return wrapped.valueCounts();
-        case 8:
-          return wrapped.nullValueCounts();
-        case 9:
-          return wrapped.lowerBounds();
-        case 10:
-          return wrapped.upperBounds();
-        case 11:
-          return wrapped.keyMetadata();
-        case 12:
-          return wrapped.splitOffsets();
-      }
-      throw new IllegalArgumentException("Unknown field ordinal: " + pos);
-    }
-
-    @Override
-    public void put(int i, Object v) {
-      throw new UnsupportedOperationException("Cannot read into IndexedDataFile");
-    }
-
-    @Override
-    public org.apache.avro.Schema getSchema() {
-      return avroSchema;
-    }
-
-    @Override
-    public CharSequence path() {
-      return wrapped.path();
-    }
-
-    @Override
-    public FileFormat format() {
-      return wrapped.format();
-    }
-
-    @Override
-    public StructLike partition() {
-      return wrapped.partition();
-    }
-
-    @Override
-    public long recordCount() {
-      return wrapped.recordCount();
-    }
-
-    @Override
-    public long fileSizeInBytes() {
-      return wrapped.fileSizeInBytes();
-    }
-
-    @Override
-    public Map<Integer, Long> columnSizes() {
-      return wrapped.columnSizes();
-    }
-
-    @Override
-    public Map<Integer, Long> valueCounts() {
-      return wrapped.valueCounts();
-    }
-
-    @Override
-    public Map<Integer, Long> nullValueCounts() {
-      return wrapped.nullValueCounts();
-    }
-
-    @Override
-    public Map<Integer, ByteBuffer> lowerBounds() {
-      return wrapped.lowerBounds();
-    }
-
-    @Override
-    public Map<Integer, ByteBuffer> upperBounds() {
-      return wrapped.upperBounds();
-    }
+  /**
+   * @return the sequence number of the snapshot in which the file was added to the table
+   */
+  Long sequenceNumber();
 
-    @Override
-    public ByteBuffer keyMetadata() {
-      return wrapped.keyMetadata();
-    }
+  /**
+   * Set the sequence number for this manifest entry.
+   *
+   * @param sequenceNumber a sequence number
+   */
+  void setSequenceNumber(long sequenceNumber);
 
-    @Override
-    public List<Long> splitOffsets() {
-      return wrapped.splitOffsets();
-    }
+  /**
+   * @return a file
+   */
+  DataFile file();
 
-    @Override
-    public DataFile copy() {
-      return wrapped.copy();
-    }
+  ManifestEntry copy();
 
-    @Override
-    public DataFile copyWithoutStats() {
-      return wrapped.copyWithoutStats();
-    }
-  }
+  ManifestEntry copyWithoutStats();
 }
diff --git a/core/src/main/java/org/apache/iceberg/ManifestFiles.java b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
index 90b72cdac35..203d6906367 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestFiles.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestFiles.java
@@ -20,10 +20,8 @@
 package org.apache.iceberg;
 
 import com.google.common.base.Preconditions;
-import com.google.common.collect.Sets;
 import java.io.IOException;
 import java.util.Map;
-import java.util.Set;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.io.InputFile;
@@ -73,7 +71,6 @@ public static ManifestReader read(ManifestFile manifest, FileIO io, Map<Integer,
    * @return a manifest writer
    */
   public static ManifestWriter write(PartitionSpec spec, OutputFile outputFile) {
-    // always use a v1 writer for appended manifests because sequence number must be inherited
     return write(1, spec, outputFile, null);
   }
 
@@ -86,31 +83,55 @@ public static ManifestWriter write(PartitionSpec spec, OutputFile outputFile) {
    * @param snapshotId a snapshot ID for the manifest entries, or null for an inherited ID
    * @return a manifest writer
    */
-  static ManifestWriter write(int formatVersion, PartitionSpec spec, OutputFile outputFile, Long snapshotId) {
+  public static ManifestWriter write(int formatVersion, PartitionSpec spec, OutputFile outputFile, Long snapshotId) {
     switch (formatVersion) {
       case 1:
         return new ManifestWriter.V1Writer(spec, outputFile, snapshotId);
+      case 2:
+        return new ManifestWriter.V2Writer(spec, outputFile, snapshotId);
     }
     throw new UnsupportedOperationException("Cannot write manifest for table version: " + formatVersion);
   }
 
-  static ManifestFile copyAppendManifest(int formatVersion, ManifestReader reader, OutputFile outputFile,
-                                         long snapshotId, SnapshotSummary.Builder summaryBuilder) {
-    return copyManifest(
-        formatVersion, reader, outputFile, snapshotId, summaryBuilder, Sets.newHashSet(ManifestEntry.Status.ADDED));
+  static ManifestFile copyAppendManifest(int formatVersion,
+                                         InputFile toCopy, Map<Integer, PartitionSpec> specsById,
+                                         OutputFile outputFile, long snapshotId,
+                                         SnapshotSummary.Builder summaryBuilder) {
+    // use metadata that will add the current snapshot's ID for the rewrite
+    InheritableMetadata inheritableMetadata = InheritableMetadataFactory.forCopy(snapshotId);
+    try (ManifestReader reader = new ManifestReader(toCopy, specsById, inheritableMetadata)) {
+      return copyManifestInternal(
+          formatVersion, reader, outputFile, snapshotId, summaryBuilder, ManifestEntry.Status.ADDED);
+    } catch (IOException e) {
+      throw new RuntimeIOException(e, "Failed to close manifest: %s", toCopy.location());
+    }
+  }
+
+  static ManifestFile copyRewriteManifest(int formatVersion,
+                                          InputFile toCopy, Map<Integer, PartitionSpec> specsById,
+                                          OutputFile outputFile, long snapshotId,
+                                          SnapshotSummary.Builder summaryBuilder) {
+    // for a rewritten manifest all snapshot ids should be set. use empty metadata to throw an exception if it is not
+    InheritableMetadata inheritableMetadata = InheritableMetadataFactory.empty();
+    try (ManifestReader reader = new ManifestReader(toCopy, specsById, inheritableMetadata)) {
+      return copyManifestInternal(
+          formatVersion, reader, outputFile, snapshotId, summaryBuilder, ManifestEntry.Status.EXISTING);
+    } catch (IOException e) {
+      throw new RuntimeIOException(e, "Failed to close manifest: %s", toCopy.location());
+    }
   }
 
-  static ManifestFile copyManifest(int formatVersion, ManifestReader reader, OutputFile outputFile, long snapshotId,
-                                   SnapshotSummary.Builder summaryBuilder,
-                                   Set<ManifestEntry.Status> allowedEntryStatuses) {
+  private static ManifestFile copyManifestInternal(int formatVersion, ManifestReader reader, OutputFile outputFile,
+                                                   long snapshotId, SnapshotSummary.Builder summaryBuilder,
+                                                   ManifestEntry.Status allowedEntryStatus) {
     ManifestWriter writer = write(formatVersion, reader.spec(), outputFile, snapshotId);
     boolean threw = true;
     try {
       for (ManifestEntry entry : reader.entries()) {
         Preconditions.checkArgument(
-            allowedEntryStatuses.contains(entry.status()),
-            "Invalid manifest entry status: %s (allowed statuses: %s)",
-            entry.status(), allowedEntryStatuses);
+            allowedEntryStatus == entry.status(),
+            "Invalid manifest entry status: %s (allowed status: %s)",
+            entry.status(), allowedEntryStatus);
         switch (entry.status()) {
           case ADDED:
             summaryBuilder.addedFile(reader.spec(), entry.file());
diff --git a/core/src/main/java/org/apache/iceberg/ManifestListWriter.java b/core/src/main/java/org/apache/iceberg/ManifestListWriter.java
index fe3afe4f8e6..1cf9c92e097 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestListWriter.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestListWriter.java
@@ -78,7 +78,7 @@ static class V2Writer extends ManifestListWriter {
           "parent-snapshot-id", String.valueOf(parentSnapshotId),
           "sequence-number", String.valueOf(sequenceNumber),
           "format-version", "2"));
-      this.wrapper = new V2Metadata.IndexedManifestFile(sequenceNumber);
+      this.wrapper = new V2Metadata.IndexedManifestFile(snapshotId, sequenceNumber);
     }
 
     @Override
@@ -117,6 +117,7 @@ protected ManifestFile prepare(ManifestFile manifest) {
       return wrapper.wrap(manifest);
     }
 
+    @Override
     protected FileAppender<ManifestFile> newAppender(OutputFile file, Map<String, String> meta) {
       try {
         return Avro.write(file)
diff --git a/core/src/main/java/org/apache/iceberg/ManifestReader.java b/core/src/main/java/org/apache/iceberg/ManifestReader.java
index 367c2c38ca4..045ebe17da5 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestReader.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestReader.java
@@ -38,8 +38,6 @@
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.types.Types;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import static org.apache.iceberg.expressions.Expressions.alwaysTrue;
 
@@ -49,8 +47,6 @@
  * Create readers using {@link ManifestFiles#read(ManifestFile, FileIO, Map)}.
  */
 public class ManifestReader extends CloseableGroup implements Filterable<FilteredManifest> {
-  private static final Logger LOG = LoggerFactory.getLogger(ManifestReader.class);
-
   static final ImmutableList<String> ALL_COLUMNS = ImmutableList.of("*");
   static final ImmutableList<String> CHANGE_COLUMNS = ImmutableList.of(
       "file_path", "file_format", "partition", "record_count", "file_size_in_bytes");
@@ -220,12 +216,12 @@ CloseableIterable<ManifestEntry> entries(Schema fileProjection) {
       case AVRO:
         AvroIterable<ManifestEntry> reader = Avro.read(file)
             .project(ManifestEntry.wrapFileSchema(fileProjection.asStruct()))
-            .rename("manifest_entry", ManifestEntry.class.getName())
+            .rename("manifest_entry", GenericManifestEntry.class.getName())
             .rename("partition", PartitionData.class.getName())
             .rename("r102", PartitionData.class.getName())
             .rename("data_file", GenericDataFile.class.getName())
             .rename("r2", GenericDataFile.class.getName())
-            .classLoader(ManifestEntry.class.getClassLoader())
+            .classLoader(GenericManifestFile.class.getClassLoader())
             .reuseContainers()
             .build();
 
diff --git a/core/src/main/java/org/apache/iceberg/ManifestWriter.java b/core/src/main/java/org/apache/iceberg/ManifestWriter.java
index e6df3ae257b..ddd74c8a3e1 100644
--- a/core/src/main/java/org/apache/iceberg/ManifestWriter.java
+++ b/core/src/main/java/org/apache/iceberg/ManifestWriter.java
@@ -25,14 +25,13 @@
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.io.OutputFile;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 /**
  * Writer for manifest files.
  */
 public abstract class ManifestWriter implements FileAppender<DataFile> {
-  private static final Logger LOG = LoggerFactory.getLogger(ManifestWriter.class);
+  // stand-in for the current sequence number that will be assigned when the commit is successful
+  // this is replaced when writing a manifest list by the ManifestFile wrapper
   static final long UNASSIGNED_SEQ = -1L;
 
   /**
@@ -55,7 +54,7 @@ public static ManifestWriter write(PartitionSpec spec, OutputFile outputFile) {
   private final int specId;
   private final FileAppender<ManifestEntry> writer;
   private final Long snapshotId;
-  private final ManifestEntry reused;
+  private final GenericManifestEntry reused;
   private final PartitionSummary stats;
 
   private boolean closed = false;
@@ -65,16 +64,21 @@ public static ManifestWriter write(PartitionSpec spec, OutputFile outputFile) {
   private long existingRows = 0L;
   private int deletedFiles = 0;
   private long deletedRows = 0L;
+  private Long minSequenceNumber = null;
 
   private ManifestWriter(PartitionSpec spec, OutputFile file, Long snapshotId) {
     this.file = file;
     this.specId = spec.specId();
-    this.writer = newAppender(FileFormat.AVRO, spec, file);
+    this.writer = newAppender(spec, file);
     this.snapshotId = snapshotId;
-    this.reused = new ManifestEntry(spec.partitionType());
+    this.reused = new GenericManifestEntry(spec.partitionType());
     this.stats = new PartitionSummary(spec);
   }
 
+  protected abstract ManifestEntry prepare(ManifestEntry entry);
+
+  protected abstract FileAppender<ManifestEntry> newAppender(PartitionSpec spec, OutputFile outputFile);
+
   void addEntry(ManifestEntry entry) {
     switch (entry.status()) {
       case ADDED:
@@ -91,7 +95,10 @@ void addEntry(ManifestEntry entry) {
         break;
     }
     stats.update(entry.file().partition());
-    writer.add(entry);
+    if (entry.sequenceNumber() != null && (minSequenceNumber == null || entry.sequenceNumber() < minSequenceNumber)) {
+      this.minSequenceNumber = entry.sequenceNumber();
+    }
+    writer.add(prepare(entry));
   }
 
   /**
@@ -115,13 +122,14 @@ void add(ManifestEntry entry) {
    *
    * @param existingFile a data file
    * @param fileSnapshotId snapshot ID when the data file was added to the table
+   * @param sequenceNumber sequence number for the data file
    */
-  public void existing(DataFile existingFile, long fileSnapshotId) {
-    addEntry(reused.wrapExisting(fileSnapshotId, existingFile));
+  public void existing(DataFile existingFile, long fileSnapshotId, long sequenceNumber) {
+    addEntry(reused.wrapExisting(fileSnapshotId, sequenceNumber, existingFile));
   }
 
   void existing(ManifestEntry entry) {
-    addEntry(reused.wrapExisting(entry.snapshotId(), entry.file()));
+    addEntry(reused.wrapExisting(entry.snapshotId(), entry.sequenceNumber(), entry.file()));
   }
 
   /**
@@ -153,7 +161,10 @@ public long length() {
 
   public ManifestFile toManifestFile() {
     Preconditions.checkState(closed, "Cannot build ManifestFile, writer is not closed");
-    return new GenericManifestFile(file.location(), writer.length(), specId, UNASSIGNED_SEQ, UNASSIGNED_SEQ, snapshotId,
+    // if the minSequenceNumber is null, then no manifests with a sequence number have been written, so the min
+    // sequence number is the one that will be assigned when this is committed. pass UNASSIGNED_SEQ to inherit it.
+    long minSeqNumber = minSequenceNumber != null ? minSequenceNumber : UNASSIGNED_SEQ;
+    return new GenericManifestFile(file.location(), writer.length(), specId, UNASSIGNED_SEQ, minSeqNumber, snapshotId,
         addedFiles, addedRows, existingFiles, existingRows, deletedFiles, deletedRows, stats.summaries());
   }
 
@@ -163,31 +174,67 @@ public void close() throws IOException {
     writer.close();
   }
 
-  private static <D> FileAppender<D> newAppender(FileFormat format, PartitionSpec spec,
-                                                 OutputFile file) {
-    Schema manifestSchema = ManifestEntry.getSchema(spec.partitionType());
-    try {
-      switch (format) {
-        case AVRO:
-          return Avro.write(file)
-              .schema(manifestSchema)
-              .named("manifest_entry")
-              .meta("schema", SchemaParser.toJson(spec.schema()))
-              .meta("partition-spec", PartitionSpecParser.toJsonFields(spec))
-              .meta("partition-spec-id", String.valueOf(spec.specId()))
-              .overwrite()
-              .build();
-        default:
-          throw new IllegalArgumentException("Unsupported format: " + format);
+  static class V2Writer extends ManifestWriter {
+    private V2Metadata.IndexedManifestEntry entryWrapper;
+
+    V2Writer(PartitionSpec spec, OutputFile file, Long snapshotId) {
+      super(spec, file, snapshotId);
+      this.entryWrapper = new V2Metadata.IndexedManifestEntry(snapshotId, spec.partitionType());
+    }
+
+    @Override
+    protected ManifestEntry prepare(ManifestEntry entry) {
+      return entryWrapper.wrap(entry);
+    }
+
+    @Override
+    protected FileAppender<ManifestEntry> newAppender(PartitionSpec spec, OutputFile file) {
+      Schema manifestSchema = V2Metadata.entrySchema(spec.partitionType());
+      try {
+        return Avro.write(file)
+            .schema(manifestSchema)
+            .named("manifest_entry")
+            .meta("schema", SchemaParser.toJson(spec.schema()))
+            .meta("partition-spec", PartitionSpecParser.toJsonFields(spec))
+            .meta("partition-spec-id", String.valueOf(spec.specId()))
+            .meta("format-version", "2")
+            .overwrite()
+            .build();
+      } catch (IOException e) {
+        throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
       }
-    } catch (IOException e) {
-      throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
     }
   }
 
   static class V1Writer extends ManifestWriter {
+    private V1Metadata.IndexedManifestEntry entryWrapper;
+
     V1Writer(PartitionSpec spec, OutputFile file, Long snapshotId) {
       super(spec, file, snapshotId);
+      this.entryWrapper = new V1Metadata.IndexedManifestEntry(spec.partitionType());
+    }
+
+    @Override
+    protected ManifestEntry prepare(ManifestEntry entry) {
+      return entryWrapper.wrap(entry);
+    }
+
+    @Override
+    protected FileAppender<ManifestEntry> newAppender(PartitionSpec spec, OutputFile file) {
+      Schema manifestSchema = V1Metadata.entrySchema(spec.partitionType());
+      try {
+        return Avro.write(file)
+            .schema(manifestSchema)
+            .named("manifest_entry")
+            .meta("schema", SchemaParser.toJson(spec.schema()))
+            .meta("partition-spec", PartitionSpecParser.toJsonFields(spec))
+            .meta("partition-spec-id", String.valueOf(spec.specId()))
+            .meta("format-version", "1")
+            .overwrite()
+            .build();
+      } catch (IOException e) {
+        throw new RuntimeIOException(e, "Failed to create manifest writer for path: " + file);
+      }
     }
   }
 }
diff --git a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
index deefbe85dad..9c83ebfbcf1 100644
--- a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
+++ b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
@@ -42,6 +42,7 @@
 import org.apache.iceberg.expressions.ManifestEvaluator;
 import org.apache.iceberg.expressions.Projections;
 import org.apache.iceberg.expressions.StrictMetricsEvaluator;
+import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.util.BinPacking.ListPacker;
 import org.apache.iceberg.util.CharSequenceWrapper;
@@ -227,13 +228,11 @@ protected void add(ManifestFile manifest) {
   }
 
   private ManifestFile copyManifest(ManifestFile manifest) {
-    try (ManifestReader reader = ManifestFiles.read(manifest, ops.io(), ops.current().specsById())) {
-      OutputFile newManifestPath = newManifestOutput();
-      return ManifestFiles.copyAppendManifest(
-          ops.current().formatVersion(), reader, newManifestPath, snapshotId(), appendedManifestsSummary);
-    } catch (IOException e) {
-      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
-    }
+    TableMetadata current = ops.current();
+    InputFile toCopy = ops.io().newInputFile(manifest.path());
+    OutputFile newManifestPath = newManifestOutput();
+    return ManifestFiles.copyAppendManifest(
+        current.formatVersion(), toCopy, current.specsById(), newManifestPath, snapshotId(), appendedManifestsSummary);
   }
 
   @Override
diff --git a/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java b/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
index f44e3725e97..0e6c0e14207 100644
--- a/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
+++ b/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
@@ -31,6 +31,7 @@
 import java.util.function.Consumer;
 import org.apache.iceberg.avro.Avro;
 import org.apache.iceberg.exceptions.CommitFailedException;
+import org.apache.iceberg.exceptions.NotFoundException;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.util.PropertyUtil;
@@ -201,120 +202,131 @@ private void cleanExpiredFiles(List<Snapshot> snapshots, Set<Long> validIds, Set
     // find manifests to clean up that are still referenced by a valid snapshot, but written by an expired snapshot
     Set<String> validManifests = Sets.newHashSet();
     Set<ManifestFile> manifestsToScan = Sets.newHashSet();
-    for (Snapshot snapshot : snapshots) {
-      try (CloseableIterable<ManifestFile> manifests = readManifestFiles(snapshot)) {
-        for (ManifestFile manifest : manifests) {
-          validManifests.add(manifest.path());
-
-          long snapshotId = manifest.snapshotId();
-          // whether the manifest was created by a valid snapshot (true) or an expired snapshot (false)
-          boolean fromValidSnapshots = validIds.contains(snapshotId);
-          // whether the snapshot that created the manifest was an ancestor of the table state
-          boolean isFromAncestor = ancestorIds.contains(snapshotId);
-          // whether the changes in this snapshot have been picked into the current table state
-          boolean isPicked = pickedAncestorSnapshotIds.contains(snapshotId);
-          // if the snapshot that wrote this manifest is no longer valid (has expired), then delete its deleted files.
-          // note that this is only for expired snapshots that are in the current table state
-          if (!fromValidSnapshots && (isFromAncestor || isPicked) && manifest.hasDeletedFiles()) {
-            manifestsToScan.add(manifest.copy());
-          }
-        }
-
-      } catch (IOException e) {
-        throw new RuntimeIOException(e,
-            "Failed to close manifest list: %s", snapshot.manifestListLocation());
-      }
-    }
+    Tasks.foreach(snapshots).retry(3).suppressFailureWhenFinished()
+        .onFailure((snapshot, exc) ->
+            LOG.warn("Failed on snapshot {} while reading manifest list: {}", snapshot.snapshotId(),
+                snapshot.manifestListLocation(), exc))
+        .run(
+            snapshot -> {
+              try (CloseableIterable<ManifestFile> manifests = readManifestFiles(snapshot)) {
+                for (ManifestFile manifest : manifests) {
+                  validManifests.add(manifest.path());
+
+                  long snapshotId = manifest.snapshotId();
+                  // whether the manifest was created by a valid snapshot (true) or an expired snapshot (false)
+                  boolean fromValidSnapshots = validIds.contains(snapshotId);
+                  // whether the snapshot that created the manifest was an ancestor of the table state
+                  boolean isFromAncestor = ancestorIds.contains(snapshotId);
+                  // whether the changes in this snapshot have been picked into the current table state
+                  boolean isPicked = pickedAncestorSnapshotIds.contains(snapshotId);
+                  // if the snapshot that wrote this manifest is no longer valid (has expired),
+                  // then delete its deleted files. note that this is only for expired snapshots that are in the
+                  // current table state
+                  if (!fromValidSnapshots && (isFromAncestor || isPicked) && manifest.hasDeletedFiles()) {
+                    manifestsToScan.add(manifest.copy());
+                  }
+                }
+
+              } catch (IOException e) {
+                throw new RuntimeIOException(e,
+                    "Failed to close manifest list: %s", snapshot.manifestListLocation());
+              }
+            });
 
     // find manifests to clean up that were only referenced by snapshots that have expired
     Set<String> manifestListsToDelete = Sets.newHashSet();
     Set<String> manifestsToDelete = Sets.newHashSet();
     Set<ManifestFile> manifestsToRevert = Sets.newHashSet();
-    for (Snapshot snapshot : base.snapshots()) {
-      long snapshotId = snapshot.snapshotId();
-      if (!validIds.contains(snapshotId)) {
-        // determine whether the changes in this snapshot are in the current table state
-        if (pickedAncestorSnapshotIds.contains(snapshotId)) {
-          // this snapshot was cherry-picked into the current table state, so skip cleaning it up. its changes will
-          // expire when the picked snapshot expires.
-          // A -- C -- D (source=B)
-          //  `- B <-- this commit
-          continue;
-        }
-
-        long sourceSnapshotId = PropertyUtil.propertyAsLong(
-            snapshot.summary(), SnapshotSummary.SOURCE_SNAPSHOT_ID_PROP, -1);
-        if (ancestorIds.contains(sourceSnapshotId)) {
-          // this commit was cherry-picked from a commit that is in the current table state. do not clean up its
-          // changes because it would revert data file additions that are in the current table.
-          // A -- B -- C
-          //  `- D (source=B) <-- this commit
-          continue;
-        }
-
-        if (pickedAncestorSnapshotIds.contains(sourceSnapshotId)) {
-          // this commit was cherry-picked from a commit that is in the current table state. do not clean up its
-          // changes because it would revert data file additions that are in the current table.
-          // A -- C -- E (source=B)
-          //  `- B `- D (source=B) <-- this commit
-          continue;
-        }
-
-        // find any manifests that are no longer needed
-        try (CloseableIterable<ManifestFile> manifests = readManifestFiles(snapshot)) {
-          for (ManifestFile manifest : manifests) {
-            if (!validManifests.contains(manifest.path())) {
-              manifestsToDelete.add(manifest.path());
-
-              boolean isFromAncestor = ancestorIds.contains(manifest.snapshotId());
-              boolean isFromExpiringSnapshot = expiredIds.contains(manifest.snapshotId());
-
-              if (isFromAncestor && manifest.hasDeletedFiles()) {
-                // Only delete data files that were deleted in by an expired snapshot if that
-                // snapshot is an ancestor of the current table state. Otherwise, a snapshot that
-                // deleted files and was rolled back will delete files that could be in the current
-                // table state.
-                manifestsToScan.add(manifest.copy());
+    Tasks.foreach(base.snapshots()).retry(3).suppressFailureWhenFinished()
+        .onFailure((snapshot, exc) ->
+            LOG.warn("Failed on snapshot {} while reading manifest list: {}", snapshot.snapshotId(),
+                snapshot.manifestListLocation(), exc))
+        .run(
+            snapshot -> {
+              long snapshotId = snapshot.snapshotId();
+              if (!validIds.contains(snapshotId)) {
+                // determine whether the changes in this snapshot are in the current table state
+                if (pickedAncestorSnapshotIds.contains(snapshotId)) {
+                  // this snapshot was cherry-picked into the current table state, so skip cleaning it up.
+                  // its changes will expire when the picked snapshot expires.
+                  // A -- C -- D (source=B)
+                  //  `- B <-- this commit
+                  return;
+                }
+
+                long sourceSnapshotId = PropertyUtil.propertyAsLong(
+                    snapshot.summary(), SnapshotSummary.SOURCE_SNAPSHOT_ID_PROP, -1);
+                if (ancestorIds.contains(sourceSnapshotId)) {
+                  // this commit was cherry-picked from a commit that is in the current table state. do not clean up its
+                  // changes because it would revert data file additions that are in the current table.
+                  // A -- B -- C
+                  //  `- D (source=B) <-- this commit
+                  return;
+                }
+
+                if (pickedAncestorSnapshotIds.contains(sourceSnapshotId)) {
+                  // this commit was cherry-picked from a commit that is in the current table state. do not clean up its
+                  // changes because it would revert data file additions that are in the current table.
+                  // A -- C -- E (source=B)
+                  //  `- B `- D (source=B) <-- this commit
+                  return;
+                }
+
+                // find any manifests that are no longer needed
+                try (CloseableIterable<ManifestFile> manifests = readManifestFiles(snapshot)) {
+                  for (ManifestFile manifest : manifests) {
+                    if (!validManifests.contains(manifest.path())) {
+                      manifestsToDelete.add(manifest.path());
+
+                      boolean isFromAncestor = ancestorIds.contains(manifest.snapshotId());
+                      boolean isFromExpiringSnapshot = expiredIds.contains(manifest.snapshotId());
+
+                      if (isFromAncestor && manifest.hasDeletedFiles()) {
+                        // Only delete data files that were deleted in by an expired snapshot if that
+                        // snapshot is an ancestor of the current table state. Otherwise, a snapshot that
+                        // deleted files and was rolled back will delete files that could be in the current
+                        // table state.
+                        manifestsToScan.add(manifest.copy());
+                      }
+
+                      if (!isFromAncestor && isFromExpiringSnapshot && manifest.hasAddedFiles()) {
+                        // Because the manifest was written by a snapshot that is not an ancestor of the
+                        // current table state, the files added in this manifest can be removed. The extra
+                        // check whether the manifest was written by a known snapshot that was expired in
+                        // this commit ensures that the full ancestor list between when the snapshot was
+                        // written and this expiration is known and there is no missing history. If history
+                        // were missing, then the snapshot could be an ancestor of the table state but the
+                        // ancestor ID set would not contain it and this would be unsafe.
+                        manifestsToRevert.add(manifest.copy());
+                      }
+                    }
+                  }
+                } catch (IOException e) {
+                  throw new RuntimeIOException(e,
+                      "Failed to close manifest list: %s", snapshot.manifestListLocation());
+                }
+
+                // add the manifest list to the delete set, if present
+                if (snapshot.manifestListLocation() != null) {
+                  manifestListsToDelete.add(snapshot.manifestListLocation());
+                }
               }
-
-              if (!isFromAncestor && isFromExpiringSnapshot && manifest.hasAddedFiles()) {
-                // Because the manifest was written by a snapshot that is not an ancestor of the
-                // current table state, the files added in this manifest can be removed. The extra
-                // check whether the manifest was written by a known snapshot that was expired in
-                // this commit ensures that the full ancestor list between when the snapshot was
-                // written and this expiration is known and there is no missing history. If history
-                // were missing, then the snapshot could be an ancestor of the table state but the
-                // ancestor ID set would not contain it and this would be unsafe.
-                manifestsToRevert.add(manifest.copy());
-              }
-            }
-          }
-        } catch (IOException e) {
-          throw new RuntimeIOException(e,
-              "Failed to close manifest list: %s", snapshot.manifestListLocation());
-        }
-
-        // add the manifest list to the delete set, if present
-        if (snapshot.manifestListLocation() != null) {
-          manifestListsToDelete.add(snapshot.manifestListLocation());
-        }
-      }
-    }
-
+            });
     deleteDataFiles(manifestsToScan, manifestsToRevert, validIds);
     deleteMetadataFiles(manifestsToDelete, manifestListsToDelete);
   }
 
   private void deleteMetadataFiles(Set<String> manifestsToDelete, Set<String> manifestListsToDelete) {
     LOG.warn("Manifests to delete: {}", Joiner.on(", ").join(manifestsToDelete));
+    LOG.warn("Manifests Lists to delete: {}", Joiner.on(", ").join(manifestListsToDelete));
 
     Tasks.foreach(manifestsToDelete)
-        .noRetry().suppressFailureWhenFinished()
+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()
         .onFailure((manifest, exc) -> LOG.warn("Delete failed for manifest: {}", manifest, exc))
         .run(deleteFunc::accept);
 
     Tasks.foreach(manifestListsToDelete)
-        .noRetry().suppressFailureWhenFinished()
+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()
         .onFailure((list, exc) -> LOG.warn("Delete failed for manifest list: {}", list, exc))
         .run(deleteFunc::accept);
   }
@@ -323,7 +335,7 @@ private void deleteDataFiles(Set<ManifestFile> manifestsToScan, Set<ManifestFile
                                Set<Long> validIds) {
     Set<String> filesToDelete = findFilesToDelete(manifestsToScan, manifestsToRevert, validIds);
     Tasks.foreach(filesToDelete)
-        .noRetry().suppressFailureWhenFinished()
+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()
         .onFailure((file, exc) -> LOG.warn("Delete failed for data file: {}", file, exc))
         .run(file -> deleteFunc.accept(file));
   }
@@ -332,7 +344,7 @@ private Set<String> findFilesToDelete(Set<ManifestFile> manifestsToScan, Set<Man
                                         Set<Long> validIds) {
     Set<String> filesToDelete = ConcurrentHashMap.newKeySet();
     Tasks.foreach(manifestsToScan)
-        .noRetry().suppressFailureWhenFinished()
+        .retry(3).suppressFailureWhenFinished()
         .executeWith(ThreadPools.getWorkerPool())
         .onFailure((item, exc) -> LOG.warn("Failed to get deleted files: this may cause orphaned data files", exc))
         .run(manifest -> {
@@ -352,7 +364,7 @@ private Set<String> findFilesToDelete(Set<ManifestFile> manifestsToScan, Set<Man
         });
 
     Tasks.foreach(manifestsToRevert)
-        .noRetry().suppressFailureWhenFinished()
+        .retry(3).suppressFailureWhenFinished()
         .executeWith(ThreadPools.getWorkerPool())
         .onFailure((item, exc) -> LOG.warn("Failed to get added files: this may cause orphaned data files", exc))
         .run(manifest -> {
diff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java
index 9b6c4042d64..5325ba2d705 100644
--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java
+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java
@@ -48,10 +48,30 @@ public class TableMetadata {
   static final int SUPPORTED_TABLE_FORMAT_VERSION = 2;
   static final int INITIAL_SPEC_ID = 0;
 
+  /**
+   * @deprecated will be removed in 0.9.0; use newTableMetadata(Schema, PartitionSpec, String, Map) instead.
+   */
+  @Deprecated
+  public static TableMetadata newTableMetadata(TableOperations ops,
+                                               Schema schema,
+                                               PartitionSpec spec,
+                                               String location,
+                                               Map<String, String> properties) {
+    return newTableMetadata(schema, spec, location, properties, DEFAULT_TABLE_FORMAT_VERSION);
+  }
+
   public static TableMetadata newTableMetadata(Schema schema,
                                                PartitionSpec spec,
                                                String location,
                                                Map<String, String> properties) {
+    return newTableMetadata(schema, spec, location, properties, DEFAULT_TABLE_FORMAT_VERSION);
+  }
+
+  static TableMetadata newTableMetadata(Schema schema,
+                                        PartitionSpec spec,
+                                        String location,
+                                        Map<String, String> properties,
+                                        int formatVersion) {
     // reassign all column ids to ensure consistency
     AtomicInteger lastColumnId = new AtomicInteger(0);
     Schema freshSchema = TypeUtil.assignFreshIds(schema, lastColumnId::incrementAndGet);
@@ -70,7 +90,7 @@ public static TableMetadata newTableMetadata(Schema schema,
     }
     PartitionSpec freshSpec = specBuilder.build();
 
-    return new TableMetadata(null, DEFAULT_TABLE_FORMAT_VERSION, UUID.randomUUID().toString(), location,
+    return new TableMetadata(null, formatVersion, UUID.randomUUID().toString(), location,
         INITIAL_SEQUENCE_NUMBER, System.currentTimeMillis(),
         lastColumnId.get(), freshSchema, INITIAL_SPEC_ID, ImmutableList.of(freshSpec),
         ImmutableMap.copyOf(properties), -1, ImmutableList.of(),
diff --git a/core/src/main/java/org/apache/iceberg/TableMetadataParser.java b/core/src/main/java/org/apache/iceberg/TableMetadataParser.java
index f746860057f..ed9d4791cf4 100644
--- a/core/src/main/java/org/apache/iceberg/TableMetadataParser.java
+++ b/core/src/main/java/org/apache/iceberg/TableMetadataParser.java
@@ -214,6 +214,14 @@ private static void toJson(TableMetadata metadata, JsonGenerator generator) thro
     generator.writeEndObject();
   }
 
+  /**
+   * @deprecated will be removed in 0.9.0; use read(FileIO, InputFile) instead.
+   */
+  @Deprecated
+  public static TableMetadata read(TableOperations ops, InputFile file) {
+    return read(ops.io(), file);
+  }
+
   public static TableMetadata read(FileIO io, String path) {
     return read(io, io.newInputFile(path));
   }
diff --git a/core/src/main/java/org/apache/iceberg/V1Metadata.java b/core/src/main/java/org/apache/iceberg/V1Metadata.java
index f581b718d9e..a905b4d6ec5 100644
--- a/core/src/main/java/org/apache/iceberg/V1Metadata.java
+++ b/core/src/main/java/org/apache/iceberg/V1Metadata.java
@@ -19,9 +19,14 @@
 
 package org.apache.iceberg;
 
+import java.nio.ByteBuffer;
 import java.util.List;
+import java.util.Map;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.iceberg.avro.AvroSchemaUtil;
+import org.apache.iceberg.types.Types;
+
+import static org.apache.iceberg.types.Types.NestedField.required;
 
 class V1Metadata {
   private V1Metadata() {
@@ -175,4 +180,231 @@ public ManifestFile copy() {
       return wrapped.copy();
     }
   }
+
+  static Schema entrySchema(Types.StructType partitionType) {
+    return wrapFileSchema(DataFile.getType(partitionType));
+  }
+
+  static Schema wrapFileSchema(Types.StructType fileSchema) {
+    // this is used to build projection schemas
+    return new Schema(
+        ManifestEntry.STATUS, ManifestEntry.SNAPSHOT_ID,
+        required(ManifestEntry.DATA_FILE_ID, "data_file", fileSchema));
+  }
+
+  static class IndexedManifestEntry implements ManifestEntry, IndexedRecord {
+    private final org.apache.avro.Schema avroSchema;
+    private final IndexedDataFile fileWrapper;
+    private ManifestEntry wrapped = null;
+
+    IndexedManifestEntry(Types.StructType partitionType) {
+      this.avroSchema = AvroSchemaUtil.convert(entrySchema(partitionType), "manifest_entry");
+      this.fileWrapper = new IndexedDataFile(avroSchema.getField("data_file").schema());
+    }
+
+    public IndexedManifestEntry wrap(ManifestEntry entry) {
+      this.wrapped = entry;
+      return this;
+    }
+
+    @Override
+    public org.apache.avro.Schema getSchema() {
+      return avroSchema;
+    }
+
+    @Override
+    public void put(int i, Object v) {
+      throw new UnsupportedOperationException("Cannot read using IndexedManifestEntry");
+    }
+
+    @Override
+    public Object get(int i) {
+      switch (i) {
+        case 0:
+          return wrapped.status().id();
+        case 1:
+          return wrapped.snapshotId();
+        case 2:
+          DataFile file = wrapped.file();
+          if (file == null || file instanceof GenericDataFile) {
+            return file;
+          } else {
+            return fileWrapper.wrap(file);
+          }
+        default:
+          throw new UnsupportedOperationException("Unknown field ordinal: " + i);
+      }
+    }
+
+    @Override
+    public Status status() {
+      return wrapped.status();
+    }
+
+    @Override
+    public Long snapshotId() {
+      return wrapped.snapshotId();
+    }
+
+    @Override
+    public void setSnapshotId(long snapshotId) {
+      wrapped.setSnapshotId(snapshotId);
+    }
+
+    @Override
+    public Long sequenceNumber() {
+      return wrapped.sequenceNumber();
+    }
+
+    @Override
+    public void setSequenceNumber(long sequenceNumber) {
+      wrapped.setSequenceNumber(sequenceNumber);
+    }
+
+    @Override
+    public DataFile file() {
+      return wrapped.file();
+    }
+
+    @Override
+    public ManifestEntry copy() {
+      return wrapped.copy();
+    }
+
+    @Override
+    public ManifestEntry copyWithoutStats() {
+      return wrapped.copyWithoutStats();
+    }
+  }
+
+  static class IndexedDataFile implements DataFile, IndexedRecord {
+    private static final long DEFAULT_BLOCK_SIZE = 64 * 1024 * 1024;
+
+    private final org.apache.avro.Schema avroSchema;
+    private final IndexedStructLike partitionWrapper;
+    private DataFile wrapped = null;
+
+    IndexedDataFile(org.apache.avro.Schema avroSchema) {
+      this.avroSchema = avroSchema;
+      this.partitionWrapper = new IndexedStructLike(avroSchema.getField("partition").schema());
+    }
+
+    IndexedDataFile wrap(DataFile file) {
+      this.wrapped = file;
+      return this;
+    }
+
+    @Override
+    public Object get(int pos) {
+      switch (pos) {
+        case 0:
+          return wrapped.path().toString();
+        case 1:
+          return wrapped.format() != null ? wrapped.format().toString() : null;
+        case 2:
+          return partitionWrapper.wrap(wrapped.partition());
+        case 3:
+          return wrapped.recordCount();
+        case 4:
+          return wrapped.fileSizeInBytes();
+        case 5:
+          return DEFAULT_BLOCK_SIZE;
+        case 6:
+          return wrapped.columnSizes();
+        case 7:
+          return wrapped.valueCounts();
+        case 8:
+          return wrapped.nullValueCounts();
+        case 9:
+          return wrapped.lowerBounds();
+        case 10:
+          return wrapped.upperBounds();
+        case 11:
+          return wrapped.keyMetadata();
+        case 12:
+          return wrapped.splitOffsets();
+      }
+      throw new IllegalArgumentException("Unknown field ordinal: " + pos);
+    }
+
+    @Override
+    public void put(int i, Object v) {
+      throw new UnsupportedOperationException("Cannot read into IndexedDataFile");
+    }
+
+    @Override
+    public org.apache.avro.Schema getSchema() {
+      return avroSchema;
+    }
+
+    @Override
+    public CharSequence path() {
+      return wrapped.path();
+    }
+
+    @Override
+    public FileFormat format() {
+      return wrapped.format();
+    }
+
+    @Override
+    public StructLike partition() {
+      return wrapped.partition();
+    }
+
+    @Override
+    public long recordCount() {
+      return wrapped.recordCount();
+    }
+
+    @Override
+    public long fileSizeInBytes() {
+      return wrapped.fileSizeInBytes();
+    }
+
+    @Override
+    public Map<Integer, Long> columnSizes() {
+      return wrapped.columnSizes();
+    }
+
+    @Override
+    public Map<Integer, Long> valueCounts() {
+      return wrapped.valueCounts();
+    }
+
+    @Override
+    public Map<Integer, Long> nullValueCounts() {
+      return wrapped.nullValueCounts();
+    }
+
+    @Override
+    public Map<Integer, ByteBuffer> lowerBounds() {
+      return wrapped.lowerBounds();
+    }
+
+    @Override
+    public Map<Integer, ByteBuffer> upperBounds() {
+      return wrapped.upperBounds();
+    }
+
+    @Override
+    public ByteBuffer keyMetadata() {
+      return wrapped.keyMetadata();
+    }
+
+    @Override
+    public List<Long> splitOffsets() {
+      return wrapped.splitOffsets();
+    }
+
+    @Override
+    public DataFile copy() {
+      return wrapped.copy();
+    }
+
+    @Override
+    public DataFile copyWithoutStats() {
+      return wrapped.copyWithoutStats();
+    }
+  }
 }
diff --git a/core/src/main/java/org/apache/iceberg/V2Metadata.java b/core/src/main/java/org/apache/iceberg/V2Metadata.java
index 600506d47ef..6ee9d3f20db 100644
--- a/core/src/main/java/org/apache/iceberg/V2Metadata.java
+++ b/core/src/main/java/org/apache/iceberg/V2Metadata.java
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg;
 
+import com.google.common.base.Preconditions;
 import java.util.List;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.iceberg.avro.AvroSchemaUtil;
@@ -68,10 +69,12 @@ static class IndexedManifestFile implements ManifestFile, IndexedRecord {
     private static final org.apache.avro.Schema AVRO_SCHEMA =
         AvroSchemaUtil.convert(MANIFEST_LIST_SCHEMA, "manifest_file");
 
+    private final long commitSnapshotId;
     private final long sequenceNumber;
     private ManifestFile wrapped = null;
 
-    IndexedManifestFile(long sequenceNumber) {
+    IndexedManifestFile(long commitSnapshotId, long sequenceNumber) {
+      this.commitSnapshotId = commitSnapshotId;
       this.sequenceNumber = sequenceNumber;
     }
 
@@ -101,12 +104,25 @@ public Object get(int pos) {
           return wrapped.partitionSpecId();
         case 3:
           if (wrapped.sequenceNumber() == ManifestWriter.UNASSIGNED_SEQ) {
+            // if the sequence number is being assigned here, then the manifest must be created by the current
+            // operation. to validate this, check that the snapshot id matches the current commit
+            Preconditions.checkState(commitSnapshotId == wrapped.snapshotId(),
+                "Found unassigned sequence number for a manifest from snapshot: %s", wrapped.snapshotId());
             return sequenceNumber;
           } else {
             return wrapped.sequenceNumber();
           }
         case 4:
-          return wrapped.minSequenceNumber();
+          if (wrapped.minSequenceNumber() == ManifestWriter.UNASSIGNED_SEQ) {
+            // same sanity check as above
+            Preconditions.checkState(commitSnapshotId == wrapped.snapshotId(),
+                "Found unassigned sequence number for a manifest from snapshot: %s", wrapped.snapshotId());
+            // if the min sequence number is not determined, then there was no assigned sequence number for any file
+            // written to the wrapped manifest. replace the unassigned sequence number with the one for this commit
+            return sequenceNumber;
+          } else {
+            return wrapped.minSequenceNumber();
+          }
         case 5:
           return wrapped.snapshotId();
         case 6:
@@ -213,4 +229,109 @@ public ManifestFile copy() {
       return wrapped.copy();
     }
   }
+
+  static Schema entrySchema(Types.StructType partitionType) {
+    return wrapFileSchema(DataFile.getType(partitionType));
+  }
+
+  static Schema wrapFileSchema(Types.StructType fileSchema) {
+    // this is used to build projection schemas
+    return new Schema(
+        ManifestEntry.STATUS, ManifestEntry.SNAPSHOT_ID, ManifestEntry.SEQUENCE_NUMBER,
+        required(ManifestEntry.DATA_FILE_ID, "data_file", fileSchema));
+  }
+
+  static class IndexedManifestEntry implements ManifestEntry, IndexedRecord {
+    private final org.apache.avro.Schema avroSchema;
+    private final Long commitSnapshotId;
+    private final V1Metadata.IndexedDataFile fileWrapper;
+    private ManifestEntry wrapped = null;
+
+    IndexedManifestEntry(Long commitSnapshotId, Types.StructType partitionType) {
+      this.avroSchema = AvroSchemaUtil.convert(entrySchema(partitionType), "manifest_entry");
+      this.commitSnapshotId = commitSnapshotId;
+      // TODO: when v2 data files differ from v1, this should use a v2 wrapper
+      this.fileWrapper = new V1Metadata.IndexedDataFile(avroSchema.getField("data_file").schema());
+    }
+
+    public IndexedManifestEntry wrap(ManifestEntry entry) {
+      this.wrapped = entry;
+      return this;
+    }
+
+    @Override
+    public org.apache.avro.Schema getSchema() {
+      return avroSchema;
+    }
+
+    @Override
+    public void put(int i, Object v) {
+      throw new UnsupportedOperationException("Cannot read using IndexedManifestEntry");
+    }
+
+    @Override
+    public Object get(int i) {
+      switch (i) {
+        case 0:
+          return wrapped.status().id();
+        case 1:
+          return wrapped.snapshotId();
+        case 2:
+          if (wrapped.sequenceNumber() == null) {
+            // if the entry's sequence number is null, then it will inherit the sequence number of the current commit.
+            // to validate that this is correct, check that the snapshot id is either null (will also be inherited) or
+            // that it matches the id of the current commit.
+            Preconditions.checkState(
+                wrapped.snapshotId() == null || wrapped.snapshotId().equals(commitSnapshotId),
+                "Found unassigned sequence number for an entry from snapshot: %s", wrapped.snapshotId());
+            return null;
+          }
+          return wrapped.sequenceNumber();
+        case 3:
+          return fileWrapper.wrap(wrapped.file());
+        default:
+          throw new UnsupportedOperationException("Unknown field ordinal: " + i);
+      }
+    }
+
+    @Override
+    public Status status() {
+      return wrapped.status();
+    }
+
+    @Override
+    public Long snapshotId() {
+      return wrapped.snapshotId();
+    }
+
+    @Override
+    public void setSnapshotId(long snapshotId) {
+      wrapped.setSnapshotId(snapshotId);
+    }
+
+    @Override
+    public Long sequenceNumber() {
+      return wrapped.sequenceNumber();
+    }
+
+    @Override
+    public void setSequenceNumber(long sequenceNumber) {
+      wrapped.setSequenceNumber(sequenceNumber);
+    }
+
+    @Override
+    public DataFile file() {
+      return wrapped.file();
+    }
+
+    @Override
+    public ManifestEntry copy() {
+      return wrapped.copy();
+    }
+
+    @Override
+    public ManifestEntry copyWithoutStats() {
+      return wrapped.copyWithoutStats();
+    }
+  }
 }
diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
index 04b8747fe43..200e979682c 100644
--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
@@ -21,13 +21,17 @@
 
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import java.io.Closeable;
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -40,9 +44,11 @@
 import org.apache.iceberg.TableMetadata;
 import org.apache.iceberg.TableOperations;
 import org.apache.iceberg.catalog.Namespace;
+import org.apache.iceberg.catalog.SupportsNamespaces;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.exceptions.AlreadyExistsException;
+import org.apache.iceberg.exceptions.NoSuchNamespaceException;
 import org.apache.iceberg.exceptions.NoSuchTableException;
-import org.apache.iceberg.exceptions.NotFoundException;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 
 /**
@@ -58,12 +64,16 @@
  *
  * Note: The HadoopCatalog requires that the underlying file system supports atomic rename.
  */
-public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable {
+public class HadoopCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {
+
   private static final String ICEBERG_HADOOP_WAREHOUSE_BASE = "iceberg/warehouse";
   private static final String TABLE_METADATA_FILE_EXTENSION = ".metadata.json";
+  private static final Joiner SLASH = Joiner.on("/");
   private static final PathFilter TABLE_FILTER = path -> path.getName().endsWith(TABLE_METADATA_FILE_EXTENSION);
+
   private final Configuration conf;
-  private String warehouseLocation;
+  private final String warehouseLocation;
+  private final FileSystem fs;
 
   /**
    * The constructor of the HadoopCatalog. It uses the passed location as its warehouse directory.
@@ -77,6 +87,7 @@ public HadoopCatalog(Configuration conf, String warehouseLocation) {
 
     this.conf = conf;
     this.warehouseLocation = warehouseLocation.replaceAll("/*$", "");
+    this.fs = Util.getFs(new Path(warehouseLocation), conf);
   }
 
   /**
@@ -89,6 +100,7 @@ public HadoopCatalog(Configuration conf, String warehouseLocation) {
   public HadoopCatalog(Configuration conf) {
     this.conf = conf;
     this.warehouseLocation = conf.get("fs.defaultFS") + "/" + ICEBERG_HADOOP_WAREHOUSE_BASE;
+    this.fs = Util.getFs(new Path(warehouseLocation), conf);
   }
 
   @Override
@@ -101,14 +113,12 @@ public List<TableIdentifier> listTables(Namespace namespace) {
     Preconditions.checkArgument(namespace.levels().length >= 1,
         "Missing database in table identifier: %s", namespace);
 
-    Joiner slash = Joiner.on("/");
-    Path nsPath = new Path(slash.join(warehouseLocation, slash.join(namespace.levels())));
-    FileSystem fs = Util.getFs(nsPath, conf);
+    Path nsPath = new Path(warehouseLocation, SLASH.join(namespace.levels()));
     Set<TableIdentifier> tblIdents = Sets.newHashSet();
 
     try {
       if (!fs.exists(nsPath) || !fs.isDirectory(nsPath)) {
-        throw new NotFoundException("Unknown namespace " + namespace);
+        throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
       }
 
       for (FileStatus s : fs.listStatus(nsPath)) {
@@ -128,7 +138,7 @@ public List<TableIdentifier> listTables(Namespace namespace) {
         }
       }
     } catch (IOException ioe) {
-      throw new RuntimeException("Failed to list tables under " + namespace, ioe);
+      throw new RuntimeIOException(ioe, "Failed to list tables under: %s", namespace);
     }
 
     return Lists.newArrayList(tblIdents);
@@ -180,7 +190,6 @@ public boolean dropTable(TableIdentifier identifier, boolean purge) {
       lastMetadata = null;
     }
 
-    FileSystem fs = Util.getFs(tablePath, conf);
     try {
       if (purge && lastMetadata != null) {
         // Since the data files and the metadata files may store in different locations,
@@ -200,7 +209,104 @@ public void renameTable(TableIdentifier from, TableIdentifier to) {
   }
 
   @Override
-  public void close() throws IOException {
+  public void createNamespace(Namespace namespace, Map<String, String> meta) {
+    Preconditions.checkArgument(
+        !namespace.isEmpty(),
+        "Cannot create namespace with invalid name: %s", namespace);
+    if (!meta.isEmpty()) {
+      throw new UnsupportedOperationException("Cannot create namespace " + namespace + " : metadata is not supported");
+    }
+
+    Path nsPath = new Path(warehouseLocation, SLASH.join(namespace.levels()));
+
+    if (isNamespace(nsPath)) {
+      throw new AlreadyExistsException("Namespace '%s' already exists!", namespace);
+    }
+
+    try {
+      fs.mkdirs(nsPath);
+
+    } catch (IOException e) {
+      throw new RuntimeIOException(e, "Create namespace failed: %s", namespace);
+    }
   }
 
+  @Override
+  public List<Namespace> listNamespaces(Namespace namespace) {
+    Path nsPath = namespace.isEmpty() ? new Path(warehouseLocation)
+        : new Path(warehouseLocation, SLASH.join(namespace.levels()));
+    if (!isNamespace(nsPath)) {
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
+    }
+
+    try {
+      return Stream.of(fs.listStatus(nsPath))
+        .map(FileStatus::getPath)
+        .filter(path -> isNamespace(path))
+        .map(path -> append(namespace, path.getName()))
+        .collect(Collectors.toList());
+    } catch (IOException ioe) {
+      throw new RuntimeIOException(ioe, "Failed to list namespace under: %s", namespace);
+    }
+  }
+
+  private Namespace append(Namespace ns, String name) {
+    String[] levels = Arrays.copyOfRange(ns.levels(), 0, ns.levels().length + 1);
+    levels[ns.levels().length] = name;
+    return Namespace.of(levels);
+  }
+
+  @Override
+  public boolean dropNamespace(Namespace namespace) {
+    Path nsPath = new Path(warehouseLocation, SLASH.join(namespace.levels()));
+
+    if (!isNamespace(nsPath) || namespace.isEmpty()) {
+      return false;
+    }
+
+    try {
+      return fs.delete(nsPath, false /* recursive */);
+
+    } catch (IOException e) {
+      throw new RuntimeIOException(e, "Namespace delete failed: %s", namespace);
+    }
+  }
+
+  @Override
+  public boolean setProperties(Namespace namespace,  Map<String, String> properties) {
+    throw new UnsupportedOperationException(
+        "Cannot set namespace properties " + namespace + " : setProperties is not supported");
+  }
+
+  @Override
+  public boolean removeProperties(Namespace namespace,  Set<String> properties) {
+    throw new UnsupportedOperationException(
+        "Cannot remove properties " + namespace + " : removeProperties is not supported");
+  }
+
+  @Override
+  public Map<String, String> loadNamespaceMetadata(Namespace namespace) {
+    Path nsPath = new Path(warehouseLocation, SLASH.join(namespace.levels()));
+
+    if (!isNamespace(nsPath) || namespace.isEmpty()) {
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
+    }
+
+    return ImmutableMap.of("location", nsPath.toString());
+  }
+
+  private boolean isNamespace(Path path) {
+    Path metadataPath = new Path(path, "metadata");
+    try {
+      return fs.isDirectory(path) && !(fs.exists(metadataPath) && fs.isDirectory(metadataPath) &&
+          (fs.listStatus(metadataPath, TABLE_FILTER).length >= 1));
+
+    } catch (IOException ioe) {
+      throw new RuntimeIOException(ioe, "Failed to list namespace info: %s ", path);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+  }
 }
diff --git a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
index 8bad1488411..431fc8991a2 100644
--- a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
+++ b/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
@@ -42,6 +42,7 @@
 import org.apache.iceberg.exceptions.ValidationException;
 import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.io.LocationProvider;
+import org.apache.iceberg.util.Pair;
 import org.apache.iceberg.util.Tasks;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -56,11 +57,12 @@ public class HadoopTableOperations implements TableOperations {
 
   private final Configuration conf;
   private final Path location;
-  private TableMetadata currentMetadata = null;
-  private Integer version = null;
-  private boolean shouldRefresh = true;
   private HadoopFileIO defaultFileIo = null;
 
+  private volatile TableMetadata currentMetadata = null;
+  private volatile Integer version = null;
+  private volatile boolean shouldRefresh = true;
+
   protected HadoopTableOperations(Path location, Configuration conf) {
     this.conf = conf;
     this.location = location;
@@ -74,6 +76,18 @@ public TableMetadata current() {
     return currentMetadata;
   }
 
+  private synchronized Pair<Integer, TableMetadata> versionAndMetadata() {
+    return Pair.of(version, currentMetadata);
+  }
+
+  private synchronized void updateVersionAndMetadata(int newVersion, String metadataFile) {
+    // update if the current version is out of date
+    if (version == null || version != newVersion) {
+      this.version = newVersion;
+      this.currentMetadata = checkUUID(currentMetadata, TableMetadataParser.read(io(), metadataFile));
+    }
+  }
+
   @Override
   public TableMetadata refresh() {
     int ver = version != null ? version : readVersionHint();
@@ -93,12 +107,7 @@ public TableMetadata refresh() {
         nextMetadataFile = getMetadataFile(ver + 1);
       }
 
-      // only load if the current version is out of date
-      if (version == null || version != ver) {
-        this.version = ver;
-
-        this.currentMetadata = checkUUID(currentMetadata, TableMetadataParser.read(io(), metadataFile.toString()));
-      }
+      updateVersionAndMetadata(ver, metadataFile.toString());
 
       this.shouldRefresh = false;
       return currentMetadata;
@@ -108,8 +117,9 @@ public TableMetadata refresh() {
   }
 
   @Override
-  public synchronized void commit(TableMetadata base, TableMetadata metadata) {
-    if (base != current()) {
+  public void commit(TableMetadata base, TableMetadata metadata) {
+    Pair<Integer, TableMetadata> current = versionAndMetadata();
+    if (base != current.second()) {
       throw new CommitFailedException("Cannot commit changes based on stale table metadata");
     }
 
@@ -131,7 +141,7 @@ public synchronized void commit(TableMetadata base, TableMetadata metadata) {
     Path tempMetadataFile = metadataPath(UUID.randomUUID().toString() + fileExtension);
     TableMetadataParser.write(metadata, io().newOutputFile(tempMetadataFile.toString()));
 
-    int nextVersion = (version != null ? version : 0) + 1;
+    int nextVersion = (current.first() != null ? current.first() : 0) + 1;
     Path finalMetadataFile = metadataFilePath(nextVersion, codec);
     FileSystem fs = getFileSystem(tempMetadataFile, conf);
 
diff --git a/core/src/test/java/org/apache/iceberg/TableMetadataParserTest.java b/core/src/test/java/org/apache/iceberg/TableMetadataParserTest.java
index a03a38a17e5..f353989f11e 100644
--- a/core/src/test/java/org/apache/iceberg/TableMetadataParserTest.java
+++ b/core/src/test/java/org/apache/iceberg/TableMetadataParserTest.java
@@ -30,6 +30,7 @@
 import java.util.zip.GZIPInputStream;
 import java.util.zip.ZipException;
 import org.apache.iceberg.TableMetadataParser.Codec;
+import org.apache.iceberg.io.FileIO;
 import org.apache.iceberg.io.OutputFile;
 import org.apache.iceberg.types.Types.BooleanType;
 import org.junit.After;
@@ -74,7 +75,7 @@ public void testCompressionProperty() throws IOException {
     TableMetadata metadata = newTableMetadata(SCHEMA, unpartitioned(), location, properties);
     TableMetadataParser.write(metadata, outputFile);
     Assert.assertEquals(codec == Codec.GZIP, isCompressed(fileName));
-    TableMetadata actualMetadata = TableMetadataParser.read(null, Files.localInput(new File(fileName)));
+    TableMetadata actualMetadata = TableMetadataParser.read((FileIO) null, Files.localInput(new File(fileName)));
     verifyMetadata(metadata, actualMetadata);
   }
 
diff --git a/core/src/test/java/org/apache/iceberg/TableTestBase.java b/core/src/test/java/org/apache/iceberg/TableTestBase.java
index 4398c2420b2..ef4466dec2c 100644
--- a/core/src/test/java/org/apache/iceberg/TableTestBase.java
+++ b/core/src/test/java/org/apache/iceberg/TableTestBase.java
@@ -86,6 +86,12 @@ public class TableTestBase {
   File metadataDir = null;
   public TestTables.TestTable table = null;
 
+  protected final int formatVersion;
+
+  public TableTestBase(int formatVersion) {
+    this.formatVersion = formatVersion;
+  }
+
   @Before
   public void setupTable() throws Exception {
     this.tableDir = temp.newFolder();
@@ -109,8 +115,8 @@ List<File> listManifestFiles(File tableDirToList) {
         !name.startsWith("snap") && Files.getFileExtension(name).equalsIgnoreCase("avro")));
   }
 
-  private TestTables.TestTable create(Schema schema, PartitionSpec spec) {
-    return TestTables.create(tableDir, "test", schema, spec);
+  TestTables.TestTable create(Schema schema, PartitionSpec spec) {
+    return TestTables.create(tableDir, "test", schema, spec, formatVersion);
   }
 
   TestTables.TestTable load() {
@@ -130,7 +136,7 @@ ManifestFile writeManifest(DataFile... files) throws IOException {
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = table.ops().io().newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(table.spec(), outputFile);
+    ManifestWriter writer = ManifestFiles.write(formatVersion, table.spec(), outputFile, null);
     try {
       for (DataFile file : files) {
         writer.add(file);
@@ -147,7 +153,7 @@ ManifestFile writeManifest(String fileName, ManifestEntry... entries) throws IOE
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = table.ops().io().newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(table.spec(), outputFile);
+    ManifestWriter writer = ManifestFiles.write(formatVersion, table.spec(), outputFile, null);
     try {
       for (ManifestEntry entry : entries) {
         writer.addEntry(entry);
@@ -164,7 +170,7 @@ ManifestFile writeManifestWithName(String name, DataFile... files) throws IOExce
     Assert.assertTrue(manifestFile.delete());
     OutputFile outputFile = table.ops().io().newOutputFile(manifestFile.getCanonicalPath());
 
-    ManifestWriter writer = ManifestFiles.write(table.spec(), outputFile);
+    ManifestWriter writer = ManifestFiles.write(formatVersion, table.spec(), outputFile, null);
     try {
       for (DataFile file : files) {
         writer.add(file);
@@ -177,12 +183,12 @@ ManifestFile writeManifestWithName(String name, DataFile... files) throws IOExce
   }
 
   ManifestEntry manifestEntry(ManifestEntry.Status status, Long snapshotId, DataFile file) {
-    ManifestEntry entry = new ManifestEntry(table.spec().partitionType());
+    GenericManifestEntry entry = new GenericManifestEntry(table.spec().partitionType());
     switch (status) {
       case ADDED:
         return entry.wrapAppend(snapshotId, file);
       case EXISTING:
-        return entry.wrapExisting(snapshotId, file);
+        return entry.wrapExisting(snapshotId, 0L, file);
       case DELETED:
         return entry.wrapDelete(snapshotId, file);
       default:
diff --git a/core/src/test/java/org/apache/iceberg/TestCreateTransaction.java b/core/src/test/java/org/apache/iceberg/TestCreateTransaction.java
index 90b46444e8f..342e91306b1 100644
--- a/core/src/test/java/org/apache/iceberg/TestCreateTransaction.java
+++ b/core/src/test/java/org/apache/iceberg/TestCreateTransaction.java
@@ -26,10 +26,25 @@
 import org.apache.iceberg.types.TypeUtil;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.PartitionSpec.unpartitioned;
 
+@RunWith(Parameterized.class)
 public class TestCreateTransaction extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestCreateTransaction(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testCreateTransaction() throws IOException {
     File tableDir = temp.newFolder();
@@ -273,7 +288,7 @@ public void testCreateTransactionConflict() throws IOException {
     Assert.assertNull("Should have no metadata version",
         TestTables.metadataVersion("test_conflict"));
 
-    Table conflict = TestTables.create(tableDir, "test_conflict", SCHEMA, unpartitioned());
+    Table conflict = TestTables.create(tableDir, "test_conflict", SCHEMA, unpartitioned(), formatVersion);
 
     Assert.assertEquals("Table schema should match with reassigned IDs",
         TypeUtil.assignIncreasingFreshIds(SCHEMA).asStruct(), conflict.schema().asStruct());
diff --git a/core/src/test/java/org/apache/iceberg/TestDataTableScan.java b/core/src/test/java/org/apache/iceberg/TestDataTableScan.java
index 5c3dc90898f..ff75d529e2f 100644
--- a/core/src/test/java/org/apache/iceberg/TestDataTableScan.java
+++ b/core/src/test/java/org/apache/iceberg/TestDataTableScan.java
@@ -19,42 +19,30 @@
 
 package org.apache.iceberg;
 
-import java.io.File;
-import java.io.IOException;
 import org.apache.iceberg.types.Types;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Rule;
 import org.junit.Test;
-import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
 import static org.junit.Assert.assertEquals;
 
-public class TestDataTableScan {
-
-  @Rule
-  public TemporaryFolder temp = new TemporaryFolder();
-  private final Schema schema = new Schema(
-      required(1, "id", Types.IntegerType.get()),
-      required(2, "data", Types.StringType.get()));
-  private File tableDir = null;
-
-  @Before
-  public void setupTableDir() throws IOException {
-    this.tableDir = temp.newFolder();
+@RunWith(Parameterized.class)
+public class TestDataTableScan extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
   }
 
-  @After
-  public void cleanupTables() {
-    TestTables.clearTables();
+  public TestDataTableScan(int formatVersion) {
+    super(formatVersion);
   }
 
   @Test
   public void testTableScanHonorsSelect() {
-    PartitionSpec spec = PartitionSpec.unpartitioned();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
-
     TableScan scan = table.newScan().select("id");
 
     Schema expectedSchema = new Schema(required(1, "id", Types.IntegerType.get()));
@@ -66,9 +54,6 @@ public void testTableScanHonorsSelect() {
 
   @Test
   public void testTableScanHonorsSelectWithoutCaseSensitivity() {
-    PartitionSpec spec = PartitionSpec.unpartitioned();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
-
     TableScan scan1 = table.newScan().caseSensitive(false).select("ID");
     // order of refinements shouldn't matter
     TableScan scan2 = table.newScan().select("ID").caseSensitive(false);
diff --git a/core/src/test/java/org/apache/iceberg/TestDeleteFiles.java b/core/src/test/java/org/apache/iceberg/TestDeleteFiles.java
index cb95d02357c..bdc5537b3e2 100644
--- a/core/src/test/java/org/apache/iceberg/TestDeleteFiles.java
+++ b/core/src/test/java/org/apache/iceberg/TestDeleteFiles.java
@@ -22,8 +22,23 @@
 import org.apache.iceberg.ManifestEntry.Status;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestDeleteFiles extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestDeleteFiles(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testMultipleDeletes() {
     table.newAppend()
diff --git a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java
index 5e0ff86197c..2255b6e3fb3 100644
--- a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java
+++ b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java
@@ -22,10 +22,24 @@
 import com.google.common.collect.Iterables;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.junit.Assert.assertEquals;
 
+@RunWith(Parameterized.class)
 public class TestEntriesMetadataTable extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestEntriesMetadataTable(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testEntriesTable() {
@@ -66,52 +80,38 @@ public void testEntriesTableScan() {
   }
 
   @Test
-  public void testSplitPlanningWithSplitSizeOption() {
+  public void testSplitPlanningWithMetadataSplitSizeProperty() {
     table.newAppend()
         .appendFile(FILE_A)
         .appendFile(FILE_B)
         .commit();
 
-    int splitSize = 2 * 1024; // 2 KB split size
+    table.newAppend()
+        .appendFile(FILE_C)
+        .appendFile(FILE_D)
+        .commit();
 
+    // set the split size to a large value so that both manifests are in 1 split
     table.updateProperties()
-        .set(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(2 * splitSize))
+        .set(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(128 * 1024 * 1024))
         .commit();
 
     Table entriesTable = new ManifestEntriesTable(table.ops(), table);
-    Assert.assertEquals(1, entriesTable.currentSnapshot().manifests().size());
-
-    int expectedSplits =
-        ((int) entriesTable.currentSnapshot().manifests().get(0).length() + splitSize - 1) / splitSize;
-
-    TableScan scan = entriesTable.newScan()
-        .option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));
-
-    Assert.assertEquals(expectedSplits, Iterables.size(scan.planTasks()));
-  }
 
-  @Test
-  public void testSplitPlanningWithMetadataSplitSizeProperty() {
-    table.newAppend()
-        .appendFile(FILE_A)
-        .appendFile(FILE_B)
-        .commit();
-
-    int splitSize = 2 * 1024; // 2 KB split size
+    Assert.assertEquals(1, Iterables.size(entriesTable.newScan().planTasks()));
 
+    // set the split size to a small value so that manifests end up in different splits
     table.updateProperties()
-        .set(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(splitSize))
+        .set(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(1))
         .commit();
 
-    Table entriesTable = new ManifestEntriesTable(table.ops(), table);
-    Assert.assertEquals(1, entriesTable.currentSnapshot().manifests().size());
-
-    int expectedSplits =
-        ((int) entriesTable.currentSnapshot().manifests().get(0).length() + splitSize - 1) / splitSize;
+    Assert.assertEquals(2, Iterables.size(entriesTable.newScan().planTasks()));
 
-    TableScan scan = entriesTable.newScan();
+    // override the table property with a large value so that both manifests are in 1 split
+    TableScan scan = entriesTable.newScan()
+        .option(TableProperties.SPLIT_SIZE, String.valueOf(128 * 1024 * 1024));
 
-    Assert.assertEquals(expectedSplits, Iterables.size(scan.planTasks()));
+    Assert.assertEquals(1, Iterables.size(scan.planTasks()));
   }
 
   @Test
diff --git a/core/src/test/java/org/apache/iceberg/TestFastAppend.java b/core/src/test/java/org/apache/iceberg/TestFastAppend.java
index f8cb6d28f65..4f071e1570e 100644
--- a/core/src/test/java/org/apache/iceberg/TestFastAppend.java
+++ b/core/src/test/java/org/apache/iceberg/TestFastAppend.java
@@ -29,8 +29,22 @@
 import org.apache.iceberg.exceptions.CommitFailedException;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestFastAppend extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestFastAppend(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testEmptyTableAppend() {
diff --git a/core/src/test/java/org/apache/iceberg/TestFilterFiles.java b/core/src/test/java/org/apache/iceberg/TestFilterFiles.java
index 3a1c8867e96..d023048de54 100644
--- a/core/src/test/java/org/apache/iceberg/TestFilterFiles.java
+++ b/core/src/test/java/org/apache/iceberg/TestFilterFiles.java
@@ -34,11 +34,27 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
 import static org.junit.Assert.assertEquals;
 
+@RunWith(Parameterized.class)
 public class TestFilterFiles {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public final int formatVersion;
+
+  public TestFilterFiles(int formatVersion) {
+    this.formatVersion = formatVersion;
+  }
 
   @Rule
   public TemporaryFolder temp = new TemporaryFolder();
@@ -60,28 +76,28 @@ public void cleanupTables() {
   @Test
   public void testFilterFilesUnpartitionedTable() {
     PartitionSpec spec = PartitionSpec.unpartitioned();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
+    Table table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
     testFilterFiles(table);
   }
 
   @Test
   public void testCaseInsensitiveFilterFilesUnpartitionedTable() {
     PartitionSpec spec = PartitionSpec.unpartitioned();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
+    Table table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
     testCaseInsensitiveFilterFiles(table);
   }
 
   @Test
   public void testFilterFilesPartitionedTable() {
     PartitionSpec spec = PartitionSpec.builderFor(schema).bucket("data", 16).build();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
+    Table table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
     testFilterFiles(table);
   }
 
   @Test
   public void testCaseInsensitiveFilterFilesPartitionedTable() {
     PartitionSpec spec = PartitionSpec.builderFor(schema).bucket("data", 16).build();
-    Table table = TestTables.create(tableDir, "test", schema, spec);
+    Table table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
     testCaseInsensitiveFilterFiles(table);
   }
 
diff --git a/core/src/test/java/org/apache/iceberg/TestFindFiles.java b/core/src/test/java/org/apache/iceberg/TestFindFiles.java
index 4cb5bdd974d..68121d3f6be 100644
--- a/core/src/test/java/org/apache/iceberg/TestFindFiles.java
+++ b/core/src/test/java/org/apache/iceberg/TestFindFiles.java
@@ -29,8 +29,23 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestFindFiles extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestFindFiles(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testBasicBehavior() {
     table.newAppend()
diff --git a/core/src/test/java/org/apache/iceberg/TestFormatVersions.java b/core/src/test/java/org/apache/iceberg/TestFormatVersions.java
index f58d70beeeb..1d04834c288 100644
--- a/core/src/test/java/org/apache/iceberg/TestFormatVersions.java
+++ b/core/src/test/java/org/apache/iceberg/TestFormatVersions.java
@@ -23,6 +23,10 @@
 import org.junit.Test;
 
 public class TestFormatVersions extends TableTestBase {
+  public TestFormatVersions() {
+    super(1);
+  }
+
   @Test
   public void testDefaultFormatVersion() {
     Assert.assertEquals("Should default to v1", 1, table.ops().current().formatVersion());
diff --git a/core/src/test/java/org/apache/iceberg/TestIncrementalDataTableScan.java b/core/src/test/java/org/apache/iceberg/TestIncrementalDataTableScan.java
index 22b5848408a..485d4c2fea7 100644
--- a/core/src/test/java/org/apache/iceberg/TestIncrementalDataTableScan.java
+++ b/core/src/test/java/org/apache/iceberg/TestIncrementalDataTableScan.java
@@ -27,8 +27,22 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestIncrementalDataTableScan extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestIncrementalDataTableScan(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Before
   public void setupTableProperties() {
diff --git a/core/src/test/java/org/apache/iceberg/TestManifestCleanup.java b/core/src/test/java/org/apache/iceberg/TestManifestCleanup.java
index 917862d4a97..acb08bb9d1e 100644
--- a/core/src/test/java/org/apache/iceberg/TestManifestCleanup.java
+++ b/core/src/test/java/org/apache/iceberg/TestManifestCleanup.java
@@ -22,8 +22,23 @@
 import org.apache.iceberg.expressions.Expressions;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestManifestCleanup extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestManifestCleanup(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testDelete() {
     Assert.assertEquals("Table should start with no manifests",
diff --git a/core/src/test/java/org/apache/iceberg/TestManifestFileVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestListVersions.java
similarity index 99%
rename from core/src/test/java/org/apache/iceberg/TestManifestFileVersions.java
rename to core/src/test/java/org/apache/iceberg/TestManifestListVersions.java
index 0155cca7f77..65ec4b24b29 100644
--- a/core/src/test/java/org/apache/iceberg/TestManifestFileVersions.java
+++ b/core/src/test/java/org/apache/iceberg/TestManifestListVersions.java
@@ -39,7 +39,7 @@
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 
-public class TestManifestFileVersions {
+public class TestManifestListVersions {
   private static final String PATH = "s3://bucket/table/m1.avro";
   private static final long LENGTH = 1024L;
   private static final int SPEC_ID = 1;
diff --git a/core/src/test/java/org/apache/iceberg/TestManifestReader.java b/core/src/test/java/org/apache/iceberg/TestManifestReader.java
index 76ede625ffa..058a845e47e 100644
--- a/core/src/test/java/org/apache/iceberg/TestManifestReader.java
+++ b/core/src/test/java/org/apache/iceberg/TestManifestReader.java
@@ -26,8 +26,22 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestManifestReader extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestManifestReader(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   @SuppressWarnings("deprecation")
diff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriter.java b/core/src/test/java/org/apache/iceberg/TestManifestWriter.java
index 9a67ac7b45d..a344e748381 100644
--- a/core/src/test/java/org/apache/iceberg/TestManifestWriter.java
+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriter.java
@@ -24,22 +24,36 @@
 import org.apache.iceberg.ManifestEntry.Status;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestManifestWriter extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestManifestWriter(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testManifestStats() throws IOException {
     ManifestFile manifest = writeManifest(
         "manifest.avro",
-        manifestEntry(Status.ADDED, 100L, newFile(10)),
-        manifestEntry(Status.ADDED, 100L, newFile(20)),
-        manifestEntry(Status.ADDED, 100L, newFile(5)),
-        manifestEntry(Status.ADDED, 100L, newFile(5)),
-        manifestEntry(Status.EXISTING, 100L, newFile(15)),
-        manifestEntry(Status.EXISTING, 100L, newFile(10)),
-        manifestEntry(Status.EXISTING, 100L, newFile(1)),
-        manifestEntry(Status.DELETED, 100L, newFile(5)),
-        manifestEntry(Status.DELETED, 100L, newFile(2)));
+        manifestEntry(Status.ADDED, null, newFile(10)),
+        manifestEntry(Status.ADDED, null, newFile(20)),
+        manifestEntry(Status.ADDED, null, newFile(5)),
+        manifestEntry(Status.ADDED, null, newFile(5)),
+        manifestEntry(Status.EXISTING, null, newFile(15)),
+        manifestEntry(Status.EXISTING, null, newFile(10)),
+        manifestEntry(Status.EXISTING, null, newFile(1)),
+        manifestEntry(Status.DELETED, null, newFile(5)),
+        manifestEntry(Status.DELETED, null, newFile(2)));
 
     Assert.assertTrue("Added files should be present", manifest.hasAddedFiles());
     Assert.assertEquals("Added files count should match", 4, (int) manifest.addedFilesCount());
diff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java
new file mode 100644
index 00000000000..423cd51074d
--- /dev/null
+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java
@@ -0,0 +1,236 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg;
+
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import java.io.IOException;
+import java.util.List;
+import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.io.FileAppender;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.io.OutputFile;
+import org.apache.iceberg.types.Conversions;
+import org.apache.iceberg.types.Types;
+import org.junit.Assert;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import static org.apache.iceberg.types.Types.NestedField.required;
+
+public class TestManifestWriterVersions {
+  private static final FileIO FILE_IO = new TestTables.LocalFileIO();
+
+  private static final Schema SCHEMA = new Schema(
+      required(1, "id", Types.LongType.get()),
+      required(2, "timestamp", Types.TimestampType.withZone()),
+      required(3, "category", Types.StringType.get()),
+      required(4, "data", Types.StringType.get()));
+
+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)
+      .identity("category")
+      .hour("timestamp")
+      .bucket("id", 16)
+      .build();
+
+  private static final long SEQUENCE_NUMBER = 34L;
+  private static final long SNAPSHOT_ID = 987134631982734L;
+  private static final String PATH = "s3://bucket/table/category=cheesy/timestamp_hour=10/id_bucket=3/file.avro";
+  private static final FileFormat FORMAT = FileFormat.AVRO;
+  private static final PartitionData PARTITION = DataFiles.data(SPEC, "category=cheesy/timestamp_hour=10/id_bucket=3");
+  private static final Metrics METRICS = new Metrics(
+      1587L,
+      ImmutableMap.of(1, 15L, 2, 122L, 3, 4021L, 4, 9411L), // sizes
+      ImmutableMap.of(1, 100L, 2, 100L, 3, 100L, 4, 100L),  // value counts
+      ImmutableMap.of(1, 0L, 2, 0L, 3, 0L, 4, 0L),          // null value counts
+      ImmutableMap.of(1, Conversions.toByteBuffer(Types.IntegerType.get(), 1)),  // lower bounds
+      ImmutableMap.of(1, Conversions.toByteBuffer(Types.IntegerType.get(), 1))); // upper bounds
+  private static final List<Long> OFFSETS = ImmutableList.of(4L);
+
+  private static final DataFile DATA_FILE = new GenericDataFile(
+      PATH, FORMAT, PARTITION, 150972L, METRICS, null, OFFSETS);
+
+  @Rule
+  public TemporaryFolder temp = new TemporaryFolder();
+
+  @Test
+  public void testV1Write() throws IOException {
+    ManifestFile manifest = writeManifest(1);
+    checkManifest(manifest, ManifestWriter.UNASSIGNED_SEQ);
+    checkEntry(readManifest(manifest), ManifestWriter.UNASSIGNED_SEQ);
+  }
+
+  @Test
+  public void testV1WriteWithInheritance() throws IOException {
+    ManifestFile manifest = writeAndReadManifestList(writeManifest(1), 1);
+    checkManifest(manifest, 0L);
+
+    // v1 should be read using sequence number 0 because it was missing from the manifest list file
+    checkEntry(readManifest(manifest), 0L);
+  }
+
+  @Test
+  public void testV2Write() throws IOException {
+    ManifestFile manifest = writeManifest(1);
+    checkManifest(manifest, ManifestWriter.UNASSIGNED_SEQ);
+    checkEntry(readManifest(manifest), ManifestWriter.UNASSIGNED_SEQ);
+  }
+
+  @Test
+  public void testV2WriteWithInheritance() throws IOException {
+    ManifestFile manifest = writeAndReadManifestList(writeManifest(2), 2);
+    checkManifest(manifest, SEQUENCE_NUMBER);
+
+    // v2 should use the correct sequence number by inheriting it
+    checkEntry(readManifest(manifest), SEQUENCE_NUMBER);
+  }
+
+  @Test
+  public void testV2ManifestListRewriteWithInheritance() throws IOException {
+    // write with v1
+    ManifestFile manifest = writeAndReadManifestList(writeManifest(1), 1);
+    checkManifest(manifest, 0L);
+
+    // rewrite existing metadata with v2 manifest list
+    ManifestFile manifest2 = writeAndReadManifestList(manifest, 2);
+    // the ManifestFile did not change and should still have its original sequence number, 0
+    checkManifest(manifest2, 0L);
+
+    // should not inherit the v2 sequence number because it was a rewrite
+    checkEntry(readManifest(manifest2), 0L);
+  }
+
+  @Test
+  public void testV2ManifestRewriteWithInheritance() throws IOException {
+    // write with v1
+    ManifestFile manifest = writeAndReadManifestList(writeManifest(1), 1);
+    checkManifest(manifest, 0L);
+
+    // rewrite the manifest file using a v2 manifest
+    ManifestFile rewritten = rewriteManifest(manifest, 2);
+    checkRewrittenManifest(rewritten, ManifestWriter.UNASSIGNED_SEQ, 0L);
+
+    // add the v2 manifest to a v2 manifest list, with a sequence number
+    ManifestFile manifest2 = writeAndReadManifestList(rewritten, 2);
+    // the ManifestFile is new so it has a sequence number, but the min sequence number 0 is from the entry
+    checkRewrittenManifest(manifest2, SEQUENCE_NUMBER, 0L);
+
+    // should not inherit the v2 sequence number because it was written into the v2 manifest
+    checkRewrittenEntry(readManifest(manifest2), 0L);
+  }
+
+  void checkEntry(ManifestEntry entry, Long expectedSequenceNumber) {
+    Assert.assertEquals("Status", ManifestEntry.Status.ADDED, entry.status());
+    Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, entry.snapshotId());
+    Assert.assertEquals("Sequence number", expectedSequenceNumber, entry.sequenceNumber());
+    checkDataFile(entry.file());
+  }
+
+  void checkRewrittenEntry(ManifestEntry entry, Long expectedSequenceNumber) {
+    Assert.assertEquals("Status", ManifestEntry.Status.EXISTING, entry.status());
+    Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, entry.snapshotId());
+    Assert.assertEquals("Sequence number", expectedSequenceNumber, entry.sequenceNumber());
+    checkDataFile(entry.file());
+  }
+
+  void checkDataFile(DataFile dataFile) {
+    Assert.assertEquals("Path", PATH, dataFile.path());
+    Assert.assertEquals("Format", FORMAT, dataFile.format());
+    Assert.assertEquals("Partition", PARTITION, dataFile.partition());
+    Assert.assertEquals("Record count", METRICS.recordCount(), (Long) dataFile.recordCount());
+    Assert.assertEquals("Column sizes", METRICS.columnSizes(), dataFile.columnSizes());
+    Assert.assertEquals("Value counts", METRICS.valueCounts(), dataFile.valueCounts());
+    Assert.assertEquals("Null value counts", METRICS.nullValueCounts(), dataFile.nullValueCounts());
+    Assert.assertEquals("Lower bounds", METRICS.lowerBounds(), dataFile.lowerBounds());
+    Assert.assertEquals("Upper bounds", METRICS.upperBounds(), dataFile.upperBounds());
+  }
+
+  void checkManifest(ManifestFile manifest, long expectedSequenceNumber) {
+    Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, manifest.snapshotId());
+    Assert.assertEquals("Sequence number", expectedSequenceNumber, manifest.sequenceNumber());
+    Assert.assertEquals("Min sequence number", expectedSequenceNumber, manifest.minSequenceNumber());
+    Assert.assertEquals("Added files count", (Integer) 1, manifest.addedFilesCount());
+    Assert.assertEquals("Existing files count", (Integer) 0, manifest.existingFilesCount());
+    Assert.assertEquals("Deleted files count", (Integer) 0, manifest.deletedFilesCount());
+    Assert.assertEquals("Added rows count", METRICS.recordCount(), manifest.addedRowsCount());
+    Assert.assertEquals("Existing rows count", (Long) 0L, manifest.existingRowsCount());
+    Assert.assertEquals("Deleted rows count", (Long) 0L, manifest.deletedRowsCount());
+  }
+
+  void checkRewrittenManifest(ManifestFile manifest, long expectedSequenceNumber, long expectedMinSequenceNumber) {
+    Assert.assertEquals("Snapshot ID", (Long) SNAPSHOT_ID, manifest.snapshotId());
+    Assert.assertEquals("Sequence number", expectedSequenceNumber, manifest.sequenceNumber());
+    Assert.assertEquals("Min sequence number", expectedMinSequenceNumber, manifest.minSequenceNumber());
+    Assert.assertEquals("Added files count", (Integer) 0, manifest.addedFilesCount());
+    Assert.assertEquals("Existing files count", (Integer) 1, manifest.existingFilesCount());
+    Assert.assertEquals("Deleted files count", (Integer) 0, manifest.deletedFilesCount());
+    Assert.assertEquals("Added rows count", (Long) 0L, manifest.addedRowsCount());
+    Assert.assertEquals("Existing rows count", METRICS.recordCount(), manifest.existingRowsCount());
+    Assert.assertEquals("Deleted rows count", (Long) 0L, manifest.deletedRowsCount());
+  }
+
+  private InputFile writeManifestList(ManifestFile manifest, int formatVersion) throws IOException {
+    OutputFile manifestList = Files.localOutput(temp.newFile());
+    try (FileAppender<ManifestFile> writer = ManifestLists.write(
+        formatVersion, manifestList, SNAPSHOT_ID, SNAPSHOT_ID - 1, formatVersion > 1 ? SEQUENCE_NUMBER : 0)) {
+      writer.add(manifest);
+    }
+    return manifestList.toInputFile();
+  }
+
+  private ManifestFile writeAndReadManifestList(ManifestFile manifest, int formatVersion) throws IOException {
+    List<ManifestFile> manifests = ManifestLists.read(writeManifestList(manifest, formatVersion));
+    Assert.assertEquals("Should contain one manifest", 1, manifests.size());
+    return manifests.get(0);
+  }
+
+  private ManifestFile rewriteManifest(ManifestFile manifest, int formatVersion) throws IOException {
+    OutputFile manifestFile = Files.localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
+    ManifestWriter writer = ManifestFiles.write(formatVersion, SPEC, manifestFile, SNAPSHOT_ID);
+    try {
+      writer.existing(readManifest(manifest));
+    } finally {
+      writer.close();
+    }
+    return writer.toManifestFile();
+  }
+
+  private ManifestFile writeManifest(int formatVersion) throws IOException {
+    OutputFile manifestFile = Files.localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
+    ManifestWriter writer = ManifestFiles.write(formatVersion, SPEC, manifestFile, SNAPSHOT_ID);
+    try {
+      writer.add(DATA_FILE);
+    } finally {
+      writer.close();
+    }
+    return writer.toManifestFile();
+  }
+
+  private ManifestEntry readManifest(ManifestFile manifest) throws IOException {
+    try (CloseableIterable<ManifestEntry> reader = ManifestFiles.read(manifest, FILE_IO).entries()) {
+      List<ManifestEntry> files = Lists.newArrayList(reader);
+      Assert.assertEquals("Should contain only one data file", 1, files.size());
+      return files.get(0);
+    }
+  }
+}
diff --git a/core/src/test/java/org/apache/iceberg/TestMergeAppend.java b/core/src/test/java/org/apache/iceberg/TestMergeAppend.java
index d4dcff9c542..88dc997e004 100644
--- a/core/src/test/java/org/apache/iceberg/TestMergeAppend.java
+++ b/core/src/test/java/org/apache/iceberg/TestMergeAppend.java
@@ -30,10 +30,25 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static com.google.common.collect.Iterators.concat;
 
+@RunWith(Parameterized.class)
 public class TestMergeAppend extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestMergeAppend(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testEmptyTableAppend() {
     Assert.assertEquals("Table should start empty", 0, listManifestFiles().size());
diff --git a/core/src/test/java/org/apache/iceberg/TestOverwrite.java b/core/src/test/java/org/apache/iceberg/TestOverwrite.java
index 81f1a992edd..dca9875ff3e 100644
--- a/core/src/test/java/org/apache/iceberg/TestOverwrite.java
+++ b/core/src/test/java/org/apache/iceberg/TestOverwrite.java
@@ -30,6 +30,8 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.expressions.Expressions.and;
 import static org.apache.iceberg.expressions.Expressions.equal;
@@ -37,6 +39,7 @@
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestOverwrite extends TableTestBase {
   private static final Schema DATE_SCHEMA = new Schema(
       required(1, "id", Types.LongType.get()),
@@ -89,6 +92,18 @@ public class TestOverwrite extends TableTestBase {
       ))
       .build();
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestOverwrite(int formatVersion) {
+    super(formatVersion);
+  }
+
   private static ByteBuffer longToBuffer(long value) {
     return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putLong(0, value);
   }
@@ -100,7 +115,7 @@ public void createTestTable() throws IOException {
     File tableDir = temp.newFolder();
     Assert.assertTrue(tableDir.delete());
 
-    this.table = TestTables.create(tableDir, TABLE_NAME, DATE_SCHEMA, PARTITION_BY_DATE);
+    this.table = TestTables.create(tableDir, TABLE_NAME, DATE_SCHEMA, PARTITION_BY_DATE, formatVersion);
 
     table.newAppend()
         .appendFile(FILE_0_TO_4)
diff --git a/core/src/test/java/org/apache/iceberg/TestOverwriteWithValidation.java b/core/src/test/java/org/apache/iceberg/TestOverwriteWithValidation.java
index 1997067e76c..9eea57c03d0 100644
--- a/core/src/test/java/org/apache/iceberg/TestOverwriteWithValidation.java
+++ b/core/src/test/java/org/apache/iceberg/TestOverwriteWithValidation.java
@@ -31,6 +31,8 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.expressions.Expressions.alwaysTrue;
 import static org.apache.iceberg.expressions.Expressions.and;
@@ -40,6 +42,7 @@
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestOverwriteWithValidation extends TableTestBase {
 
   private static final String TABLE_NAME = "overwrite_table";
@@ -116,6 +119,18 @@ public class TestOverwriteWithValidation extends TableTestBase {
       greaterThanOrEqual("id", 5L),
       lessThanOrEqual("id", 9L));
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestOverwriteWithValidation(int formatVersion) {
+    super(formatVersion);
+  }
+
   private static ByteBuffer longToBuffer(long value) {
     return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putLong(0, value);
   }
@@ -126,7 +141,7 @@ private static ByteBuffer longToBuffer(long value) {
   public void before() throws IOException {
     File tableDir = temp.newFolder();
     Assert.assertTrue(tableDir.delete());
-    this.table = TestTables.create(tableDir, TABLE_NAME, DATE_SCHEMA, PARTITION_SPEC);
+    this.table = TestTables.create(tableDir, TABLE_NAME, DATE_SCHEMA, PARTITION_SPEC, formatVersion);
   }
 
   @Test
diff --git a/core/src/test/java/org/apache/iceberg/TestPartitionSpecInfo.java b/core/src/test/java/org/apache/iceberg/TestPartitionSpecInfo.java
index 4c7d7c4f273..2d70bbe9081 100644
--- a/core/src/test/java/org/apache/iceberg/TestPartitionSpecInfo.java
+++ b/core/src/test/java/org/apache/iceberg/TestPartitionSpecInfo.java
@@ -29,9 +29,12 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestPartitionSpecInfo {
 
   @Rule
@@ -41,6 +44,20 @@ public class TestPartitionSpecInfo {
       required(2, "data", Types.StringType.get()));
   private File tableDir = null;
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  private final int formatVersion;
+
+  public TestPartitionSpecInfo(int formatVersion) {
+    this.formatVersion = formatVersion;
+  }
+
   @Before
   public void setupTableDir() throws IOException {
     this.tableDir = temp.newFolder();
@@ -54,7 +71,7 @@ public void cleanupTables() {
   @Test
   public void testSpecInfoUnpartitionedTable() {
     PartitionSpec spec = PartitionSpec.unpartitioned();
-    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec);
+    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
 
     Assert.assertEquals(spec, table.spec());
     Assert.assertEquals(spec.lastAssignedFieldId(), table.spec().lastAssignedFieldId());
@@ -65,7 +82,7 @@ public void testSpecInfoUnpartitionedTable() {
   @Test
   public void testSpecInfoPartitionedTable() {
     PartitionSpec spec = PartitionSpec.builderFor(schema).identity("data").build();
-    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec);
+    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
 
     Assert.assertEquals(spec, table.spec());
     Assert.assertEquals(spec.lastAssignedFieldId(), table.spec().lastAssignedFieldId());
@@ -78,7 +95,7 @@ public void testSpecInfoPartitionSpecEvolutionForV1Table() {
     PartitionSpec spec = PartitionSpec.builderFor(schema)
         .bucket("data", 4)
         .build();
-    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec);
+    TestTables.TestTable table = TestTables.create(tableDir, "test", schema, spec, formatVersion);
 
     Assert.assertEquals(spec, table.spec());
 
diff --git a/core/src/test/java/org/apache/iceberg/TestPartitionSpecParser.java b/core/src/test/java/org/apache/iceberg/TestPartitionSpecParser.java
index c31f89406b4..847ff4283ab 100644
--- a/core/src/test/java/org/apache/iceberg/TestPartitionSpecParser.java
+++ b/core/src/test/java/org/apache/iceberg/TestPartitionSpecParser.java
@@ -23,6 +23,10 @@
 import org.junit.Test;
 
 public class TestPartitionSpecParser extends TableTestBase {
+  public TestPartitionSpecParser() {
+    super(1);
+  }
+
   @Test
   public void testToJsonForV1Table() {
     String expected = "{\n" +
diff --git a/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java b/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java
index ed6b2509a0f..ae3b45f9bd8 100644
--- a/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java
+++ b/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java
@@ -30,8 +30,22 @@
 import org.apache.iceberg.ManifestEntry.Status;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestRemoveSnapshots extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestRemoveSnapshots(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testRetainLastWithExpireOlderThan() {
diff --git a/core/src/test/java/org/apache/iceberg/TestReplacePartitions.java b/core/src/test/java/org/apache/iceberg/TestReplacePartitions.java
index e677d384850..811cc7ee31f 100644
--- a/core/src/test/java/org/apache/iceberg/TestReplacePartitions.java
+++ b/core/src/test/java/org/apache/iceberg/TestReplacePartitions.java
@@ -25,7 +25,10 @@
 import org.apache.iceberg.exceptions.ValidationException;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestReplacePartitions extends TableTestBase {
 
   static final DataFile FILE_E = DataFiles.builder(SPEC)
@@ -49,6 +52,18 @@ public class TestReplacePartitions extends TableTestBase {
       .withRecordCount(0)
       .build();
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestReplacePartitions(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testReplaceOnePartition() {
     table.newFastAppend()
@@ -114,7 +129,7 @@ public void testReplaceWithUnpartitionedTable() throws IOException {
     Assert.assertTrue(tableDir.delete());
 
     Table unpartitioned = TestTables.create(
-        tableDir, "unpartitioned", SCHEMA, PartitionSpec.unpartitioned());
+        tableDir, "unpartitioned", SCHEMA, PartitionSpec.unpartitioned(), formatVersion);
 
     Assert.assertEquals("Table version should be 0",
         0, (long) TestTables.metadataVersion("unpartitioned"));
@@ -153,7 +168,7 @@ public void testReplaceAndMergeWithUnpartitionedTable() throws IOException {
     Assert.assertTrue(tableDir.delete());
 
     Table unpartitioned = TestTables.create(
-        tableDir, "unpartitioned", SCHEMA, PartitionSpec.unpartitioned());
+        tableDir, "unpartitioned", SCHEMA, PartitionSpec.unpartitioned(), formatVersion);
 
     // ensure the overwrite results in a merge
     unpartitioned.updateProperties().set(TableProperties.MANIFEST_MIN_MERGE_COUNT, "1").commit();
diff --git a/core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java b/core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java
index 68ef9b9291e..9a35dfb1e03 100644
--- a/core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java
+++ b/core/src/test/java/org/apache/iceberg/TestReplaceTransaction.java
@@ -29,11 +29,26 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.PartitionSpec.unpartitioned;
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestReplaceTransaction extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestReplaceTransaction(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testReplaceTransaction() {
     Schema newSchema = new Schema(
diff --git a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java
index 9c4258bf902..b4b64e7c358 100644
--- a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java
+++ b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java
@@ -25,13 +25,27 @@
 import org.apache.iceberg.exceptions.ValidationException;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 import org.mockito.internal.util.collections.Sets;
 
 import static org.apache.iceberg.ManifestEntry.Status.ADDED;
 import static org.apache.iceberg.ManifestEntry.Status.DELETED;
 import static org.apache.iceberg.ManifestEntry.Status.EXISTING;
 
+@RunWith(Parameterized.class)
 public class TestRewriteFiles extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestRewriteFiles(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testEmptyTable() {
diff --git a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java
index 1d85a2eb7af..c664bf50c26 100644
--- a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java
+++ b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java
@@ -32,13 +32,27 @@
 import org.apache.iceberg.expressions.Expressions;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.TableProperties.MANIFEST_MERGE_ENABLED;
 import static org.apache.iceberg.TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED;
 import static org.mockito.Mockito.spy;
 import static org.mockito.Mockito.when;
 
+@RunWith(Parameterized.class)
 public class TestRewriteManifests extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestRewriteManifests(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testRewriteManifestsAppendedDirectly() throws IOException {
@@ -941,9 +955,10 @@ public void testManifestReplacementCombinedWithRewriteConcurrentDelete() throws
 
     Assert.assertEquals(3, Iterables.size(table.snapshots()));
 
-    ManifestFile newManifest = writeManifest(
-        "manifest-file-1.avro",
-        manifestEntry(ManifestEntry.Status.EXISTING, firstSnapshot.snapshotId(), FILE_A));
+    ManifestEntry entry = manifestEntry(ManifestEntry.Status.EXISTING, firstSnapshot.snapshotId(), FILE_A);
+    // update the entry's sequence number or else it will be rejected by the writer
+    entry.setSequenceNumber(firstSnapshot.sequenceNumber());
+    ManifestFile newManifest = writeManifest("manifest-file-1.avro", entry);
 
     RewriteManifests rewriteManifests = table.rewriteManifests()
         .deleteManifest(firstSnapshotManifest)
@@ -990,9 +1005,11 @@ public void testInvalidUsage() throws IOException {
     Assert.assertEquals(1, manifests.size());
     ManifestFile manifest = manifests.get(0);
 
-    ManifestFile invalidAddedFileManifest = writeManifest(
-        "manifest-file-2.avro",
-        manifestEntry(ManifestEntry.Status.ADDED, snapshot.snapshotId(), FILE_A));
+    ManifestEntry appendEntry = manifestEntry(ManifestEntry.Status.ADDED, snapshot.snapshotId(), FILE_A);
+    // update the entry's sequence number or else it will be rejected by the writer
+    appendEntry.setSequenceNumber(snapshot.sequenceNumber());
+
+    ManifestFile invalidAddedFileManifest = writeManifest("manifest-file-2.avro", appendEntry);
 
     AssertHelpers.assertThrows("Should reject commit",
         IllegalArgumentException.class, "Cannot add manifest with added files",
@@ -1001,9 +1018,11 @@ public void testInvalidUsage() throws IOException {
             .addManifest(invalidAddedFileManifest)
             .commit());
 
-    ManifestFile invalidDeletedFileManifest = writeManifest(
-        "manifest-file-3.avro",
-        manifestEntry(ManifestEntry.Status.DELETED, snapshot.snapshotId(), FILE_A));
+    ManifestEntry deleteEntry = manifestEntry(ManifestEntry.Status.DELETED, snapshot.snapshotId(), FILE_A);
+    // update the entry's sequence number or else it will be rejected by the writer
+    deleteEntry.setSequenceNumber(snapshot.sequenceNumber());
+
+    ManifestFile invalidDeletedFileManifest = writeManifest("manifest-file-3.avro", deleteEntry);
 
     AssertHelpers.assertThrows("Should reject commit",
         IllegalArgumentException.class, "Cannot add manifest with deleted files",
diff --git a/core/src/test/java/org/apache/iceberg/TestScanSummary.java b/core/src/test/java/org/apache/iceberg/TestScanSummary.java
index 2110bef9373..77068f9e7a4 100644
--- a/core/src/test/java/org/apache/iceberg/TestScanSummary.java
+++ b/core/src/test/java/org/apache/iceberg/TestScanSummary.java
@@ -24,6 +24,8 @@
 import org.apache.iceberg.util.Pair;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.ScanSummary.timestampRange;
 import static org.apache.iceberg.ScanSummary.toMillis;
@@ -33,7 +35,19 @@
 import static org.apache.iceberg.expressions.Expressions.lessThan;
 import static org.apache.iceberg.expressions.Expressions.lessThanOrEqual;
 
+@RunWith(Parameterized.class)
 public class TestScanSummary extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestScanSummary(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testSnapshotTimeRangeValidation() {
diff --git a/core/src/test/java/org/apache/iceberg/TestScansAndSchemaEvolution.java b/core/src/test/java/org/apache/iceberg/TestScansAndSchemaEvolution.java
index 6add3a80336..274ee80df9a 100644
--- a/core/src/test/java/org/apache/iceberg/TestScansAndSchemaEvolution.java
+++ b/core/src/test/java/org/apache/iceberg/TestScansAndSchemaEvolution.java
@@ -35,9 +35,12 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestScansAndSchemaEvolution {
   private static final Schema SCHEMA = new Schema(
       required(1, "id", Types.LongType.get()),
@@ -48,6 +51,20 @@ public class TestScansAndSchemaEvolution {
       .identity("part")
       .build();
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public final int formatVersion;
+
+  public TestScansAndSchemaEvolution(int formatVersion) {
+    this.formatVersion = formatVersion;
+  }
+
   @Rule
   public TemporaryFolder temp = new TemporaryFolder();
 
@@ -85,7 +102,7 @@ public void testPartitionSourceRename() throws IOException {
     File dataLocation = new File(location, "data");
     Assert.assertTrue(location.delete()); // should be created by table create
 
-    Table table = TestTables.create(location, "test", SCHEMA, SPEC);
+    Table table = TestTables.create(location, "test", SCHEMA, SPEC, formatVersion);
 
     DataFile fileOne = createDataFile(dataLocation, "one");
     DataFile fileTwo = createDataFile(dataLocation, "two");
diff --git a/core/src/test/java/org/apache/iceberg/TestSchemaAndMappingUpdate.java b/core/src/test/java/org/apache/iceberg/TestSchemaAndMappingUpdate.java
index 349a797ce2f..07e5a26e0d1 100644
--- a/core/src/test/java/org/apache/iceberg/TestSchemaAndMappingUpdate.java
+++ b/core/src/test/java/org/apache/iceberg/TestSchemaAndMappingUpdate.java
@@ -32,8 +32,23 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestSchemaAndMappingUpdate extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestSchemaAndMappingUpdate(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testAddPrimitiveColumn() {
     NameMapping mapping = MappingUtil.create(table.schema());
diff --git a/core/src/test/java/org/apache/iceberg/TestSnapshot.java b/core/src/test/java/org/apache/iceberg/TestSnapshot.java
index 1374fbe6545..91b8716b9b5 100644
--- a/core/src/test/java/org/apache/iceberg/TestSnapshot.java
+++ b/core/src/test/java/org/apache/iceberg/TestSnapshot.java
@@ -20,8 +20,22 @@
 package org.apache.iceberg;
 
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestSnapshot extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestSnapshot(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testAppendFilesFromTable() {
diff --git a/core/src/test/java/org/apache/iceberg/TestSnapshotSelection.java b/core/src/test/java/org/apache/iceberg/TestSnapshotSelection.java
index 9da4c4b1eb9..17d54c2808c 100644
--- a/core/src/test/java/org/apache/iceberg/TestSnapshotSelection.java
+++ b/core/src/test/java/org/apache/iceberg/TestSnapshotSelection.java
@@ -25,8 +25,22 @@
 import java.nio.ByteOrder;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestSnapshotSelection extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestSnapshotSelection(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testSnapshotSelectionById() {
diff --git a/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java b/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java
index 0d788709ea6..e827def67da 100644
--- a/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java
+++ b/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java
@@ -34,9 +34,12 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.optional;
 
+@RunWith(Parameterized.class)
 public class TestSplitPlanning extends TableTestBase {
 
   private static final Configuration CONF = new Configuration();
@@ -50,6 +53,18 @@ public class TestSplitPlanning extends TableTestBase {
   public TemporaryFolder temp = new TemporaryFolder();
   private Table table = null;
 
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestSplitPlanning(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Before
   public void setupTable() throws IOException {
     File tableDir = temp.newFolder();
diff --git a/core/src/test/java/org/apache/iceberg/TestTables.java b/core/src/test/java/org/apache/iceberg/TestTables.java
index 41974e37ac2..e7897ce5eea 100644
--- a/core/src/test/java/org/apache/iceberg/TestTables.java
+++ b/core/src/test/java/org/apache/iceberg/TestTables.java
@@ -38,12 +38,20 @@ public class TestTables {
 
   private TestTables() {}
 
-  public static TestTable create(File temp, String name, Schema schema, PartitionSpec spec) {
+  private static TestTable upgrade(File temp, String name, int newFormatVersion) {
+    TestTable table = load(temp, name);
+    TableOperations ops = table.ops();
+    TableMetadata base = ops.current();
+    ops.commit(base, ops.current().upgradeToFormatVersion(newFormatVersion));
+    return table;
+  }
+
+  public static TestTable create(File temp, String name, Schema schema, PartitionSpec spec, int formatVersion) {
     TestTableOperations ops = new TestTableOperations(name, temp);
     if (ops.current() != null) {
       throw new AlreadyExistsException("Table %s already exists at location: %s", name, temp);
     }
-    ops.commit(null, TableMetadata.newTableMetadata(schema, spec, temp.toString(), ImmutableMap.of()));
+    ops.commit(null, TableMetadata.newTableMetadata(schema, spec, temp.toString(), ImmutableMap.of(), formatVersion));
     return new TestTable(ops, name);
   }
 
@@ -53,7 +61,8 @@ public static Transaction beginCreate(File temp, String name, Schema schema, Par
       throw new AlreadyExistsException("Table %s already exists at location: %s", name, temp);
     }
 
-    TableMetadata metadata = TableMetadata.newTableMetadata(schema, spec, temp.toString(), ImmutableMap.of());
+    TableMetadata metadata = TableMetadata.newTableMetadata(
+        schema, spec, temp.toString(), ImmutableMap.of(), 1);
 
     return Transactions.createTableTransaction(ops, metadata);
   }
diff --git a/core/src/test/java/org/apache/iceberg/TestTimestampPartitions.java b/core/src/test/java/org/apache/iceberg/TestTimestampPartitions.java
index fa6c98f5491..2343b7cdd8d 100644
--- a/core/src/test/java/org/apache/iceberg/TestTimestampPartitions.java
+++ b/core/src/test/java/org/apache/iceberg/TestTimestampPartitions.java
@@ -24,11 +24,25 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.optional;
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestTimestampPartitions extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestTimestampPartitions(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Test
   public void testPartitionAppend() throws IOException {
@@ -52,7 +66,7 @@ public void testPartitionAppend() throws IOException {
     File tableDir = temp.newFolder();
     Assert.assertTrue(tableDir.delete());
 
-    this.table = TestTables.create(tableDir, "test_date_partition", dateSchema, partitionSpec);
+    this.table = TestTables.create(tableDir, "test_date_partition", dateSchema, partitionSpec, formatVersion);
 
     table.newAppend()
         .appendFile(dataFile)
diff --git a/core/src/test/java/org/apache/iceberg/TestTransaction.java b/core/src/test/java/org/apache/iceberg/TestTransaction.java
index 8f0fedbf018..d855e16e2e4 100644
--- a/core/src/test/java/org/apache/iceberg/TestTransaction.java
+++ b/core/src/test/java/org/apache/iceberg/TestTransaction.java
@@ -31,8 +31,23 @@
 import org.apache.iceberg.io.OutputFile;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestTransaction extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestTransaction(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testEmptyTransaction() {
     Assert.assertEquals("Table should be on version 0", 0, (int) version());
diff --git a/core/src/test/java/org/apache/iceberg/TestWapWorkflow.java b/core/src/test/java/org/apache/iceberg/TestWapWorkflow.java
index 9f148609513..e4aceaf714c 100644
--- a/core/src/test/java/org/apache/iceberg/TestWapWorkflow.java
+++ b/core/src/test/java/org/apache/iceberg/TestWapWorkflow.java
@@ -27,8 +27,22 @@
 import org.junit.Assert;
 import org.junit.Before;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
+@RunWith(Parameterized.class)
 public class TestWapWorkflow extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestWapWorkflow(int formatVersion) {
+    super(formatVersion);
+  }
 
   @Before
   public void setupTableProperties() {
@@ -149,7 +163,6 @@ public void testSetCurrentSnapshotNoWAP() {
     table.newAppend()
         .appendFile(FILE_B)
         .commit();
-    base = readMetadata();
 
     // do setCurrentSnapshot
     table.manageSnapshots().setCurrentSnapshot(firstSnapshotId).commit();
@@ -278,8 +291,6 @@ public void testRollbackToTime() {
     table.newAppend()
         .appendFile(FILE_C)
         .commit();
-    base = readMetadata();
-    Snapshot thirdSnapshot = base.currentSnapshot();
 
     // rollback to before the second snapshot's time
     table.manageSnapshots().rollbackToTime(secondSnapshot.timestampMillis()).commit();
@@ -537,29 +548,18 @@ public void testWithCherryPickingWithCommitRetry() {
         .stageOnly()
         .commit();
 
-    // // second WAP commit
-    // table.newAppend()
-    //     .appendFile(FILE_C)
-    //     .set(SnapshotSummary.STAGED_WAP_ID_PROP, "987654321")
-    //     .stageOnly()
-    //     .commit();
     base = readMetadata();
 
     // pick the snapshot that's staged but not committed
     Snapshot wap1Snapshot = base.snapshots().get(1);
-    // Snapshot wap2Snapshot = base.snapshots().get(2);
 
-    Assert.assertEquals("Should have three snapshots", 2, base.snapshots().size());
+    Assert.assertEquals("Should have two snapshots", 2, base.snapshots().size());
     Assert.assertEquals("Should have first wap id in summary", "123456789",
         wap1Snapshot.summary().get("wap.id"));
-    // Assert.assertEquals("Should have second wap id in summary", "987654321",
-    //     wap2Snapshot.summary().get(SnapshotSummary.STAGED_WAP_ID_PROP));
     Assert.assertEquals("Current snapshot should be first commit's snapshot",
         firstSnapshotId, base.currentSnapshot().snapshotId());
     Assert.assertEquals("Parent snapshot id should be same for first WAP snapshot",
         firstSnapshotId, wap1Snapshot.parentId().longValue());
-    // Assert.assertEquals("Parent snapshot id should be same for second WAP snapshot",
-    //     firstSnapshotId, wap2Snapshot.parentId().longValue());
     Assert.assertEquals("Snapshot log should indicate number of snapshots committed", 1,
         base.snapshotLog().size());
 
diff --git a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
index 1e867206372..1cafc25eeae 100644
--- a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
+++ b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
@@ -19,6 +19,7 @@
 
 package org.apache.iceberg.hadoop;
 
+import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import java.io.IOException;
@@ -33,11 +34,14 @@
 import org.apache.iceberg.Transaction;
 import org.apache.iceberg.catalog.Namespace;
 import org.apache.iceberg.catalog.TableIdentifier;
-import org.apache.iceberg.exceptions.NotFoundException;
+import org.apache.iceberg.exceptions.NoSuchNamespaceException;
+import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.junit.Assert;
 import org.junit.Test;
 
 public class TestHadoopCatalog extends HadoopTableTestBase {
+  private static ImmutableMap<String, String> meta = ImmutableMap.of();
+
   @Test
   public void testBasicCatalog() throws Exception {
     Configuration conf = new Configuration();
@@ -129,8 +133,8 @@ public void testListTables() throws Exception {
     Assert.assertEquals(tbls2.size(), 1);
     Assert.assertTrue(tbls2.get(0).name().equals("tbl3"));
 
-    AssertHelpers.assertThrows("should throw exception", NotFoundException.class,
-        "Unknown namespace", () -> {
+    AssertHelpers.assertThrows("should throw exception", NoSuchNamespaceException.class,
+        "Namespace does not exist: ", () -> {
         catalog.listTables(Namespace.of("db", "ns1", "ns2"));
       });
   }
@@ -149,4 +153,162 @@ public void testCallingLocationProviderWhenNoCurrentMetadata() throws IOExceptio
     Assert.assertEquals("1 table expected", 1, catalog.listTables(Namespace.of("ns1", "ns2")).size());
     catalog.dropTable(tableIdent, true);
   }
+
+  @Test
+  public void testCreateNamespace() throws Exception {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+
+    TableIdentifier tbl1 = TableIdentifier.of("db", "ns1", "ns2", "metadata");
+    TableIdentifier tbl2 = TableIdentifier.of("db", "ns2", "ns3", "tbl2");
+
+    Lists.newArrayList(tbl1, tbl2).forEach(t ->
+        catalog.createNamespace(t.namespace(), meta)
+    );
+
+    String metaLocation1 = warehousePath + "/" + "db/ns1/ns2";
+    FileSystem fs1 = Util.getFs(new Path(metaLocation1), conf);
+    Assert.assertTrue(fs1.isDirectory(new Path(metaLocation1)));
+
+    String metaLocation2 = warehousePath + "/" + "db/ns2/ns3";
+    FileSystem fs2 = Util.getFs(new Path(metaLocation2), conf);
+    Assert.assertTrue(fs2.isDirectory(new Path(metaLocation2)));
+
+    AssertHelpers.assertThrows("Should fail to create when namespace already exist: " + tbl1.namespace(),
+        org.apache.iceberg.exceptions.AlreadyExistsException.class,
+        "Namespace '" + tbl1.namespace() + "' already exists!", () -> {
+          catalog.createNamespace(tbl1.namespace());
+        });
+  }
+
+  @Test
+  public void testListNamespace() throws Exception {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+
+    TableIdentifier tbl1 = TableIdentifier.of("db", "ns1", "ns2", "metadata");
+    TableIdentifier tbl2 = TableIdentifier.of("db", "ns2", "ns3", "tbl2");
+    TableIdentifier tbl3 = TableIdentifier.of("db", "ns3", "tbl4");
+    TableIdentifier tbl4 = TableIdentifier.of("db", "metadata");
+    TableIdentifier tbl5 = TableIdentifier.of("db2", "metadata");
+
+    Lists.newArrayList(tbl1, tbl2, tbl3, tbl4, tbl5).forEach(t ->
+        catalog.createTable(t, SCHEMA, PartitionSpec.unpartitioned())
+    );
+
+    List<Namespace> nsp1 = catalog.listNamespaces(Namespace.of("db"));
+    Set<String> tblSet = Sets.newHashSet(nsp1.stream().map(t -> t.toString()).iterator());
+    Assert.assertEquals(tblSet.size(), 3);
+    Assert.assertTrue(tblSet.contains("db.ns1"));
+    Assert.assertTrue(tblSet.contains("db.ns2"));
+    Assert.assertTrue(tblSet.contains("db.ns3"));
+
+    List<Namespace> nsp2 = catalog.listNamespaces(Namespace.of("db", "ns1"));
+    Assert.assertEquals(nsp2.size(), 1);
+    Assert.assertTrue(nsp2.get(0).toString().equals("db.ns1.ns2"));
+
+    List<Namespace> nsp3 = catalog.listNamespaces();
+    Set<String> tblSet2 = Sets.newHashSet(nsp3.stream().map(t -> t.toString()).iterator());
+    Assert.assertEquals(tblSet2.size(), 2);
+    Assert.assertTrue(tblSet2.contains("db"));
+    Assert.assertTrue(tblSet2.contains("db2"));
+
+    List<Namespace> nsp4 = catalog.listNamespaces();
+    Set<String> tblSet3 = Sets.newHashSet(nsp4.stream().map(t -> t.toString()).iterator());
+    Assert.assertEquals(tblSet3.size(), 2);
+    Assert.assertTrue(tblSet3.contains("db"));
+    Assert.assertTrue(tblSet3.contains("db2"));
+
+    AssertHelpers.assertThrows("Should fail to list namespace doesn't exist", NoSuchNamespaceException.class,
+        "Namespace does not exist: ", () -> {
+          catalog.listNamespaces(Namespace.of("db", "db2", "ns2"));
+        });
+  }
+
+  @Test
+  public void testLoadNamespaceMeta() throws IOException {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+
+    TableIdentifier tbl1 = TableIdentifier.of("db", "ns1", "ns2", "metadata");
+    TableIdentifier tbl2 = TableIdentifier.of("db", "ns2", "ns3", "tbl2");
+    TableIdentifier tbl3 = TableIdentifier.of("db", "ns3", "tbl4");
+    TableIdentifier tbl4 = TableIdentifier.of("db", "metadata");
+
+    Lists.newArrayList(tbl1, tbl2, tbl3, tbl4).forEach(t ->
+        catalog.createTable(t, SCHEMA, PartitionSpec.unpartitioned())
+    );
+    catalog.loadNamespaceMetadata(Namespace.of("db"));
+
+    AssertHelpers.assertThrows("Should fail to load namespace doesn't exist", NoSuchNamespaceException.class,
+        "Namespace does not exist: ", () -> {
+          catalog.loadNamespaceMetadata(Namespace.of("db", "db2", "ns2"));
+        });
+  }
+
+  @Test
+  public void testNamespaceExists() throws IOException {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+
+    TableIdentifier tbl1 = TableIdentifier.of("db", "ns1", "ns2", "metadata");
+    TableIdentifier tbl2 = TableIdentifier.of("db", "ns2", "ns3", "tbl2");
+    TableIdentifier tbl3 = TableIdentifier.of("db", "ns3", "tbl4");
+    TableIdentifier tbl4 = TableIdentifier.of("db", "metadata");
+
+    Lists.newArrayList(tbl1, tbl2, tbl3, tbl4).forEach(t ->
+        catalog.createTable(t, SCHEMA, PartitionSpec.unpartitioned())
+    );
+    Assert.assertTrue("Should true to namespace exist",
+        catalog.namespaceExists(Namespace.of("db", "ns1", "ns2")));
+    Assert.assertTrue("Should false to namespace doesn't exist",
+        !catalog.namespaceExists(Namespace.of("db", "db2", "ns2")));
+  }
+
+  @Test
+  public void testAlterNamespaceMeta() throws IOException {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+    AssertHelpers.assertThrows("Should fail to change namespace", UnsupportedOperationException.class,
+        "Cannot set namespace properties db.db2.ns2 : setProperties is not supported", () -> {
+          catalog.setProperties(Namespace.of("db", "db2", "ns2"), ImmutableMap.of("property", "test"));
+        });
+  }
+
+  @Test
+  public void testDropNamespace() throws IOException {
+    Configuration conf = new Configuration();
+    String warehousePath = temp.newFolder().getAbsolutePath();
+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);
+    Namespace namespace1 = Namespace.of("db");
+    Namespace namespace2 = Namespace.of("db", "ns1");
+
+    TableIdentifier tbl1 = TableIdentifier.of(namespace1, "tbl1");
+    TableIdentifier tbl2 = TableIdentifier.of(namespace2, "tbl1");
+
+    Lists.newArrayList(tbl1, tbl2).forEach(t ->
+        catalog.createTable(t, SCHEMA, PartitionSpec.unpartitioned())
+    );
+
+    AssertHelpers.assertThrows("Should fail to drop namespace is not empty " + namespace1,
+        RuntimeIOException.class,
+        "Namespace delete failed: " + namespace1, () -> {
+          catalog.dropNamespace(Namespace.of("db"));
+        });
+    Assert.assertFalse("Should fail to drop namespace doesn't exist",
+          catalog.dropNamespace(Namespace.of("db2")));
+    Assert.assertTrue(catalog.dropTable(tbl1));
+    Assert.assertTrue(catalog.dropTable(tbl2));
+    Assert.assertTrue(catalog.dropNamespace(namespace2));
+    Assert.assertTrue(catalog.dropNamespace(namespace1));
+    String metaLocation = warehousePath + "/" + "db";
+    FileSystem fs = Util.getFs(new Path(metaLocation), conf);
+    Assert.assertFalse(fs.isDirectory(new Path(metaLocation)));
+  }
 }
+
diff --git a/core/src/test/java/org/apache/iceberg/mapping/TestMappingUpdates.java b/core/src/test/java/org/apache/iceberg/mapping/TestMappingUpdates.java
index 8f23b644a87..ae30bc67f64 100644
--- a/core/src/test/java/org/apache/iceberg/mapping/TestMappingUpdates.java
+++ b/core/src/test/java/org/apache/iceberg/mapping/TestMappingUpdates.java
@@ -25,10 +25,25 @@
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
 
 import static org.apache.iceberg.types.Types.NestedField.required;
 
+@RunWith(Parameterized.class)
 public class TestMappingUpdates extends TableTestBase {
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { 1 },
+        new Object[] { 2 },
+    };
+  }
+
+  public TestMappingUpdates(int formatVersion) {
+    super(formatVersion);
+  }
+
   @Test
   public void testAddColumnMappingUpdate() {
     NameMapping mapping = MappingUtil.create(table.schema());
diff --git a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java
index 03f03be8969..2db582295b8 100644
--- a/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java
+++ b/data/src/main/java/org/apache/iceberg/data/orc/GenericOrcReader.java
@@ -101,7 +101,7 @@ default T convert(ColumnVector vector, int row) {
       if (!vector.noNulls && vector.isNull[rowIndex]) {
         return null;
       } else {
-        return convertNonNullValue(vector, row);
+        return convertNonNullValue(vector, rowIndex);
       }
     }
 
diff --git a/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java b/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java
index bc6767f7470..acb35ed29ca 100644
--- a/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java
+++ b/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java
@@ -30,7 +30,6 @@
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
 import java.time.temporal.ChronoUnit;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import org.apache.iceberg.Schema;
@@ -297,32 +296,6 @@ public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveTy
     MessageType type() {
       return type;
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    protected String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);
diff --git a/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java b/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java
index 922a9701fb9..d20db91af8c 100644
--- a/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java
+++ b/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java
@@ -27,7 +27,6 @@
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
 import java.time.temporal.ChronoUnit;
-import java.util.Iterator;
 import java.util.List;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.parquet.ParquetTypeVisitor;
@@ -172,32 +171,6 @@ public ParquetValueWriter<?> primitive(PrimitiveType primitive) {
           throw new UnsupportedOperationException("Unsupported type: " + primitive);
       }
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    private String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);
diff --git a/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java b/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java
index 670b455aa9c..058379b906f 100644
--- a/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java
+++ b/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.time.LocalDateTime;
 import java.time.OffsetDateTime;
+import java.util.Collections;
 import java.util.List;
 import java.util.TimeZone;
 import org.apache.iceberg.Files;
@@ -48,29 +49,18 @@ public class TestGenericData extends DataTest {
   protected void writeAndValidate(Schema schema) throws IOException {
     List<Record> expected = RandomGenericData.generate(schema, 100, 0L);
 
-    File testFile = temp.newFile();
-    Assert.assertTrue("Delete should succeed", testFile.delete());
-
-    try (FileAppender<Record> writer = ORC.write(Files.localOutput(testFile))
-        .schema(schema)
-        .createWriterFunc(GenericOrcWriter::buildWriter)
-        .build()) {
-      for (Record rec : expected) {
-        writer.add(rec);
-      }
-    }
+    writeAndValidateRecords(schema, expected);
+  }
 
-    List<Record> rows;
-    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(testFile))
-        .project(schema)
-        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))
-        .build()) {
-      rows = Lists.newArrayList(reader);
-    }
+  @Test
+  public void writeAndValidateRepeatingRecords() throws IOException {
+    Schema structSchema = new Schema(
+        required(100, "id", Types.LongType.get()),
+        required(101, "data", Types.StringType.get())
+    );
+    List<Record> expectedRepeating = Collections.nCopies(100, RandomGenericData.generate(structSchema, 1, 0L).get(0));
 
-    for (int i = 0; i < expected.size(); i += 1) {
-      DataTestHelpers.assertEquals(schema.asStruct(), expected.get(i), rows.get(i));
-    }
+    writeAndValidateRecords(structSchema, expectedRepeating);
   }
 
   @Test
@@ -127,4 +117,30 @@ public void writeAndValidateTimestamps() throws IOException {
     Assert.assertEquals(OffsetDateTime.parse("1935-05-17T01:10:34Z"), rows.get(3).getField("tsTzCol"));
     Assert.assertEquals(LocalDateTime.parse("1935-05-01T00:01:00"), rows.get(3).getField("tsCol"));
   }
+
+  private void writeAndValidateRecords(Schema schema, List<Record> expected) throws IOException {
+    File testFile = temp.newFile();
+    Assert.assertTrue("Delete should succeed", testFile.delete());
+
+    try (FileAppender<Record> writer = ORC.write(Files.localOutput(testFile))
+        .schema(schema)
+        .createWriterFunc(GenericOrcWriter::buildWriter)
+        .build()) {
+      for (Record rec : expected) {
+        writer.add(rec);
+      }
+    }
+
+    List<Record> rows;
+    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(testFile))
+        .project(schema)
+        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))
+        .build()) {
+      rows = Lists.newArrayList(reader);
+    }
+
+    for (int i = 0; i < expected.size(); i += 1) {
+      DataTestHelpers.assertEquals(schema.asStruct(), expected.get(i), rows.get(i));
+    }
+  }
 }
diff --git a/dev/.rat-excludes b/dev/.rat-excludes
index ad33fe2a858..898ff81a556 100644
--- a/dev/.rat-excludes
+++ b/dev/.rat-excludes
@@ -1,6 +1,8 @@
 version.txt
 versions.lock
 versions.props
+books.json
+new-books.json
 build
 .gitignore
 .rat-excludes
diff --git a/dev/source-release.sh b/dev/source-release.sh
index ceca5b14ae8..b0f09060cdb 100755
--- a/dev/source-release.sh
+++ b/dev/source-release.sh
@@ -64,7 +64,7 @@ tarball=$tag.tar.gz
 
 # be conservative and use the release hash, even though git produces the same
 # archive (identical hashes) using the scm tag
-git archive $release_hash --prefix $tag/ -o $tarball .baseline api common core data dev gradle gradlew hive orc parquet pig project runtime spark LICENSE NOTICE DISCLAIMER README.md build.gradle gradle.properties settings.gradle versions.lock versions.props version.txt
+git archive $release_hash --prefix $tag/ -o $tarball .baseline api arrow common core data dev gradle gradlew hive mr orc parquet pig project spark spark-runtime LICENSE NOTICE DISCLAIMER README.md build.gradle baseline.gradle deploy.gradle tasks.gradle jmh.gradle gradle.properties settings.gradle versions.lock versions.props version.txt
 
 # sign the archive
 gpg --armor --output ${tarball}.asc --detach-sig $tarball
diff --git a/hive/src/main/java/org/apache/iceberg/hive/HiveCatalog.java b/hive/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
index d00a1519c29..4973b063f74 100644
--- a/hive/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
+++ b/hive/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
@@ -21,12 +21,20 @@
 
 import com.google.common.base.Joiner;
 import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Maps;
 import java.io.Closeable;
 import java.util.Arrays;
 import java.util.List;
+import java.util.Map;
+import java.util.Set;
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
 import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.InvalidOperationException;
 import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;
 import org.apache.hadoop.hive.metastore.api.Table;
 import org.apache.hadoop.hive.metastore.api.UnknownDBException;
@@ -34,14 +42,16 @@
 import org.apache.iceberg.TableMetadata;
 import org.apache.iceberg.TableOperations;
 import org.apache.iceberg.catalog.Namespace;
+import org.apache.iceberg.catalog.SupportsNamespaces;
 import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;
+import org.apache.iceberg.exceptions.NoSuchNamespaceException;
 import org.apache.iceberg.exceptions.NoSuchTableException;
-import org.apache.iceberg.exceptions.NotFoundException;
 import org.apache.thrift.TException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-public class HiveCatalog extends BaseMetastoreCatalog implements Closeable {
+public class HiveCatalog extends BaseMetastoreCatalog implements Closeable, SupportsNamespaces {
   private static final Logger LOG = LoggerFactory.getLogger(HiveCatalog.class);
 
   private final HiveClientPool clients;
@@ -58,7 +68,7 @@ public HiveCatalog(Configuration conf) {
 
   @Override
   public List<TableIdentifier> listTables(Namespace namespace) {
-    Preconditions.checkArgument(namespace.levels().length == 1,
+    Preconditions.checkArgument(isValidateNamespace(namespace),
         "Missing database in namespace: %s", namespace);
     String database = namespace.level(0);
 
@@ -69,10 +79,10 @@ public List<TableIdentifier> listTables(Namespace namespace) {
           .collect(Collectors.toList());
 
     } catch (UnknownDBException e) {
-      throw new NotFoundException(e, "Unknown namespace " + namespace.toString());
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
 
     } catch (TException e) {
-      throw new RuntimeException("Failed to list all tables under namespace " + namespace.toString(), e);
+      throw new RuntimeException("Failed to list all tables under namespace " + namespace, e);
 
     } catch (InterruptedException e) {
       Thread.currentThread().interrupt();
@@ -119,7 +129,7 @@ public boolean dropTable(TableIdentifier identifier, boolean purge) {
       return false;
 
     } catch (TException e) {
-      throw new RuntimeException("Failed to drop " + identifier.toString(), e);
+      throw new RuntimeException("Failed to drop " + identifier, e);
 
     } catch (InterruptedException e) {
       Thread.currentThread().interrupt();
@@ -155,7 +165,7 @@ public void renameTable(TableIdentifier from, TableIdentifier to) {
       throw new org.apache.iceberg.exceptions.AlreadyExistsException("Table already exists: %s", to);
 
     } catch (TException e) {
-      throw new RuntimeException("Failed to rename " + from.toString() + " to " + to.toString(), e);
+      throw new RuntimeException("Failed to rename " + from + " to " + to, e);
 
     } catch (InterruptedException e) {
       Thread.currentThread().interrupt();
@@ -163,11 +173,168 @@ public void renameTable(TableIdentifier from, TableIdentifier to) {
     }
   }
 
+  @Override
+  public void createNamespace(Namespace namespace, Map<String, String> meta) {
+    Preconditions.checkArgument(
+        !namespace.isEmpty(),
+        "Cannot create namespace with invalid name: %s", namespace);
+    Preconditions.checkArgument(isValidateNamespace(namespace),
+        "Cannot support multi part namespace in Hive MetaStore: %s", namespace);
+
+    try {
+      clients.run(client -> {
+        client.createDatabase(convertToDatabase(namespace, meta));
+        return null;
+      });
+
+    } catch (AlreadyExistsException e) {
+      throw new org.apache.iceberg.exceptions.AlreadyExistsException(e, "Namespace '%s' already exists!",
+            namespace);
+
+    } catch (TException e) {
+      throw new RuntimeException("Failed to create namespace " + namespace + " in Hive MataStore", e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException(
+          "Interrupted in call to createDatabase(name) " + namespace + " in Hive MataStore", e);
+    }
+  }
+
+  @Override
+  public List<Namespace> listNamespaces(Namespace namespace) {
+    if (!isValidateNamespace(namespace) && !namespace.isEmpty()) {
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
+    }
+    if (!namespace.isEmpty()) {
+      return ImmutableList.of();
+    }
+    try {
+      return clients.run(
+          HiveMetaStoreClient::getAllDatabases)
+          .stream()
+          .map(Namespace::of)
+          .collect(Collectors.toList());
+
+    } catch (TException e) {
+      throw new RuntimeException("Failed to list all namespace: " + namespace + " in Hive MataStore",  e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException(
+          "Interrupted in call to getAllDatabases() " + namespace + " in Hive MataStore", e);
+    }
+  }
+
+  @Override
+  public boolean dropNamespace(Namespace namespace) {
+    if (!isValidateNamespace(namespace)) {
+      return false;
+    }
+
+    try {
+      clients.run(client -> {
+        client.dropDatabase(namespace.level(0),
+            false /* deleteData */,
+            false /* ignoreUnknownDb */,
+            false /* cascade */);
+        return null;
+      });
+
+      return true;
+
+    } catch (InvalidOperationException e) {
+      throw new NamespaceNotEmptyException("Namespace " + namespace + " is not empty. One or more tables exist.", e);
+
+    } catch (NoSuchObjectException e) {
+      return false;
+
+    } catch (TException e) {
+      throw new RuntimeException("Failed to drop namespace " + namespace + " in Hive MataStore", e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException(
+          "Interrupted in call to drop dropDatabase(name) " + namespace + " in Hive MataStore", e);
+    }
+  }
+
+  @Override
+  public boolean setProperties(Namespace namespace,  Map<String, String> properties) {
+    Map<String, String> parameter = Maps.newHashMap();
+
+    parameter.putAll(loadNamespaceMetadata(namespace));
+    parameter.putAll(properties);
+    Database database = convertToDatabase(namespace, parameter);
+
+    return alterHiveDataBase(namespace, database);
+  }
+
+  @Override
+  public boolean removeProperties(Namespace namespace,  Set<String> properties) {
+    Map<String, String> parameter = Maps.newHashMap();
+
+    parameter.putAll(loadNamespaceMetadata(namespace));
+    properties.forEach(key -> parameter.put(key, null));
+    Database database = convertToDatabase(namespace, parameter);
+
+    return alterHiveDataBase(namespace, database);
+  }
+
+  private boolean alterHiveDataBase(Namespace namespace,  Database database) {
+    try {
+      clients.run(client -> {
+        client.alterDatabase(namespace.level(0), database);
+        return null;
+      });
+
+      return true;
+
+    } catch (NoSuchObjectException | UnknownDBException e) {
+      throw new NoSuchNamespaceException(e, "Namespace does not exist: %s", namespace);
+
+    } catch (TException e) {
+      throw new RuntimeException(
+          "Failed to list namespace under namespace: " + namespace + " in Hive MataStore", e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException("Interrupted in call to getDatabase(name) " + namespace + " in Hive MataStore", e);
+    }
+  }
+
+  @Override
+  public Map<String, String> loadNamespaceMetadata(Namespace namespace) {
+    if (!isValidateNamespace(namespace)) {
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
+    }
+
+    try {
+      Database database = clients.run(client -> client.getDatabase(namespace.level(0)));
+      return convertToMetadata(database);
+
+    } catch (NoSuchObjectException | UnknownDBException e) {
+      throw new NoSuchNamespaceException(e, "Namespace does not exist: %s", namespace);
+
+    } catch (TException e) {
+      throw new RuntimeException("Failed to list namespace under namespace: " + namespace + " in Hive MataStore", e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException(
+          "Interrupted in call to getDatabase(name) " + namespace + " in Hive MataStore", e);
+    }
+  }
+
   @Override
   protected boolean isValidIdentifier(TableIdentifier tableIdentifier) {
     return tableIdentifier.namespace().levels().length == 1;
   }
 
+  private boolean isValidateNamespace(Namespace namespace) {
+    return namespace.levels().length == 1;
+  }
+
   @Override
   public TableOperations newTableOps(TableIdentifier tableIdentifier) {
     String dbName = tableIdentifier.namespace().level(0);
@@ -188,6 +355,46 @@ protected String defaultWarehouseLocation(TableIdentifier tableIdentifier) {
         tableIdentifier.name());
   }
 
+  private Map<String, String> convertToMetadata(Database database) {
+
+    Map<String, String> meta = Maps.newHashMap();
+
+    meta.putAll(database.getParameters());
+    meta.put("location", database.getLocationUri());
+    meta.put("comment", database.getDescription());
+
+    return meta;
+  }
+
+  Database convertToDatabase(Namespace namespace, Map<String, String> meta) {
+    String warehouseLocation = conf.get("hive.metastore.warehouse.dir");
+
+    if (!isValidateNamespace(namespace)) {
+      throw new NoSuchNamespaceException("Namespace does not exist: %s", namespace);
+    }
+
+    Database database  = new Database();
+    Map<String, String> parameter = Maps.newHashMap();
+
+    database.setName(namespace.level(0));
+    database.setLocationUri(new Path(warehouseLocation, namespace.level(0)).toString() + ".db");
+
+    meta.forEach((key, value) -> {
+      if (key.equals("comment")) {
+        database.setDescription(value);
+      } else if (key.equals("location")) {
+        database.setLocationUri(value);
+      } else {
+        if (value != null) {
+          parameter.put(key, value);
+        }
+      }
+    });
+    database.setParameters(parameter);
+
+    return database;
+  }
+
   @Override
   public void close() {
     if (!closed) {
diff --git a/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
index 0823184117c..e6d33b9a1d9 100644
--- a/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
+++ b/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
@@ -296,13 +296,18 @@ private long acquireLock() throws UnknownHostException, TException, InterruptedE
   private void unlock(Optional<Long> lockId) {
     if (lockId.isPresent()) {
       try {
-        metaClients.run(client -> {
-          client.unlock(lockId.get());
-          return null;
-        });
+        doUnlock(lockId.get());
       } catch (Exception e) {
-        throw new RuntimeException(String.format("Failed to unlock %s.%s", database, tableName), e);
+        LOG.warn("Failed to unlock {}.{}", database, tableName, e);
       }
     }
   }
+
+  // visible for testing
+  protected void doUnlock(long lockId) throws TException, InterruptedException {
+    metaClients.run(client -> {
+      client.unlock(lockId);
+      return null;
+    });
+  }
 }
diff --git a/hive/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java b/hive/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java
new file mode 100644
index 00000000000..744295c1a1d
--- /dev/null
+++ b/hive/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java
@@ -0,0 +1,192 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.hive;
+
+import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.ImmutableSet;
+import java.util.List;
+import java.util.Map;
+import java.util.UUID;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.iceberg.AssertHelpers;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.catalog.Namespace;
+import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.exceptions.AlreadyExistsException;
+import org.apache.iceberg.exceptions.NamespaceNotEmptyException;
+import org.apache.iceberg.exceptions.NoSuchNamespaceException;
+import org.apache.iceberg.types.Types;
+import org.apache.thrift.TException;
+import org.junit.Assert;
+import org.junit.Test;
+
+import static org.apache.iceberg.types.Types.NestedField.required;
+
+public class TestHiveCatalog extends HiveMetastoreTest {
+  private static final String hiveLocalDir = "file:/tmp/hive/" + UUID.randomUUID().toString();
+  private static ImmutableMap meta = ImmutableMap.of(
+      "owner", "apache",
+      "group", "iceberg",
+      "comment", "iceberg  hiveCatalog test");
+
+  @Test
+  public void testCreateNamespace() throws TException {
+    Namespace namespace1 = Namespace.of("noLocation");
+    catalog.createNamespace(namespace1, meta);
+    Database database1 = metastoreClient.getDatabase(namespace1.toString());
+
+    Assert.assertTrue(database1.getParameters().get("owner").equals("apache"));
+    Assert.assertTrue(database1.getParameters().get("group").equals("iceberg"));
+
+    Assert.assertEquals("There no same location for db and namespace",
+        database1.getLocationUri(), defaultUri(namespace1));
+
+    AssertHelpers.assertThrows("Should fail to create when namespace already exist " + namespace1,
+        AlreadyExistsException.class, "Namespace '" + namespace1 + "' already exists!", () -> {
+          catalog.createNamespace(namespace1);
+        });
+    ImmutableMap newMeta = ImmutableMap.<String, String>builder()
+        .putAll(meta)
+        .put("location", hiveLocalDir)
+        .build();
+    Namespace namespace2 = Namespace.of("haveLocation");
+
+    catalog.createNamespace(namespace2, newMeta);
+    Database database2 = metastoreClient.getDatabase(namespace2.toString());
+    Assert.assertEquals("There no same location for db and namespace",
+        database2.getLocationUri(), hiveLocalDir);
+  }
+
+  @Test
+  public void testListNamespace() throws TException {
+    List<Namespace> namespaces;
+    Namespace namespace1 = Namespace.of("dbname1");
+    catalog.createNamespace(namespace1, meta);
+    namespaces = catalog.listNamespaces(namespace1);
+    Assert.assertTrue("Hive db not hive the namespace 'dbname1'", namespaces.isEmpty());
+
+    Namespace namespace2 = Namespace.of("dbname2");
+    catalog.createNamespace(namespace2, meta);
+    namespaces = catalog.listNamespaces();
+
+    Assert.assertTrue("Hive db not hive the namespace 'dbname2'", namespaces.contains(namespace2));
+  }
+
+  @Test
+  public void testLoadNamespaceMeta() throws TException {
+    Namespace namespace = Namespace.of("dbname_load");
+
+    catalog.createNamespace(namespace, meta);
+
+    Map<String, String> nameMata = catalog.loadNamespaceMetadata(namespace);
+    Assert.assertTrue(nameMata.get("owner").equals("apache"));
+    Assert.assertTrue(nameMata.get("group").equals("iceberg"));
+    Assert.assertEquals("There no same location for db and namespace",
+        nameMata.get("location"), catalog.convertToDatabase(namespace, meta).getLocationUri());
+  }
+
+  @Test
+  public void testNamespaceExists() throws TException {
+    Namespace namespace = Namespace.of("dbname_exists");
+
+    catalog.createNamespace(namespace, meta);
+
+    Assert.assertTrue("Should true to namespace exist",
+        catalog.namespaceExists(namespace));
+    Assert.assertTrue("Should false to namespace doesn't exist",
+        !catalog.namespaceExists(Namespace.of("db2", "db2", "ns2")));
+  }
+
+  @Test
+  public void testSetNamespaceProperties() throws TException {
+    Namespace namespace = Namespace.of("dbname_set");
+
+    catalog.createNamespace(namespace, meta);
+    catalog.setProperties(namespace,
+        ImmutableMap.of(
+            "owner", "alter_apache",
+            "test", "test",
+            "location", "file:/data/tmp",
+            "comment", "iceberg test")
+    );
+
+    Database database = metastoreClient.getDatabase(namespace.level(0));
+    Assert.assertEquals(database.getParameters().get("owner"), "alter_apache");
+    Assert.assertEquals(database.getParameters().get("test"), "test");
+    Assert.assertEquals(database.getParameters().get("group"), "iceberg");
+    AssertHelpers.assertThrows("Should fail to namespace not exist" + namespace,
+        NoSuchNamespaceException.class, "Namespace does not exist: ", () -> {
+          catalog.setProperties(Namespace.of("db2", "db2", "ns2"), meta);
+        });
+  }
+
+  @Test
+  public void testRemoveNamespaceProperties() throws TException {
+    Namespace namespace = Namespace.of("dbname_remove");
+
+    catalog.createNamespace(namespace, meta);
+
+    catalog.removeProperties(namespace, ImmutableSet.of("comment", "owner"));
+
+    Database database = metastoreClient.getDatabase(namespace.level(0));
+
+    Assert.assertEquals(database.getParameters().get("owner"), null);
+    Assert.assertEquals(database.getParameters().get("group"), "iceberg");
+    AssertHelpers.assertThrows("Should fail to namespace not exist" + namespace,
+        NoSuchNamespaceException.class, "Namespace does not exist: ", () -> {
+          catalog.removeProperties(Namespace.of("db2", "db2", "ns2"), ImmutableSet.of("comment", "owner"));
+        });
+  }
+
+  @Test
+  public void testDropNamespace() throws TException {
+    Namespace namespace = Namespace.of("dbname_drop");
+    TableIdentifier identifier = TableIdentifier.of(namespace, "table");
+    Schema schema = new Schema(Types.StructType.of(
+        required(1, "id", Types.LongType.get())).fields());
+
+    catalog.createNamespace(namespace, meta);
+    catalog.createTable(identifier, schema);
+    Map<String, String> nameMata = catalog.loadNamespaceMetadata(namespace);
+    Assert.assertTrue(nameMata.get("owner").equals("apache"));
+    Assert.assertTrue(nameMata.get("group").equals("iceberg"));
+
+    AssertHelpers.assertThrows("Should fail to drop namespace is not empty" + namespace,
+        NamespaceNotEmptyException.class,
+        "Namespace dbname_drop is not empty. One or more tables exist.", () -> {
+          catalog.dropNamespace(namespace);
+        });
+    Assert.assertTrue(catalog.dropTable(identifier, true));
+    Assert.assertTrue("Should fail to drop namespace if it is not empty",
+        catalog.dropNamespace(namespace));
+    Assert.assertFalse("Should fail to drop when namespace doesn't exist",
+        catalog.dropNamespace(Namespace.of("db.ns1")));
+    AssertHelpers.assertThrows("Should fail to drop namespace exist" + namespace,
+        NoSuchNamespaceException.class, "Namespace does not exist: ", () -> {
+          catalog.loadNamespaceMetadata(namespace);
+        });
+  }
+
+  private String defaultUri(Namespace namespace) throws TException {
+    return metastoreClient.getConfigValue(
+        "hive.metastore.warehouse.dir", "") +  "/" + namespace.level(0) + ".db";
+  }
+
+}
diff --git a/hive/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java b/hive/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java
new file mode 100644
index 00000000000..81ac5043ae4
--- /dev/null
+++ b/hive/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.hive;
+
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableMetadata;
+import org.apache.iceberg.types.Types;
+import org.apache.thrift.TException;
+import org.junit.Assert;
+import org.junit.Test;
+import org.mockito.ArgumentCaptor;
+
+import static org.mockito.Mockito.doThrow;
+import static org.mockito.Mockito.spy;
+
+public class TestHiveCommits extends HiveTableBaseTest {
+
+  @Test
+  public void testSuppressUnlockExceptions() throws TException, InterruptedException {
+    Table table = catalog.loadTable(TABLE_IDENTIFIER);
+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();
+
+    TableMetadata metadataV1 = ops.current();
+
+    table.updateSchema()
+        .addColumn("n", Types.IntegerType.get())
+        .commit();
+
+    ops.refresh();
+
+    TableMetadata metadataV2 = ops.current();
+
+    Assert.assertEquals(2, ops.current().schema().columns().size());
+
+    HiveTableOperations spyOps = spy(ops);
+
+    ArgumentCaptor<Long> lockId = ArgumentCaptor.forClass(Long.class);
+    doThrow(new RuntimeException()).when(spyOps).doUnlock(lockId.capture());
+
+    try {
+      spyOps.commit(metadataV2, metadataV1);
+    } finally {
+      ops.doUnlock(lockId.getValue());
+    }
+
+    ops.refresh();
+
+    // the commit must succeed
+    Assert.assertEquals(1, ops.current().schema().columns().size());
+  }
+}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java b/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java
new file mode 100644
index 00000000000..8ddb543f449
--- /dev/null
+++ b/mr/src/main/java/org/apache/iceberg/mr/IcebergRecordReader.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.FileScanTask;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.avro.Avro;
+import org.apache.iceberg.data.avro.DataReader;
+import org.apache.iceberg.data.orc.GenericOrcReader;
+import org.apache.iceberg.data.parquet.GenericParquetReaders;
+import org.apache.iceberg.expressions.Evaluator;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.expressions.Expressions;
+import org.apache.iceberg.hadoop.HadoopInputFile;
+import org.apache.iceberg.io.CloseableIterable;
+import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.orc.ORC;
+import org.apache.iceberg.parquet.Parquet;
+
+public class IcebergRecordReader {
+
+  private boolean applyResidual;
+  private boolean caseSensitive;
+  private boolean reuseContainers;
+
+  private void initialize(Configuration conf) {
+    this.applyResidual = !conf.getBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, false);
+    this.caseSensitive = conf.getBoolean(InputFormatConfig.CASE_SENSITIVE, true);
+    this.reuseContainers = conf.getBoolean(InputFormatConfig.REUSE_CONTAINERS, false);
+  }
+
+  public CloseableIterable createReader(Configuration config, FileScanTask currentTask, Schema readSchema) {
+    initialize(config);
+    DataFile file = currentTask.file();
+    // TODO we should make use of FileIO to create inputFile
+    InputFile inputFile = HadoopInputFile.fromLocation(file.path(), config);
+    switch (file.format()) {
+      case AVRO:
+        return newAvroIterable(inputFile, currentTask, readSchema);
+      case ORC:
+        return newOrcIterable(inputFile, currentTask, readSchema);
+      case PARQUET:
+        return newParquetIterable(inputFile, currentTask, readSchema);
+      default:
+        throw new UnsupportedOperationException(
+            String.format("Cannot read %s file: %s", file.format().name(), file.path()));
+    }
+  }
+
+  private CloseableIterable newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile).project(readSchema).split(task.start(), task.length());
+    if (reuseContainers) {
+      avroReadBuilder.reuseContainers();
+    }
+    avroReadBuilder.createReaderFunc(DataReader::create);
+    return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    Parquet.ReadBuilder parquetReadBuilder = Parquet
+        .read(inputFile)
+        .project(readSchema)
+        .filter(task.residual())
+        .caseSensitive(caseSensitive)
+        .split(task.start(), task.length());
+    if (reuseContainers) {
+      parquetReadBuilder.reuseContainers();
+    }
+
+    parquetReadBuilder.createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));
+
+    return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
+    ORC.ReadBuilder orcReadBuilder = ORC
+        .read(inputFile)
+        .project(readSchema)
+        .caseSensitive(caseSensitive)
+        .split(task.start(), task.length());
+    // ORC does not support reuse containers yet
+    orcReadBuilder.createReaderFunc(fileSchema -> GenericOrcReader.buildReader(readSchema, fileSchema));
+    return applyResidualFiltering(orcReadBuilder.build(), task.residual(), readSchema);
+  }
+
+  private CloseableIterable applyResidualFiltering(CloseableIterable iter, Expression residual, Schema readSchema) {
+    if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
+      Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
+      return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
+    } else {
+      return iter;
+    }
+  }
+
+}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java b/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
new file mode 100644
index 00000000000..bbfecd0e6b2
--- /dev/null
+++ b/mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java
@@ -0,0 +1,155 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import com.google.common.base.Preconditions;
+import java.util.function.Function;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.SchemaParser;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.catalog.Catalog;
+import org.apache.iceberg.catalog.TableIdentifier;
+import org.apache.iceberg.common.DynConstructors;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.hadoop.HadoopTables;
+
+public class InputFormatConfig {
+
+  private InputFormatConfig() {}
+
+  public static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
+  public static final String CASE_SENSITIVE = "iceberg.mr.case.sensitive";
+  public static final String SKIP_RESIDUAL_FILTERING = "skip.residual.filtering";
+  public static final String AS_OF_TIMESTAMP = "iceberg.mr.as.of.time";
+  public static final String FILTER_EXPRESSION = "iceberg.mr.filter.expression";
+  public static final String IN_MEMORY_DATA_MODEL = "iceberg.mr.in.memory.data.model";
+  public static final String READ_SCHEMA = "iceberg.mr.read.schema";
+  public static final String SNAPSHOT_ID = "iceberg.mr.snapshot.id";
+  public static final String SPLIT_SIZE = "iceberg.mr.split.size";
+  public static final String TABLE_PATH = "iceberg.mr.table.path";
+  public static final String TABLE_SCHEMA = "iceberg.mr.table.schema";
+  public static final String LOCALITY = "iceberg.mr.locality";
+  public static final String CATALOG = "iceberg.mr.catalog";
+
+  public static class ConfigBuilder {
+    private final Configuration conf;
+
+    public ConfigBuilder(Configuration conf) {
+      this.conf = conf;
+      // defaults
+      conf.setBoolean(SKIP_RESIDUAL_FILTERING, false);
+      conf.setBoolean(CASE_SENSITIVE, true);
+      conf.setBoolean(REUSE_CONTAINERS, false);
+      conf.setBoolean(LOCALITY, false);
+    }
+
+    public ConfigBuilder readFrom(String path) {
+      conf.set(TABLE_PATH, path);
+      Table table = findTable(conf);
+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));
+      return this;
+    }
+
+    public ConfigBuilder filter(Expression expression) {
+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));
+      return this;
+    }
+
+    public ConfigBuilder project(Schema schema) {
+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));
+      return this;
+    }
+
+    public ConfigBuilder reuseContainers(boolean reuse) {
+      conf.setBoolean(InputFormatConfig.REUSE_CONTAINERS, reuse);
+      return this;
+    }
+
+    public ConfigBuilder caseSensitive(boolean caseSensitive) {
+      conf.setBoolean(InputFormatConfig.CASE_SENSITIVE, caseSensitive);
+      return this;
+    }
+
+    public ConfigBuilder snapshotId(long snapshotId) {
+      conf.setLong(SNAPSHOT_ID, snapshotId);
+      return this;
+    }
+
+    public ConfigBuilder asOfTime(long asOfTime) {
+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);
+      return this;
+    }
+
+    public ConfigBuilder splitSize(long splitSize) {
+      conf.setLong(SPLIT_SIZE, splitSize);
+      return this;
+    }
+
+    /**
+     * If this API is called. The input splits
+     * constructed will have host location information
+     */
+    public ConfigBuilder preferLocality() {
+      conf.setBoolean(LOCALITY, true);
+      return this;
+    }
+
+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {
+      conf.setClass(CATALOG, catalogFuncClass, Function.class);
+      return this;
+    }
+
+    /**
+     * Compute platforms pass down filters to data sources. If the data source cannot apply some filters, or only
+     * partially applies the filter, it will return the residual filter back. If the platform can correctly apply
+     * the residual filters, then it should call this api. Otherwise the current api will throw an exception if the
+     * passed in filter is not completely satisfied.
+     */
+    public ConfigBuilder skipResidualFiltering() {
+      conf.setBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, true);
+      return this;
+    }
+  }
+
+  public static Table findTable(Configuration conf) {
+    String path = conf.get(InputFormatConfig.TABLE_PATH);
+    Preconditions.checkArgument(path != null, "Table path should not be null");
+    if (path.contains("/")) {
+      HadoopTables tables = new HadoopTables(conf);
+      return tables.load(path);
+    }
+
+    String catalogFuncClass = conf.get(InputFormatConfig.CATALOG);
+    if (catalogFuncClass != null) {
+      Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)
+          DynConstructors.builder(Function.class)
+                         .impl(catalogFuncClass)
+                         .build()
+                         .newInstance();
+      Catalog catalog = catalogFunc.apply(conf);
+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);
+      return catalog.loadTable(tableIdentifier);
+    } else {
+      throw new IllegalArgumentException("No custom catalog specified to load table " + path);
+    }
+  }
+
+}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
index 82ca2bc9bfe..32900b3888c 100644
--- a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
+++ b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergInputFormat.java
@@ -38,15 +38,12 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.iceberg.CombinedScanTask;
-import org.apache.iceberg.DataFile;
 import org.apache.iceberg.FileScanTask;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.hadoop.HadoopInputFile;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.mr.SerializationUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -59,7 +56,7 @@
 public class IcebergInputFormat<T> implements InputFormat<Void, T>, CombineHiveInputFormat.AvoidSplitCombination {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);
 
-  static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
+  static final String TABLE_LOCATION = "location";
 
   private Table table;
 
@@ -75,7 +72,7 @@ public InputSplit[] getSplits(JobConf conf, int numSplits) throws IOException {
 
   private Table findTable(JobConf conf) throws IOException {
     HadoopTables tables = new HadoopTables(conf);
-    String tableDir = conf.get("location");
+    String tableDir = conf.get(TABLE_LOCATION);
     if (tableDir == null) {
       throw new IllegalArgumentException("Table 'location' not set in JobConf");
     }
@@ -115,12 +112,10 @@ public class IcebergRecordReader implements RecordReader<Void, IcebergWritable>
     private CloseableIterable<Record> reader;
     private Iterator<Record> recordIterator;
     private Record currentRecord;
-    private boolean reuseContainers;
 
     public IcebergRecordReader(InputSplit split, JobConf conf) throws IOException {
       this.split = (IcebergSplit) split;
       this.conf = conf;
-      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);
       initialise();
     }
 
@@ -131,11 +126,9 @@ private void initialise() {
 
     private void nextTask() {
       FileScanTask currentTask = tasks.next();
-      DataFile file = currentTask.file();
-      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), conf);
       Schema tableSchema = table.schema();
-
-      reader = IcebergReaderFactory.createReader(file, currentTask, inputFile, tableSchema, reuseContainers);
+      org.apache.iceberg.mr.IcebergRecordReader wrappedReader = new org.apache.iceberg.mr.IcebergRecordReader();
+      reader = wrappedReader.createReader(conf, currentTask, tableSchema);
       recordIterator = reader.iterator();
     }
 
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java b/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java
deleted file mode 100644
index 37bb40f54bb..00000000000
--- a/mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergReaderFactory.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *   http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing,
- * software distributed under the License is distributed on an
- * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- * KIND, either express or implied.  See the License for the
- * specific language governing permissions and limitations
- * under the License.
- */
-
-package org.apache.iceberg.mr.mapred;
-
-import org.apache.iceberg.DataFile;
-import org.apache.iceberg.FileScanTask;
-import org.apache.iceberg.Schema;
-import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
-import org.apache.iceberg.data.orc.GenericOrcReader;
-import org.apache.iceberg.data.parquet.GenericParquetReaders;
-import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
-import org.apache.iceberg.orc.ORC;
-import org.apache.iceberg.parquet.Parquet;
-
-class IcebergReaderFactory {
-
-  private IcebergReaderFactory() {
-  }
-
-  public static CloseableIterable<Record> createReader(DataFile file, FileScanTask currentTask, InputFile inputFile,
-      Schema tableSchema, boolean reuseContainers) {
-    switch (file.format()) {
-      case AVRO:
-        return buildAvroReader(currentTask, inputFile, tableSchema, reuseContainers);
-      case ORC:
-        return buildOrcReader(currentTask, inputFile, tableSchema, reuseContainers);
-      case PARQUET:
-        return buildParquetReader(currentTask, inputFile, tableSchema, reuseContainers);
-
-      default:
-        throw new UnsupportedOperationException(String.format("Cannot read %s file: %s", file.format().name(),
-            file.path()));
-    }
-  }
-
-  private static CloseableIterable buildAvroReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    Avro.ReadBuilder builder = Avro.read(inputFile)
-        .createReaderFunc(DataReader::create)
-        .project(schema)
-        .split(task.start(), task.length());
-
-    if (reuseContainers) {
-      builder.reuseContainers();
-    }
-
-    return builder.build();
-  }
-
-  private static CloseableIterable buildOrcReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    ORC.ReadBuilder builder = ORC.read(inputFile)
-        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))
-        .project(schema)
-        .split(task.start(), task.length());
-
-    return builder.build();
-  }
-
-  private static CloseableIterable buildParquetReader(FileScanTask task, InputFile inputFile, Schema schema,
-      boolean reuseContainers) {
-    Parquet.ReadBuilder builder = Parquet.read(inputFile)
-        .createReaderFunc(fileSchema  -> GenericParquetReaders.buildReader(schema, fileSchema))
-        .project(schema)
-        .split(task.start(), task.length());
-
-    if (reuseContainers) {
-      builder.reuseContainers();
-    }
-
-    return builder.build();
-  }
-}
diff --git a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
index 4f31abca95d..9371ec70e65 100644
--- a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
+++ b/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
@@ -19,7 +19,6 @@
 
 package org.apache.iceberg.mr.mapreduce;
 
-import com.google.common.base.Preconditions;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -32,7 +31,6 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.function.Function;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.InputFormat;
@@ -52,26 +50,15 @@
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.TableScan;
-import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.catalog.Catalog;
-import org.apache.iceberg.catalog.TableIdentifier;
-import org.apache.iceberg.common.DynConstructors;
 import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataReader;
-import org.apache.iceberg.data.orc.GenericOrcReader;
-import org.apache.iceberg.data.parquet.GenericParquetReaders;
 import org.apache.iceberg.exceptions.RuntimeIOException;
 import org.apache.iceberg.expressions.Expression;
 import org.apache.iceberg.expressions.Expressions;
-import org.apache.iceberg.hadoop.HadoopInputFile;
-import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.hadoop.Util;
 import org.apache.iceberg.io.CloseableIterable;
-import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.mr.InputFormatConfig;
 import org.apache.iceberg.mr.SerializationUtil;
-import org.apache.iceberg.orc.ORC;
-import org.apache.iceberg.parquet.Parquet;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
 import org.slf4j.Logger;
@@ -84,20 +71,6 @@
 public class IcebergInputFormat<T> extends InputFormat<Void, T> {
   private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);
 
-  static final String AS_OF_TIMESTAMP = "iceberg.mr.as.of.time";
-  static final String CASE_SENSITIVE = "iceberg.mr.case.sensitive";
-  static final String FILTER_EXPRESSION = "iceberg.mr.filter.expression";
-  static final String IN_MEMORY_DATA_MODEL = "iceberg.mr.in.memory.data.model";
-  static final String READ_SCHEMA = "iceberg.mr.read.schema";
-  static final String REUSE_CONTAINERS = "iceberg.mr.reuse.containers";
-  static final String SNAPSHOT_ID = "iceberg.mr.snapshot.id";
-  static final String SPLIT_SIZE = "iceberg.mr.split.size";
-  static final String TABLE_PATH = "iceberg.mr.table.path";
-  static final String TABLE_SCHEMA = "iceberg.mr.table.schema";
-  static final String LOCALITY = "iceberg.mr.locality";
-  static final String CATALOG = "iceberg.mr.catalog";
-  static final String SKIP_RESIDUAL_FILTERING = "skip.residual.filtering";
-
   private transient List<InputSplit> splits;
 
   private enum InMemoryDataModel {
@@ -112,100 +85,9 @@ private enum InMemoryDataModel {
    *
    * @param job the {@code Job} to configure
    */
-  public static ConfigBuilder configure(Job job) {
+  public static InputFormatConfig.ConfigBuilder configure(Job job) {
     job.setInputFormatClass(IcebergInputFormat.class);
-    return new ConfigBuilder(job.getConfiguration());
-  }
-
-  public static class ConfigBuilder {
-    private final Configuration conf;
-
-    public ConfigBuilder(Configuration conf) {
-      this.conf = conf;
-      // defaults
-      conf.setEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
-      conf.setBoolean(SKIP_RESIDUAL_FILTERING, false);
-      conf.setBoolean(CASE_SENSITIVE, true);
-      conf.setBoolean(REUSE_CONTAINERS, false);
-      conf.setBoolean(LOCALITY, false);
-    }
-
-    public ConfigBuilder readFrom(String path) {
-      conf.set(TABLE_PATH, path);
-      Table table = findTable(conf);
-      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));
-      return this;
-    }
-
-    public ConfigBuilder filter(Expression expression) {
-      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));
-      return this;
-    }
-
-    public ConfigBuilder project(Schema schema) {
-      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));
-      return this;
-    }
-
-    public ConfigBuilder reuseContainers(boolean reuse) {
-      conf.setBoolean(REUSE_CONTAINERS, reuse);
-      return this;
-    }
-
-    public ConfigBuilder caseSensitive(boolean caseSensitive) {
-      conf.setBoolean(CASE_SENSITIVE, caseSensitive);
-      return this;
-    }
-
-    public ConfigBuilder snapshotId(long snapshotId) {
-      conf.setLong(SNAPSHOT_ID, snapshotId);
-      return this;
-    }
-
-    public ConfigBuilder asOfTime(long asOfTime) {
-      conf.setLong(AS_OF_TIMESTAMP, asOfTime);
-      return this;
-    }
-
-    public ConfigBuilder splitSize(long splitSize) {
-      conf.setLong(SPLIT_SIZE, splitSize);
-      return this;
-    }
-
-    /**
-     * If this API is called. The input splits
-     * constructed will have host location information
-     */
-    public ConfigBuilder preferLocality() {
-      conf.setBoolean(LOCALITY, true);
-      return this;
-    }
-
-    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {
-      conf.setClass(CATALOG, catalogFuncClass, Function.class);
-      return this;
-    }
-
-    public ConfigBuilder useHiveRows() {
-      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());
-      return this;
-    }
-
-    public ConfigBuilder usePigTuples() {
-      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());
-      return this;
-    }
-
-    /**
-     * Compute platforms pass down filters to data sources. If the data source cannot apply some filters, or only
-     * partially applies the filter, it will return the residual filter back. If the platform can correctly apply
-     * the residual filters, then it should call this api. Otherwise the current api will throw an exception if the
-     * passed in filter is not completely satisfied.
-     */
-    public ConfigBuilder skipResidualFiltering() {
-      conf.setBoolean(SKIP_RESIDUAL_FILTERING, true);
-      return this;
-    }
+    return new InputFormatConfig.ConfigBuilder(job.getConfiguration());
   }
 
   @Override
@@ -216,38 +98,39 @@ public List<InputSplit> getSplits(JobContext context) {
     }
 
     Configuration conf = context.getConfiguration();
-    Table table = findTable(conf);
+    Table table = InputFormatConfig.findTable(conf);
     TableScan scan = table.newScan()
-                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));
-    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);
+                          .caseSensitive(conf.getBoolean(InputFormatConfig.CASE_SENSITIVE, true));
+    long snapshotId = conf.getLong(InputFormatConfig.SNAPSHOT_ID, -1);
     if (snapshotId != -1) {
       scan = scan.useSnapshot(snapshotId);
     }
-    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);
+    long asOfTime = conf.getLong(InputFormatConfig.AS_OF_TIMESTAMP, -1);
     if (asOfTime != -1) {
       scan = scan.asOfTime(asOfTime);
     }
-    long splitSize = conf.getLong(SPLIT_SIZE, 0);
+    long splitSize = conf.getLong(InputFormatConfig.SPLIT_SIZE, 0);
     if (splitSize > 0) {
       scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));
     }
-    String schemaStr = conf.get(READ_SCHEMA);
+    String schemaStr = conf.get(InputFormatConfig.READ_SCHEMA);
     if (schemaStr != null) {
       scan.project(SchemaParser.fromJson(schemaStr));
     }
 
     // TODO add a filter parser to get rid of Serialization
-    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));
+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(InputFormatConfig.FILTER_EXPRESSION));
     if (filter != null) {
       scan = scan.filter(filter);
     }
 
     splits = Lists.newArrayList();
-    boolean applyResidualFiltering = !conf.getBoolean(SKIP_RESIDUAL_FILTERING, false);
+    boolean applyResidual = !conf.getBoolean(InputFormatConfig.SKIP_RESIDUAL_FILTERING, false);
+    InMemoryDataModel model = conf.getEnum(InputFormatConfig.IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
     try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {
       tasksIterable.forEach(task -> {
-        if (applyResidualFiltering) {
-          //TODO: We do not support residual evaluation yet
+        if (applyResidual && (model == InMemoryDataModel.HIVE || model == InMemoryDataModel.PIG)) {
+          //TODO: We do not support residual evaluation for HIVE and PIG in memory data model yet
           checkResiduals(task);
         }
         splits.add(new IcebergSplit(conf, task));
@@ -280,8 +163,6 @@ public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptCon
     private TaskAttemptContext context;
     private Schema tableSchema;
     private Schema expectedSchema;
-    private boolean reuseContainers;
-    private boolean caseSensitive;
     private InMemoryDataModel inMemoryDataModel;
     private Map<String, Integer> namesToPos;
     private Iterator<FileScanTask> tasks;
@@ -296,13 +177,11 @@ public void initialize(InputSplit split, TaskAttemptContext newContext) {
       CombinedScanTask task = ((IcebergSplit) split).task;
       this.context = newContext;
       this.tasks = task.files().iterator();
-      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));
-      String readSchemaStr = conf.get(READ_SCHEMA);
+      this.tableSchema = SchemaParser.fromJson(conf.get(InputFormatConfig.TABLE_SCHEMA));
+      String readSchemaStr = conf.get(InputFormatConfig.READ_SCHEMA);
       this.expectedSchema = readSchemaStr != null ? SchemaParser.fromJson(readSchemaStr) : tableSchema;
       this.namesToPos = buildNameToPos(expectedSchema);
-      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);
-      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);
-      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
+      this.inMemoryDataModel = conf.getEnum(InputFormatConfig.IN_MEMORY_DATA_MODEL, InMemoryDataModel.GENERIC);
       this.currentIterator = open(tasks.next());
     }
 
@@ -316,6 +195,7 @@ public boolean nextKeyValue() throws IOException {
           currentCloseable.close();
           currentIterator = open(tasks.next());
         } else {
+          currentCloseable.close();
           return false;
         }
       }
@@ -375,26 +255,9 @@ private Iterator<T> open(FileScanTask currentTask) {
     }
 
     private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {
-      DataFile file = currentTask.file();
-      // TODO we should make use of FileIO to create inputFile
-      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());
-      CloseableIterable<T> iterable;
-      switch (file.format()) {
-        case AVRO:
-          iterable = newAvroIterable(inputFile, currentTask, readSchema);
-          break;
-        case ORC:
-          iterable = newOrcIterable(inputFile, currentTask, readSchema);
-          break;
-        case PARQUET:
-          iterable = newParquetIterable(inputFile, currentTask, readSchema);
-          break;
-        default:
-          throw new UnsupportedOperationException(
-              String.format("Cannot read %s file: %s", file.format().name(), file.path()));
-      }
+      org.apache.iceberg.mr.IcebergRecordReader wrappedReader = new org.apache.iceberg.mr.IcebergRecordReader();
+      CloseableIterable<T> iterable = wrappedReader.createReader(context.getConfiguration(), currentTask, readSchema);
       currentCloseable = iterable;
-      //TODO: Apply residual filtering before returning the iterator
       return iterable.iterator();
     }
 
@@ -441,87 +304,6 @@ private Record withIdentityPartitionColumns(
       return row;
     }
 
-    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)
-          .project(readSchema)
-          .split(task.start(), task.length());
-      if (reuseContainers) {
-        avroReadBuilder.reuseContainers();
-      }
-
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("Avro support not yet supported for Pig and Hive");
-        case GENERIC:
-          avroReadBuilder.createReaderFunc(DataReader::create);
-      }
-      return avroReadBuilder.build();
-    }
-
-    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)
-          .project(readSchema)
-          .filter(task.residual())
-          .caseSensitive(caseSensitive)
-          .split(task.start(), task.length());
-      if (reuseContainers) {
-        parquetReadBuilder.reuseContainers();
-      }
-
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("Parquet support not yet supported for Pig and Hive");
-        case GENERIC:
-          parquetReadBuilder.createReaderFunc(
-              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));
-      }
-      return parquetReadBuilder.build();
-    }
-
-    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {
-      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)
-          .project(readSchema)
-          .caseSensitive(caseSensitive)
-          .split(task.start(), task.length());
-      // ORC does not support reuse containers yet
-      switch (inMemoryDataModel) {
-        case PIG:
-        case HIVE:
-          //TODO: implement value readers for Pig and Hive
-          throw new UnsupportedOperationException("ORC support not yet supported for Pig and Hive");
-        case GENERIC:
-          orcReadBuilder.createReaderFunc(fileSchema -> GenericOrcReader.buildReader(readSchema, fileSchema));
-      }
-
-      return orcReadBuilder.build();
-    }
-  }
-
-  private static Table findTable(Configuration conf) {
-    String path = conf.get(TABLE_PATH);
-    Preconditions.checkArgument(path != null, "Table path should not be null");
-    if (path.contains("/")) {
-      HadoopTables tables = new HadoopTables(conf);
-      return tables.load(path);
-    }
-
-    String catalogFuncClass = conf.get(CATALOG);
-    if (catalogFuncClass != null) {
-      Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)
-          DynConstructors.builder(Function.class)
-                         .impl(catalogFuncClass)
-                         .build()
-                         .newInstance();
-      Catalog catalog = catalogFunc.apply(conf);
-      TableIdentifier tableIdentifier = TableIdentifier.parse(path);
-      return catalog.loadTable(tableIdentifier);
-    } else {
-      throw new IllegalArgumentException("No custom catalog specified to load table " + path);
-    }
   }
 
   static class IcebergSplit extends InputSplit implements Writable {
@@ -542,7 +324,7 @@ public long getLength() {
 
     @Override
     public String[] getLocations() {
-      boolean localityPreferred = conf.getBoolean(LOCALITY, false);
+      boolean localityPreferred = conf.getBoolean(InputFormatConfig.LOCALITY, false);
       if (!localityPreferred) {
         return ANYWHERE;
       }
diff --git a/mr/src/test/java/org/apache/iceberg/mr/BaseInputFormatTest.java b/mr/src/test/java/org/apache/iceberg/mr/BaseInputFormatTest.java
new file mode 100644
index 00000000000..8aa24aeca52
--- /dev/null
+++ b/mr/src/test/java/org/apache/iceberg/mr/BaseInputFormatTest.java
@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import com.google.common.collect.ImmutableMap;
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.FileFormat;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.data.RandomGenericData;
+import org.apache.iceberg.data.Record;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.mr.TestHelpers.Row;
+import org.apache.iceberg.types.Types;
+import org.junit.Assert;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import static org.apache.iceberg.mr.TestHelpers.writeFile;
+import static org.apache.iceberg.types.Types.NestedField.required;
+
+
+@RunWith(Parameterized.class)
+public abstract class BaseInputFormatTest {
+
+  @Rule
+  public TemporaryFolder temp = new TemporaryFolder();
+
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][]{
+        new Object[]{"parquet"},
+        new Object[]{"avro"},
+        new Object[]{"orc"}
+    };
+  }
+
+  protected static final Schema SCHEMA = new Schema(
+      required(1, "data", Types.StringType.get()),
+      required(2, "id", Types.LongType.get()),
+      required(3, "date", Types.StringType.get()));
+
+  protected static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)
+      .identity("date")
+      .bucket("id", 1)
+      .build();
+
+  protected Configuration conf = new Configuration();
+  protected  HadoopTables tables = new HadoopTables(conf);
+
+  protected FileFormat fileFormat;
+
+  protected abstract void runAndValidate(File tableLocation, List<Record> expectedRecords) throws IOException;
+
+  @Test
+  public void testUnpartitionedTable() throws Exception {
+    File tableLocation = temp.newFolder(fileFormat.name());
+    Table table = tables
+        .create(SCHEMA, PartitionSpec.unpartitioned(),
+            ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()), tableLocation.toString());
+    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
+    DataFile dataFile = writeFile(temp.newFile(), table, null, fileFormat, expectedRecords);
+    table.newAppend().appendFile(dataFile).commit();
+    runAndValidate(tableLocation, expectedRecords);
+  }
+
+  @Test
+  public void testPartitionedTable() throws Exception {
+    File tableLocation = temp.newFolder(fileFormat.name());
+    Assert.assertTrue(tableLocation.delete());
+    Table table = tables.create(SCHEMA, SPEC,
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
+                                tableLocation.toString());
+    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
+    expectedRecords.get(0).set(2, "2020-03-20");
+    DataFile dataFile = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, expectedRecords);
+    table.newAppend()
+         .appendFile(dataFile)
+         .commit();
+
+    runAndValidate(tableLocation, expectedRecords);
+  }
+
+}
diff --git a/mr/src/test/java/org/apache/iceberg/mr/TestHelpers.java b/mr/src/test/java/org/apache/iceberg/mr/TestHelpers.java
new file mode 100644
index 00000000000..bddddd6afef
--- /dev/null
+++ b/mr/src/test/java/org/apache/iceberg/mr/TestHelpers.java
@@ -0,0 +1,128 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.mr;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.DataFiles;
+import org.apache.iceberg.FileFormat;
+import org.apache.iceberg.Files;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.avro.Avro;
+import org.apache.iceberg.data.Record;
+import org.apache.iceberg.data.avro.DataWriter;
+import org.apache.iceberg.data.orc.GenericOrcWriter;
+import org.apache.iceberg.data.parquet.GenericParquetWriter;
+import org.apache.iceberg.io.FileAppender;
+import org.apache.iceberg.orc.ORC;
+import org.apache.iceberg.parquet.Parquet;
+
+/**
+ *
+ */
+public class TestHelpers {
+
+  private TestHelpers() {}
+
+  /**
+   * Implements {@link StructLike#get} for passing data in tests.
+   */
+  public static class Row implements StructLike {
+    public static Row of(Object... values) {
+      return new Row(values);
+    }
+
+    private final Object[] values;
+
+    private Row(Object... values) {
+      this.values = values;
+    }
+
+    @Override
+    public int size() {
+      return values.length;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public <T> T get(int pos, Class<T> javaClass) {
+      return javaClass.cast(values[pos]);
+    }
+
+    @Override
+    public <T> void set(int pos, T value) {
+      throw new UnsupportedOperationException("Setting values is not supported");
+    }
+  }
+
+  public static DataFile writeFile(File targetFile,
+      Table table, StructLike partitionData, FileFormat fileFormat, List<Record> records) throws IOException {
+    if (targetFile.exists()) {
+      if (!targetFile.delete()) {
+        throw new IOException("Unable to delete " + targetFile.getAbsolutePath());
+      }
+    }
+    FileAppender<Record> appender;
+    switch (fileFormat) {
+      case AVRO:
+        appender = Avro.write(Files.localOutput(targetFile))
+            .schema(table.schema())
+            .createWriterFunc(DataWriter::create)
+            .named(fileFormat.name())
+            .build();
+        break;
+      case PARQUET:
+        appender = Parquet.write(Files.localOutput(targetFile))
+            .schema(table.schema())
+            .createWriterFunc(GenericParquetWriter::buildWriter)
+            .named(fileFormat.name())
+            .build();
+        break;
+      case ORC:
+        appender = ORC.write(Files.localOutput(targetFile))
+            .schema(table.schema())
+            .createWriterFunc(GenericOrcWriter::buildWriter)
+            .build();
+        break;
+      default:
+        throw new UnsupportedOperationException("Cannot write format: " + fileFormat);
+    }
+
+    try {
+      appender.addAll(records);
+    } finally {
+      appender.close();
+    }
+
+    DataFiles.Builder builder = DataFiles.builder(table.spec())
+        .withPath(targetFile.toString())
+        .withFormat(fileFormat)
+        .withFileSizeInBytes(targetFile.length())
+        .withMetrics(appender.metrics());
+    if (partitionData != null) {
+      builder.withPartition(partitionData);
+    }
+    return builder.build();
+  }
+
+}
diff --git a/mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergInputFormat.java b/mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergInputFormat.java
index cf1fb878a83..e64ff021ea3 100644
--- a/mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergInputFormat.java
+++ b/mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergInputFormat.java
@@ -21,91 +21,75 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
-import org.apache.commons.compress.utils.Lists;
-import org.apache.commons.io.FileUtils;
+import java.util.Locale;
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RecordReader;
-import org.apache.iceberg.DataFile;
-import org.apache.iceberg.DataFiles;
-import org.apache.iceberg.PartitionSpec;
-import org.apache.iceberg.Schema;
-import org.apache.iceberg.Table;
+import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.hadoop.HadoopTables;
-import org.apache.iceberg.types.Types;
-import org.junit.After;
-import org.junit.Before;
+import org.apache.iceberg.mr.BaseInputFormatTest;
+import org.junit.Assert;
 import org.junit.Test;
+import org.junit.runners.Parameterized;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.apache.iceberg.types.Types.NestedField.optional;
-import static org.junit.Assert.assertEquals;
-
-public class TestIcebergInputFormat {
+public class TestIcebergInputFormat extends BaseInputFormatTest {
 
   private static final Logger LOG = LoggerFactory.getLogger(TestIcebergInputFormat.class);
 
-  private File tableLocation;
-  private Table table;
-  private IcebergInputFormat format = new IcebergInputFormat();
-  private JobConf conf = new JobConf();
-
-  @Before
-  public void before() throws IOException {
-    tableLocation = java.nio.file.Files.createTempDirectory("temp").toFile();
-    Schema schema = new Schema(optional(1, "name", Types.StringType.get()),
-        optional(2, "salary", Types.LongType.get()));
-    PartitionSpec spec = PartitionSpec.unpartitioned();
-    HadoopTables tables = new HadoopTables();
-    table = tables.create(schema, spec, tableLocation.getAbsolutePath());
-
-    DataFile fileA = DataFiles
-        .builder(spec)
-        .withPath("src/test/resources/test-table/data/00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet")
-        .withFileSizeInBytes(1024)
-        .withRecordCount(3) // needs at least one record or else metrics will filter it out
-        .build();
+  private IcebergInputFormat inputFormat = new IcebergInputFormat();
 
-    table.newAppend().appendFile(fileA).commit();
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] { new Object[] { "parquet" }, new Object[] { "avro" }
+        /*
+         * , TODO: put orc back, seems to be an issue with different versions of Orc in Hive and Iceberg new
+         * Object[]{"orc"}
+         */
+    };
   }
 
-  @Test
-  public void testGetSplits() throws IOException {
-    conf.set("location", "file:" + tableLocation);
-    InputSplit[] splits = format.getSplits(conf, 1);
-    assertEquals(splits.length, 1);
+  public TestIcebergInputFormat(String fileFormat) {
+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));
   }
 
   @Test(expected = IllegalArgumentException.class)
   public void testGetSplitsNoLocation() throws IOException {
-    format.getSplits(conf, 1);
+    JobConf jobConf = new JobConf();
+    inputFormat.getSplits(jobConf, 1);
   }
 
   @Test(expected = IOException.class)
   public void testGetSplitsInvalidLocationUri() throws IOException {
-    conf.set("location", "http:");
-    format.getSplits(conf, 1);
+    JobConf jobConf = new JobConf();
+    jobConf.set(IcebergInputFormat.TABLE_LOCATION, "http:");
+    inputFormat.getSplits(jobConf, 1);
   }
 
-  @Test
-  public void testGetRecordReader() throws IOException {
-    conf.set("location", "file:" + tableLocation);
-    InputSplit[] splits = format.getSplits(conf, 1);
-    RecordReader reader = format.getRecordReader(splits[0], conf, null);
-    IcebergWritable value = (IcebergWritable) reader.createValue();
+  @Override
+  protected void runAndValidate(File tableLocation, List<Record> expectedRecords) throws IOException {
+    JobConf jobConf = new JobConf();
+    jobConf.set(IcebergInputFormat.TABLE_LOCATION, "file:" + tableLocation);
+    validate(jobConf, expectedRecords);
+  }
 
-    List<Record> records = Lists.newArrayList();
+  private void validate(JobConf jobConf, List<Record> expectedRecords) throws IOException {
+    List<Record> actualRecords = readRecords(jobConf);
+    Assert.assertEquals(expectedRecords, actualRecords);
+  }
+
+  private List<Record> readRecords(JobConf jobConf) throws IOException {
+    InputSplit[] splits = inputFormat.getSplits(jobConf, 1);
+    RecordReader reader = inputFormat.getRecordReader(splits[0], jobConf, null);
+    List<Record> records = new ArrayList<>();
+    IcebergWritable value = (IcebergWritable) reader.createValue();
     while (reader.next(null, value)) {
       records.add(value.getRecord().copy());
     }
-    assertEquals(3, records.size());
+    return records;
   }
 
-  @After
-  public void after() throws IOException {
-    FileUtils.deleteDirectory(tableLocation);
-  }
 }
diff --git a/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java b/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
index 1f84890f826..7b8258db8ee 100644
--- a/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
+++ b/mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java
@@ -40,138 +40,66 @@
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.iceberg.AppendFiles;
-import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.DataFile;
-import org.apache.iceberg.DataFiles;
 import org.apache.iceberg.FileFormat;
-import org.apache.iceberg.Files;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.Schema;
-import org.apache.iceberg.StructLike;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.TableProperties;
-import org.apache.iceberg.TestHelpers.Row;
-import org.apache.iceberg.avro.Avro;
 import org.apache.iceberg.catalog.Catalog;
 import org.apache.iceberg.catalog.TableIdentifier;
 import org.apache.iceberg.data.RandomGenericData;
 import org.apache.iceberg.data.Record;
-import org.apache.iceberg.data.avro.DataWriter;
-import org.apache.iceberg.data.orc.GenericOrcWriter;
-import org.apache.iceberg.data.parquet.GenericParquetWriter;
 import org.apache.iceberg.expressions.Expressions;
 import org.apache.iceberg.hadoop.HadoopCatalog;
-import org.apache.iceberg.hadoop.HadoopTables;
-import org.apache.iceberg.io.FileAppender;
-import org.apache.iceberg.orc.ORC;
-import org.apache.iceberg.parquet.Parquet;
+import org.apache.iceberg.mr.BaseInputFormatTest;
+import org.apache.iceberg.mr.InputFormatConfig;
+import org.apache.iceberg.mr.TestHelpers.Row;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
 import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Rule;
 import org.junit.Test;
-import org.junit.rules.TemporaryFolder;
 import org.junit.runner.RunWith;
 import org.junit.runners.Parameterized;
 
-import static org.apache.iceberg.types.Types.NestedField.required;
+import static org.apache.iceberg.mr.TestHelpers.writeFile;
 
 @RunWith(Parameterized.class)
-public class TestIcebergInputFormat {
-  static final Schema SCHEMA = new Schema(
-      required(1, "data", Types.StringType.get()),
-      required(2, "id", Types.LongType.get()),
-      required(3, "date", Types.StringType.get()));
-
-  static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)
-      .identity("date")
-      .bucket("id", 1)
-      .build();
-
-  @Rule
-  public TemporaryFolder temp = new TemporaryFolder();
-  private HadoopTables tables;
-  private Configuration conf;
-
-  @Parameterized.Parameters
-  public static Object[][] parameters() {
-    return new Object[][]{
-        new Object[]{"parquet"},
-        new Object[]{"avro"},
-        new Object[]{"orc"}
-    };
-  }
-
-  private final FileFormat format;
+public class TestIcebergInputFormat extends BaseInputFormatTest {
 
   public TestIcebergInputFormat(String format) {
-    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
-  }
-
-  @Before
-  public void before() {
-    conf = new Configuration();
-    tables = new HadoopTables(conf);
-  }
-
-  @Test
-  public void testUnpartitionedTable() throws Exception {
-    File location = temp.newFolder(format.name());
-    Assert.assertTrue(location.delete());
-    Table table = tables.create(SCHEMA, PartitionSpec.unpartitioned(),
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
-                                location.toString());
-    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
-    DataFile dataFile = writeFile(table, null, format, expectedRecords);
-    table.newAppend()
-         .appendFile(dataFile)
-         .commit();
-    Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
-    configBuilder.readFrom(location.toString());
-    validate(job, expectedRecords);
+    this.fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
   }
 
-  @Test
-  public void testPartitionedTable() throws Exception {
-    File location = temp.newFolder(format.name());
-    Assert.assertTrue(location.delete());
-    Table table = tables.create(SCHEMA, SPEC,
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
-                                location.toString());
-    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
-    expectedRecords.get(0).set(2, "2020-03-20");
-    DataFile dataFile = writeFile(table, Row.of("2020-03-20", 0), format, expectedRecords);
-    table.newAppend()
-         .appendFile(dataFile)
-         .commit();
-
+  @Override
+  protected void runAndValidate(File tableLocation, List<Record> expectedRecords) throws IOException {
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
-    configBuilder.readFrom(location.toString());
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    configBuilder.readFrom(tableLocation.toString());
     validate(job, expectedRecords);
   }
 
+  //TODO: try move as many methods below into base class (once functionality is implemented in
+  //      mapred InputFormat)
   @Test
   public void testFilterExp() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Table table = tables.create(SCHEMA, SPEC,
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
     List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 2, 0L);
     expectedRecords.get(0).set(2, "2020-03-20");
     expectedRecords.get(1).set(2, "2020-03-20");
-    DataFile dataFile1 = writeFile(table, Row.of("2020-03-20", 0), format, expectedRecords);
-    DataFile dataFile2 = writeFile(table, Row.of("2020-03-21", 0), format,
+    DataFile dataFile1 = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, expectedRecords);
+    DataFile dataFile2 = writeFile(temp.newFile(), table, Row.of("2020-03-21", 0), fileFormat,
                                    RandomGenericData.generate(table.schema(), 2, 0L));
     table.newAppend()
          .appendFile(dataFile1)
          .appendFile(dataFile2)
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString())
                  .filter(Expressions.equal("date", "2020-03-20"));
     validate(job, expectedRecords);
@@ -179,47 +107,61 @@ public void testFilterExp() throws Exception {
 
   @Test
   public void testResiduals() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Table table = tables.create(SCHEMA, SPEC,
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
-    List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 2, 0L);
-    expectedRecords.get(0).set(2, "2020-03-20");
-    expectedRecords.get(1).set(2, "2020-03-20");
-    DataFile dataFile = writeFile(table, Row.of("2020-03-20", 0), format, expectedRecords);
+    List<Record> writeRecords = RandomGenericData.generate(table.schema(), 2, 0L);
+    writeRecords.get(0).set(1, 123L);
+    writeRecords.get(0).set(2, "2020-03-20");
+    writeRecords.get(1).set(1, 456L);
+    writeRecords.get(1).set(2, "2020-03-20");
+
+    List<Record> expectedRecords = new ArrayList<>();
+    expectedRecords.add(writeRecords.get(0));
+
+    DataFile dataFile1 = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, writeRecords);
+    DataFile dataFile2 = writeFile(temp.newFile(), table, Row.of("2020-03-21", 0), fileFormat,
+        RandomGenericData.generate(table.schema(), 2, 0L));
     table.newAppend()
-         .appendFile(dataFile)
+         .appendFile(dataFile1)
+         .appendFile(dataFile2)
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString())
-                 .filter(Expressions.and(
-                     Expressions.equal("date", "2020-03-20"),
-                     Expressions.equal("id", 0)));
-
-    AssertHelpers.assertThrows(
-        "Residuals are not evaluated today for Iceberg Generics In memory model",
-        UnsupportedOperationException.class, "Filter expression ref(name=\"id\") == 0 is not completely satisfied.",
-        () -> validate(job, expectedRecords));
+        .filter(Expressions.and(
+            Expressions.equal("date", "2020-03-20"),
+            Expressions.equal("id", 123)));
+    validate(job, expectedRecords);
+
+    // skip residual filtering
+    job = Job.getInstance(conf);
+    configBuilder = IcebergInputFormat.configure(job);
+    configBuilder.skipResidualFiltering().readFrom(location.toString())
+        .filter(Expressions.and(
+            Expressions.equal("date", "2020-03-20"),
+            Expressions.equal("id", 123)));
+    validate(job, writeRecords);
   }
 
   @Test
   public void testProjection() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Schema projectedSchema = TypeUtil.select(SCHEMA, ImmutableSet.of(1));
     Table table = tables.create(SCHEMA, SPEC,
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
     List<Record> inputRecords = RandomGenericData.generate(table.schema(), 1, 0L);
-    DataFile dataFile = writeFile(table, Row.of("2020-03-20", 0), format, inputRecords);
+    DataFile dataFile = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, inputRecords);
     table.newAppend()
          .appendFile(dataFile)
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(location.toString())
         .project(projectedSchema);
@@ -240,10 +182,10 @@ public void testProjection() throws Exception {
 
   @Test
   public void testIdentityPartitionProjections() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Table table = tables.create(LOG_SCHEMA, IDENTITY_PARTITION_SPEC,
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
 
     List<Record> inputRecords = RandomGenericData.generate(LOG_SCHEMA, 10, 0);
@@ -252,7 +194,8 @@ public void testIdentityPartitionProjections() throws Exception {
     for (Record record : inputRecords) {
       record.set(1, "2020-03-2" + idx);
       record.set(2, idx.toString());
-      append.appendFile(writeFile(table, Row.of("2020-03-2" + idx, idx.toString()), format, ImmutableList.of(record)));
+      append.appendFile(writeFile(temp.newFile(), table, Row.of("2020-03-2" + idx, idx.toString()),
+                        fileFormat, ImmutableList.of(record)));
       idx += 1;
     }
     append.commit();
@@ -293,7 +236,7 @@ private static Schema withColumns(String... names) {
   private void validateIdentityPartitionProjections(
       String tablePath, Schema projectedSchema, List<Record> inputRecords) throws Exception {
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(tablePath)
         .project(projectedSchema);
@@ -313,22 +256,23 @@ private void validateIdentityPartitionProjections(
 
   @Test
   public void testSnapshotReads() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Table table = tables.create(SCHEMA, PartitionSpec.unpartitioned(),
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
     List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
     table.newAppend()
-         .appendFile(writeFile(table, null, format, expectedRecords))
+         .appendFile(writeFile(temp.newFile(), table, null, fileFormat, expectedRecords))
          .commit();
     long snapshotId = table.currentSnapshot().snapshotId();
     table.newAppend()
-         .appendFile(writeFile(table, null, format, RandomGenericData.generate(table.schema(), 1, 0L)))
+         .appendFile(writeFile(temp.newFile(), table, null, fileFormat,
+                     RandomGenericData.generate(table.schema(), 1, 0L)))
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .readFrom(location.toString())
         .snapshotId(snapshotId);
@@ -338,17 +282,17 @@ public void testSnapshotReads() throws Exception {
 
   @Test
   public void testLocality() throws Exception {
-    File location = temp.newFolder(format.name());
+    File location = temp.newFolder(fileFormat.name());
     Assert.assertTrue(location.delete());
     Table table = tables.create(SCHEMA, PartitionSpec.unpartitioned(),
-                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()),
+                                ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()),
                                 location.toString());
     List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
     table.newAppend()
-         .appendFile(writeFile(table, null, format, expectedRecords))
+         .appendFile(writeFile(temp.newFile(), table, null, fileFormat, expectedRecords))
          .commit();
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder.readFrom(location.toString());
 
     for (InputSplit split : splits(job.getConfiguration())) {
@@ -376,16 +320,16 @@ public void testCustomCatalog() throws Exception {
     Catalog catalog = new HadoopCatalogFunc().apply(conf);
     TableIdentifier tableIdentifier = TableIdentifier.of("db", "t");
     Table table = catalog.createTable(tableIdentifier, SCHEMA, SPEC,
-                                      ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name()));
+                                      ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name()));
     List<Record> expectedRecords = RandomGenericData.generate(table.schema(), 1, 0L);
     expectedRecords.get(0).set(2, "2020-03-20");
-    DataFile dataFile = writeFile(table, Row.of("2020-03-20", 0), format, expectedRecords);
+    DataFile dataFile = writeFile(temp.newFile(), table, Row.of("2020-03-20", 0), fileFormat, expectedRecords);
     table.newAppend()
          .appendFile(dataFile)
          .commit();
 
     Job job = Job.getInstance(conf);
-    IcebergInputFormat.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
+    InputFormatConfig.ConfigBuilder configBuilder = IcebergInputFormat.configure(job);
     configBuilder
         .catalogFunc(HadoopCatalogFunc.class)
         .readFrom(tableIdentifier.toString());
@@ -429,50 +373,4 @@ private static <T> Iterable<T> readRecords(
     return records;
   }
 
-  private DataFile writeFile(
-      Table table, StructLike partitionData, FileFormat fileFormat, List<Record> records) throws IOException {
-    File file = temp.newFile();
-    Assert.assertTrue(file.delete());
-    FileAppender<Record> appender;
-    switch (fileFormat) {
-      case AVRO:
-        appender = Avro.write(Files.localOutput(file))
-            .schema(table.schema())
-            .createWriterFunc(DataWriter::create)
-            .named(fileFormat.name())
-            .build();
-        break;
-      case PARQUET:
-        appender = Parquet.write(Files.localOutput(file))
-            .schema(table.schema())
-            .createWriterFunc(GenericParquetWriter::buildWriter)
-            .named(fileFormat.name())
-            .build();
-        break;
-      case ORC:
-        appender = ORC.write(Files.localOutput(file))
-            .schema(table.schema())
-            .createWriterFunc(GenericOrcWriter::buildWriter)
-            .build();
-        break;
-      default:
-        throw new UnsupportedOperationException("Cannot write format: " + fileFormat);
-    }
-
-    try {
-      appender.addAll(records);
-    } finally {
-      appender.close();
-    }
-
-    DataFiles.Builder builder = DataFiles.builder(table.spec())
-        .withPath(file.toString())
-        .withFormat(format)
-        .withFileSizeInBytes(file.length())
-        .withMetrics(appender.metrics());
-    if (partitionData != null) {
-      builder.withPartition(partitionData);
-    }
-    return builder.build();
-  }
 }
diff --git a/mr/src/test/resources/test-table/data/.00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet.crc b/mr/src/test/resources/test-table-to-delete/data/.00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet.crc
similarity index 100%
rename from mr/src/test/resources/test-table/data/.00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet.crc
rename to mr/src/test/resources/test-table-to-delete/data/.00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet.crc
diff --git a/mr/src/test/resources/test-table/data/00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet b/mr/src/test/resources/test-table-to-delete/data/00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet
similarity index 100%
rename from mr/src/test/resources/test-table/data/00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet
rename to mr/src/test/resources/test-table-to-delete/data/00000-1-c7557bc3-ae0d-46fb-804e-e9806abf81c7-00000.parquet
diff --git a/mr/src/test/resources/test-table/metadata/.1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro.crc b/mr/src/test/resources/test-table-to-delete/metadata/.1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro.crc
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/.1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro.crc
rename to mr/src/test/resources/test-table-to-delete/metadata/.1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro.crc
diff --git a/mr/src/test/resources/test-table/metadata/.snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro.crc b/mr/src/test/resources/test-table-to-delete/metadata/.snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro.crc
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/.snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro.crc
rename to mr/src/test/resources/test-table-to-delete/metadata/.snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro.crc
diff --git a/mr/src/test/resources/test-table/metadata/.v1.metadata.json.crc b/mr/src/test/resources/test-table-to-delete/metadata/.v1.metadata.json.crc
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/.v1.metadata.json.crc
rename to mr/src/test/resources/test-table-to-delete/metadata/.v1.metadata.json.crc
diff --git a/mr/src/test/resources/test-table/metadata/.v2.metadata.json.crc b/mr/src/test/resources/test-table-to-delete/metadata/.v2.metadata.json.crc
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/.v2.metadata.json.crc
rename to mr/src/test/resources/test-table-to-delete/metadata/.v2.metadata.json.crc
diff --git a/mr/src/test/resources/test-table/metadata/.version-hint.text.crc b/mr/src/test/resources/test-table-to-delete/metadata/.version-hint.text.crc
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/.version-hint.text.crc
rename to mr/src/test/resources/test-table-to-delete/metadata/.version-hint.text.crc
diff --git a/mr/src/test/resources/test-table/metadata/1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro b/mr/src/test/resources/test-table-to-delete/metadata/1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro
rename to mr/src/test/resources/test-table-to-delete/metadata/1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c-m0.avro
diff --git a/mr/src/test/resources/test-table/metadata/snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro b/mr/src/test/resources/test-table-to-delete/metadata/snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro
rename to mr/src/test/resources/test-table-to-delete/metadata/snap-7829799286772121706-1-1a3ffe32-d8da-47cf-9a8c-0e4c889a3a4c.avro
diff --git a/mr/src/test/resources/test-table/metadata/v1.metadata.json b/mr/src/test/resources/test-table-to-delete/metadata/v1.metadata.json
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/v1.metadata.json
rename to mr/src/test/resources/test-table-to-delete/metadata/v1.metadata.json
diff --git a/mr/src/test/resources/test-table/metadata/v2.metadata.json b/mr/src/test/resources/test-table-to-delete/metadata/v2.metadata.json
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/v2.metadata.json
rename to mr/src/test/resources/test-table-to-delete/metadata/v2.metadata.json
diff --git a/mr/src/test/resources/test-table/metadata/version-hint.text b/mr/src/test/resources/test-table-to-delete/metadata/version-hint.text
similarity index 100%
rename from mr/src/test/resources/test-table/metadata/version-hint.text
rename to mr/src/test/resources/test-table-to-delete/metadata/version-hint.text
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java
index a26fe92a12b..6a272745660 100644
--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java
+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java
@@ -26,7 +26,6 @@
 import java.math.BigInteger;
 import java.nio.ByteBuffer;
 import java.nio.ByteOrder;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.UUID;
@@ -231,32 +230,6 @@ public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveTy
           throw new UnsupportedOperationException("Unsupported type: " + primitive);
       }
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    private String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   static class DecimalReader extends ParquetValueReaders.PrimitiveReader<BigDecimal> {
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java
index b76a88de9e1..a900669cd09 100644
--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java
+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroWriter.java
@@ -20,7 +20,6 @@
 package org.apache.iceberg.parquet;
 
 import com.google.common.collect.Lists;
-import java.util.Iterator;
 import java.util.List;
 import org.apache.avro.generic.GenericData.Fixed;
 import org.apache.avro.generic.IndexedRecord;
@@ -163,32 +162,6 @@ public ParquetValueWriter<?> primitive(PrimitiveType primitive) {
           throw new UnsupportedOperationException("Unsupported type: " + primitive);
       }
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    private String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static class FixedWriter extends PrimitiveWriter<Fixed> {
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
index 9d830531482..6b68c84edb7 100644
--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
@@ -160,4 +160,14 @@ public T map(GroupType map, T key, T value) {
   public T primitive(PrimitiveType primitive) {
     return null;
   }
+
+  protected String[] currentPath() {
+    return Lists.newArrayList(fieldNames.descendingIterator()).toArray(new String[0]);
+  }
+
+  protected String[] path(String name) {
+    List<String> list = Lists.newArrayList(fieldNames.descendingIterator());
+    list.add(name);
+    return list.toArray(new String[0]);
+  }
 }
diff --git a/parquet/src/main/java/org/apache/iceberg/parquet/TypeWithSchemaVisitor.java b/parquet/src/main/java/org/apache/iceberg/parquet/TypeWithSchemaVisitor.java
index 7b741c304d9..a13705db9a2 100644
--- a/parquet/src/main/java/org/apache/iceberg/parquet/TypeWithSchemaVisitor.java
+++ b/parquet/src/main/java/org/apache/iceberg/parquet/TypeWithSchemaVisitor.java
@@ -192,4 +192,14 @@ public T primitive(org.apache.iceberg.types.Type.PrimitiveType iPrimitive,
                      PrimitiveType primitive) {
     return null;
   }
+
+  protected String[] currentPath() {
+    return Lists.newArrayList(fieldNames.descendingIterator()).toArray(new String[0]);
+  }
+
+  protected String[] path(String name) {
+    List<String> list = Lists.newArrayList(fieldNames.descendingIterator());
+    list.add(name);
+    return list.toArray(new String[0]);
+  }
 }
diff --git a/pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java b/pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java
index aa3f1dc7d6a..fd9e9ac2dfe 100644
--- a/pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java
+++ b/pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java
@@ -26,7 +26,6 @@
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
 import java.time.temporal.ChronoUnit;
-import java.util.Iterator;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
@@ -273,32 +272,6 @@ public ParquetValueReader<?> primitive(
           throw new UnsupportedOperationException("Unsupported type: " + primitive);
       }
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    protected String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static class DateReader extends PrimitiveReader<String> {
diff --git a/site/docs/presto.md b/site/docs/presto.md
index 24d0e67d489..9886de56157 100644
--- a/site/docs/presto.md
+++ b/site/docs/presto.md
@@ -17,4 +17,4 @@
 
 # Presto
 
-An Iceberg connector for Presto is available in [pull request #458 on prestosql/presto](https://github.com/prestosql/presto/pull/458)
+Iceberg connector is part of Presto SQL's master branch, although support for many features is still under development. Current status of the connector is detailed in the [presto-iceberg README](https://github.com/prestosql/presto/blob/master/presto-iceberg/README.md).
diff --git a/site/docs/spec.md b/site/docs/spec.md
index 24b93602148..9940af84ca6 100644
--- a/site/docs/spec.md
+++ b/site/docs/spec.md
@@ -19,6 +19,17 @@
 
 This is a specification for the Iceberg table format that is designed to manage a large, slow-changing collection of files in a distributed file system or key-value store as a table.
 
+#### Version 1: Analytic Data Tables
+
+**Iceberg format version 1 is the current version**. It defines how to manage large analytic tables using immutable file formats, like Parquet, Avro, and ORC.
+
+#### Version 2: Row-level Deletes
+
+The Iceberg community is currently working on version 2 of the Iceberg format that supports encoding row-level deletes. **The v2 specification is incomplete and may change until it is finished and adopted.** This document includes tentative v2 format requirements, but there are currently no compatibility guarantees with the unfinished v2 spec.
+
+The goal of version 2 is to provide a way to encode row-level deletes. This update can be used to delete or replace individual rows in an immutable data file without rewriting the file.
+
+
 ## Goals
 
 * **Snapshot isolation** -- Reads will be isolated from concurrent writes and always use a committed snapshot of a table’s data. Writes will support removing and adding files in a single operation and are never partially visible. Readers will not acquire locks.
@@ -139,6 +150,7 @@ Data files are stored in manifests with a tuple of partition values that are use
 Tables are configured with a **partition spec** that defines how to produce a tuple of partition values from a record. A partition spec has a list of fields that consist of:
 
 *   A **source column id** from the table’s schema
+*   A **partition field id** that is used to identify a partition field, which is unique within a partition spec. In v2 table metadata, it will be unique across all partition specs.
 *   A **transform** that is applied to the source column to produce a partition value
 *   A **partition name**
 
@@ -334,38 +346,6 @@ Table metadata is stored as JSON. Each table metadata change creates a new table
 
 The atomic operation used to commit metadata depends on how tables are tracked and is not standardized by this spec. See the sections below for examples.
 
-### Delete Format
-
-This section details how to encode row-level deletes in Iceberg metadata. Row-level deletes are not supported in the current format version 1. This part of the spec is not yet complete and will be completed as format version 2.
-
-#### Position-based Delete Files
-
-Position-based delete files identify rows in one or more data files that have been deleted.
-
-Position-based delete files store `file_position_delete`, a struct with the following fields:
-
-| Field id, name          | Type                            | Description                                                                                                              |
-|-------------------------|---------------------------------|--------------------------------------------------------------------------------------------------------------------------|
-| **`1  file_path`**     | `required string`               | The full URI of a data file with FS scheme. This must match the `file_path` of the target data file in a manifest entry.   |
-| **`2  position`**      | `required long`                 | The ordinal position of a deleted row in the target data file identified by `file_path`, starting at `0`.                    |
-
-The rows in the delete file must be sorted by `file_path` then `position` to optimize filtering rows while scanning. 
-
-*  Sorting by `file_path` allows filter pushdown by file in columnar storage formats.
-*  Sorting by `position` allows filtering rows while scanning, to avoid keeping deletes in memory.
- 
-Though the delete files can be written using any supported data file format in Iceberg, it is recommended to write delete files with same file format as the table's file format.
-
-#### Commit Conflict Resolution and Retry
-
-When two commits happen at the same time and are based on the same version, only one commit will succeed. In most cases, the failed commit can be applied to the new current version of table metadata and retried. Updates verify the conditions under which they can be applied to a new version and retry if those conditions are met.
-
-*   Append operations have no requirements and can always be applied.
-*   Replace operations must verify that the files that will be deleted are still in the table. Examples of replace operations include format changes (replace an Avro file with a Parquet file) and compactions (several files are replaced with a single file that contains the same rows).
-*   Delete operations must verify that specific files to delete are still in the table. Delete operations based on expressions can always be applied (e.g., where timestamp < X).
-*   Table schema updates and partition spec changes must validate that the schema has not changed between the base version and the current version.
-
-
 #### Table Metadata Fields
 
 Table metadata consists of the following fields:
@@ -390,6 +370,16 @@ Table metadata consists of the following fields:
 For serialization details, see Appendix C.
 
 
+#### Commit Conflict Resolution and Retry
+
+When two commits happen at the same time and are based on the same version, only one commit will succeed. In most cases, the failed commit can be applied to the new current version of table metadata and retried. Updates verify the conditions under which they can be applied to a new version and retry if those conditions are met.
+
+*   Append operations have no requirements and can always be applied.
+*   Replace operations must verify that the files that will be deleted are still in the table. Examples of replace operations include format changes (replace an Avro file with a Parquet file) and compactions (several files are replaced with a single file that contains the same rows).
+*   Delete operations must verify that specific files to delete are still in the table. Delete operations based on expressions can always be applied (e.g., where timestamp < X).
+*   Table schema updates and partition spec changes must validate that the schema has not changed between the base version and the current version.
+
+
 #### File System Tables
 
 An atomic swap can be implemented using atomic rename in file systems that support it, like HDFS or most local file systems [1].
@@ -423,6 +413,30 @@ Notes:
 
 1. The metastore table scheme is partly implemented in [BaseMetastoreTableOperations](/javadoc/master/index.html?org/apache/iceberg/BaseMetastoreTableOperations.html).
 
+
+### Delete Formats
+
+This section details how to encode row-level deletes in Iceberg metadata. Row-level deletes are not supported in the current format version 1. This part of the spec is not yet complete and will be completed as format version 2.
+
+#### Position-based Delete Files
+
+Position-based delete files identify rows in one or more data files that have been deleted.
+
+Position-based delete files store `file_position_delete`, a struct with the following fields:
+
+| Field id, name          | Type                            | Description                                                                                                              |
+|-------------------------|---------------------------------|--------------------------------------------------------------------------------------------------------------------------|
+| **`1  file_path`**     | `required string`               | The full URI of a data file with FS scheme. This must match the `file_path` of the target data file in a manifest entry.   |
+| **`2  position`**      | `required long`                 | The ordinal position of a deleted row in the target data file identified by `file_path`, starting at `0`.                    |
+
+The rows in the delete file must be sorted by `file_path` then `position` to optimize filtering rows while scanning. 
+
+*  Sorting by `file_path` allows filter pushdown by file in columnar storage formats.
+*  Sorting by `position` allows filtering rows while scanning, to avoid keeping deletes in memory.
+
+Though the delete files can be written using any supported data file format in Iceberg, it is recommended to write delete files with same file format as the table's file format.
+
+
 ## Appendix A: Format-specific Requirements
 
 
@@ -615,7 +629,7 @@ Partition specs are serialized as a JSON object with the following fields:
 |Field|JSON representation|Example|
 |--- |--- |--- |
 |**`spec-id`**|`JSON int`|`0`|
-|**`fields`**|`JSON list: [`<br />&nbsp;&nbsp;`<partition field JSON>,`<br />&nbsp;&nbsp;`...`<br />`]`|`[ {`<br />&nbsp;&nbsp;`"source-id": 4,`<br />&nbsp;&nbsp;`"name": "ts_day",`<br />&nbsp;&nbsp;`"transform": "day"`<br />`}, {`<br />&nbsp;&nbsp;`"source-id": 1,`<br />&nbsp;&nbsp;`"name": "id_bucket",`<br />&nbsp;&nbsp;`"transform": "bucket[16]"`<br />`} ]`|
+|**`fields`**|`JSON list: [`<br />&nbsp;&nbsp;`<partition field JSON>,`<br />&nbsp;&nbsp;`...`<br />`]`|`[ {`<br />&nbsp;&nbsp;`"source-id": 4,`<br />&nbsp;&nbsp;`"field-id": 1000,`<br />&nbsp;&nbsp;`"name": "ts_day",`<br />&nbsp;&nbsp;`"transform": "day"`<br />`}, {`<br />&nbsp;&nbsp;`"source-id": 1,`<br />&nbsp;&nbsp;`"field-id": 1001,`<br />&nbsp;&nbsp;`"name": "id_bucket",`<br />&nbsp;&nbsp;`"transform": "bucket[16]"`<br />`} ]`|
 
 Each partition field in the fields list is stored as an object. See the table for more detail:
 
@@ -628,7 +642,7 @@ Each partition field in the fields list is stored as an object. See the table fo
 |**`month`**|`JSON string: "month"`|`"month"`|
 |**`day`**|`JSON string: "day"`|`"day"`|
 |**`hour`**|`JSON string: "hour"`|`"hour"`|
-|**`Partition Field`**|`JSON object: {`<br />&nbsp;&nbsp;`"source-id": <id int>,`<br />&nbsp;&nbsp;`"name": <name string>,`<br />&nbsp;&nbsp;`"transform": <transform JSON>`<br />`}`|`{`<br />&nbsp;&nbsp;`"source-id": 1,`<br />&nbsp;&nbsp;`"name": "id_bucket",`<br />&nbsp;&nbsp;`"transform": "bucket[16]"`<br />`}`|
+|**`Partition Field`**|`JSON object: {`<br />&nbsp;&nbsp;`"source-id": <id int>,`<br />&nbsp;&nbsp;`"field-id": <field id int>,`<br />&nbsp;&nbsp;`"name": <name string>,`<br />&nbsp;&nbsp;`"transform": <transform JSON>`<br />`}`|`{`<br />&nbsp;&nbsp;`"source-id": 1,`<br />&nbsp;&nbsp;`"field-id": 1000,`<br />&nbsp;&nbsp;`"name": "id_bucket",`<br />&nbsp;&nbsp;`"transform": "bucket[16]"`<br />`}`|
 
 In some cases partition specs are stored using only the field list instead of the object format that includes the spec ID, like the deprecated `partition-spec` field in table metadata. The object format should be used unless otherwise noted in this spec.
 
diff --git a/spark-runtime/DISCLAIMER b/spark-runtime/DISCLAIMER
new file mode 100644
index 00000000000..f33cec3dee4
--- /dev/null
+++ b/spark-runtime/DISCLAIMER
@@ -0,0 +1,10 @@
+Apache Iceberg is an effort undergoing incubation at the Apache Software
+Foundation (ASF), sponsored by the Apache Incubator PMC.
+
+Incubation is required of all newly accepted projects until a further review
+indicates that the infrastructure, communications, and decision making process
+have stabilized in a manner consistent with other successful ASF projects.
+
+While incubation status is not necessarily a reflection of the completeness
+or stability of the code, it does indicate that the project has yet to be
+fully endorsed by the ASF.
diff --git a/spark-runtime/LICENSE b/spark-runtime/LICENSE
index 2e38567d795..e5fa87dd6e7 100644
--- a/spark-runtime/LICENSE
+++ b/spark-runtime/LICENSE
@@ -296,6 +296,57 @@ License: http://www.apache.org/licenses/LICENSE-2.0
 
 --------------------------------------------------------------------------------
 
+This binary artifact contains Apache Hive's storage API via ORC.
+
+Copyright: 2013-2019 The Apache Software Foundation.
+Home page: https://hive.apache.org/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains Google protobuf via ORC.
+
+Copyright: 2008 Google Inc.
+Home page: https://developers.google.com/protocol-buffers
+License: https://github.com/protocolbuffers/protobuf/blob/master/LICENSE (BSD)
+
+License text:
+
+| Copyright 2008 Google Inc.  All rights reserved.
+|
+| Redistribution and use in source and binary forms, with or without
+| modification, are permitted provided that the following conditions are
+| met:
+|
+|     * Redistributions of source code must retain the above copyright
+| notice, this list of conditions and the following disclaimer.
+|     * Redistributions in binary form must reproduce the above
+| copyright notice, this list of conditions and the following disclaimer
+| in the documentation and/or other materials provided with the
+| distribution.
+|     * Neither the name of Google Inc. nor the names of its
+| contributors may be used to endorse or promote products derived from
+| this software without specific prior written permission.
+|
+| THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+| "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+| LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+| A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+| OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+| SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+| LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+| DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+| THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+| (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+| OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+|
+| Code generated by the Protocol Buffer compiler is owned by the owner
+| of the input file used when generating it.  This code is not
+| standalone and requires a support library to be linked with it.  This
+| support library is itself covered by the above license.
+
+--------------------------------------------------------------------------------
+
 This binary artifact contains Airlift Aircompressor.
 
 Copyright: 2011-2019 Aircompressor authors.
@@ -304,6 +355,22 @@ License: http://www.apache.org/licenses/LICENSE-2.0
 
 --------------------------------------------------------------------------------
 
+This binary artifact contains Airlift Slice.
+
+Copyright: 2013-2019 Slice authors.
+Home page: https://github.com/airlift/slice
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains JetBrains annotations.
+
+Copyright: 2000-2020 JetBrains s.r.o.
+Home page: https://github.com/JetBrains/java-annotations
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
 This binary artifact contains code from Cloudera Kite.
 
 Copyright: 2013-2017 Cloudera Inc.
@@ -426,3 +493,51 @@ Copyright: 2014-2019 Ben Manes and contributors
 Home page: https://github.com/ben-manes/caffeine
 License: http://www.apache.org/licenses/LICENSE-2.0
 
+--------------------------------------------------------------------------------
+
+This binary artifact contains Apache Arrow.
+
+Copyright: 2016-2019 The Apache Software Foundation.
+Home page: https://arrow.apache.org/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains Netty's buffer library.
+
+Copyright: 2014-2020 The Netty Project
+Home page: https://netty.io/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains Google FlatBuffers.
+
+Copyright: 2013-2020 Google Inc.
+Home page: https://google.github.io/flatbuffers/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains Carrot Search Labs HPPC.
+
+Copyright: 2002-2019 Carrot Search s.c.
+Home page: http://labs.carrotsearch.com/hppc.html
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains code from Apache Lucene via Carrot Search HPPC.
+
+Copyright: 2011-2020 The Apache Software Foundation.
+Home page: https://lucene.apache.org/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
+--------------------------------------------------------------------------------
+
+This binary artifact contains Apache Yetus audience annotations.
+
+Copyright: 2008-2020 The Apache Software Foundation.
+Home page: https://yetus.apache.org/
+License: http://www.apache.org/licenses/LICENSE-2.0
+
diff --git a/spark-runtime/NOTICE b/spark-runtime/NOTICE
index 8b631b478c0..d76bea81e51 100644
--- a/spark-runtime/NOTICE
+++ b/spark-runtime/NOTICE
@@ -42,7 +42,7 @@ This binary artifact includes Apache ORC with the following in its NOTICE file:
 This binary artifact includes Airlift Aircompressor with the following in its
 NOTICE file:
 
-| Snappy Copyright Notices 
+| Snappy Copyright Notices
 | =========================
 |
 | * Copyright 2011 Dain Sundstrom <dain@iq80.com>
@@ -79,3 +79,417 @@ NOTICE file:
 | THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 | (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 | OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+--------------------------------------------------------------------------------
+
+This binary artifact includes Carrot Search Labs HPPC with the following in its
+NOTICE file:
+
+| ACKNOWLEDGEMENT
+| ===============
+|
+| HPPC borrowed code, ideas or both from:
+|
+|  * Apache Lucene, http://lucene.apache.org/
+|    (Apache license)
+|  * Fastutil, http://fastutil.di.unimi.it/
+|    (Apache license)
+|  * Koloboke, https://github.com/OpenHFT/Koloboke
+|    (Apache license)
+
+--------------------------------------------------------------------------------
+
+This binary artifact includes Apache Yetus with the following in its NOTICE
+file:
+
+| Apache Yetus
+| Copyright 2008-2020 The Apache Software Foundation
+|
+| This product includes software developed at
+| The Apache Software Foundation (https://www.apache.org/).
+|
+| ---
+| Additional licenses for the Apache Yetus Source/Website:
+| ---
+|
+|
+| See LICENSE for terms.
+
+--------------------------------------------------------------------------------
+
+This binary artifact includes Google Protobuf with the following copyright
+notice:
+
+| Copyright 2008 Google Inc.  All rights reserved.
+|
+| Redistribution and use in source and binary forms, with or without
+| modification, are permitted provided that the following conditions are
+| met:
+|
+|     * Redistributions of source code must retain the above copyright
+| notice, this list of conditions and the following disclaimer.
+|     * Redistributions in binary form must reproduce the above
+| copyright notice, this list of conditions and the following disclaimer
+| in the documentation and/or other materials provided with the
+| distribution.
+|     * Neither the name of Google Inc. nor the names of its
+| contributors may be used to endorse or promote products derived from
+| this software without specific prior written permission.
+|
+| THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+| "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+| LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+| A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+| OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+| SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+| LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+| DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+| THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+| (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+| OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+|
+| Code generated by the Protocol Buffer compiler is owned by the owner
+| of the input file used when generating it.  This code is not
+| standalone and requires a support library to be linked with it.  This
+| support library is itself covered by the above license.
+
+This binary artifact includes Apache Arrow with the following in its NOTICE file:
+
+| Apache Arrow
+| Copyright 2016-2019 The Apache Software Foundation
+|
+| This product includes software developed at
+| The Apache Software Foundation (http://www.apache.org/).
+|
+| This product includes software from the SFrame project (BSD, 3-clause).
+| * Copyright (C) 2015 Dato, Inc.
+| * Copyright (c) 2009 Carnegie Mellon University.
+|
+| This product includes software from the Feather project (Apache 2.0)
+| https://github.com/wesm/feather
+|
+| This product includes software from the DyND project (BSD 2-clause)
+| https://github.com/libdynd
+|
+| This product includes software from the LLVM project
+|  * distributed under the University of Illinois Open Source
+|
+| This product includes software from the google-lint project
+|  * Copyright (c) 2009 Google Inc. All rights reserved.
+|
+| This product includes software from the mman-win32 project
+|  * Copyright https://code.google.com/p/mman-win32/
+|  * Licensed under the MIT License;
+|
+| This product includes software from the LevelDB project
+|  * Copyright (c) 2011 The LevelDB Authors. All rights reserved.
+|  * Use of this source code is governed by a BSD-style license that can be
+|  * Moved from Kudu http://github.com/cloudera/kudu
+|
+| This product includes software from the CMake project
+|  * Copyright 2001-2009 Kitware, Inc.
+|  * Copyright 2012-2014 Continuum Analytics, Inc.
+|  * All rights reserved.
+|
+| This product includes software from https://github.com/matthew-brett/multibuild (BSD 2-clause)
+|  * Copyright (c) 2013-2016, Matt Terry and Matthew Brett; all rights reserved.
+|
+| This product includes software from the Ibis project (Apache 2.0)
+|  * Copyright (c) 2015 Cloudera, Inc.
+|  * https://github.com/cloudera/ibis
+|
+| This product includes software from Dremio (Apache 2.0)
+|   * Copyright (C) 2017-2018 Dremio Corporation
+|   * https://github.com/dremio/dremio-oss
+|
+| This product includes software from Google Guava (Apache 2.0)
+|   * Copyright (C) 2007 The Guava Authors
+|   * https://github.com/google/guava
+|
+| This product include software from CMake (BSD 3-Clause)
+|   * CMake - Cross Platform Makefile Generator
+|   * Copyright 2000-2019 Kitware, Inc. and Contributors
+|
+| The web site includes files generated by Jekyll.
+|
+| --------------------------------------------------------------------------------
+|
+| This product includes code from Apache Kudu, which includes the following in
+| its NOTICE file:
+|
+|   Apache Kudu
+|   Copyright 2016 The Apache Software Foundation
+|
+|   This product includes software developed at
+|   The Apache Software Foundation (http://www.apache.org/).
+|
+|   Portions of this software were developed at
+|   Cloudera, Inc (http://www.cloudera.com/).
+|
+| --------------------------------------------------------------------------------
+|
+| This product includes code from Apache ORC, which includes the following in
+| its NOTICE file:
+|
+|   Apache ORC
+|   Copyright 2013-2019 The Apache Software Foundation
+|
+|   This product includes software developed by The Apache Software
+|   Foundation (http://www.apache.org/).
+|
+|   This product includes software developed by Hewlett-Packard:
+|   (c) Copyright [2014-2015] Hewlett-Packard Development Company, L.P
+
+--------------------------------------------------------------------------------
+
+This binary artifact includes Netty buffers with the following in its NOTICE
+file:
+
+|                             The Netty Project
+|                             =================
+|
+| Please visit the Netty web site for more information:
+|
+|   * https://netty.io/
+|
+| Copyright 2014 The Netty Project
+|
+| The Netty Project licenses this file to you under the Apache License,
+| version 2.0 (the "License"); you may not use this file except in compliance
+| with the License. You may obtain a copy of the License at:
+|
+|   http://www.apache.org/licenses/LICENSE-2.0
+|
+| Unless required by applicable law or agreed to in writing, software
+| distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+| WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+| License for the specific language governing permissions and limitations
+| under the License.
+|
+| Also, please refer to each LICENSE.<component>.txt file, which is located in
+| the 'license' directory of the distribution file, for the license terms of the
+| components that this product depends on.
+|
+| -------------------------------------------------------------------------------
+| This product contains the extensions to Java Collections Framework which has
+| been derived from the works by JSR-166 EG, Doug Lea, and Jason T. Greene:
+|
+|   * LICENSE:
+|     * license/LICENSE.jsr166y.txt (Public Domain)
+|   * HOMEPAGE:
+|     * http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/
+|     * http://viewvc.jboss.org/cgi-bin/viewvc.cgi/jbosscache/experimental/jsr166/
+|
+| This product contains a modified version of Robert Harder's Public Domain
+| Base64 Encoder and Decoder, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.base64.txt (Public Domain)
+|   * HOMEPAGE:
+|     * http://iharder.sourceforge.net/current/java/base64/
+|
+| This product contains a modified portion of 'Webbit', an event based
+| WebSocket and HTTP server, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.webbit.txt (BSD License)
+|   * HOMEPAGE:
+|     * https://github.com/joewalnes/webbit
+|
+| This product contains a modified portion of 'SLF4J', a simple logging
+| facade for Java, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.slf4j.txt (MIT License)
+|   * HOMEPAGE:
+|     * http://www.slf4j.org/
+|
+| This product contains a modified portion of 'Apache Harmony', an open source
+| Java SE, which can be obtained at:
+|
+|   * NOTICE:
+|     * license/NOTICE.harmony.txt
+|   * LICENSE:
+|     * license/LICENSE.harmony.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * http://archive.apache.org/dist/harmony/
+|
+| This product contains a modified portion of 'jbzip2', a Java bzip2 compression
+| and decompression library written by Matthew J. Francis. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.jbzip2.txt (MIT License)
+|   * HOMEPAGE:
+|     * https://code.google.com/p/jbzip2/
+|
+| This product contains a modified portion of 'libdivsufsort', a C API library to construct
+| the suffix array and the Burrows-Wheeler transformed string for any input string of
+| a constant-size alphabet written by Yuta Mori. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.libdivsufsort.txt (MIT License)
+|   * HOMEPAGE:
+|     * https://github.com/y-256/libdivsufsort
+|
+| This product contains a modified portion of Nitsan Wakart's 'JCTools', Java Concurrency Tools for the JVM,
+|  which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.jctools.txt (ASL2 License)
+|   * HOMEPAGE:
+|     * https://github.com/JCTools/JCTools
+|
+| This product optionally depends on 'JZlib', a re-implementation of zlib in
+| pure Java, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.jzlib.txt (BSD style License)
+|   * HOMEPAGE:
+|     * http://www.jcraft.com/jzlib/
+|
+| This product optionally depends on 'Compress-LZF', a Java library for encoding and
+| decoding data in LZF format, written by Tatu Saloranta. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.compress-lzf.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/ning/compress
+|
+| This product optionally depends on 'lz4', a LZ4 Java compression
+| and decompression library written by Adrien Grand. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.lz4.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/jpountz/lz4-java
+|
+| This product optionally depends on 'lzma-java', a LZMA Java compression
+| and decompression library, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.lzma-java.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/jponge/lzma-java
+|
+| This product contains a modified portion of 'jfastlz', a Java port of FastLZ compression
+| and decompression library written by William Kinney. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.jfastlz.txt (MIT License)
+|   * HOMEPAGE:
+|     * https://code.google.com/p/jfastlz/
+|
+| This product contains a modified portion of and optionally depends on 'Protocol Buffers', Google's data
+| interchange format, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.protobuf.txt (New BSD License)
+|   * HOMEPAGE:
+|     * https://github.com/google/protobuf
+|
+| This product optionally depends on 'Bouncy Castle Crypto APIs' to generate
+| a temporary self-signed X.509 certificate when the JVM does not provide the
+| equivalent functionality.  It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.bouncycastle.txt (MIT License)
+|   * HOMEPAGE:
+|     * http://www.bouncycastle.org/
+|
+| This product optionally depends on 'Snappy', a compression library produced
+| by Google Inc, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.snappy.txt (New BSD License)
+|   * HOMEPAGE:
+|     * https://github.com/google/snappy
+|
+| This product optionally depends on 'JBoss Marshalling', an alternative Java
+| serialization API, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.jboss-marshalling.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/jboss-remoting/jboss-marshalling
+|
+| This product optionally depends on 'Caliper', Google's micro-
+| benchmarking framework, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.caliper.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/google/caliper
+|
+| This product optionally depends on 'Apache Commons Logging', a logging
+| framework, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.commons-logging.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * http://commons.apache.org/logging/
+|
+| This product optionally depends on 'Apache Log4J', a logging framework, which
+| can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.log4j.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * http://logging.apache.org/log4j/
+|
+| This product optionally depends on 'Aalto XML', an ultra-high performance
+| non-blocking XML processor, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.aalto-xml.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * http://wiki.fasterxml.com/AaltoHome
+|
+| This product contains a modified version of 'HPACK', a Java implementation of
+| the HTTP/2 HPACK algorithm written by Twitter. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.hpack.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/twitter/hpack
+|
+| This product contains a modified version of 'HPACK', a Java implementation of
+| the HTTP/2 HPACK algorithm written by Cory Benfield. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.hyper-hpack.txt (MIT License)
+|   * HOMEPAGE:
+|     * https://github.com/python-hyper/hpack/
+|
+| This product contains a modified version of 'HPACK', a Java implementation of
+| the HTTP/2 HPACK algorithm written by Tatsuhiro Tsujikawa. It can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.nghttp2-hpack.txt (MIT License)
+|   * HOMEPAGE:
+|     * https://github.com/nghttp2/nghttp2/
+|
+| This product contains a modified portion of 'Apache Commons Lang', a Java library
+| provides utilities for the java.lang API, which can be obtained at:
+|
+|   * LICENSE:
+|     * license/LICENSE.commons-lang.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://commons.apache.org/proper/commons-lang/
+|
+|
+| This product contains the Maven wrapper scripts from 'Maven Wrapper', that provides an easy way to ensure a user has everything necessary to run the Maven build.
+|
+|   * LICENSE:
+|     * license/LICENSE.mvn-wrapper.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * https://github.com/takari/maven-wrapper
+|
+| This product contains the dnsinfo.h header file, that provides a way to retrieve the system DNS configuration on MacOS.
+| This private header is also used by Apple's open source
+|  mDNSResponder (https://opensource.apple.com/tarballs/mDNSResponder/).
+|
+|  * LICENSE:
+|     * license/LICENSE.dnsinfo.txt (Apache License 2.0)
+|   * HOMEPAGE:
+|     * http://www.opensource.apple.com/source/configd/configd-453.19/dnsinfo/dnsinfo.h
+
diff --git a/spark/src/main/java/org/apache/iceberg/actions/Actions.java b/spark/src/main/java/org/apache/iceberg/actions/Actions.java
index 549d541b29f..f2d386d213e 100644
--- a/spark/src/main/java/org/apache/iceberg/actions/Actions.java
+++ b/spark/src/main/java/org/apache/iceberg/actions/Actions.java
@@ -43,4 +43,8 @@ public static Actions forTable(Table table) {
   public RemoveOrphanFilesAction removeOrphanFiles() {
     return new RemoveOrphanFilesAction(spark, table);
   }
+
+  public RewriteManifestsAction rewriteManifests() {
+    return new RewriteManifestsAction(spark, table);
+  }
 }
diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java
new file mode 100644
index 00000000000..67d971422fa
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java
@@ -0,0 +1,40 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+import org.apache.iceberg.MetadataTableType;
+import org.apache.iceberg.Table;
+
+abstract class BaseAction<R> implements Action<R> {
+
+  protected abstract Table table();
+
+  protected String metadataTableName(MetadataTableType type) {
+    String tableName = table().toString();
+    if (tableName.contains("/")) {
+      return tableName + "#" + type;
+    } else if (tableName.startsWith("hadoop.") || tableName.startsWith("hive.")) {
+      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4
+      return tableName.replaceFirst("(hadoop\\.)|(hive\\.)", "") + "." + type;
+    } else {
+      return tableName + "." + type;
+    }
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/actions/BaseSnapshotUpdateAction.java b/spark/src/main/java/org/apache/iceberg/actions/BaseSnapshotUpdateAction.java
new file mode 100644
index 00000000000..b9a8c949b1a
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/actions/BaseSnapshotUpdateAction.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+import com.google.common.collect.Maps;
+import java.util.Map;
+import org.apache.iceberg.SnapshotUpdate;
+
+abstract class BaseSnapshotUpdateAction<ThisT, R> extends BaseAction<R> implements SnapshotUpdateAction<ThisT, R> {
+
+  private final Map<String, String> summary = Maps.newHashMap();
+
+  protected abstract ThisT self();
+
+  @Override
+  public ThisT set(String property, String value) {
+    summary.put(property, value);
+    return self();
+  }
+
+  protected void commit(SnapshotUpdate<?> update) {
+    summary.forEach(update::set);
+    update.commit();
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java b/spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java
index 8f10812c524..b782f0839bd 100644
--- a/spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java
+++ b/spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java
@@ -66,7 +66,7 @@
  * <em>Note:</em> It is dangerous to call this action with a short retention interval as it might corrupt
  * the state of the table if another operation is writing at the same time.
  */
-public class RemoveOrphanFilesAction implements Action<List<String>> {
+public class RemoveOrphanFilesAction extends BaseAction<List<String>> {
 
   private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);
 
@@ -96,6 +96,11 @@ public void accept(String file) {
     this.location = table.location();
   }
 
+  @Override
+  protected Table table() {
+    return table;
+  }
+
   /**
    * Removes orphan files in the given location.
    *
@@ -250,18 +255,6 @@ private static void listDirRecursively(
     }
   }
 
-  private String metadataTableName(MetadataTableType type) {
-    String tableName = table.toString();
-    if (tableName.contains("/")) {
-      return tableName + "#" + type;
-    } else if (tableName.startsWith("hadoop.") || tableName.startsWith("hive.")) {
-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4
-      return tableName.replaceFirst("(hadoop\\.)|(hive\\.)", "") + "." + type;
-    } else {
-      return tableName + "." + type;
-    }
-  }
-
   private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(
       Broadcast<SerializableConfiguration> conf,
       long olderThanTimestamp) {
@@ -272,13 +265,17 @@ private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(
 
       Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;
 
-      int maxDepth = Integer.MAX_VALUE;
+      int maxDepth = 2000;
       int maxDirectSubDirs = Integer.MAX_VALUE;
 
       dirs.forEachRemaining(dir -> {
         listDirRecursively(dir, predicate, conf.value().value(), maxDepth, maxDirectSubDirs, subDirs, files);
       });
 
+      if (!subDirs.isEmpty()) {
+        throw new RuntimeException("Could not list subdirectories, reached maximum subdirectory depth: " + maxDepth);
+      }
+
       return files.iterator();
     };
   }
diff --git a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java
new file mode 100644
index 00000000000..b8f88ea54e6
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java
@@ -0,0 +1,390 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+import java.util.UUID;
+import java.util.function.Function;
+import java.util.function.Predicate;
+import java.util.stream.Collectors;
+import org.apache.hadoop.fs.Path;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.FileFormat;
+import org.apache.iceberg.HasTableOperations;
+import org.apache.iceberg.ManifestFile;
+import org.apache.iceberg.ManifestFiles;
+import org.apache.iceberg.ManifestWriter;
+import org.apache.iceberg.MetadataTableType;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.RewriteManifests;
+import org.apache.iceberg.Snapshot;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableOperations;
+import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.exceptions.ValidationException;
+import org.apache.iceberg.hadoop.HadoopFileIO;
+import org.apache.iceberg.io.FileIO;
+import org.apache.iceberg.io.OutputFile;
+import org.apache.iceberg.spark.SparkDataFile;
+import org.apache.iceberg.types.Types;
+import org.apache.iceberg.util.PropertyUtil;
+import org.apache.iceberg.util.Tasks;
+import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.api.java.function.MapFunction;
+import org.apache.spark.api.java.function.MapPartitionsFunction;
+import org.apache.spark.broadcast.Broadcast;
+import org.apache.spark.sql.Column;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoder;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SparkSession;
+import org.apache.spark.sql.internal.SQLConf;
+import org.apache.spark.sql.types.StructType;
+import org.apache.spark.util.SerializableConfiguration;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.
+ * <p>
+ * By default, this action rewrites all manifests for the current partition spec and writes the result
+ * to the metadata folder. The behavior can be modified by passing a custom predicate to {@link #rewriteIf(Predicate)}
+ * and a custom spec id to {@link #specId(int)}. In addition, there is a way to configure a custom location
+ * for new manifests via {@link #stagingLocation}.
+ */
+public class RewriteManifestsAction
+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {
+
+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);
+
+  private final SparkSession spark;
+  private final JavaSparkContext sparkContext;
+  private final Encoder<ManifestFile> manifestEncoder;
+  private final Table table;
+  private final int formatVersion;
+  private final FileIO fileIO;
+  private final long targetManifestSizeBytes;
+
+  private PartitionSpec spec = null;
+  private Predicate<ManifestFile> predicate = manifest -> true;
+  private String stagingLocation = null;
+  private boolean useCaching = true;
+
+  RewriteManifestsAction(SparkSession spark, Table table) {
+    this.spark = spark;
+    this.sparkContext = new JavaSparkContext(spark.sparkContext());
+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);
+    this.table = table;
+    this.spec = table.spec();
+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(
+        table.properties(),
+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,
+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);
+
+    if (table.io() instanceof HadoopFileIO) {
+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization
+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());
+      this.fileIO = new HadoopFileIO(conf::value);
+    } else {
+      this.fileIO = table.io();
+    }
+
+    // default the staging location to the metadata location
+    TableOperations ops = ((HasTableOperations) table).operations();
+    Path metadataFilePath = new Path(ops.metadataFileLocation("file"));
+    this.stagingLocation = metadataFilePath.getParent().toString();
+
+    // use the current table format version for new manifests
+    this.formatVersion = ops.current().formatVersion();
+  }
+
+  @Override
+  protected RewriteManifestsAction self() {
+    return this;
+  }
+
+  @Override
+  protected Table table() {
+    return table;
+  }
+
+  public RewriteManifestsAction specId(int specId) {
+    Preconditions.checkArgument(table.specs().containsKey(specId), "Invalid spec id %d", specId);
+    this.spec = table.specs().get(specId);
+    return this;
+  }
+
+  /**
+   * Rewrites only manifests that match the given predicate.
+   *
+   * @param newPredicate a predicate
+   * @return this for method chaining
+   */
+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {
+    this.predicate = newPredicate;
+    return this;
+  }
+
+  /**
+   * Passes a location where the manifests should be written.
+   *
+   * @param newStagingLocation a staging location
+   * @return this for method chaining
+   */
+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {
+    this.stagingLocation = newStagingLocation;
+    return this;
+  }
+
+  /**
+   * Configures whether the action should cache manifest entries used in multiple jobs.
+   *
+   * @param newUseCaching a flag whether to use caching
+   * @return this for method chaining
+   */
+  public RewriteManifestsAction useCaching(boolean newUseCaching) {
+    this.useCaching = newUseCaching;
+    return this;
+  }
+
+  @Override
+  public RewriteManifestsActionResult execute() {
+    List<ManifestFile> matchingManifests = findMatchingManifests();
+    if (matchingManifests.isEmpty()) {
+      return RewriteManifestsActionResult.empty();
+    }
+
+    long totalSizeBytes = 0L;
+    int numEntries = 0;
+
+    for (ManifestFile manifest : matchingManifests) {
+      ValidationException.check(hasFileCounts(manifest), "No file counts in manifest: " + manifest.path());
+
+      totalSizeBytes += manifest.length();
+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();
+    }
+
+    int targetNumManifests = targetNumManifests(totalSizeBytes);
+    int targetNumManifestEntries = targetNumManifestEntries(numEntries, targetNumManifests);
+
+    Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);
+
+    List<ManifestFile> newManifests;
+    if (spec.fields().size() < 1) {
+      newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);
+    } else {
+      newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests, targetNumManifestEntries);
+    }
+
+    replaceManifests(matchingManifests, newManifests);
+
+    return new RewriteManifestsActionResult(matchingManifests, newManifests);
+  }
+
+  private Dataset<Row> buildManifestEntryDF(List<ManifestFile> manifests) {
+    Dataset<Row> manifestDF = spark
+        .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())
+        .toDF("manifest");
+
+    String entriesMetadataTable = metadataTableName(MetadataTableType.ENTRIES);
+    Dataset<Row> manifestEntryDF = spark.read().format("iceberg")
+        .load(entriesMetadataTable)
+        .filter("status < 2") // select only live entries
+        .selectExpr("input_file_name() as manifest", "snapshot_id", "sequence_number", "data_file");
+
+    Column joinCond = manifestDF.col("manifest").equalTo(manifestEntryDF.col("manifest"));
+    return manifestEntryDF
+        .join(manifestDF, joinCond, "left_semi")
+        .select("snapshot_id", "sequence_number", "data_file");
+  }
+
+  private List<ManifestFile> writeManifestsForUnpartitionedTable(Dataset<Row> manifestEntryDF, int numManifests) {
+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);
+    StructType sparkType = (StructType) manifestEntryDF.schema().apply("data_file").dataType();
+
+    // we rely only on the target number of manifests for unpartitioned tables
+    // as we should not worry about having too much metadata per partition
+    long maxNumManifestEntries = Long.MAX_VALUE;
+
+    return manifestEntryDF
+        .repartition(numManifests)
+        .mapPartitions(
+            toManifests(io, maxNumManifestEntries, stagingLocation, formatVersion, spec, sparkType),
+            manifestEncoder
+        )
+        .collectAsList();
+  }
+
+  private List<ManifestFile> writeManifestsForPartitionedTable(
+      Dataset<Row> manifestEntryDF, int numManifests,
+      int targetNumManifestEntries) {
+
+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);
+    StructType sparkType = (StructType) manifestEntryDF.schema().apply("data_file").dataType();
+
+    // we allow the actual size of manifests to be 10% higher if the estimation is not precise enough
+    long maxNumManifestEntries = (long) (1.1 * targetNumManifestEntries);
+
+    return withReusableDS(manifestEntryDF, df -> {
+      Column partitionColumn = df.col("data_file.partition");
+      return df.repartitionByRange(numManifests, partitionColumn)
+          .sortWithinPartitions(partitionColumn)
+          .mapPartitions(
+              toManifests(io, maxNumManifestEntries, stagingLocation, formatVersion, spec, sparkType),
+              manifestEncoder
+          )
+          .collectAsList();
+    });
+  }
+
+  private <T, U> U withReusableDS(Dataset<T> ds, Function<Dataset<T>, U> func) {
+    Dataset<T> reusableDS;
+    if (useCaching) {
+      reusableDS = ds.cache();
+    } else {
+      int parallelism = SQLConf.get().numShufflePartitions();
+      reusableDS = ds.repartition(parallelism).map((MapFunction<T, T>) value -> value, ds.exprEnc());
+    }
+
+    try {
+      return func.apply(reusableDS);
+    } finally {
+      if (useCaching) {
+        reusableDS.unpersist(false);
+      }
+    }
+  }
+
+  private List<ManifestFile> findMatchingManifests() {
+    Snapshot currentSnapshot = table.currentSnapshot();
+
+    if (currentSnapshot == null) {
+      return ImmutableList.of();
+    }
+
+    return currentSnapshot.manifests().stream()
+        .filter(manifest -> manifest.partitionSpecId() == spec.specId() && predicate.test(manifest))
+        .collect(Collectors.toList());
+  }
+
+  private int targetNumManifests(long totalSizeBytes) {
+    return (int) ((totalSizeBytes + targetManifestSizeBytes - 1) / targetManifestSizeBytes);
+  }
+
+  private int targetNumManifestEntries(int numEntries, int numManifests) {
+    return (numEntries + numManifests - 1) / numManifests;
+  }
+
+  private boolean hasFileCounts(ManifestFile manifest) {
+    return manifest.addedFilesCount() != null &&
+        manifest.existingFilesCount() != null &&
+        manifest.deletedFilesCount() != null;
+  }
+
+  private void replaceManifests(Iterable<ManifestFile> deletedManifests, Iterable<ManifestFile> addedManifests) {
+    try {
+      boolean snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(
+          table.properties(),
+          TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,
+          TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);
+
+      RewriteManifests rewriteManifests = table.rewriteManifests();
+      deletedManifests.forEach(rewriteManifests::deleteManifest);
+      addedManifests.forEach(rewriteManifests::addManifest);
+      commit(rewriteManifests);
+
+      if (!snapshotIdInheritanceEnabled) {
+        // delete new manifests as they were rewritten before the commit
+        deleteFiles(Iterables.transform(addedManifests, ManifestFile::path));
+      }
+    } catch (Exception e) {
+      // delete all new manifests because the rewrite failed
+      deleteFiles(Iterables.transform(addedManifests, ManifestFile::path));
+      throw e;
+    }
+  }
+
+  private void deleteFiles(Iterable<String> locations) {
+    Tasks.foreach(locations)
+        .noRetry()
+        .suppressFailureWhenFinished()
+        .onFailure((location, exc) -> LOG.warn("Failed to delete: {}", location, exc))
+        .run(fileIO::deleteFile);
+  }
+
+  private static ManifestFile writeManifest(
+      List<Row> rows, int startIndex, int endIndex, Broadcast<FileIO> io,
+      String location, int format, PartitionSpec spec, StructType sparkType) throws IOException {
+
+    String manifestName = "optimized-m-" + UUID.randomUUID();
+    Path manifestPath = new Path(location, manifestName);
+    OutputFile outputFile = io.value().newOutputFile(FileFormat.AVRO.addExtension(manifestPath.toString()));
+
+    Types.StructType dataFileType = DataFile.getType(spec.partitionType());
+    SparkDataFile wrapper = new SparkDataFile(dataFileType, sparkType);
+
+    ManifestWriter writer = ManifestFiles.write(format, spec, outputFile, null);
+
+    try {
+      for (int index = startIndex; index < endIndex; index++) {
+        Row row = rows.get(index);
+        long snapshotId = row.getLong(0);
+        long sequenceNumber = row.getLong(1);
+        Row file = row.getStruct(2);
+        writer.existing(wrapper.wrap(file), snapshotId, sequenceNumber);
+      }
+    } finally {
+      writer.close();
+    }
+
+    return writer.toManifestFile();
+  }
+
+  private static MapPartitionsFunction<Row, ManifestFile> toManifests(
+      Broadcast<FileIO> io, long maxNumManifestEntries, String location,
+      int format, PartitionSpec spec, StructType sparkType) {
+
+    return (MapPartitionsFunction<Row, ManifestFile>) rows -> {
+      List<Row> rowsAsList = Lists.newArrayList(rows);
+
+      if (rowsAsList.isEmpty()) {
+        return Collections.emptyIterator();
+      }
+
+      List<ManifestFile> manifests = Lists.newArrayList();
+      if (rowsAsList.size() <= maxNumManifestEntries) {
+        manifests.add(writeManifest(rowsAsList, 0, rowsAsList.size(), io, location, format, spec, sparkType));
+      } else {
+        int midIndex = rowsAsList.size() / 2;
+        manifests.add(writeManifest(rowsAsList, 0, midIndex, io, location, format, spec, sparkType));
+        manifests.add(writeManifest(rowsAsList,  midIndex, rowsAsList.size(), io, location, format, spec, sparkType));
+      }
+
+      return manifests.iterator();
+    };
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsActionResult.java b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsActionResult.java
new file mode 100644
index 00000000000..b122450129d
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsActionResult.java
@@ -0,0 +1,50 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+import com.google.common.collect.ImmutableList;
+import java.util.List;
+import org.apache.iceberg.ManifestFile;
+
+public class RewriteManifestsActionResult {
+
+  private static final RewriteManifestsActionResult EMPTY =
+      new RewriteManifestsActionResult(ImmutableList.of(), ImmutableList.of());
+
+  private List<ManifestFile> deletedManifests;
+  private List<ManifestFile> addedManifests;
+
+  public RewriteManifestsActionResult(List<ManifestFile> deletedManifests, List<ManifestFile> addedManifests) {
+    this.deletedManifests = deletedManifests;
+    this.addedManifests = addedManifests;
+  }
+
+  static RewriteManifestsActionResult empty() {
+    return EMPTY;
+  }
+
+  public List<ManifestFile> deletedManifests() {
+    return deletedManifests;
+  }
+
+  public List<ManifestFile> addedManifests() {
+    return addedManifests;
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/actions/SnapshotUpdateAction.java b/spark/src/main/java/org/apache/iceberg/actions/SnapshotUpdateAction.java
new file mode 100644
index 00000000000..7bf6c7fc231
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/actions/SnapshotUpdateAction.java
@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+public interface SnapshotUpdateAction<ThisT, R> extends Action<R> {
+  ThisT set(String property, String value);
+}
diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java b/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
new file mode 100644
index 00000000000..4991980d83d
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java
@@ -0,0 +1,179 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.spark;
+
+import com.google.common.collect.Maps;
+import java.nio.ByteBuffer;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.FileFormat;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.types.Type;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.types.StructType;
+
+public class SparkDataFile implements DataFile {
+
+  private final int filePathPosition;
+  private final int fileFormatPosition;
+  private final int partitionPosition;
+  private final int recordCountPosition;
+  private final int fileSizeInBytesPosition;
+  private final int columnSizesPosition;
+  private final int valueCountsPosition;
+  private final int nullValueCountsPosition;
+  private final int lowerBoundsPosition;
+  private final int upperBoundsPosition;
+  private final int keyMetadataPosition;
+  private final int splitOffsetsPosition;
+  private final Type lowerBoundsType;
+  private final Type upperBoundsType;
+  private final Type keyMetadataType;
+
+  private final SparkStructLike wrappedPartition;
+  private Row wrapped;
+
+  public SparkDataFile(Types.StructType type, StructType sparkType) {
+    this.lowerBoundsType = type.fieldType("lower_bounds");
+    this.upperBoundsType = type.fieldType("upper_bounds");
+    this.keyMetadataType = type.fieldType("key_metadata");
+    this.wrappedPartition = new SparkStructLike(type.fieldType("partition").asStructType());
+
+    Map<String, Integer> positions = Maps.newHashMap();
+    type.fields().forEach(field -> {
+      String fieldName = field.name();
+      positions.put(fieldName, fieldPosition(fieldName, sparkType));
+    });
+
+    filePathPosition = positions.get("file_path");
+    fileFormatPosition = positions.get("file_format");
+    partitionPosition = positions.get("partition");
+    recordCountPosition = positions.get("record_count");
+    fileSizeInBytesPosition = positions.get("file_size_in_bytes");
+    columnSizesPosition = positions.get("column_sizes");
+    valueCountsPosition = positions.get("value_counts");
+    nullValueCountsPosition = positions.get("null_value_counts");
+    lowerBoundsPosition = positions.get("lower_bounds");
+    upperBoundsPosition = positions.get("upper_bounds");
+    keyMetadataPosition = positions.get("key_metadata");
+    splitOffsetsPosition = positions.get("split_offsets");
+  }
+
+  public SparkDataFile wrap(Row row) {
+    this.wrapped = row;
+    if (wrappedPartition.size() > 0) {
+      this.wrappedPartition.wrap(row.getAs(partitionPosition));
+    }
+    return this;
+  }
+
+  @Override
+  public CharSequence path() {
+    return wrapped.getAs(filePathPosition);
+  }
+
+  @Override
+  public FileFormat format() {
+    String formatAsString = wrapped.getString(fileFormatPosition).toUpperCase(Locale.ROOT);
+    return FileFormat.valueOf(formatAsString);
+  }
+
+  @Override
+  public StructLike partition() {
+    return wrappedPartition;
+  }
+
+  @Override
+  public long recordCount() {
+    return wrapped.getAs(recordCountPosition);
+  }
+
+  @Override
+  public long fileSizeInBytes() {
+    return wrapped.getAs(fileSizeInBytesPosition);
+  }
+
+  @Override
+  public Map<Integer, Long> columnSizes() {
+    return wrapped.isNullAt(columnSizesPosition) ? null : wrapped.getJavaMap(columnSizesPosition);
+  }
+
+  @Override
+  public Map<Integer, Long> valueCounts() {
+    return wrapped.isNullAt(valueCountsPosition) ? null : wrapped.getJavaMap(valueCountsPosition);
+  }
+
+  @Override
+  public Map<Integer, Long> nullValueCounts() {
+    return wrapped.isNullAt(nullValueCountsPosition) ? null : wrapped.getJavaMap(nullValueCountsPosition);
+  }
+
+  @Override
+  public Map<Integer, ByteBuffer> lowerBounds() {
+    Map<?, ?> lowerBounds = wrapped.isNullAt(lowerBoundsPosition) ? null : wrapped.getJavaMap(lowerBoundsPosition);
+    return convert(lowerBoundsType, lowerBounds);
+  }
+
+  @Override
+  public Map<Integer, ByteBuffer> upperBounds() {
+    Map<?, ?> upperBounds = wrapped.isNullAt(upperBoundsPosition) ? null : wrapped.getJavaMap(upperBoundsPosition);
+    return convert(upperBoundsType, upperBounds);
+  }
+
+  @Override
+  public ByteBuffer keyMetadata() {
+    return convert(keyMetadataType, wrapped.get(keyMetadataPosition));
+  }
+
+  @Override
+  public DataFile copy() {
+    throw new UnsupportedOperationException("Not implemented: copy");
+  }
+
+  @Override
+  public DataFile copyWithoutStats() {
+    throw new UnsupportedOperationException("Not implemented: copyWithoutStats");
+  }
+
+  @Override
+  public List<Long> splitOffsets() {
+    return wrapped.isNullAt(splitOffsetsPosition) ? null : wrapped.getList(splitOffsetsPosition);
+  }
+
+  private int fieldPosition(String name, StructType sparkType) {
+    try {
+      return sparkType.fieldIndex(name);
+    } catch (IllegalArgumentException e) {
+      // the partition field is absent for unpartitioned tables
+      if (name.equals("partition") && wrappedPartition.size() == 0) {
+        return -1;
+      }
+      throw e;
+    }
+  }
+
+  @SuppressWarnings("unchecked")
+  private <T> T convert(Type valueType, Object value) {
+    return (T) SparkValueConverter.convert(valueType, value);
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkStructLike.java b/spark/src/main/java/org/apache/iceberg/spark/SparkStructLike.java
new file mode 100644
index 00000000000..30509e3381d
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkStructLike.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.spark;
+
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Row;
+
+public class SparkStructLike implements StructLike {
+
+  private final Types.StructType type;
+  private Row wrapped;
+
+  public SparkStructLike(Types.StructType type) {
+    this.type = type;
+  }
+
+  public SparkStructLike wrap(Row row) {
+    this.wrapped = row;
+    return this;
+  }
+
+  @Override
+  public int size() {
+    return type.fields().size();
+  }
+
+  @Override
+  public <T> T get(int pos, Class<T> javaClass) {
+    Types.NestedField field = type.fields().get(pos);
+    return javaClass.cast(SparkValueConverter.convert(field.type(), wrapped.get(pos)));
+  }
+
+  @Override
+  public <T> void set(int pos, T value) {
+    throw new UnsupportedOperationException("Not implemented: set");
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java b/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
new file mode 100644
index 00000000000..8e028272156
--- /dev/null
+++ b/spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.spark;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import java.nio.ByteBuffer;
+import java.sql.Date;
+import java.sql.Timestamp;
+import java.util.List;
+import java.util.Map;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.data.GenericRecord;
+import org.apache.iceberg.data.Record;
+import org.apache.iceberg.types.Type;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.catalyst.util.DateTimeUtils;
+
+/**
+ * A utility class that converts Spark values to Iceberg's internal representation.
+ */
+public class SparkValueConverter {
+
+  private SparkValueConverter() {}
+
+  public static Record convert(Schema schema, Row row) {
+    return convert(schema.asStruct(), row);
+  }
+
+  public static Object convert(Type type, Object object) {
+    if (object == null) {
+      return null;
+    }
+
+    switch (type.typeId()) {
+      case STRUCT:
+        return convert(type.asStructType(), (Row) object);
+
+      case LIST:
+        List<Object> convertedList = Lists.newArrayList();
+        List<?> list = (List<?>) object;
+        for (Object element : list) {
+          convertedList.add(convert(type.asListType().elementType(), element));
+        }
+        return convertedList;
+
+      case MAP:
+        Map<Object, Object> convertedMap = Maps.newLinkedHashMap();
+        Map<?, ?> map = (Map<?, ?>) object;
+        for (Map.Entry<?, ?> entry : map.entrySet()) {
+          convertedMap.put(
+              convert(type.asMapType().keyType(), entry.getKey()),
+              convert(type.asMapType().valueType(), entry.getValue()));
+        }
+        return convertedMap;
+
+      case DATE:
+        return DateTimeUtils.fromJavaDate((Date) object);
+      case TIMESTAMP:
+        return DateTimeUtils.fromJavaTimestamp((Timestamp) object);
+      case BINARY:
+        return ByteBuffer.wrap((byte[]) object);
+      case BOOLEAN:
+      case INTEGER:
+      case LONG:
+      case FLOAT:
+      case DOUBLE:
+      case DECIMAL:
+      case STRING:
+      case FIXED:
+        return object;
+      default:
+        throw new UnsupportedOperationException("Not a supported type: " + type);
+    }
+  }
+
+  private static Record convert(Types.StructType struct, Row row) {
+    Record record = GenericRecord.create(struct);
+    List<Types.NestedField> fields = struct.fields();
+    for (int i = 0; i < fields.size(); i += 1) {
+      Types.NestedField field = fields.get(i);
+
+      Type fieldType = field.type();
+
+      switch (fieldType.typeId()) {
+        case STRUCT:
+          record.set(i, convert(fieldType.asStructType(), row.getStruct(i)));
+          break;
+        case LIST:
+          record.set(i, convert(fieldType.asListType(), row.getList(i)));
+          break;
+        case MAP:
+          record.set(i, convert(fieldType.asMapType(), row.getJavaMap(i)));
+          break;
+        default:
+          record.set(i, convert(fieldType, row.get(i)));
+      }
+    }
+    return record;
+  }
+}
diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
index 190b708db14..60a41380e42 100644
--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
@@ -27,7 +27,6 @@
 import java.math.BigInteger;
 import java.nio.ByteBuffer;
 import java.util.Arrays;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import org.apache.iceberg.Schema;
@@ -281,35 +280,9 @@ public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveTy
       }
     }
 
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
     protected MessageType type() {
       return type;
     }
-
-    protected String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static class BinaryDecimalReader extends PrimitiveReader<Decimal> {
diff --git a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
index 3cd0d5ef245..52ebd823335 100644
--- a/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
+++ b/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
@@ -185,32 +185,6 @@ public ParquetValueWriter<?> primitive(PrimitiveType primitive) {
           throw new UnsupportedOperationException("Unsupported type: " + primitive);
       }
     }
-
-    private String[] currentPath() {
-      String[] path = new String[fieldNames.size()];
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
-
-    private String[] path(String name) {
-      String[] path = new String[fieldNames.size() + 1];
-      path[fieldNames.size()] = name;
-
-      if (!fieldNames.isEmpty()) {
-        Iterator<String> iter = fieldNames.descendingIterator();
-        for (int i = 0; iter.hasNext(); i += 1) {
-          path[i] = iter.next();
-        }
-      }
-
-      return path;
-    }
   }
 
   private static PrimitiveWriter<UTF8String> utf8Strings(ColumnDescriptor desc) {
diff --git a/spark/src/test/java/org/apache/iceberg/actions/TestRewriteManifestsAction.java b/spark/src/test/java/org/apache/iceberg/actions/TestRewriteManifestsAction.java
new file mode 100644
index 00000000000..c9249da1f5a
--- /dev/null
+++ b/spark/src/test/java/org/apache/iceberg/actions/TestRewriteManifestsAction.java
@@ -0,0 +1,442 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.actions;
+
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import java.io.File;
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.ManifestFile;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.Snapshot;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.spark.SparkTableUtil;
+import org.apache.iceberg.spark.source.ThreeColumnRecord;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Encoders;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SparkSession;
+import org.apache.spark.sql.catalyst.TableIdentifier;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+
+import static org.apache.iceberg.types.Types.NestedField.optional;
+
+@RunWith(Parameterized.class)
+public class TestRewriteManifestsAction {
+
+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());
+  private static final Schema SCHEMA = new Schema(
+      optional(1, "c1", Types.IntegerType.get()),
+      optional(2, "c2", Types.StringType.get()),
+      optional(3, "c3", Types.StringType.get())
+  );
+
+  private static SparkSession spark;
+
+  @Parameterized.Parameters
+  public static Object[][] parameters() {
+    return new Object[][] {
+        new Object[] { "true" },
+        new Object[] { "false" }
+    };
+  }
+
+  @BeforeClass
+  public static void startSpark() {
+    TestRewriteManifestsAction.spark = SparkSession.builder()
+        .master("local[2]")
+        .getOrCreate();
+  }
+
+  @AfterClass
+  public static void stopSpark() {
+    SparkSession currentSpark = TestRewriteManifestsAction.spark;
+    TestRewriteManifestsAction.spark = null;
+    currentSpark.stop();
+  }
+
+  @Rule
+  public TemporaryFolder temp = new TemporaryFolder();
+
+  private final String snapshotIdInheritanceEnabled;
+  private String tableLocation = null;
+
+  public TestRewriteManifestsAction(String snapshotIdInheritanceEnabled) {
+    this.snapshotIdInheritanceEnabled = snapshotIdInheritanceEnabled;
+  }
+
+  @Before
+  public void setupTableLocation() throws Exception {
+    File tableDir = temp.newFolder();
+    this.tableLocation = tableDir.toURI().toString();
+  }
+
+  @Test
+  public void testRewriteManifestsEmptyTable() throws IOException {
+    PartitionSpec spec = PartitionSpec.unpartitioned();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    Assert.assertNull("Table must be empty", table.currentSnapshot());
+
+    Actions actions = Actions.forTable(table);
+
+    actions.rewriteManifests()
+        .rewriteIf(manifest -> true)
+        .stagingLocation(temp.newFolder().toString())
+        .execute();
+
+    Assert.assertNull("Table must stay empty", table.currentSnapshot());
+  }
+
+  @Test
+  public void testRewriteSmallManifestsNonPartitionedTable() throws IOException {
+    PartitionSpec spec = PartitionSpec.unpartitioned();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    List<ThreeColumnRecord> records1 = Lists.newArrayList(
+        new ThreeColumnRecord(1, null, "AAAA"),
+        new ThreeColumnRecord(1, "BBBBBBBBBB", "BBBB")
+    );
+    writeRecords(records1);
+
+    List<ThreeColumnRecord> records2 = Lists.newArrayList(
+        new ThreeColumnRecord(2, "CCCCCCCCCC", "CCCC"),
+        new ThreeColumnRecord(2, "DDDDDDDDDD", "DDDD")
+    );
+    writeRecords(records2);
+
+    table.refresh();
+
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 2 manifests before rewrite", 2, manifests.size());
+
+    Actions actions = Actions.forTable(table);
+
+    RewriteManifestsActionResult result = actions.rewriteManifests()
+        .rewriteIf(manifest -> true)
+        .execute();
+
+    Assert.assertEquals("Action should rewrite 2 manifests", 2, result.deletedManifests().size());
+    Assert.assertEquals("Action should add 1 manifests", 1, result.addedManifests().size());
+
+    table.refresh();
+
+    List<ManifestFile> newManifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 1 manifests after rewrite", 1, newManifests.size());
+
+    Assert.assertEquals(4, (long) newManifests.get(0).existingFilesCount());
+    Assert.assertFalse(newManifests.get(0).hasAddedFiles());
+    Assert.assertFalse(newManifests.get(0).hasDeletedFiles());
+
+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();
+    expectedRecords.addAll(records1);
+    expectedRecords.addAll(records2);
+
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(tableLocation);
+    List<ThreeColumnRecord> actualRecords = resultDF.sort("c1", "c2")
+        .as(Encoders.bean(ThreeColumnRecord.class))
+        .collectAsList();
+
+    Assert.assertEquals("Rows must match", expectedRecords, actualRecords);
+  }
+
+  @Test
+  public void testRewriteSmallManifestsPartitionedTable() throws IOException {
+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)
+        .identity("c1")
+        .truncate("c2", 2)
+        .build();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    List<ThreeColumnRecord> records1 = Lists.newArrayList(
+        new ThreeColumnRecord(1, null, "AAAA"),
+        new ThreeColumnRecord(1, "BBBBBBBBBB", "BBBB")
+    );
+    writeRecords(records1);
+
+    List<ThreeColumnRecord> records2 = Lists.newArrayList(
+        new ThreeColumnRecord(2, "CCCCCCCCCC", "CCCC"),
+        new ThreeColumnRecord(2, "DDDDDDDDDD", "DDDD")
+    );
+    writeRecords(records2);
+
+    List<ThreeColumnRecord> records3 = Lists.newArrayList(
+        new ThreeColumnRecord(3, "EEEEEEEEEE", "EEEE"),
+        new ThreeColumnRecord(3, "FFFFFFFFFF", "FFFF")
+    );
+    writeRecords(records3);
+
+    List<ThreeColumnRecord> records4 = Lists.newArrayList(
+        new ThreeColumnRecord(4, "GGGGGGGGGG", "GGGG"),
+        new ThreeColumnRecord(4, "HHHHHHHHHG", "HHHH")
+    );
+    writeRecords(records4);
+
+    table.refresh();
+
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 4 manifests before rewrite", 4, manifests.size());
+
+    Actions actions = Actions.forTable(table);
+
+    // we will expect to have 2 manifests with 4 entries in each after rewrite
+    long manifestEntrySizeBytes = computeManifestEntrySizeBytes(manifests);
+    long targetManifestSizeBytes = (long) (1.05 * 4 * manifestEntrySizeBytes);
+
+    table.updateProperties()
+        .set(TableProperties.MANIFEST_TARGET_SIZE_BYTES, String.valueOf(targetManifestSizeBytes))
+        .commit();
+
+    RewriteManifestsActionResult result = actions.rewriteManifests()
+        .rewriteIf(manifest -> true)
+        .execute();
+
+    Assert.assertEquals("Action should rewrite 4 manifests", 4, result.deletedManifests().size());
+    Assert.assertEquals("Action should add 2 manifests", 2, result.addedManifests().size());
+
+    table.refresh();
+
+    List<ManifestFile> newManifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 2 manifests after rewrite", 2, newManifests.size());
+
+    Assert.assertEquals(4, (long) newManifests.get(0).existingFilesCount());
+    Assert.assertFalse(newManifests.get(0).hasAddedFiles());
+    Assert.assertFalse(newManifests.get(0).hasDeletedFiles());
+
+    Assert.assertEquals(4, (long) newManifests.get(1).existingFilesCount());
+    Assert.assertFalse(newManifests.get(1).hasAddedFiles());
+    Assert.assertFalse(newManifests.get(1).hasDeletedFiles());
+
+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();
+    expectedRecords.addAll(records1);
+    expectedRecords.addAll(records2);
+    expectedRecords.addAll(records3);
+    expectedRecords.addAll(records4);
+
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(tableLocation);
+    List<ThreeColumnRecord> actualRecords = resultDF.sort("c1", "c2")
+        .as(Encoders.bean(ThreeColumnRecord.class))
+        .collectAsList();
+
+    Assert.assertEquals("Rows must match", expectedRecords, actualRecords);
+  }
+
+  @Test
+  public void testRewriteImportedManifests() throws IOException {
+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)
+        .identity("c3")
+        .build();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    List<ThreeColumnRecord> records = Lists.newArrayList(
+        new ThreeColumnRecord(1, null, "AAAA"),
+        new ThreeColumnRecord(1, "BBBBBBBBBB", "BBBB")
+    );
+    File parquetTableDir = temp.newFolder("parquet_table");
+    String parquetTableLocation = parquetTableDir.toURI().toString();
+
+    try {
+      Dataset<Row> inputDF = spark.createDataFrame(records, ThreeColumnRecord.class);
+      inputDF.select("c1", "c2", "c3")
+          .write()
+          .format("parquet")
+          .mode("overwrite")
+          .option("path", parquetTableLocation)
+          .partitionBy("c3")
+          .saveAsTable("parquet_table");
+
+      File stagingDir = temp.newFolder("staging-dir");
+      SparkTableUtil.importSparkTable(spark, new TableIdentifier("parquet_table"), table, stagingDir.toString());
+
+      Snapshot snapshot = table.currentSnapshot();
+
+      Actions actions = Actions.forTable(table);
+
+      RewriteManifestsActionResult result = actions.rewriteManifests()
+          .rewriteIf(manifest -> true)
+          .stagingLocation(temp.newFolder().toString())
+          .execute();
+
+      Assert.assertEquals("Action should rewrite all manifests", snapshot.manifests(), result.deletedManifests());
+      Assert.assertEquals("Action should add 1 manifest", 1, result.addedManifests().size());
+
+    } finally {
+      spark.sql("DROP TABLE parquet_table");
+    }
+  }
+
+  @Test
+  public void testRewriteLargeManifestsPartitionedTable() throws IOException {
+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)
+        .identity("c3")
+        .build();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    // all records belong to the same partition
+    List<ThreeColumnRecord> records = Lists.newArrayList();
+    for (int i = 0; i < 50; i++) {
+      records.add(new ThreeColumnRecord(i, String.valueOf(i), "0"));
+    }
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);
+    // repartition to create separate files
+    writeDF(df.repartition(50, df.col("c1")));
+
+    table.refresh();
+
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 1 manifests before rewrite", 1, manifests.size());
+
+    // set the target manifest size to a small value to force splitting records into multiple files
+    table.updateProperties()
+        .set(TableProperties.MANIFEST_TARGET_SIZE_BYTES, String.valueOf(manifests.get(0).length() / 2))
+        .commit();
+
+    Actions actions = Actions.forTable(table);
+
+    RewriteManifestsActionResult result = actions.rewriteManifests()
+        .rewriteIf(manifest -> true)
+        .stagingLocation(temp.newFolder().toString())
+        .execute();
+
+    Assert.assertEquals("Action should rewrite 1 manifest", 1, result.deletedManifests().size());
+    Assert.assertEquals("Action should add 2 manifests", 2, result.addedManifests().size());
+
+    table.refresh();
+
+    List<ManifestFile> newManifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 2 manifests after rewrite", 2, newManifests.size());
+
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(tableLocation);
+    List<ThreeColumnRecord> actualRecords = resultDF.sort("c1", "c2")
+        .as(Encoders.bean(ThreeColumnRecord.class))
+        .collectAsList();
+
+    Assert.assertEquals("Rows must match", records, actualRecords);
+  }
+
+  @Test
+  public void testRewriteManifestsWithPredicate() throws IOException {
+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)
+        .identity("c1")
+        .truncate("c2", 2)
+        .build();
+    Map<String, String> options = Maps.newHashMap();
+    options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);
+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);
+
+    List<ThreeColumnRecord> records1 = Lists.newArrayList(
+        new ThreeColumnRecord(1, null, "AAAA"),
+        new ThreeColumnRecord(1, "BBBBBBBBBB", "BBBB")
+    );
+    writeRecords(records1);
+
+    List<ThreeColumnRecord> records2 = Lists.newArrayList(
+        new ThreeColumnRecord(2, "CCCCCCCCCC", "CCCC"),
+        new ThreeColumnRecord(2, "DDDDDDDDDD", "DDDD")
+    );
+    writeRecords(records2);
+
+    table.refresh();
+
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 2 manifests before rewrite", 2, manifests.size());
+
+    Actions actions = Actions.forTable(table);
+
+    // rewrite only the first manifest without caching
+    RewriteManifestsActionResult result = actions.rewriteManifests()
+        .rewriteIf(manifest -> manifest.path().equals(manifests.get(0).path()))
+        .stagingLocation(temp.newFolder().toString())
+        .useCaching(false)
+        .execute();
+
+    Assert.assertEquals("Action should rewrite 1 manifest", 1, result.deletedManifests().size());
+    Assert.assertEquals("Action should add 1 manifests", 1, result.addedManifests().size());
+
+    table.refresh();
+
+    List<ManifestFile> newManifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 2 manifests after rewrite", 2, newManifests.size());
+
+    Assert.assertFalse("First manifest must be rewritten", newManifests.contains(manifests.get(0)));
+    Assert.assertTrue("Second manifest must not be rewritten", newManifests.contains(manifests.get(1)));
+
+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();
+    expectedRecords.addAll(records1);
+    expectedRecords.addAll(records2);
+
+    Dataset<Row> resultDF = spark.read().format("iceberg").load(tableLocation);
+    List<ThreeColumnRecord> actualRecords = resultDF.sort("c1", "c2")
+        .as(Encoders.bean(ThreeColumnRecord.class))
+        .collectAsList();
+
+    Assert.assertEquals("Rows must match", expectedRecords, actualRecords);
+  }
+
+  private void writeRecords(List<ThreeColumnRecord> records) {
+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);
+    writeDF(df);
+  }
+
+  private void writeDF(Dataset<Row> df) {
+    df.select("c1", "c2", "c3")
+        .write()
+        .format("iceberg")
+        .mode("append")
+        .save(tableLocation);
+  }
+
+  private long computeManifestEntrySizeBytes(List<ManifestFile> manifests) {
+    long totalSize = 0L;
+    int numEntries = 0;
+
+    for (ManifestFile manifest : manifests) {
+      totalSize += manifest.length();
+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();
+    }
+
+    return totalSize / numEntries;
+  }
+}
diff --git a/spark/src/test/java/org/apache/iceberg/spark/TestSparkDataFile.java b/spark/src/test/java/org/apache/iceberg/spark/TestSparkDataFile.java
new file mode 100644
index 00000000000..2ebb3dd473f
--- /dev/null
+++ b/spark/src/test/java/org/apache/iceberg/spark/TestSparkDataFile.java
@@ -0,0 +1,208 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.spark;
+
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import java.io.File;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.iceberg.DataFile;
+import org.apache.iceberg.ManifestFile;
+import org.apache.iceberg.ManifestFiles;
+import org.apache.iceberg.ManifestReader;
+import org.apache.iceberg.PartitionSpec;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.StructLike;
+import org.apache.iceberg.Table;
+import org.apache.iceberg.TableProperties;
+import org.apache.iceberg.hadoop.HadoopTables;
+import org.apache.iceberg.spark.data.RandomData;
+import org.apache.iceberg.types.Types;
+import org.apache.spark.api.java.JavaRDD;
+import org.apache.spark.api.java.JavaSparkContext;
+import org.apache.spark.sql.Column;
+import org.apache.spark.sql.ColumnName;
+import org.apache.spark.sql.Dataset;
+import org.apache.spark.sql.Row;
+import org.apache.spark.sql.SparkSession;
+import org.apache.spark.sql.catalyst.InternalRow;
+import org.apache.spark.sql.types.StructType;
+import org.junit.AfterClass;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import static org.apache.iceberg.types.Types.NestedField.optional;
+import static org.apache.iceberg.types.Types.NestedField.required;
+
+public class TestSparkDataFile {
+
+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());
+  private static final Schema SCHEMA = new Schema(
+      required(100, "id", Types.LongType.get()),
+      optional(101, "data", Types.StringType.get()),
+      required(102, "b", Types.BooleanType.get()),
+      optional(103, "i", Types.IntegerType.get()),
+      required(104, "l", Types.LongType.get()),
+      optional(105, "f", Types.FloatType.get()),
+      required(106, "d", Types.DoubleType.get()),
+      optional(107, "date", Types.DateType.get()),
+      required(108, "ts", Types.TimestampType.withZone()),
+      required(110, "s", Types.StringType.get()),
+      optional(113, "bytes", Types.BinaryType.get()),
+      required(114, "dec_9_0", Types.DecimalType.of(9, 0)),
+      required(115, "dec_11_2", Types.DecimalType.of(11, 2)),
+      required(116, "dec_38_10", Types.DecimalType.of(38, 10)) // maximum precision
+  );
+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)
+      .identity("b")
+      .bucket("i", 2)
+      .identity("l")
+      .identity("f")
+      .identity("d")
+      .identity("date")
+      .hour("ts")
+      .identity("ts")
+      .truncate("s", 2)
+      .identity("bytes")
+      .bucket("dec_9_0", 2)
+      .bucket("dec_11_2", 2)
+      .bucket("dec_38_10", 2)
+      .build();
+
+  private static SparkSession spark;
+  private static JavaSparkContext sparkContext = null;
+
+  @BeforeClass
+  public static void startSpark() {
+    TestSparkDataFile.spark = SparkSession.builder().master("local[2]").getOrCreate();
+    TestSparkDataFile.sparkContext = new JavaSparkContext(spark.sparkContext());
+  }
+
+  @AfterClass
+  public static void stopSpark() {
+    SparkSession currentSpark = TestSparkDataFile.spark;
+    TestSparkDataFile.spark = null;
+    TestSparkDataFile.sparkContext = null;
+    currentSpark.stop();
+  }
+
+  @Rule
+  public TemporaryFolder temp = new TemporaryFolder();
+  private String tableLocation = null;
+
+  @Before
+  public void setupTableLocation() throws Exception {
+    File tableDir = temp.newFolder();
+    this.tableLocation = tableDir.toURI().toString();
+  }
+
+  @Test
+  public void testValueConversion() throws IOException {
+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);
+    checkSparkDataFile(table);
+  }
+
+  @Test
+  public void testValueConversionPartitionedTable() throws IOException {
+    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);
+    checkSparkDataFile(table);
+  }
+
+  @Test
+  public void testValueConversionWithEmptyStats() throws IOException {
+    Map<String, String> props = Maps.newHashMap();
+    props.put(TableProperties.DEFAULT_WRITE_METRICS_MODE, "none");
+    Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);
+    checkSparkDataFile(table);
+  }
+
+  private void checkSparkDataFile(Table table) throws IOException {
+    Iterable<InternalRow> rows = RandomData.generateSpark(table.schema(), 200, 0);
+    JavaRDD<InternalRow> rdd = sparkContext.parallelize(Lists.newArrayList(rows));
+    Dataset<Row> df = spark.internalCreateDataFrame(JavaRDD.toRDD(rdd), SparkSchemaUtil.convert(table.schema()), false);
+
+    df.write().format("iceberg").mode("append").save(tableLocation);
+
+    table.refresh();
+
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
+    Assert.assertEquals("Should have 1 manifest", 1, manifests.size());
+
+    List<DataFile> dataFiles = Lists.newArrayList();
+    try (ManifestReader reader = ManifestFiles.read(manifests.get(0), table.io())) {
+      reader.forEach(dataFile -> dataFiles.add(dataFile.copy()));
+    }
+
+    Dataset<Row> dataFileDF = spark.read().format("iceberg").load(tableLocation + "#files");
+
+    // reorder columns to test arbitrary projections
+    List<Column> columns = Arrays.stream(dataFileDF.columns())
+        .map(ColumnName::new)
+        .collect(Collectors.toList());
+    Collections.shuffle(columns);
+
+    List<Row> sparkDataFiles = dataFileDF
+        .select(Iterables.toArray(columns, Column.class))
+        .collectAsList();
+
+    Assert.assertEquals("The number of files should match", dataFiles.size(), sparkDataFiles.size());
+
+    Types.StructType dataFileType = DataFile.getType(table.spec().partitionType());
+    StructType sparkDataFileType = sparkDataFiles.get(0).schema();
+    SparkDataFile wrapper = new SparkDataFile(dataFileType, sparkDataFileType);
+
+    for (int i = 0; i < dataFiles.size(); i++) {
+      checkDataFile(dataFiles.get(i), wrapper.wrap(sparkDataFiles.get(i)));
+    }
+  }
+
+  private void checkDataFile(DataFile expected, DataFile actual) {
+    Assert.assertEquals("Path must match", expected.path(), actual.path());
+    Assert.assertEquals("Format must match", expected.format(), actual.format());
+    Assert.assertEquals("Record count must match", expected.recordCount(), actual.recordCount());
+    Assert.assertEquals("Size must match", expected.fileSizeInBytes(), actual.fileSizeInBytes());
+    Assert.assertEquals("Record value counts must match", expected.valueCounts(), actual.valueCounts());
+    Assert.assertEquals("Record null value counts must match", expected.nullValueCounts(), actual.nullValueCounts());
+    Assert.assertEquals("Lower bounds must match", expected.lowerBounds(), actual.lowerBounds());
+    Assert.assertEquals("Upper bounds must match", expected.upperBounds(), actual.upperBounds());
+    Assert.assertEquals("Key metadata must match", expected.keyMetadata(), actual.keyMetadata());
+    Assert.assertEquals("Split offsets must match", expected.splitOffsets(), actual.splitOffsets());
+
+    checkStructLike(expected.partition(), actual.partition());
+  }
+
+  private void checkStructLike(StructLike expected, StructLike actual) {
+    Assert.assertEquals("Struct size should match", expected.size(), actual.size());
+    for (int i = 0; i < expected.size(); i++) {
+      Assert.assertEquals("Struct values must match", expected.get(i, Object.class), actual.get(i, Object.class));
+    }
+  }
+}
diff --git a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
index fefdce98029..071c903fd06 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
@@ -21,16 +21,21 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.Collections;
 import java.util.Iterator;
+import java.util.List;
 import org.apache.iceberg.Files;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.io.CloseableIterable;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.orc.ORC;
+import org.apache.iceberg.types.Types;
 import org.apache.spark.sql.catalyst.InternalRow;
 import org.junit.Assert;
+import org.junit.Test;
 
 import static org.apache.iceberg.spark.data.TestHelpers.assertEquals;
+import static org.apache.iceberg.types.Types.NestedField.required;
 
 public class TestSparkOrcReader extends AvroDataTest {
   @Override
@@ -38,6 +43,22 @@ protected void writeAndValidate(Schema schema) throws IOException {
     final Iterable<InternalRow> expected = RandomData
         .generateSpark(schema, 100, 0L);
 
+    writeAndValidateRecords(schema, expected);
+  }
+
+  @Test
+  public void writeAndValidateRepeatingRecords() throws IOException {
+    Schema structSchema = new Schema(
+        required(100, "id", Types.LongType.get()),
+        required(101, "data", Types.StringType.get())
+    );
+    List<InternalRow> expectedRepeating = Collections.nCopies(100,
+        RandomData.generateSpark(structSchema, 1, 0L).iterator().next());
+
+    writeAndValidateRecords(structSchema, expectedRepeating);
+  }
+
+  private void writeAndValidateRecords(Schema schema, Iterable<InternalRow> expected) throws IOException {
     final File testFile = temp.newFile();
     Assert.assertTrue("Delete should succeed", testFile.delete());
 
diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
index 8668b283c9c..802ac2027d0 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java
@@ -28,6 +28,7 @@
 import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.FileScanTask;
+import org.apache.iceberg.ManifestFile;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.Table;
@@ -288,68 +289,48 @@ public void testIncrementalScanOptions() throws IOException {
   @Test
   public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOException {
     String tableLocation = temp.newFolder("iceberg-table").toString();
-    int splitSize = 2 * 1024;
 
     HadoopTables tables = new HadoopTables(CONF);
     PartitionSpec spec = PartitionSpec.unpartitioned();
     Map<String, String> options = Maps.newHashMap();
-    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
-    options.put(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(32L * 1024 * 1024)); // 32MB
-    tables.create(SCHEMA, spec, options, tableLocation);
+    Table table = tables.create(SCHEMA, spec, options, tableLocation);
 
     List<SimpleRecord> expectedRecords = Lists.newArrayList(
         new SimpleRecord(1, "a"),
         new SimpleRecord(2, "b")
     );
     Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);
+    // produce 1st manifest
     originalDf.select("id", "data").write()
         .format("iceberg")
         .mode("append")
         .save(tableLocation);
-
-    int expectedSplits = ((int) tables.load(tableLocation + "#entries")
-        .currentSnapshot().manifests().get(0).length() + splitSize - 1) / splitSize;
-
-    Dataset<Row> metadataDf = spark.read()
-        .format("iceberg")
-        .option("split-size", String.valueOf(splitSize))
-        .load(tableLocation + "#entries");
-
-    int partitionNum = metadataDf.javaRDD().getNumPartitions();
-    Assert.assertEquals("Spark partitions should match", expectedSplits, partitionNum);
-  }
-
-  @Test
-  public void testMetadataSplitSizeTableProperties() throws IOException {
-    String tableLocation = temp.newFolder("iceberg-table").toString();
-    int splitSize = 2 * 1024;
-
-    HadoopTables tables = new HadoopTables(CONF);
-    PartitionSpec spec = PartitionSpec.unpartitioned();
-    Map<String, String> options = Maps.newHashMap();
-    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb
-    options.put(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(splitSize)); // 2KB
-    tables.create(SCHEMA, spec, options, tableLocation);
-
-    List<SimpleRecord> expectedRecords = Lists.newArrayList(
-        new SimpleRecord(1, "a"),
-        new SimpleRecord(2, "b")
-    );
-    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);
+    // produce 2nd manifest
     originalDf.select("id", "data").write()
         .format("iceberg")
         .mode("append")
         .save(tableLocation);
 
-    int expectedSplits = ((int) tables.load(tableLocation + "#entries")
-        .currentSnapshot().manifests().get(0).length() + splitSize - 1) / splitSize;
+    List<ManifestFile> manifests = table.currentSnapshot().manifests();
 
-    Dataset<Row> metadataDf = spark.read()
+    Assert.assertEquals("Must be 2 manifests", 2, manifests.size());
+
+    // set the target metadata split size so each manifest ends up in a separate split
+    table.updateProperties()
+        .set(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(manifests.get(0).length()))
+        .commit();
+
+    Dataset<Row> entriesDf = spark.read()
         .format("iceberg")
         .load(tableLocation + "#entries");
+    Assert.assertEquals("Num partitions must match", 2, entriesDf.javaRDD().getNumPartitions());
 
-    int partitionNum = metadataDf.javaRDD().getNumPartitions();
-    Assert.assertEquals("Spark partitions should match", expectedSplits, partitionNum);
+    // override the table property using options
+    entriesDf = spark.read()
+        .format("iceberg")
+        .option("split-size", String.valueOf(128 * 1024 * 1024))
+        .load(tableLocation + "#entries");
+    Assert.assertEquals("Num partitions must match", 1, entriesDf.javaRDD().getNumPartitions());
   }
 
   @Test
diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
index 23e7926c458..1dc91adc728 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java
@@ -19,30 +19,24 @@
 
 package org.apache.iceberg.spark.source;
 
-import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Lists;
 import java.io.File;
 import java.io.IOException;
 import java.util.List;
-import java.util.Map;
 import java.util.UUID;
 import org.apache.avro.generic.GenericData;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.iceberg.AssertHelpers;
 import org.apache.iceberg.DataFile;
 import org.apache.iceberg.DataFiles;
-import org.apache.iceberg.DataOperations;
 import org.apache.iceberg.FileFormat;
-import org.apache.iceberg.HasTableOperations;
-import org.apache.iceberg.ManifestFile;
 import org.apache.iceberg.ManifestFiles;
 import org.apache.iceberg.ManifestWriter;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.PartitionSpecParser;
 import org.apache.iceberg.Schema;
-import org.apache.iceberg.Snapshot;
 import org.apache.iceberg.Table;
-import org.apache.iceberg.TableOperations;
+import org.apache.iceberg.TableProperties;
 import org.apache.iceberg.hadoop.HadoopTables;
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.io.OutputFile;
@@ -165,6 +159,9 @@ public void testSparkCanReadUnknownTransform() throws IOException {
     HadoopTables tables = new HadoopTables(CONF);
     Table table = tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());
 
+    // enable snapshot inheritance to avoid rewriting the manifest with an unknown transform
+    table.updateProperties().set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, "true").commit();
+
     List<GenericData.Record> expected = RandomData.generateList(table.schema(), 100, 1L);
 
     File parquetFile = new File(dataFolder,
@@ -192,8 +189,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {
       manifestWriter.close();
     }
 
-    TableOperations ops = ((HasTableOperations) table).operations();
-    ops.commit(ops.current(), ops.current().replaceCurrentSnapshot(new FakeSnapshot(manifestWriter.toManifestFile())));
+    table.newFastAppend().appendManifest(manifestWriter.toManifestFile()).commit();
 
     Dataset<Row> df = spark.read()
         .format("iceberg")
@@ -206,62 +202,4 @@ public void testSparkCanReadUnknownTransform() throws IOException {
       TestHelpers.assertEqualsSafe(table.schema().asStruct(), expected.get(i), rows.get(i));
     }
   }
-
-  private static class FakeSnapshot implements Snapshot {
-    private final ManifestFile manifest;
-
-    FakeSnapshot(ManifestFile manifest) {
-      this.manifest = manifest;
-    }
-
-    @Override
-    public long sequenceNumber() {
-      return 0;
-    }
-
-    @Override
-    public long snapshotId() {
-      return 1;
-    }
-
-    @Override
-    public Long parentId() {
-      return null;
-    }
-
-    @Override
-    public long timestampMillis() {
-      return 0;
-    }
-
-    @Override
-    public List<ManifestFile> manifests() {
-      return ImmutableList.of(manifest);
-    }
-
-    @Override
-    public String operation() {
-      return DataOperations.APPEND;
-    }
-
-    @Override
-    public Map<String, String> summary() {
-      return null;
-    }
-
-    @Override
-    public Iterable<DataFile> addedFiles() {
-      return null;
-    }
-
-    @Override
-    public Iterable<DataFile> deletedFiles() {
-      return null;
-    }
-
-    @Override
-    public String manifestListLocation() {
-      return null;
-    }
-  }
 }
diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
index bc45a032f19..e993256fc63 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java
@@ -116,11 +116,19 @@ public void testEntriesTable() throws Exception {
         .load(loadLocation(tableIdentifier, "entries"))
         .collectAsList();
 
-    Assert.assertEquals("Should only contain one manifest", 1, table.currentSnapshot().manifests().size());
-    InputFile manifest = table.io().newInputFile(table.currentSnapshot().manifests().get(0).path());
-    List<GenericData.Record> expected;
+    Snapshot snapshot = table.currentSnapshot();
+
+    Assert.assertEquals("Should only contain one manifest", 1, snapshot.manifests().size());
+
+    InputFile manifest = table.io().newInputFile(snapshot.manifests().get(0).path());
+    List<GenericData.Record> expected = Lists.newArrayList();
     try (CloseableIterable<GenericData.Record> rows = Avro.read(manifest).project(entriesTable.schema()).build()) {
-      expected = Lists.newArrayList(rows);
+      // each row must inherit snapshot_id and sequence_number
+      rows.forEach(row -> {
+        row.put(1, snapshot.snapshotId());
+        row.put(2, 0L);
+        expected.add(row);
+      });
     }
 
     Assert.assertEquals("Entries table should have one row", 1, expected.size());
@@ -278,6 +286,53 @@ public void testFilesTableWithSnapshotIdInheritance() throws Exception {
 
   }
 
+  @Test
+  public void testEntriesTableWithSnapshotIdInheritance() {
+    TableIdentifier tableIdentifier = TableIdentifier.of("db", "entries_inheritance_test");
+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity("id").build();
+    Table table = createTable(tableIdentifier, SCHEMA, spec);
+
+    table.updateProperties()
+        .set(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, "true")
+        .commit();
+
+    List<SimpleRecord> records = Lists.newArrayList(
+        new SimpleRecord(1, "a"),
+        new SimpleRecord(2, "b")
+    );
+
+    try {
+      Dataset<Row> inputDF = spark.createDataFrame(records, SimpleRecord.class);
+      inputDF.select("id", "data").write()
+          .format("parquet")
+          .mode("append")
+          .partitionBy("id")
+          .saveAsTable("parquet_table");
+
+      String stagingLocation = table.location() + "/metadata";
+      SparkTableUtil.importSparkTable(
+          spark, new org.apache.spark.sql.catalyst.TableIdentifier("parquet_table"), table, stagingLocation);
+
+      List<Row> actual = spark.read()
+          .format("iceberg")
+          .load(loadLocation(tableIdentifier, "entries"))
+          .select("sequence_number", "snapshot_id", "data_file")
+          .collectAsList();
+
+      table.refresh();
+
+      long snapshotId = table.currentSnapshot().snapshotId();
+
+      Assert.assertEquals("Entries table should have 2 rows", 2, actual.size());
+      Assert.assertEquals("Sequence number must match", 0, actual.get(0).getLong(0));
+      Assert.assertEquals("Snapshot id must match", snapshotId, actual.get(0).getLong(1));
+      Assert.assertEquals("Sequence number must match", 0, actual.get(1).getLong(0));
+      Assert.assertEquals("Snapshot id must match", snapshotId, actual.get(1).getLong(1));
+    } finally {
+      spark.sql("DROP TABLE parquet_table");
+    }
+  }
+
   @Test
   public void testFilesUnpartitionedTable() throws Exception {
     TableIdentifier tableIdentifier = TableIdentifier.of("db", "unpartitioned_files_test");
diff --git a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
index 95f50b08d58..9abfe63f56b 100644
--- a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
+++ b/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
@@ -23,12 +23,6 @@
 import com.google.common.collect.Maps;
 import java.io.File;
 import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.time.Instant;
-import java.time.LocalDate;
-import java.time.OffsetDateTime;
-import java.time.ZoneOffset;
-import java.time.temporal.ChronoUnit;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
@@ -40,7 +34,6 @@
 import org.apache.iceberg.Schema;
 import org.apache.iceberg.Table;
 import org.apache.iceberg.avro.Avro;
-import org.apache.iceberg.data.GenericRecord;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.data.avro.DataWriter;
 import org.apache.iceberg.data.orc.GenericOrcWriter;
@@ -48,6 +41,7 @@
 import org.apache.iceberg.io.FileAppender;
 import org.apache.iceberg.orc.ORC;
 import org.apache.iceberg.parquet.Parquet;
+import org.apache.iceberg.spark.SparkValueConverter;
 import org.apache.iceberg.types.Type;
 import org.apache.iceberg.types.TypeUtil;
 import org.apache.iceberg.types.Types;
@@ -66,8 +60,6 @@
 
 @RunWith(Parameterized.class)
 public class TestSparkReadProjection extends TestReadProjection {
-  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);
-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();
 
   private static SparkSession spark = null;
 
@@ -169,90 +161,13 @@ protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema
           .option("iceberg.table.name", desc)
           .load();
 
-      // convert to Avro using the read schema so that the record schemas match
-      return convert(readSchema, df.collectAsList().get(0));
+      return SparkValueConverter.convert(readSchema, df.collectAsList().get(0));
 
     } finally {
       TestTables.clearTables();
     }
   }
 
-  @SuppressWarnings("unchecked")
-  private Object convert(Type type, Object object) {
-    switch (type.typeId()) {
-      case STRUCT:
-        return convert(type.asStructType(), (Row) object);
-
-      case LIST:
-        List<Object> convertedList = Lists.newArrayList();
-        List<?> list = (List<?>) object;
-        for (Object element : list) {
-          convertedList.add(convert(type.asListType().elementType(), element));
-        }
-        return convertedList;
-
-      case MAP:
-        Map<Object, Object> convertedMap = Maps.newLinkedHashMap();
-        Map<?, ?> map = (Map<?, ?>) object;
-        for (Map.Entry<?, ?> entry : map.entrySet()) {
-          convertedMap.put(
-              convert(type.asMapType().keyType(), entry.getKey()),
-              convert(type.asMapType().valueType(), entry.getValue()));
-        }
-        return convertedMap;
-
-      case DATE:
-        LocalDate date = (LocalDate) object;
-        return ChronoUnit.DAYS.between(EPOCH_DAY, date);
-      case TIMESTAMP:
-        OffsetDateTime timestamp = (OffsetDateTime) object;
-        return ChronoUnit.MICROS.between(EPOCH, timestamp);
-      case UUID:
-        return ((UUID) object).toString();
-      case BINARY:
-        return ByteBuffer.wrap((byte[]) object);
-      case BOOLEAN:
-      case INTEGER:
-      case LONG:
-      case FLOAT:
-      case DOUBLE:
-      case DECIMAL:
-      case STRING:
-      case FIXED:
-        return object;
-      default:
-        throw new UnsupportedOperationException("Not a supported type: " + type);
-    }
-  }
-  private Record convert(Schema schema, Row row) {
-    return convert(schema.asStruct(), row);
-  }
-
-  private Record convert(Types.StructType struct, Row row) {
-    Record record = GenericRecord.create(struct);
-    List<Types.NestedField> fields = struct.fields();
-    for (int i = 0; i < fields.size(); i += 1) {
-      Types.NestedField field = fields.get(i);
-
-      Type fieldType = field.type();
-
-      switch (fieldType.typeId()) {
-        case STRUCT:
-          record.set(i, convert(fieldType.asStructType(), row.getStruct(i)));
-          break;
-        case LIST:
-          record.set(i, convert(fieldType.asListType(), row.getList(i)));
-          break;
-        case MAP:
-          record.set(i, convert(fieldType.asMapType(), row.getJavaMap(i)));
-          break;
-        default:
-          record.set(i, convert(fieldType, row.get(i)));
-      }
-    }
-    return record;
-  }
-
   private List<Integer> allIds(Schema schema) {
     List<Integer> ids = Lists.newArrayList();
     TypeUtil.visit(schema, new TypeUtil.SchemaVisitor<Void>() {
diff --git a/versions.lock b/versions.lock
index f94a0ca1d75..3d10e607ab3 100644
--- a/versions.lock
+++ b/versions.lock
@@ -148,9 +148,9 @@ org.apache.htrace:htrace-core:3.1.0-incubating (2 constraints: cd22cffa)
 org.apache.httpcomponents:httpclient:4.5.6 (4 constraints: 573134dd)
 org.apache.httpcomponents:httpcore:4.4.10 (3 constraints: d327f763)
 org.apache.ivy:ivy:2.4.0 (3 constraints: 0826dbf1)
-org.apache.orc:orc-core:1.6.2 (3 constraints: ba1d17ad)
-org.apache.orc:orc-mapreduce:1.6.2 (1 constraints: c30cc227)
-org.apache.orc:orc-shims:1.6.2 (1 constraints: 3f0aeabc)
+org.apache.orc:orc-core:1.6.3 (3 constraints: bb1d64ad)
+org.apache.orc:orc-mapreduce:1.6.3 (1 constraints: c30cc227)
+org.apache.orc:orc-shims:1.6.3 (1 constraints: 400aebbc)
 org.apache.parquet:parquet-avro:1.11.0 (1 constraints: 35052c3b)
 org.apache.parquet:parquet-column:1.11.0 (3 constraints: 9429f2ca)
 org.apache.parquet:parquet-common:1.11.0 (2 constraints: 4c1e7785)
@@ -226,6 +226,7 @@ org.sonatype.sisu.inject:cglib:2.2.1-v20090111 (1 constraints: aa0cfd36)
 org.spark-project.hive:hive-exec:1.2.1.spark2 (1 constraints: 990fa09c)
 org.spark-project.hive:hive-metastore:1.2.1.spark2 (1 constraints: 990fa09c)
 org.spark-project.spark:unused:1.0.0 (12 constraints: 9aab75cf)
+org.threeten:threeten-extra:1.5.0 (1 constraints: 3c0ae5bc)
 org.xerial.snappy:snappy-java:1.1.7.3 (2 constraints: 681c5e46)
 oro:oro:2.0.8 (3 constraints: 3b229337)
 stax:stax-api:1.0.1 (2 constraints: ea186edd)
diff --git a/versions.props b/versions.props
index 0a1f86f530a..753653a5431 100644
--- a/versions.props
+++ b/versions.props
@@ -3,7 +3,7 @@ com.google.guava:guava = 28.0-jre
 org.apache.avro:avro = 1.9.2
 org.apache.hadoop:* = 2.7.3
 org.apache.hive:hive-metastore = 2.3.6
-org.apache.orc:* = 1.6.2
+org.apache.orc:* = 1.6.3
 org.apache.parquet:parquet-avro = 1.11.0
 org.apache.spark:spark-hive_2.11 = 2.4.5
 org.apache.spark:spark-avro_2.11 = 2.4.5
