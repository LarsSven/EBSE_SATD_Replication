diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java
index 5a2e2c2b6093..1682c9c8afb3 100644
--- a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileBasedSource.java
@@ -130,7 +130,7 @@ protected FileBasedSource(
    *
    * @throws IllegalArgumentException if this source is in {@link Mode#FILEPATTERN} mode.
    */
-  protected final MatchResult.Metadata getSingleFileMetadata() {
+  public final MatchResult.Metadata getSingleFileMetadata() {
     checkArgument(
         mode == Mode.SINGLE_FILE_OR_SUBRANGE,
         "This function should only be called for a single file, not %s",
diff --git a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileIO.java b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileIO.java
index b7e9630ad448..d321b139bf5f 100644
--- a/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileIO.java
+++ b/sdks/java/core/src/main/java/org/apache/beam/sdk/io/FileIO.java
@@ -472,9 +472,9 @@ public static MatchConfiguration create(EmptyMatchTreatment emptyMatchTreatment)
           .build();
     }
 
-    abstract EmptyMatchTreatment getEmptyMatchTreatment();
+    public abstract EmptyMatchTreatment getEmptyMatchTreatment();
 
-    abstract @Nullable Duration getWatchInterval();
+    public abstract @Nullable Duration getWatchInterval();
 
     abstract @Nullable TerminationCondition<String, ?> getWatchTerminationCondition();
 
diff --git a/sdks/java/io/contextual-text-io/build.gradle b/sdks/java/io/contextual-text-io/build.gradle
new file mode 100644
index 000000000000..8ad7b0059da8
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/build.gradle
@@ -0,0 +1,39 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * License); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an AS IS BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+plugins { id 'org.apache.beam.module' }
+applyJavaNature(
+        automaticModuleName: 'org.apache.beam.sdk.io.contextual-text-io',
+        enableChecker: false,
+        ignoreRawtypeErrors: true)
+
+description = "Apache Beam :: SDKs :: Java :: Contextual-Text-IO"
+ext.summary = "Context-aware Text IO."
+
+dependencies {
+    compile library.java.vendored_guava_26_0_jre
+    compile library.java.protobuf_java
+    compile project(path: ":sdks:java:core", configuration: "shadow")
+    testCompile project(path: ":sdks:java:core", configuration: "shadowTest")
+
+    testCompile library.java.guava_testlib
+    testCompile library.java.junit
+    testCompile library.java.hamcrest_core
+    testRuntimeOnly library.java.slf4j_jdk14
+    testCompile project(path: ":runners:direct-java", configuration: "shadow")
+}
\ No newline at end of file
diff --git a/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java
new file mode 100644
index 000000000000..0b3acc53e66b
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIO.java
@@ -0,0 +1,626 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.contextualtextio;
+
+import static org.apache.beam.sdk.io.FileIO.ReadMatches.DirectoryTreatment;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;
+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkNotNull;
+
+import com.google.auto.value.AutoValue;
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import org.apache.beam.sdk.coders.StringUtf8Coder;
+import org.apache.beam.sdk.io.CompressedSource;
+import org.apache.beam.sdk.io.Compression;
+import org.apache.beam.sdk.io.FileBasedSource;
+import org.apache.beam.sdk.io.FileIO;
+import org.apache.beam.sdk.io.FileIO.MatchConfiguration;
+import org.apache.beam.sdk.io.ReadAllViaFileBasedSource;
+import org.apache.beam.sdk.io.TextIO;
+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;
+import org.apache.beam.sdk.options.ValueProvider;
+import org.apache.beam.sdk.options.ValueProvider.StaticValueProvider;
+import org.apache.beam.sdk.schemas.NoSuchSchemaException;
+import org.apache.beam.sdk.schemas.SchemaCoder;
+import org.apache.beam.sdk.transforms.Count;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.PTransform;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.transforms.SerializableFunction;
+import org.apache.beam.sdk.transforms.View;
+import org.apache.beam.sdk.transforms.Watch.Growth.TerminationCondition;
+import org.apache.beam.sdk.transforms.display.DisplayData;
+import org.apache.beam.sdk.values.KV;
+import org.apache.beam.sdk.values.PBegin;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.sdk.values.PCollectionView;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.joda.time.Duration;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * {@link PTransform}s that read text files and collect contextual information of the elements in
+ * the input.
+ *
+ * <p>Use {@link TextIO} when not reading file with Multiline Records or additional metadata is not
+ * required.
+ *
+ * <h2>Reading from text files</h2>
+ *
+ * <p>To read a {@link PCollection} from one or more text files, use {@code
+ * ContextualTextIO.read()}. To instantiate a transform use {@link
+ * ContextualTextIO.Read#from(String)} and specify the path of the file(s) to be read.
+ * Alternatively, if the filenames to be read are themselves in a {@link PCollection} you can use
+ * {@link FileIO} to match them and {@link ContextualTextIO#readFiles()} to read them.
+ *
+ * <p>{@link #read} returns a {@link PCollection} of {@link RecordWithMetadata RecordWithMetadata},
+ * each corresponding to one line of an input UTF-8 text file (split into lines delimited by '\n',
+ * '\r', '\r\n', or specified delimiter see {@link ContextualTextIO.Read#withDelimiter})
+ *
+ * <h3>Filepattern expansion and watching</h3>
+ *
+ * <p>By default, the filepatterns are expanded only once. The combination of {@link
+ * FileIO.Match#continuously(Duration, TerminationCondition)} and {@link #readFiles()} allow
+ * streaming of new files matching the filepattern(s).
+ *
+ * <p>By default, {@link #read} prohibits filepatterns that match no files, and {@link #readFiles()}
+ * allows them in case the filepattern contains a glob wildcard character. Use {@link
+ * ContextualTextIO.Read#withEmptyMatchTreatment} or {@link
+ * FileIO.Match#withEmptyMatchTreatment(EmptyMatchTreatment)} plus {@link #readFiles()} to configure
+ * this behavior.
+ *
+ * <p>Example 1: reading a file or filepattern.
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * // A simple Read of a file:
+ * PCollection<RecordWithMetadata> records = p.apply(ContextualTextIO.read().from("/local/path/to/file.txt"));
+ * }</pre>
+ *
+ * <p>Example 2: reading a PCollection of filenames.
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * // E.g. the filenames might be computed from other data in the pipeline, or
+ * // read from a data source.
+ * PCollection<String> filenames = ...;
+ *
+ * // Read all files in the collection.
+ * PCollection<RecordWithMetadata> records =
+ *     filenames
+ *         .apply(FileIO.matchAll())
+ *         .apply(FileIO.readMatches())
+ *         .apply(ContextualTextIO.readFiles());
+ * }</pre>
+ *
+ * <p>Example 3: streaming new files matching a filepattern.
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * PCollection<RecordWithMetadata> records = p.apply(ContextualTextIO.read()
+ *     .from("/local/path/to/files/*")
+ *     .watchForNewFiles(
+ *       // Check for new files every minute
+ *       Duration.standardMinutes(1),
+ *       // Stop watching the filepattern if no new files appear within an hour
+ *       afterTimeSinceNewOutput(Duration.standardHours(1))));
+ * }</pre>
+ *
+ * <p>Example 4: reading a file or file pattern of RFC4180-compliant CSV files with fields that may
+ * contain line breaks.
+ *
+ * <p>Example of such a file could be:
+ *
+ * <p>"aaa","b CRLF bb","ccc" CRLF zzz,yyy,xxx
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * PCollection<RecordWithMetadata> records = p.apply(ContextualTextIO.read()
+ *     .from("/local/path/to/files/*.csv")
+ *      .withHasMultilineCSVRecords(true));
+ * }</pre>
+ *
+ * <p>Example 5: reading while watching for new files
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * PCollection<RecordWithMetadata> records = p.apply(FileIO.match()
+ *      .filepattern("filepattern")
+ *      .continuously(
+ *        Duration.millis(100),
+ *        Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))
+ *      .apply(FileIO.readMatches())
+ *      .apply(ContextualTextIO.readFiles());
+ * }</pre>
+ *
+ * <p>Example 6: reading without recordNum metadata, or only fileName associated Metadata. (the
+ * Objects would still contain recordNums, but these recordNums would correspond to their positions
+ * in their respective offsets rather than their positions within the entire file).
+ *
+ * <pre>{@code
+ * Pipeline p = ...;
+ *
+ * PCollection<RecordWithMetadata> records = p.apply(ContextualTextIO.read()
+ *     .from("/local/path/to/files/*.csv")
+ *      .setWithoutRecordNumMetadata(true));
+ * }</pre>
+ *
+ * <p>NOTE: When using {@link ContextualTextIO.Read#withHasMultilineCSVRecords(Boolean)} this
+ * option, a single reader will be used to process the file, rather than multiple readers which can
+ * read from different offsets. For a large file this can result in lower performance.
+ *
+ * <p>NOTE: Use {@link Read#withoutRecordNumMetadata()} when recordNum metadata is not required, for
+ * example, when when only filename metadata is required. Computing record positions currently
+ * introduces a shuffle step, which increases the resources used by the pipeline. <b> By default
+ * withoutRecordNumMetadata is set to false, so the shuffle step is performed.</b>
+ *
+ * <h3>Reading a very large number of files</h3>
+ *
+ * <p>If it is known that the filepattern will match a very large number of files (e.g. tens of
+ * thousands or more), use {@link ContextualTextIO.Read#withHintMatchesManyFiles} for better
+ * performance and scalability. Note that it may decrease performance if the filepattern matches
+ * only a small number of files.
+ */
+public class ContextualTextIO {
+  private static final long DEFAULT_BUNDLE_SIZE_BYTES = 64 * 1024 * 1024L;
+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIO.class);
+
+  /**
+   * A {@link PTransform} that reads from one or more text files and returns a bounded {@link
+   * PCollection} containing one {@link RecordWithMetadata}element for each line of the input files.
+   */
+  public static Read read() {
+    return new AutoValue_ContextualTextIO_Read.Builder()
+        .setCompression(Compression.AUTO)
+        .setHintMatchesManyFiles(false)
+        .setWithoutRecordNumMetadata(false)
+        .setMatchConfiguration(MatchConfiguration.create(EmptyMatchTreatment.DISALLOW))
+        .setHasMultilineCSVRecords(false)
+        .build();
+  }
+
+  /**
+   * Like {@link #read}, but reads each file in a {@link PCollection} of {@link
+   * FileIO.ReadableFile}, returned by {@link FileIO#readMatches}.
+   */
+  public static ReadFiles readFiles() {
+    return new AutoValue_ContextualTextIO_ReadFiles.Builder()
+        // 64MB is a reasonable value that allows to amortize the cost of opening files,
+        // but is not so large as to exhaust a typical runner's maximum amount of output per
+        // ProcessElement call.
+        .setDesiredBundleSizeBytes(DEFAULT_BUNDLE_SIZE_BYTES)
+        .setHasMultilineCSVRecords(false)
+        .build();
+  }
+
+  /** Implementation of {@link #read}. */
+  @AutoValue
+  public abstract static class Read extends PTransform<PBegin, PCollection<RecordWithMetadata>> {
+    abstract @Nullable ValueProvider<String> getFilepattern();
+
+    abstract MatchConfiguration getMatchConfiguration();
+
+    abstract boolean getHintMatchesManyFiles();
+
+    abstract boolean getWithoutRecordNumMetadata();
+
+    abstract Compression getCompression();
+
+    abstract @Nullable Boolean getHasMultilineCSVRecords();
+
+    @SuppressWarnings("mutable") // this returns an array that can be mutated by the caller
+    abstract byte @Nullable [] getDelimiter();
+
+    abstract Builder toBuilder();
+
+    @AutoValue.Builder
+    abstract static class Builder {
+      abstract Builder setFilepattern(ValueProvider<String> filepattern);
+
+      abstract Builder setMatchConfiguration(MatchConfiguration matchConfiguration);
+
+      abstract Builder setHintMatchesManyFiles(boolean hintManyFiles);
+
+      abstract Builder setWithoutRecordNumMetadata(boolean withoutLineNumMetadata);
+
+      abstract Builder setCompression(Compression compression);
+
+      abstract Builder setDelimiter(byte @Nullable [] delimiter);
+
+      abstract Builder setHasMultilineCSVRecords(Boolean hasMultilineCSVRecords);
+
+      abstract Read build();
+    }
+
+    /**
+     * Reads text from the file(s) with the given filename or filename pattern.
+     *
+     * <p>This can be a local path (if running locally), or a Google Cloud Storage filename or
+     * filename pattern of the form {@code "gs://<bucket>/<filepath>"} (if running locally or using
+     * remote execution service).
+     *
+     * <p>Standard <a href="http://docs.oracle.com/javase/tutorial/essential/io/find.html" >Java
+     * Filesystem glob patterns</a> ("*", "?", "[..]") are supported.
+     *
+     * <p>If it is known that the filepattern will match a very large number of files (at least tens
+     * of thousands), use {@link #withHintMatchesManyFiles} for better performance and scalability.
+     */
+    public Read from(String filepattern) {
+      checkArgument(filepattern != null, "filepattern can not be null");
+      return from(StaticValueProvider.of(filepattern));
+    }
+
+    /** Same as {@code from(filepattern)}, but accepting a {@link ValueProvider}. */
+    public Read from(ValueProvider<String> filepattern) {
+      checkArgument(filepattern != null, "filepattern can not be null");
+      return toBuilder().setFilepattern(filepattern).build();
+    }
+
+    /** Sets the {@link MatchConfiguration}. */
+    public Read withMatchConfiguration(MatchConfiguration matchConfiguration) {
+      return toBuilder().setMatchConfiguration(matchConfiguration).build();
+    }
+
+    /**
+     * When reading RFC4180 CSV files that have values that span multiple lines, set this to true.
+     * Note: this reduces the read performance (see: {@link ContextualTextIO}).
+     */
+    public Read withHasMultilineCSVRecords(Boolean hasMultilineCSVRecords) {
+      return toBuilder().setHasMultilineCSVRecords(hasMultilineCSVRecords).build();
+    }
+
+    /**
+     * Reads from input sources using the specified compression type.
+     *
+     * <p>If no compression type is specified, the default is {@link Compression#AUTO}.
+     */
+    public Read withCompression(Compression compression) {
+      return toBuilder().setCompression(compression).build();
+    }
+
+    /**
+     * Hints that the filepattern specified in {@link #from(String)} matches a very large number of
+     * files.
+     *
+     * <p>This hint may cause a runner to execute the transform differently, in a way that improves
+     * performance for this case, but it may worsen performance if the filepattern matches only a
+     * small number of files (e.g., in a runner that supports dynamic work rebalancing, it will
+     * happen less efficiently within individual files).
+     */
+    public Read withHintMatchesManyFiles() {
+      return toBuilder().setHintMatchesManyFiles(true).build();
+    }
+
+    /**
+     * Allows the user to opt out of getting recordNums associated with each record.
+     *
+     * <p>When set to true, it will introduce a shuffle step to assemble the recordNums for each
+     * record, which will increase the resources used by the pipeline.
+     *
+     * <p>Use this when metadata like fileNames are required and their position/order can be
+     * ignored.
+     */
+    public Read withoutRecordNumMetadata() {
+      return toBuilder().setWithoutRecordNumMetadata(true).build();
+    }
+
+    /** See {@link MatchConfiguration#withEmptyMatchTreatment}. */
+    public Read withEmptyMatchTreatment(EmptyMatchTreatment treatment) {
+      return withMatchConfiguration(getMatchConfiguration().withEmptyMatchTreatment(treatment));
+    }
+
+    /** Set the custom delimiter to be used in place of the default ones ('\r', '\n' or '\r\n'). */
+    public Read withDelimiter(byte[] delimiter) {
+      checkArgument(delimiter != null, "delimiter can not be null");
+      checkArgument(!isSelfOverlapping(delimiter), "delimiter must not self-overlap");
+      return toBuilder().setDelimiter(delimiter).build();
+    }
+
+    static boolean isSelfOverlapping(byte[] s) {
+      // s self-overlaps if v exists such as s = vu = wv with u and w non empty
+      for (int i = 1; i < s.length - 1; ++i) {
+        if (ByteBuffer.wrap(s, 0, i).equals(ByteBuffer.wrap(s, s.length - i, i))) {
+          return true;
+        }
+      }
+      return false;
+    }
+
+    @Override
+    public PCollection<RecordWithMetadata> expand(PBegin input) {
+      checkNotNull(
+          getFilepattern(), "need to set the filepattern of a ContextualTextIO.Read transform");
+      PCollection<RecordWithMetadata> records = null;
+      if (getMatchConfiguration().getWatchInterval() == null && !getHintMatchesManyFiles()) {
+        records = input.apply("Read", org.apache.beam.sdk.io.Read.from(getSource()));
+      } else {
+        // All other cases go through FileIO + ReadFiles
+        records =
+            input
+                .apply(
+                    "Create filepattern", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))
+                .apply("Match All", FileIO.matchAll().withConfiguration(getMatchConfiguration()))
+                .apply(
+                    "Read Matches",
+                    FileIO.readMatches()
+                        .withCompression(getCompression())
+                        .withDirectoryTreatment(DirectoryTreatment.PROHIBIT))
+                .apply("Via ReadFiles", readFiles().withDelimiter(getDelimiter()));
+      }
+
+      // Check if the user decided to opt out of recordNums associated with records
+      if (getWithoutRecordNumMetadata()) {
+        return records;
+      }
+
+      /*
+       * At this point the line number in RecordWithMetadata contains the relative line offset from the beginning of the read range.
+       *
+       * To compute the absolute position from the beginning of the input we group the lines within the same ranges, and evaluate the size of each range.
+       */
+
+      PCollection<KV<KV<String, Long>, RecordWithMetadata>> recordsGroupedByFileAndRange =
+          records.apply("AddFileNameAndRange", ParDo.of(new AddFileNameAndRange()));
+
+      PCollectionView<Map<KV<String, Long>, Long>> rangeSizes =
+          recordsGroupedByFileAndRange
+              .apply("CountRecordsForEachFileRange", Count.perKey())
+              .apply("SizesAsView", View.asMap());
+
+      // Get Pipeline to create a dummy PCollection with one element to help compute the lines
+      // before each Range
+      PCollection<Integer> singletonPcoll =
+          input.getPipeline().apply("CreateSingletonPcoll", Create.of(Arrays.asList(1)));
+
+      /*
+       * For each (File, Offset) pair, calculate the number of lines occurring before the Range for each file
+       *
+       * After computing the number of lines before each range, we can find the line number in original file as numLiesBeforeOffset + lineNumInCurrentOffset
+       */
+
+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange =
+          singletonPcoll
+              .apply(
+                  "ComputeRecordsBeforeRange",
+                  ParDo.of(new ComputeRecordsBeforeEachRange(rangeSizes))
+                      .withSideInputs(rangeSizes))
+              .apply("NumRecordsBeforeEachRangeAsView", View.asMap());
+
+      return recordsGroupedByFileAndRange.apply(
+          "AssignLineNums",
+          ParDo.of(new AssignRecordNums(numRecordsBeforeEachRange))
+              .withSideInputs(numRecordsBeforeEachRange));
+    }
+
+    @VisibleForTesting
+    static class AddFileNameAndRange
+        extends DoFn<RecordWithMetadata, KV<KV<String, Long>, RecordWithMetadata>> {
+      @ProcessElement
+      public void processElement(
+          @Element RecordWithMetadata record,
+          OutputReceiver<KV<KV<String, Long>, RecordWithMetadata>> out) {
+        out.output(KV.of(KV.of(record.getFileName().toString(), record.getRangeOffset()), record));
+      }
+    }
+
+    /**
+     * Helper class for computing number of record in the File preceding the beginning of the Range
+     * in this file.
+     */
+    @VisibleForTesting
+    static class ComputeRecordsBeforeEachRange extends DoFn<Integer, KV<KV<String, Long>, Long>> {
+      private final PCollectionView<Map<KV<String, Long>, Long>> rangeSizes;
+
+      public ComputeRecordsBeforeEachRange(
+          PCollectionView<Map<KV<String, Long>, Long>> rangeSizes) {
+        this.rangeSizes = rangeSizes;
+      }
+
+      // Add custom comparator as KV<K, V> is not comparable by default
+      private static class FileRangeComparator<K extends Comparable<K>, V extends Comparable<V>>
+          implements Comparator<KV<K, V>> {
+        @Override
+        public int compare(KV<K, V> a, KV<K, V> b) {
+          if (a.getKey().compareTo(b.getKey()) == 0) {
+            return a.getValue().compareTo(b.getValue());
+          }
+          return a.getKey().compareTo(b.getKey());
+        }
+      }
+
+      @ProcessElement
+      public void processElement(ProcessContext p) {
+        // Get the Map Containing the size from side-input
+        Map<KV<String, Long>, Long> rangeSizesMap = p.sideInput(rangeSizes);
+
+        // The FileRange Pair must be sorted
+        SortedMap<KV<String, Long>, Long> sorted = new TreeMap<>(new FileRangeComparator<>());
+
+        // Initialize sorted map with values
+        for (Map.Entry<KV<String, Long>, Long> entry : rangeSizesMap.entrySet()) {
+          sorted.put(entry.getKey(), entry.getValue());
+        }
+
+        // HashMap that tracks number of records passed for each file
+        Map<String, Long> pastRecords = new HashMap<>();
+
+        // For each (File, Range) Pair, compute the number of records before it
+        for (Map.Entry<KV<String, Long>, Long> entry : sorted.entrySet()) {
+          Long numRecords = (long) entry.getValue();
+          KV<String, Long> fileRange = (KV<String, Long>) entry.getKey();
+          String file = fileRange.getKey();
+          Long numRecordsBefore = 0L;
+          if (pastRecords.containsKey(file)) {
+            numRecordsBefore = pastRecords.get(file);
+          }
+          p.output(KV.of(fileRange, numRecordsBefore));
+          pastRecords.put(file, numRecordsBefore + numRecords);
+        }
+      }
+    }
+
+    static class AssignRecordNums
+        extends DoFn<KV<KV<String, Long>, RecordWithMetadata>, RecordWithMetadata> {
+      PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange;
+
+      public AssignRecordNums(
+          PCollectionView<Map<KV<String, Long>, Long>> numRecordsBeforeEachRange) {
+        this.numRecordsBeforeEachRange = numRecordsBeforeEachRange;
+      }
+
+      @ProcessElement
+      public void processElement(ProcessContext p) {
+        Long range = p.element().getKey().getValue();
+        String file = p.element().getKey().getKey();
+        RecordWithMetadata record = p.element().getValue();
+        Long numRecordsLessThanThisRange =
+            p.sideInput(numRecordsBeforeEachRange).get(KV.of(file, range));
+        // update the recordNum in record
+        record =
+            record
+                .toBuilder()
+                .setRecordNum(record.getRecordNumInOffset() + numRecordsLessThanThisRange)
+                .build();
+        p.output(record);
+      }
+    }
+
+    // Helper to create a source specific to the requested compression type.
+    protected FileBasedSource<RecordWithMetadata> getSource() {
+      return CompressedSource.from(
+              new ContextualTextIOSource(
+                  getFilepattern(),
+                  getMatchConfiguration().getEmptyMatchTreatment(),
+                  getDelimiter(),
+                  getHasMultilineCSVRecords()))
+          .withCompression(getCompression());
+    }
+
+    @Override
+    public void populateDisplayData(DisplayData.Builder builder) {
+      super.populateDisplayData(builder);
+      builder
+          .add(
+              DisplayData.item("compressionType", getCompression().toString())
+                  .withLabel("Compression Type"))
+          .addIfNotNull(DisplayData.item("filePattern", getFilepattern()).withLabel("File Pattern"))
+          .include("matchConfiguration", getMatchConfiguration())
+          .addIfNotNull(
+              DisplayData.item("delimiter", Arrays.toString(getDelimiter()))
+                  .withLabel("Custom delimiter to split records"))
+          .addIfNotNull(
+              DisplayData.item("hasMultilineCSVRecords", getHasMultilineCSVRecords())
+                  .withLabel("Has RFC4180 MultiLineCSV Records"));
+    }
+  }
+
+  /** Implementation of {@link #readFiles}. */
+  @AutoValue
+  public abstract static class ReadFiles
+      extends PTransform<PCollection<FileIO.ReadableFile>, PCollection<RecordWithMetadata>> {
+    abstract long getDesiredBundleSizeBytes();
+
+    @SuppressWarnings("mutable") // this returns an array that can be mutated by the caller
+    abstract byte @Nullable [] getDelimiter();
+
+    abstract boolean getHasMultilineCSVRecords();
+
+    abstract Builder toBuilder();
+
+    @AutoValue.Builder
+    abstract static class Builder {
+      abstract Builder setDesiredBundleSizeBytes(long desiredBundleSizeBytes);
+
+      abstract Builder setHasMultilineCSVRecords(boolean hasMultilineCSVRecords);
+
+      abstract Builder setDelimiter(byte @Nullable [] delimiter);
+
+      abstract ReadFiles build();
+    }
+
+    @VisibleForTesting
+    ReadFiles withDesiredBundleSizeBytes(long desiredBundleSizeBytes) {
+      return toBuilder().setDesiredBundleSizeBytes(desiredBundleSizeBytes).build();
+    }
+
+    /** Like {@link Read#withDelimiter}. */
+    public ReadFiles withDelimiter(byte[] delimiter) {
+      return toBuilder().setDelimiter(delimiter).build();
+    }
+
+    @Override
+    public PCollection<RecordWithMetadata> expand(PCollection<FileIO.ReadableFile> input) {
+      SchemaCoder<RecordWithMetadata> coder = null;
+      try {
+        coder = input.getPipeline().getSchemaRegistry().getSchemaCoder(RecordWithMetadata.class);
+      } catch (NoSuchSchemaException e) {
+        LOG.error("No Coder Found for RecordWithMetadata");
+      }
+      return input.apply(
+          "Read all via FileBasedSource",
+          new ReadAllViaFileBasedSource<>(
+              getDesiredBundleSizeBytes(),
+              new CreateTextSourceFn(getDelimiter(), getHasMultilineCSVRecords()),
+              coder));
+    }
+
+    @Override
+    public void populateDisplayData(DisplayData.Builder builder) {
+      super.populateDisplayData(builder);
+      builder.addIfNotNull(
+          DisplayData.item("delimiter", Arrays.toString(getDelimiter()))
+              .withLabel("Custom delimiter to split records"));
+    }
+
+    private static class CreateTextSourceFn
+        implements SerializableFunction<String, FileBasedSource<RecordWithMetadata>> {
+      private byte[] delimiter;
+      private boolean hasMultilineCSVRecords;
+
+      private CreateTextSourceFn(byte[] delimiter, boolean hasMultilineCSVRecords) {
+        this.delimiter = delimiter;
+        this.hasMultilineCSVRecords = hasMultilineCSVRecords;
+      }
+
+      @Override
+      public FileBasedSource<RecordWithMetadata> apply(String input) {
+        return new ContextualTextIOSource(
+            StaticValueProvider.of(input),
+            EmptyMatchTreatment.DISALLOW,
+            delimiter,
+            hasMultilineCSVRecords);
+      }
+    }
+  }
+
+  /** Disable construction of utility class. */
+  private ContextualTextIO() {}
+}
diff --git a/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOSource.java b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOSource.java
new file mode 100644
index 000000000000..51db88405ce1
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOSource.java
@@ -0,0 +1,361 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.contextualtextio;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.ReadableByteChannel;
+import java.nio.channels.SeekableByteChannel;
+import java.util.NoSuchElementException;
+import org.apache.beam.sdk.coders.Coder;
+import org.apache.beam.sdk.io.FileBasedSource;
+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;
+import org.apache.beam.sdk.io.fs.MatchResult;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.options.ValueProvider;
+import org.apache.beam.sdk.schemas.NoSuchSchemaException;
+import org.apache.beam.sdk.schemas.SchemaCoder;
+import org.apache.beam.sdk.schemas.SchemaRegistry;
+import org.apache.beam.vendor.grpc.v1p26p0.com.google.protobuf.ByteString;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions;
+import org.checkerframework.checker.nullness.qual.Nullable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Implementation detail of {@link ContextualTextIO.Read}.
+ *
+ * <p>A {@link FileBasedSource} which can decode records delimited by newline characters.
+ *
+ * <p>This source splits the data into records using {@code UTF-8} {@code \n}, {@code \r}, or {@code
+ * \r\n} as the delimiter. This source is not strict and supports decoding the last record even if
+ * it is not delimited. Finally, no records are decoded if the stream is empty.
+ *
+ * <p>This source supports reading from any arbitrary byte position within the stream. If the
+ * starting position is not {@code 0}, then bytes are skipped until the first delimiter is found
+ * representing the beginning of the first record to be decoded.
+ */
+@VisibleForTesting
+class ContextualTextIOSource extends FileBasedSource<RecordWithMetadata> {
+  byte[] delimiter;
+
+  private static final Logger LOG = LoggerFactory.getLogger(ContextualTextIOSource.class);
+
+  // Used to Override isSplittable
+  private boolean hasMultilineCSVRecords;
+
+  @Override
+  protected boolean isSplittable() throws Exception {
+    if (hasMultilineCSVRecords) {
+      // When Having Multiline CSV Records,
+      // Splitting the file may cause a split to be within a record,
+      // Disabling split prevents this from happening
+      return false;
+    }
+    return super.isSplittable();
+  }
+
+  ContextualTextIOSource(
+      ValueProvider<String> fileSpec,
+      EmptyMatchTreatment emptyMatchTreatment,
+      byte[] delimiter,
+      boolean hasMultilineCSVRecords) {
+    super(fileSpec, emptyMatchTreatment, 1L);
+    this.delimiter = delimiter;
+    this.hasMultilineCSVRecords = hasMultilineCSVRecords;
+  }
+
+  private ContextualTextIOSource(
+      MatchResult.Metadata metadata,
+      long start,
+      long end,
+      byte[] delimiter,
+      boolean hasMultilineCSVRecords) {
+    super(metadata, 1L, start, end);
+    this.delimiter = delimiter;
+    this.hasMultilineCSVRecords = hasMultilineCSVRecords;
+  }
+
+  @Override
+  protected FileBasedSource<RecordWithMetadata> createForSubrangeOfFile(
+      MatchResult.Metadata metadata, long start, long end) {
+    return new ContextualTextIOSource(metadata, start, end, delimiter, hasMultilineCSVRecords);
+  }
+
+  @Override
+  protected FileBasedReader<RecordWithMetadata> createSingleFileReader(PipelineOptions options) {
+    return new MultiLineTextBasedReader(this, delimiter, hasMultilineCSVRecords);
+  }
+
+  /** Returns the {@link Coder Coder} for {@link RecordWithMetadata RecordWithMetadata} */
+  @Override
+  public Coder<RecordWithMetadata> getOutputCoder() {
+    SchemaCoder<RecordWithMetadata> coder = null;
+    try {
+      coder = SchemaRegistry.createDefault().getSchemaCoder(RecordWithMetadata.class);
+    } catch (NoSuchSchemaException e) {
+      LOG.error("No Coder Found for RecordWithMetadata");
+    }
+    return coder;
+  }
+
+  /**
+   * A {@link FileBasedReader FileBasedReader} which can decode records delimited by delimiter
+   * characters.
+   *
+   * <p>See {@link ContextualTextIOSource} for further details.
+   */
+  @VisibleForTesting
+  static class MultiLineTextBasedReader extends FileBasedReader<RecordWithMetadata> {
+    public static final int READ_BUFFER_SIZE = 8192;
+    private static final ByteString UTF8_BOM =
+        ByteString.copyFrom(new byte[] {(byte) 0xEF, (byte) 0xBB, (byte) 0xBF});
+    private final ByteBuffer readBuffer = ByteBuffer.allocate(READ_BUFFER_SIZE);
+    private ByteString buffer;
+    private int startOfDelimiterInBuffer;
+    private int endOfDelimiterInBuffer;
+    private long startOfRecord;
+    private volatile long startOfNextRecord;
+    private volatile boolean eof;
+    private volatile boolean elementIsPresent;
+    private @Nullable RecordWithMetadata currentValue;
+    private @Nullable ReadableByteChannel inChannel;
+    private byte @Nullable [] delimiter;
+
+    // Add to override the isSplittable
+    private boolean hasMultilineCSVRecords;
+
+    private long startingOffset;
+    private long totalRecordCount;
+
+    private MultiLineTextBasedReader(
+        ContextualTextIOSource source, byte[] delimiter, boolean hasMultilineCSVRecords) {
+      super(source);
+      buffer = ByteString.EMPTY;
+      this.delimiter = delimiter;
+      this.hasMultilineCSVRecords = hasMultilineCSVRecords;
+      startingOffset = getCurrentSource().getStartOffset(); // Start offset;
+    }
+
+    @Override
+    protected long getCurrentOffset() throws NoSuchElementException {
+      if (!elementIsPresent) {
+        throw new NoSuchElementException();
+      }
+      return startOfRecord;
+    }
+
+    @Override
+    public long getSplitPointsRemaining() {
+      if (isStarted() && startOfNextRecord >= getCurrentSource().getEndOffset()) {
+        return isDone() ? 0 : 1;
+      }
+      return super.getSplitPointsRemaining();
+    }
+
+    @Override
+    public RecordWithMetadata getCurrent() throws NoSuchElementException {
+      if (!elementIsPresent) {
+        throw new NoSuchElementException();
+      }
+      return currentValue;
+    }
+
+    @Override
+    protected void startReading(ReadableByteChannel channel) throws IOException {
+      this.inChannel = channel;
+      // If the first offset is greater than zero, we need to skip bytes until we see our
+      // first delimiter.
+      long startOffset = getCurrentSource().getStartOffset();
+      if (startOffset > 0) {
+        Preconditions.checkState(
+            channel instanceof SeekableByteChannel,
+            "%s only supports reading from a SeekableByteChannel when given a start offset"
+                + " greater than 0.",
+            ContextualTextIOSource.class.getSimpleName());
+        long requiredPosition = startOffset - 1;
+        if (delimiter != null && startOffset >= delimiter.length) {
+          // we need to move back the offset of at worse delimiter.size to be sure to see
+          // all the bytes of the delimiter in the call to findDelimiterBounds() below
+          requiredPosition = startOffset - delimiter.length;
+        }
+        ((SeekableByteChannel) channel).position(requiredPosition);
+        findDelimiterBounds();
+        buffer = buffer.substring(endOfDelimiterInBuffer);
+        startOfNextRecord = requiredPosition + endOfDelimiterInBuffer;
+        endOfDelimiterInBuffer = 0;
+        startOfDelimiterInBuffer = 0;
+      }
+    }
+
+    /**
+     * Locates the start position and end position of the next delimiter. Will consume the channel
+     * till either EOF or the delimiter bounds are found.
+     *
+     * <p>If {@link ContextualTextIOSource#hasMultilineCSVRecords} is set then the behaviour will
+     * change from the standard read seen in {@link org.apache.beam.sdk.io.TextIO}. The assumption
+     * when {@link ContextualTextIOSource#hasMultilineCSVRecords} is set is that the file is being
+     * read with a single thread.
+     *
+     * <p>This fills the buffer and updates the positions as follows:
+     *
+     * <pre>{@code
+     * ------------------------------------------------------
+     * | element bytes | delimiter bytes | unconsumed bytes |
+     * ------------------------------------------------------
+     * 0            start of          end of              buffer
+     *              delimiter         delimiter           size
+     *              in buffer         in buffer
+     * }</pre>
+     */
+    private void findDelimiterBounds() throws IOException {
+      int bytePositionInBuffer = 0;
+      boolean doubleQuoteClosed = true;
+
+      while (true) {
+        if (!tryToEnsureNumberOfBytesInBuffer(bytePositionInBuffer + 1)) {
+          startOfDelimiterInBuffer = endOfDelimiterInBuffer = bytePositionInBuffer;
+          break;
+        }
+
+        byte currentByte = buffer.byteAt(bytePositionInBuffer);
+        if (hasMultilineCSVRecords) {
+          // Check if we are inside an open Quote
+          if (currentByte == '"') {
+            doubleQuoteClosed = !doubleQuoteClosed;
+          }
+        } else {
+          doubleQuoteClosed = true;
+        }
+
+        if (delimiter == null) {
+          // default delimiter
+          if (currentByte == '\n') {
+            startOfDelimiterInBuffer = bytePositionInBuffer;
+            endOfDelimiterInBuffer = startOfDelimiterInBuffer + 1;
+            if (doubleQuoteClosed) {
+              break;
+            }
+          } else if (currentByte == '\r') {
+            startOfDelimiterInBuffer = bytePositionInBuffer;
+            endOfDelimiterInBuffer = startOfDelimiterInBuffer + 1;
+            if (tryToEnsureNumberOfBytesInBuffer(bytePositionInBuffer + 2)) {
+              currentByte = buffer.byteAt(bytePositionInBuffer + 1);
+              if (currentByte == '\n') {
+                endOfDelimiterInBuffer += 1;
+              }
+            }
+            if (doubleQuoteClosed) {
+              break;
+            }
+          }
+        } else {
+          // when the user defines a delimiter
+          int i = 0;
+          startOfDelimiterInBuffer = endOfDelimiterInBuffer = bytePositionInBuffer;
+          while ((i < delimiter.length) && (currentByte == delimiter[i])) {
+            // read next byte;
+            i++;
+            if (tryToEnsureNumberOfBytesInBuffer(bytePositionInBuffer + i + 1)) {
+              currentByte = buffer.byteAt(bytePositionInBuffer + i);
+            } else {
+              // corner case: delimiter truncate at the end of file
+              startOfDelimiterInBuffer = endOfDelimiterInBuffer = bytePositionInBuffer;
+              break;
+            }
+          }
+          if (i == delimiter.length) {
+            endOfDelimiterInBuffer = bytePositionInBuffer + i;
+            if (doubleQuoteClosed) {
+              break;
+            }
+          }
+        }
+        bytePositionInBuffer += 1;
+      }
+    }
+
+    @Override
+    protected boolean readNextRecord() throws IOException {
+      startOfRecord = startOfNextRecord;
+
+      findDelimiterBounds();
+
+      // If we have reached EOF file and consumed all of the buffer then we know
+      // that there are no more records.
+      if (eof && buffer.isEmpty()) {
+        elementIsPresent = false;
+        return false;
+      }
+
+      decodeCurrentElement();
+      startOfNextRecord = startOfRecord + endOfDelimiterInBuffer;
+      return true;
+    }
+
+    /**
+     * Decodes the current element updating the buffer to only contain the unconsumed bytes.
+     *
+     * <p>This invalidates the currently stored {@code startOfDelimiterInBuffer} and {@code
+     * endOfDelimiterInBuffer}.
+     */
+    private void decodeCurrentElement() throws IOException {
+      ByteString dataToDecode = buffer.substring(0, startOfDelimiterInBuffer);
+      // If present, the UTF8 Byte Order Mark (BOM) will be removed.
+      if (startOfRecord == 0 && dataToDecode.startsWith(UTF8_BOM)) {
+        dataToDecode = dataToDecode.substring(UTF8_BOM.size());
+      }
+
+      // The line num is:
+      Long recordUniqueNum = totalRecordCount++;
+      // The Complete FileName (with uri if this is a web url eg: temp/abc.txt) is:
+      String fileName = getCurrentSource().getSingleFileMetadata().resourceId().toString();
+
+      // The single filename can be found as:
+      // fileName.substring(fileName.lastIndexOf('/') + 1);
+
+      currentValue =
+          RecordWithMetadata.newBuilder()
+              .setRecordNumInOffset(recordUniqueNum)
+              .setRangeOffset(startingOffset)
+              .setRecordOffset(startOfRecord)
+              .setRecordNum(recordUniqueNum)
+              .setFileName(fileName)
+              .setValue(dataToDecode.toStringUtf8())
+              .build();
+
+      elementIsPresent = true;
+      buffer = buffer.substring(endOfDelimiterInBuffer);
+    }
+
+    /** Returns false if we were unable to ensure the minimum capacity by consuming the channel. */
+    private boolean tryToEnsureNumberOfBytesInBuffer(int minCapacity) throws IOException {
+      // While we aren't at EOF or haven't fulfilled the minimum buffer capacity,
+      // attempt to read more bytes.
+      while (buffer.size() <= minCapacity && !eof) {
+        eof = inChannel.read(readBuffer) == -1;
+        readBuffer.flip();
+        buffer = buffer.concat(ByteString.copyFrom(readBuffer));
+        readBuffer.clear();
+      }
+      // Return true if we were able to honor the minimum buffer capacity request
+      return buffer.size() >= minCapacity;
+    }
+  }
+}
diff --git a/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/RecordWithMetadata.java b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/RecordWithMetadata.java
new file mode 100644
index 000000000000..1d2446855f70
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/RecordWithMetadata.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.contextualtextio;
+
+import com.google.auto.value.AutoValue;
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.schemas.AutoValueSchema;
+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;
+
+/**
+ * Helper Class based on {@link AutoValueSchema}, it provides Metadata associated with each Record
+ * when reading from file(s) using {@link ContextualTextIO}.
+ *
+ * <h3>Fields:</h3>
+ *
+ * <ul>
+ *   <li>recordOffset: The offset of a record (the byte at which the record begins) in a file. This
+ *       information can be useful if you wish to reconstruct the file. {@link
+ *       RecordWithMetadata#getRecordOffset()}
+ *   <li>recordNum: The ordinal number of the record in its file. {@link
+ *       RecordWithMetadata#getRecordNum()}
+ *   <li>recordValue: The value / contents of the record. {@link RecordWithMetadata#getValue()}
+ *   <li>rangeOffset: The starting offset of the range (split), which contained the record, when the
+ *       record was read. {@link RecordWithMetadata#getRangeOffset()}
+ *   <li>recordNumInOffset: The record number relative to the Range. (line number within the range)
+ *       {@link RecordWithMetadata#getRecordNumInOffset()}
+ *   <li>fileName: Name of the file to which the record belongs (this is the full filename,
+ *       eg:path/to/file.txt). {@link RecordWithMetadata#getFileName()}
+ * </ul>
+ */
+@Experimental(Experimental.Kind.SCHEMAS)
+@DefaultSchema(AutoValueSchema.class)
+@AutoValue
+public abstract class RecordWithMetadata {
+  /**
+   * Returns the offset of the record (the byte at which the record begins) in a file. This
+   * information can be useful if you wish to reconstruct the file.
+   */
+  public abstract long getRecordOffset();
+
+  /** Returns the ordinal number of the record in its file. */
+  public abstract long getRecordNum();
+
+  /** Returns the value / content of the Record */
+  public abstract String getValue();
+
+  /**
+   * Returns the starting offset of the range (split), which contained the record, when the record
+   * was read.
+   */
+  public abstract long getRangeOffset();
+
+  /** Returns the record number relative to the Range. */
+  public abstract long getRecordNumInOffset();
+
+  /**
+   * Returns the name of the file to which the record belongs (this is the full filename,
+   * eg:path/to/file.txt).
+   */
+  public abstract String getFileName();
+
+  public abstract Builder toBuilder();
+
+  public static Builder newBuilder() {
+    return new AutoValue_RecordWithMetadata.Builder();
+  }
+
+  @AutoValue.Builder
+  public abstract static class Builder {
+    public abstract Builder setRecordNum(long lineNum);
+
+    public abstract Builder setRecordOffset(long recordOffset);
+
+    public abstract Builder setValue(String value);
+
+    public abstract Builder setFileName(String fileName);
+
+    public abstract Builder setRecordNumInOffset(long recordNumInOffset);
+
+    public abstract Builder setRangeOffset(long startingOffset);
+
+    public abstract RecordWithMetadata build();
+  }
+}
diff --git a/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/package-info.java b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/package-info.java
new file mode 100644
index 000000000000..74d4c5cf3537
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/src/main/java/org/apache/beam/sdk/io/contextualtextio/package-info.java
@@ -0,0 +1,22 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+/** Transforms for reading from Files with contextual Information. */
+@Experimental(Experimental.Kind.UNSPECIFIED)
+package org.apache.beam.sdk.io.contextualtextio;
+
+import org.apache.beam.sdk.annotations.Experimental;
diff --git a/sdks/java/io/contextual-text-io/src/test/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOTest.java b/sdks/java/io/contextual-text-io/src/test/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOTest.java
new file mode 100644
index 000000000000..89498ade90fe
--- /dev/null
+++ b/sdks/java/io/contextual-text-io/src/test/java/org/apache/beam/sdk/io/contextualtextio/ContextualTextIOTest.java
@@ -0,0 +1,1266 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.contextualtextio;
+
+import static java.nio.charset.StandardCharsets.UTF_8;
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.assertFalse;
+import static junit.framework.Assert.assertNotNull;
+import static junit.framework.Assert.assertTrue;
+import static org.apache.beam.sdk.TestUtils.LINES_ARRAY;
+import static org.apache.beam.sdk.TestUtils.NO_LINES_ARRAY;
+import static org.apache.beam.sdk.io.Compression.AUTO;
+import static org.apache.beam.sdk.io.Compression.BZIP2;
+import static org.apache.beam.sdk.io.Compression.DEFLATE;
+import static org.apache.beam.sdk.io.Compression.GZIP;
+import static org.apache.beam.sdk.io.Compression.UNCOMPRESSED;
+import static org.apache.beam.sdk.io.Compression.ZIP;
+import static org.apache.beam.sdk.transforms.display.DisplayDataMatchers.hasDisplayItem;
+import static org.apache.beam.sdk.values.TypeDescriptors.strings;
+import static org.hamcrest.Matchers.containsInAnyOrder;
+import static org.hamcrest.Matchers.equalTo;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.hasSize;
+import static org.junit.Assert.assertThat;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.io.PrintStream;
+import java.io.Writer;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.IntStream;
+import java.util.zip.GZIPOutputStream;
+import java.util.zip.ZipEntry;
+import java.util.zip.ZipOutputStream;
+import org.apache.beam.sdk.Pipeline;
+import org.apache.beam.sdk.coders.StringUtf8Coder;
+import org.apache.beam.sdk.io.BoundedSource;
+import org.apache.beam.sdk.io.Compression;
+import org.apache.beam.sdk.io.FileBasedSource;
+import org.apache.beam.sdk.io.FileIO;
+import org.apache.beam.sdk.io.GenerateSequence;
+import org.apache.beam.sdk.io.TextIO;
+import org.apache.beam.sdk.io.fs.EmptyMatchTreatment;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.options.PipelineOptionsFactory;
+import org.apache.beam.sdk.options.ValueProvider;
+import org.apache.beam.sdk.testing.NeedsRunner;
+import org.apache.beam.sdk.testing.PAssert;
+import org.apache.beam.sdk.testing.SourceTestUtils;
+import org.apache.beam.sdk.testing.TestPipeline;
+import org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo;
+import org.apache.beam.sdk.transforms.Create;
+import org.apache.beam.sdk.transforms.DoFn;
+import org.apache.beam.sdk.transforms.MapElements;
+import org.apache.beam.sdk.transforms.ParDo;
+import org.apache.beam.sdk.transforms.ToString;
+import org.apache.beam.sdk.transforms.Watch;
+import org.apache.beam.sdk.transforms.display.DisplayData;
+import org.apache.beam.sdk.transforms.windowing.AfterPane;
+import org.apache.beam.sdk.transforms.windowing.FixedWindows;
+import org.apache.beam.sdk.transforms.windowing.Repeatedly;
+import org.apache.beam.sdk.transforms.windowing.Window;
+import org.apache.beam.sdk.util.CoderUtils;
+import org.apache.beam.sdk.values.PCollection;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Charsets;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Joiner;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;
+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;
+import org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream;
+import org.apache.commons.compress.compressors.deflate.DeflateCompressorOutputStream;
+import org.joda.time.Duration;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.TemporaryFolder;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+import org.junit.runners.Parameterized;
+
+/** Tests for {@link ContextualTextIO.Read}. */
+public class ContextualTextIOTest {
+  private static final int NUM_LINES_FOR_LARGE = 1024;
+
+  private static final List<String> EMPTY = Collections.emptyList();
+
+  private static final List<String> TINY = Arrays.asList("ABC", "DEF", "HIJ");
+
+  private static final List<String> LARGE = makeLines(NUM_LINES_FOR_LARGE);
+
+  private static File writeToFile(
+      List<String> lines, TemporaryFolder folder, String fileName, Compression compression)
+      throws IOException {
+    File file = folder.getRoot().toPath().resolve(fileName).toFile();
+    OutputStream output = new FileOutputStream(file);
+    switch (compression) {
+      case UNCOMPRESSED:
+        break;
+      case GZIP:
+        output = new GZIPOutputStream(output);
+        break;
+      case BZIP2:
+        output = new BZip2CompressorOutputStream(output);
+        break;
+      case ZIP:
+        ZipOutputStream zipOutput = new ZipOutputStream(output);
+        zipOutput.putNextEntry(new ZipEntry("entry"));
+        output = zipOutput;
+        break;
+      case DEFLATE:
+        output = new DeflateCompressorOutputStream(output);
+        break;
+      default:
+        throw new UnsupportedOperationException(compression.toString());
+    }
+    writeToStreamAndClose(lines, output);
+    return file;
+  }
+
+  /**
+   * Helper that writes the given lines (adding a newline in between) to a stream, then closes the
+   * stream.
+   */
+  private static void writeToStreamAndClose(List<String> lines, OutputStream outputStream) {
+    try (PrintStream writer = new PrintStream(outputStream)) {
+      for (String line : lines) {
+        writer.println(line);
+      }
+    }
+  }
+
+  /** Helper to make an array of compressible strings. Returns ["line" i] for i in range(0,n). */
+  private static List<String> makeLines(int n) {
+    List<String> lines = new ArrayList<>();
+    for (int i = 0; i < n; ++i) {
+      lines.add("Line " + i);
+    }
+    return lines;
+  }
+
+  private static class ConvertRecordWithMetadataToString extends DoFn<RecordWithMetadata, String> {
+    @ProcessElement
+    public void processElement(@Element RecordWithMetadata record, OutputReceiver<String> out) {
+      String resourceId = record.getFileName();
+      String file = resourceId.substring(resourceId.lastIndexOf('/') + 1);
+      out.output(file + " " + record.getRecordNum() + " " + record.getValue());
+    }
+  }
+
+  /**
+   * Helper method that runs a variety of ways to read a single file using ContextualTextIO and
+   * checks that they all match the given expected output.
+   *
+   * <p>The transforms being verified are:
+   *
+   * <ul>
+   *   <li>ContextualTextIO.read().from(filename).withCompression(compressionType).withHintMatchesManyFiles()
+   *   <li>ContextualTextIO.read().from(filename).withCompression(compressionType)
+   *   <li>ContextualTextIO.read().from(filename).withCompression(compressionType).withHasMultilineCSV(true)
+   *   <li>ContextualTextIO.readFiles().withCompression(compressionType)
+   * </ul>
+   */
+  private static void assertReadingCompressedFileMatchesExpected(
+      File file, Compression compression, List<String> expected, Pipeline p) {
+
+    ContextualTextIO.Read read =
+        ContextualTextIO.read().from(file.getPath()).withCompression(compression);
+
+    // Convert the expected output into RecordWithMetadata output Format
+    List<String> expectedOutput = new ArrayList<>();
+    for (int lineNum = 0; lineNum < expected.size(); ++lineNum) {
+      expectedOutput.add(file.getName() + " " + lineNum + " " + expected.get(lineNum));
+    }
+
+    PAssert.that(
+            p.apply("Read_" + file + "_" + compression.toString(), read)
+                .apply(
+                    "ConvertRecordWithMetadataToString",
+                    ParDo.of(new ConvertRecordWithMetadataToString())))
+        .containsInAnyOrder(expectedOutput);
+    PAssert.that(
+            p.apply(
+                    "Read_" + file + "_" + compression.toString() + "_many",
+                    read.withHintMatchesManyFiles())
+                .apply(
+                    "ConvertRecordWithMetadataToString" + "_many",
+                    ParDo.of(new ConvertRecordWithMetadataToString())))
+        .containsInAnyOrder(expectedOutput);
+
+    PAssert.that(
+            p.apply(
+                    "Read_" + file + "_" + compression.toString() + "_withRFC4180",
+                    read.withHasMultilineCSVRecords(true))
+                .apply(
+                    "ConvertRecordWithMetadataToString" + "_withRFC4180",
+                    ParDo.of(new ConvertRecordWithMetadataToString())))
+        .containsInAnyOrder(expectedOutput);
+
+    PAssert.that(
+            p.apply("Create_Paths_ReadFiles_" + file, Create.of(file.getPath()))
+                .apply("Match_" + file, FileIO.matchAll())
+                .apply("ReadMatches_" + file, FileIO.readMatches().withCompression(compression))
+                .apply("ReadFiles_" + compression.toString(), ContextualTextIO.readFiles())
+                .apply(
+                    "ConvertRecordWithMetadataToStringWithFileIO",
+                    ParDo.of(new ConvertRecordWithMetadataToString())))
+        .containsInAnyOrder(expectedOutput);
+  }
+
+  /**
+   * Create a zip file with the given lines.
+   *
+   * @param expected A list of expected lines, populated in the zip file.
+   * @param folder A temporary folder used to create files.
+   * @param filename Optionally zip file name (can be null).
+   * @param fieldsEntries Fields to write in zip entries.
+   * @return The zip filename.
+   * @throws Exception In case of a failure during zip file creation.
+   */
+  private static File createZipFile(
+      List<String> expected, TemporaryFolder folder, String filename, String[]... fieldsEntries)
+      throws Exception {
+    File tmpFile = folder.getRoot().toPath().resolve(filename).toFile();
+
+    ZipOutputStream out = new ZipOutputStream(new FileOutputStream(tmpFile));
+    PrintStream writer = new PrintStream(out, true /* auto-flush on write */);
+
+    int index = 0;
+    for (String[] entry : fieldsEntries) {
+      out.putNextEntry(new ZipEntry(Integer.toString(index)));
+      for (String field : entry) {
+        writer.println(field);
+        expected.add(field);
+      }
+      out.closeEntry();
+      index++;
+    }
+
+    writer.close();
+    out.close();
+
+    return tmpFile;
+  }
+
+  private static ContextualTextIOSource prepareSource(
+      TemporaryFolder temporaryFolder, byte[] data, byte[] delimiter, boolean hasRFC4180Multiline)
+      throws IOException {
+    Path path = temporaryFolder.newFile().toPath();
+    Files.write(path, data);
+    return new ContextualTextIOSource(
+        ValueProvider.StaticValueProvider.of(path.toString()),
+        EmptyMatchTreatment.DISALLOW,
+        delimiter,
+        hasRFC4180Multiline);
+  }
+
+  private static String getFileSuffix(Compression compression) {
+    switch (compression) {
+      case UNCOMPRESSED:
+        return ".txt";
+      case GZIP:
+        return ".gz";
+      case BZIP2:
+        return ".bz2";
+      case ZIP:
+        return ".zip";
+      case DEFLATE:
+        return ".deflate";
+      default:
+        return "";
+    }
+  }
+  /** Tests for reading from different size of files with various Compression. */
+  @RunWith(Parameterized.class)
+  public static class CompressedReadTest {
+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();
+    @Rule public TestPipeline p = TestPipeline.create();
+
+    @Parameterized.Parameters(name = "{index}: {1}")
+    public static Iterable<Object[]> data() {
+      return ImmutableList.<Object[]>builder()
+          .add(new Object[] {EMPTY, UNCOMPRESSED})
+          .add(new Object[] {EMPTY, GZIP})
+          .add(new Object[] {EMPTY, BZIP2})
+          .add(new Object[] {EMPTY, ZIP})
+          .add(new Object[] {EMPTY, DEFLATE})
+          .add(new Object[] {TINY, UNCOMPRESSED})
+          .add(new Object[] {TINY, GZIP})
+          .add(new Object[] {TINY, BZIP2})
+          .add(new Object[] {TINY, ZIP})
+          .add(new Object[] {TINY, DEFLATE})
+          .add(new Object[] {LARGE, UNCOMPRESSED})
+          .add(new Object[] {LARGE, GZIP})
+          .add(new Object[] {LARGE, BZIP2})
+          .add(new Object[] {LARGE, ZIP})
+          .add(new Object[] {LARGE, DEFLATE})
+          .build();
+    }
+
+    @Parameterized.Parameter(0)
+    public List<String> lines;
+
+    @Parameterized.Parameter(1)
+    public Compression compression;
+
+    /** Tests reading from a small, compressed file with no extension. */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testCompressedReadWithoutExtension() throws Exception {
+      String fileName = lines.size() + "_" + compression + "_no_extension";
+      File fileWithNoExtension = writeToFile(lines, tempFolder, fileName, compression);
+      assertReadingCompressedFileMatchesExpected(fileWithNoExtension, compression, lines, p);
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testCompressedReadWithExtension() throws Exception {
+      String fileName =
+          lines.size() + "_" + compression + "_no_extension" + getFileSuffix(compression);
+      File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);
+
+      // Sanity check that we're properly testing compression.
+      if (lines.size() == NUM_LINES_FOR_LARGE && !compression.equals(UNCOMPRESSED)) {
+        File uncompressedFile = writeToFile(lines, tempFolder, "large.txt", UNCOMPRESSED);
+        assertThat(uncompressedFile.length(), greaterThan(fileWithExtension.length()));
+      }
+
+      assertReadingCompressedFileMatchesExpected(fileWithExtension, compression, lines, p);
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadWithAuto() throws Exception {
+      // Files with non-compressed extensions should work in AUTO and UNCOMPRESSED modes.
+      String fileName =
+          lines.size() + "_" + compression + "_no_extension" + getFileSuffix(compression);
+      File fileWithExtension = writeToFile(lines, tempFolder, fileName, compression);
+      assertReadingCompressedFileMatchesExpected(fileWithExtension, AUTO, lines, p);
+      p.run();
+    }
+  }
+
+  /** Tests for reading files with various delimiters. */
+  @RunWith(Parameterized.class)
+  public static class ReadWithDelimiterTest {
+    private static final ImmutableList<String> EXPECTED = ImmutableList.of("asdf", "hjkl", "xyz");
+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();
+
+    @Parameterized.Parameters(name = "{index}: {0}")
+    public static Iterable<Object[]> data() {
+      return ImmutableList.<Object[]>builder()
+          .add(new Object[] {"asdf\nhjkl\nxyz\n", EXPECTED})
+          .add(new Object[] {"asdf\rhjkl\rxyz\r", EXPECTED})
+          .add(new Object[] {"asdf\r\nhjkl\r\nxyz\r\n", EXPECTED})
+          .add(new Object[] {"asdf\rhjkl\r\nxyz\n", EXPECTED})
+          .add(new Object[] {"asdf\nhjkl\nxyz", EXPECTED})
+          .add(new Object[] {"asdf\rhjkl\rxyz", EXPECTED})
+          .add(new Object[] {"asdf\r\nhjkl\r\nxyz", EXPECTED})
+          .add(new Object[] {"asdf\rhjkl\r\nxyz", EXPECTED})
+          .build();
+    }
+
+    @Parameterized.Parameter(0)
+    public String line;
+
+    @Parameterized.Parameter(1)
+    public ImmutableList<String> expected;
+
+    @Test
+    public void testReadLinesWithDelimiter() throws Exception {
+      runTestReadWithData(line.getBytes(UTF_8), expected);
+    }
+
+    private ContextualTextIOSource prepareSource(byte[] data, boolean hasRFC4180Multiline)
+        throws IOException {
+      return ContextualTextIOTest.prepareSource(tempFolder, data, null, hasRFC4180Multiline);
+    }
+
+    private void runTestReadWithData(byte[] data, List<String> expectedResults) throws Exception {
+      ContextualTextIOSource source = prepareSource(data, false);
+      List<RecordWithMetadata> actual =
+          SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());
+      List<String> actualOutput = new ArrayList<>();
+      actual.forEach(
+          (RecordWithMetadata L) -> {
+            actualOutput.add(L.getValue());
+          });
+      assertThat(
+          actualOutput,
+          containsInAnyOrder(new ArrayList<>(expectedResults).toArray(new String[0])));
+    }
+  }
+
+  @RunWith(Parameterized.class)
+  public static class ReadWithDelimiterAndRFC4180 {
+    static final ImmutableList<String> EXPECTED = ImmutableList.of("\"asdf\nhjkl\nmnop\"", "xyz");
+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();
+
+    @Parameterized.Parameters(name = "{index}: {0}")
+    public static Iterable<Object[]> data() {
+      return ImmutableList.<Object[]>builder()
+          .add(new Object[] {"\n\n\n", ImmutableList.of("", "", "")})
+          .add(new Object[] {"\"asdf\nhjkl\"\nxyz\n", ImmutableList.of("\"asdf\nhjkl\"", "xyz")})
+          .add(new Object[] {"\"asdf\nhjkl\nmnop\"\nxyz\n", EXPECTED})
+          .add(new Object[] {"\"asdf\nhjkl\nmnop\"\nxyz\r", EXPECTED})
+          .add(new Object[] {"\"asdf\nhjkl\nmnop\"\r\nxyz\n", EXPECTED})
+          .add(new Object[] {"\"asdf\nhjkl\nmnop\"\r\nxyz\r\n", EXPECTED})
+          .add(new Object[] {"\"asdf\nhjkl\nmnop\"\rxyz\r\n", EXPECTED})
+          .build();
+    }
+
+    @Parameterized.Parameter(0)
+    public String line;
+
+    @Parameterized.Parameter(1)
+    public ImmutableList<String> expected;
+
+    @Test
+    public void testReadLinesWithDelimiter() throws Exception {
+      runTestReadWithData(line.getBytes(UTF_8), expected);
+    }
+
+    private ContextualTextIOSource prepareSource(byte[] data, boolean hasRFC4180Multiline)
+        throws IOException {
+      return ContextualTextIOTest.prepareSource(tempFolder, data, null, hasRFC4180Multiline);
+    }
+
+    private void runTestReadWithData(byte[] data, List<String> expectedResults) throws Exception {
+      ContextualTextIOSource source = prepareSource(data, true);
+      List<RecordWithMetadata> actual =
+          SourceTestUtils.readFromSource(source, PipelineOptionsFactory.create());
+      List<String> actualOutput = new ArrayList<>();
+      actual.forEach(
+          (RecordWithMetadata L) -> {
+            actualOutput.add(L.getValue());
+          });
+      assertThat(
+          actualOutput,
+          containsInAnyOrder(new ArrayList<>(expectedResults).toArray(new String[0])));
+    }
+  }
+
+  /** Tests Specific for checking functionality of ContextualTextIO. */
+  @RunWith(JUnit4.class)
+  public static class ContextualTextIOSpecificTests {
+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();
+    @Rule public TestPipeline p = TestPipeline.create();
+
+    public static final char CR = (char) 0x0D;
+    public static final char LF = (char) 0x0A;
+
+    public static final String CRLF = "" + CR + LF;
+
+    public String createFileFromList(List<String> input) throws Exception {
+
+      File tmpFile = tempFolder.newFile();
+      String filename = tmpFile.getPath();
+
+      try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {
+        for (String elem : input) {
+          byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);
+          String line = new String(encodedElem, Charsets.UTF_8);
+          writer.println(line);
+        }
+      }
+      return filename;
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void multipleFilesTest() throws Exception {
+      List<File> files =
+          Arrays.asList(
+              tempFolder.newFile("File1"),
+              tempFolder.newFile("File2"),
+              tempFolder.newFile("File3"));
+
+      int numLines = 10;
+      for (File tmpFile : files) {
+        numLines += 2;
+        String filename = tmpFile.getPath();
+        try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {
+          for (int lineNum = 0; lineNum < numLines; ++lineNum) {
+            String elem = filename + " " + lineNum;
+            byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);
+            String line = new String(encodedElem, Charsets.UTF_8);
+            writer.println(line);
+          }
+        }
+      }
+      String filePath = tempFolder.getRoot().toPath() + "/*";
+      p.apply(ContextualTextIO.read().from(filePath))
+          .apply(
+              MapElements.into(strings())
+                  .via(
+                      (RecordWithMetadata L) -> {
+                        String expectedLineNum =
+                            L.getValue().substring(L.getValue().lastIndexOf(' ') + 1);
+                        assertEquals(Long.parseLong(expectedLineNum), (long) L.getRecordNum());
+                        return "";
+                      }));
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testWithHintMatchesManyFiles() throws IOException {
+      List<File> files =
+          Arrays.asList(
+              tempFolder.newFile("File1"),
+              tempFolder.newFile("File2"),
+              tempFolder.newFile("File3"));
+
+      int num = 0;
+      for (File tmpFile : files) {
+        num += 2;
+        String filename = tmpFile.getPath();
+        try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {
+          for (int lineNum = 0; lineNum < 10 + num; ++lineNum) {
+            String elem = filename + " " + lineNum;
+            byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);
+            String line = new String(encodedElem, Charsets.UTF_8);
+            writer.println(line);
+          }
+        }
+      }
+      String filePath = tempFolder.getRoot().toPath() + "/*";
+      p.apply(ContextualTextIO.read().from(filePath).withHintMatchesManyFiles())
+          .apply(
+              MapElements.into(strings())
+                  .via(
+                      (RecordWithMetadata L) -> {
+                        String expectedLineNum =
+                            L.getValue().substring(L.getValue().lastIndexOf(' ') + 1);
+                        assertEquals(Long.parseLong(expectedLineNum), (long) L.getRecordNum());
+                        return "";
+                      }));
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void runBasicReadTest() throws Exception {
+
+      List<String> input = ImmutableList.of("1", "2");
+      ContextualTextIO.Read read = ContextualTextIO.read().from(createFileFromList(input));
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder("1", "2");
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void runBasicReadTestWithRFC4180Set() throws Exception {
+
+      List<String> input = ImmutableList.of("1", "2");
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder("1", "2");
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    /** Test to read files with using MultiLine columns as per RFC4180 */
+    public void runSmallRFC4180MultiLineReadTest() throws Exception {
+
+      // Generate lines of format "1\n1" where number changes per line.
+      List<String> input =
+          IntStream.range(0, 2)
+              .<String>mapToObj(x -> "\"" + x + CRLF + x + "\"")
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(
+              MapElements.into(strings())
+                  .via(
+                      x -> {
+                        return String.valueOf(x.getValue());
+                      }));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    /** Test to read files with using MultiLine columns as per RFC4180 */
+    public void runSmallRFC4180EscapedCharcatersReadTest() throws Exception {
+
+      // Generate lines of format  "aaa","b""bb","ccc" where number changes per line.
+      List<String> input =
+          IntStream.range(0, 2)
+              .<String>mapToObj(x -> "\"aaa\",\"b\"\"bb\",\"ccc\"")
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(
+              MapElements.into(strings())
+                  .via(
+                      x -> {
+                        return String.valueOf(x.getValue());
+                      }));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    /** Test to read files with using MultiLine columns as per RFC4180 */
+    public void runLargeRFC4180MultiLineReadTest() throws Exception {
+
+      // Generate lines of format "1\n1" where number changes per line.
+      List<String> input =
+          IntStream.range(0, 1000)
+              .<String>mapToObj(x -> "\"" + x + CRLF + x + "\"")
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    /** Test to read files with using MultiLine columns as per RFC4180 */
+    public void runLargeRFC4180MultiLineAndEscapedReadTest() throws Exception {
+
+      // Generate lines of format  "aaa","b""\nbb","ccc","""\nHello" where number changes per line.
+      List<String> input =
+          IntStream.range(0, 1000)
+              .<String>mapToObj(
+                  x -> "\"a" + CRLF + "aa\",\"b\"\"" + CRLF + "bb\",\"ccc\",\"\"\"\\nHello\"")
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    /** Test to read files with using MultiLine columns as per RFC4180 */
+    public void testFileNameIsPreserved() throws Exception {
+
+      List<String> input =
+          IntStream.range(1, 1000)
+              .<String>mapToObj(x -> Integer.toString(x))
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read().from(createFileFromList(input)).withHasMultilineCSVRecords(true);
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testFileNameIsPreservedWithoutLineMetadata() throws Exception {
+
+      List<String> input =
+          IntStream.range(1, 1000)
+              .<String>mapToObj(x -> Integer.toString(x))
+              .collect(Collectors.toList());
+
+      ContextualTextIO.Read read =
+          ContextualTextIO.read()
+              .from(createFileFromList(input))
+              .withHasMultilineCSVRecords(true)
+              .withoutRecordNumMetadata();
+      PCollection<RecordWithMetadata> output = p.apply(read);
+
+      PCollection<String> result =
+          output.apply(MapElements.into(strings()).via(x -> String.valueOf(x.getValue())));
+
+      PAssert.that(result).containsInAnyOrder(input);
+
+      p.run();
+    }
+  }
+
+  /** Tests for some basic operations in {@link ContextualTextIO.Read}. */
+  @RunWith(JUnit4.class)
+  public static class BasicIOTest {
+    @Rule public TemporaryFolder tempFolder = new TemporaryFolder();
+    @Rule public TestPipeline p = TestPipeline.create();
+
+    public static class GetLines extends DoFn<RecordWithMetadata, String> {
+      @ProcessElement
+      public void processElement(@Element RecordWithMetadata record, OutputReceiver<String> out) {
+        out.output(record.getValue());
+      }
+    }
+
+    public static class GetDetails extends DoFn<RecordWithMetadata, String> {
+      @ProcessElement
+      public void processElement(@Element RecordWithMetadata record, OutputReceiver<String> out) {
+        out.output(record.getFileName() + " " + record.getRecordNum() + " " + record.getValue());
+      }
+    }
+
+    private void runTestRead(String[] expected) throws Exception {
+      File tmpFile = tempFolder.newFile();
+      String filename = tmpFile.getPath();
+
+      try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {
+        for (String elem : expected) {
+          byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);
+          String line = new String(encodedElem, Charsets.UTF_8);
+          writer.println(line);
+        }
+      }
+
+      ContextualTextIO.Read read = ContextualTextIO.read().from(filename);
+      PCollection<String> output =
+          p.apply(read)
+              .apply(MapElements.into(strings()).via((RecordWithMetadata L) -> L.getValue()));
+
+      PAssert.that(output).containsInAnyOrder(expected);
+      p.run();
+    }
+
+    private void runTestReadLineNumsAndFileName(String[] expected) throws Exception {
+      File tmpFile = tempFolder.newFile();
+      String filename = tmpFile.getPath();
+
+      List<String> actualExpected = new ArrayList<>();
+
+      try (PrintStream writer = new PrintStream(new FileOutputStream(tmpFile))) {
+        int lineNum = 0;
+        for (String elem : expected) {
+          byte[] encodedElem = CoderUtils.encodeToByteArray(StringUtf8Coder.of(), elem);
+          String line = new String(encodedElem, Charsets.UTF_8);
+          writer.println(line);
+          actualExpected.add(lineNum + " " + filename + " " + line);
+          lineNum++;
+        }
+      }
+
+      ContextualTextIO.Read read = ContextualTextIO.read().from(filename);
+      PCollection<String> output =
+          p.apply(read)
+              .apply(
+                  MapElements.into(strings())
+                      .via(
+                          (RecordWithMetadata L) ->
+                              L.getRecordNum() + " " + L.getFileName() + " " + L.getValue()));
+
+      PAssert.that(output).containsInAnyOrder(actualExpected);
+      p.run();
+    }
+
+    @Test
+    public void testDelimiterSelfOverlaps() {
+      assertFalse(ContextualTextIO.Read.isSelfOverlapping(new byte[] {'a', 'b', 'c'}));
+      assertFalse(
+          ContextualTextIO.Read.isSelfOverlapping(new byte[] {'c', 'a', 'b', 'd', 'a', 'b'}));
+      assertFalse(
+          ContextualTextIO.Read.isSelfOverlapping(new byte[] {'a', 'b', 'c', 'a', 'b', 'd'}));
+      assertTrue(ContextualTextIO.Read.isSelfOverlapping(new byte[] {'a', 'b', 'a'}));
+      assertTrue(ContextualTextIO.Read.isSelfOverlapping(new byte[] {'a', 'b', 'c', 'a', 'b'}));
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadStringsWithCustomDelimiter() throws Exception {
+      final String[] inputStrings =
+          new String[] {
+            // incomplete delimiter
+            "To be, or not to be: that |is the question: ",
+            // incomplete delimiter
+            "To be, or not to be: that *is the question: ",
+            // complete delimiter
+            "Whether 'tis nobler in the mind to suffer |*",
+            // truncated delimiter
+            "The slings and arrows of outrageous fortune,|"
+          };
+
+      File tmpFile = tempFolder.newFile("tmpfile.txt");
+      String filename = tmpFile.getPath();
+
+      try (Writer writer = Files.newBufferedWriter(tmpFile.toPath(), UTF_8)) {
+        writer.write(Joiner.on("").join(inputStrings));
+      }
+
+      PAssert.that(
+              p.apply(ContextualTextIO.read().from(filename).withDelimiter(new byte[] {'|', '*'}))
+                  .apply(ParDo.of(new GetLines())))
+          .containsInAnyOrder(
+              "To be, or not to be: that |is the question: To be, or not to be: "
+                  + "that *is the question: Whether 'tis nobler in the mind to suffer ",
+              "The slings and arrows of outrageous fortune,|");
+
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadStringsWithCustomDelimiterAndContext() throws Exception {
+      final String[] inputStrings =
+          new String[] {
+            // incomplete delimiter
+            "To be, or not to be: that |is the question: ",
+            // incomplete delimiter
+            "To be, or not to be: that *is the question: ",
+            // complete delimiter
+            "Whether 'tis nobler in the mind to suffer |*",
+            // truncated delimiter
+            "The slings and arrows of outrageous fortune,|"
+          };
+
+      File tmpFile = tempFolder.newFile("tmpfile.txt");
+      String filename = tmpFile.getPath();
+
+      try (Writer writer = Files.newBufferedWriter(tmpFile.toPath(), UTF_8)) {
+        writer.write(Joiner.on("").join(inputStrings));
+      }
+
+      PAssert.that(
+              p.apply(ContextualTextIO.read().from(filename).withDelimiter(new byte[] {'|', '*'}))
+                  .apply(ParDo.of(new GetDetails())))
+          .containsInAnyOrder(
+              filename
+                  + " 0 To be, or not to be: that |is the question: To be, or not to be: "
+                  + "that *is the question: Whether 'tis nobler in the mind to suffer ",
+              filename + " 1 The slings and arrows of outrageous fortune,|");
+      p.run();
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadStrings() throws Exception {
+      runTestRead(LINES_ARRAY);
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadStringsWithContext() throws Exception {
+      runTestReadLineNumsAndFileName(LINES_ARRAY);
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadEmptyStrings() throws Exception {
+      runTestRead(NO_LINES_ARRAY);
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadEmptyStringsWithContext() throws Exception {
+      runTestReadLineNumsAndFileName(NO_LINES_ARRAY);
+    }
+
+    @Test
+    public void testReadDisplayData() {
+      ContextualTextIO.Read read = ContextualTextIO.read().from("foo.*").withCompression(BZIP2);
+
+      DisplayData displayData = DisplayData.from(read);
+
+      assertThat(displayData, hasDisplayItem("filePattern", "foo.*"));
+      assertThat(displayData, hasDisplayItem("compressionType", BZIP2.toString()));
+    }
+
+    /** Options for testing. */
+    public interface RuntimeTestOptions extends PipelineOptions {
+      ValueProvider<String> getInput();
+
+      void setInput(ValueProvider<String> value);
+    }
+
+    @Test
+    public void testRuntimeOptionsNotCalledInApply() throws Exception {
+      p.enableAbandonedNodeEnforcement(false);
+
+      RuntimeTestOptions options = PipelineOptionsFactory.as(RuntimeTestOptions.class);
+
+      p.apply(ContextualTextIO.read().from(options.getInput()));
+    }
+
+    @Test
+    public void testCompressionIsSet() throws Exception {
+      ContextualTextIO.Read read = ContextualTextIO.read().from("/tmp/test");
+      assertEquals(AUTO, read.getCompression());
+      read = ContextualTextIO.read().from("/tmp/test").withCompression(GZIP);
+      assertEquals(GZIP, read.getCompression());
+    }
+
+    /**
+     * Tests reading from a small, uncompressed file with .gz extension. This must work in GZIP
+     * modes. This is needed because some network file systems / HTTP clients will transparently
+     * decompress gzipped content.
+     */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testSmallCompressedGzipReadActuallyUncompressed() throws Exception {
+      File smallGzNotCompressed =
+          writeToFile(TINY, tempFolder, "tiny_uncompressed.gz", UNCOMPRESSED);
+      // Should work with GZIP compression set.
+      assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, GZIP, TINY, p);
+      p.run();
+    }
+
+    /**
+     * Tests reading from a small, uncompressed file with .gz extension. This must work in AUTO
+     * modes. This is needed because some network file systems / HTTP clients will transparently
+     * decompress gzipped content.
+     */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testSmallCompressedAutoReadActuallyUncompressed() throws Exception {
+      File smallGzNotCompressed =
+          writeToFile(TINY, tempFolder, "tiny_uncompressed.gz", UNCOMPRESSED);
+      // Should also work with AUTO mode set.
+      assertReadingCompressedFileMatchesExpected(smallGzNotCompressed, AUTO, TINY, p);
+      p.run();
+    }
+
+    /**
+     * Tests a zip file with no entries. This is a corner case not tested elsewhere as the default
+     * test zip files have a single entry.
+     */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testZipCompressedReadWithNoEntries() throws Exception {
+      File file = createZipFile(new ArrayList<>(), tempFolder, "empty zip file");
+      assertReadingCompressedFileMatchesExpected(file, ZIP, EMPTY, p);
+      p.run();
+    }
+
+    /**
+     * Tests a zip file with multiple entries. This is a corner case not tested elsewhere as the
+     * default test zip files have a single entry.
+     */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testZipCompressedReadWithMultiEntriesFile() throws Exception {
+      String[] entry0 = new String[] {"first", "second", "three"};
+      String[] entry1 = new String[] {"four", "five", "six"};
+      String[] entry2 = new String[] {"seven", "eight", "nine"};
+
+      List<String> expected = new ArrayList<>();
+
+      File file = createZipFile(expected, tempFolder, "multiple entries", entry0, entry1, entry2);
+      assertReadingCompressedFileMatchesExpected(file, ZIP, expected, p);
+      p.run();
+    }
+
+    /**
+     * Read a ZIP compressed file containing data, multiple empty entries, and then more data. We
+     * expect just the data back.
+     */
+    @Test
+    @Category(NeedsRunner.class)
+    public void testZipCompressedReadWithComplexEmptyAndPresentEntries() throws Exception {
+      File file =
+          createZipFile(
+              new ArrayList<>(),
+              tempFolder,
+              "complex empty and present entries",
+              new String[] {"cat"},
+              new String[] {},
+              new String[] {},
+              new String[] {"dog"});
+
+      assertReadingCompressedFileMatchesExpected(file, ZIP, Arrays.asList("cat", "dog"), p);
+      p.run();
+    }
+
+    @Test
+    public void testContextualTextIOGetName() {
+      assertEquals("ContextualTextIO.Read", ContextualTextIO.read().from("somefile").getName());
+      assertEquals("ContextualTextIO.Read", ContextualTextIO.read().from("somefile").toString());
+    }
+
+    private ContextualTextIOSource prepareSource(byte[] data, boolean hasRFC4180Multiline)
+        throws IOException {
+      return ContextualTextIOTest.prepareSource(tempFolder, data, null, hasRFC4180Multiline);
+    }
+
+    private ContextualTextIOSource prepareSource(byte[] data) throws IOException {
+      return ContextualTextIOTest.prepareSource(tempFolder, data, null, false);
+    }
+
+    @Test
+    public void testProgressEmptyFile() throws IOException {
+      try (BoundedSource.BoundedReader<RecordWithMetadata> reader =
+          prepareSource(new byte[0]).createReader(PipelineOptionsFactory.create())) {
+        // Check preconditions before starting.
+        assertEquals(0.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // Assert empty
+        assertFalse(reader.start());
+
+        // Check postconditions after finishing
+        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(0, reader.getSplitPointsRemaining());
+      }
+    }
+
+    @Test
+    public void testProgressTextFile() throws IOException {
+      String file = "line1\nline2\nline3";
+      try (BoundedSource.BoundedReader<RecordWithMetadata> reader =
+          prepareSource(file.getBytes(Charsets.UTF_8))
+              .createReader(PipelineOptionsFactory.create())) {
+        // Check preconditions before starting
+        assertEquals(0.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // Line 1
+        assertTrue(reader.start());
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // Line 2
+        assertTrue(reader.advance());
+        assertEquals(1, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // Line 3
+        assertTrue(reader.advance());
+        assertEquals(2, reader.getSplitPointsConsumed());
+        assertEquals(1, reader.getSplitPointsRemaining());
+
+        // Check postconditions after finishing
+        assertFalse(reader.advance());
+        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(3, reader.getSplitPointsConsumed());
+        assertEquals(0, reader.getSplitPointsRemaining());
+      }
+    }
+
+    @Test
+    public void testProgressAfterSplitting() throws IOException {
+      String file = "line1\nline2\nline3";
+      BoundedSource<RecordWithMetadata> source = prepareSource(file.getBytes(Charsets.UTF_8));
+      BoundedSource<RecordWithMetadata> remainder;
+
+      // Create the remainder, verifying properties pre- and post-splitting.
+      try (BoundedSource.BoundedReader<RecordWithMetadata> readerOrig =
+          source.createReader(PipelineOptionsFactory.create())) {
+        // Preconditions.
+        assertEquals(0.0, readerOrig.getFractionConsumed(), 1e-6);
+        assertEquals(0, readerOrig.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());
+
+        // First record, before splitting.
+        assertTrue(readerOrig.start());
+        assertEquals(0, readerOrig.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, readerOrig.getSplitPointsRemaining());
+
+        // Split. 0.1 is in line1, so should now be able to detect last record.
+        remainder = readerOrig.splitAtFraction(0.1);
+        System.err.println(readerOrig.getCurrentSource());
+        assertNotNull(remainder);
+
+        // First record, after splitting.
+        assertEquals(0, readerOrig.getSplitPointsConsumed());
+        assertEquals(1, readerOrig.getSplitPointsRemaining());
+
+        // Finish and postconditions.
+        assertFalse(readerOrig.advance());
+        assertEquals(1.0, readerOrig.getFractionConsumed(), 1e-6);
+        assertEquals(1, readerOrig.getSplitPointsConsumed());
+        assertEquals(0, readerOrig.getSplitPointsRemaining());
+      }
+
+      // Check the properties of the remainder.
+      try (BoundedSource.BoundedReader<RecordWithMetadata> reader =
+          remainder.createReader(PipelineOptionsFactory.create())) {
+        // Preconditions.
+        assertEquals(0.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // First record should be line 2.
+        assertTrue(reader.start());
+        assertEquals(0, reader.getSplitPointsConsumed());
+        assertEquals(
+            BoundedSource.BoundedReader.SPLIT_POINTS_UNKNOWN, reader.getSplitPointsRemaining());
+
+        // Second record is line 3
+        assertTrue(reader.advance());
+        assertEquals(1, reader.getSplitPointsConsumed());
+        assertEquals(1, reader.getSplitPointsRemaining());
+
+        // Check postconditions after finishing
+        assertFalse(reader.advance());
+        assertEquals(1.0, reader.getFractionConsumed(), 1e-6);
+        assertEquals(2, reader.getSplitPointsConsumed());
+        assertEquals(0, reader.getSplitPointsRemaining());
+      }
+    }
+
+    @Test
+    public void testInitialSplitAutoModeGz() throws Exception {
+      PipelineOptions options = TestPipeline.testingPipelineOptions();
+      long desiredBundleSize = 1000;
+      File largeGz = writeToFile(LARGE, tempFolder, "large.gz", GZIP);
+      // Sanity check: file is at least 2 bundles long.
+      assertThat(largeGz.length(), greaterThan(2 * desiredBundleSize));
+
+      FileBasedSource<RecordWithMetadata> source =
+          ContextualTextIO.read().from(largeGz.getPath()).getSource();
+      List<? extends FileBasedSource<RecordWithMetadata>> splits =
+          source.split(desiredBundleSize, options);
+
+      // Exactly 1 split, even in AUTO mode, since it is a gzip file.
+      assertThat(splits, hasSize(equalTo(1)));
+      SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);
+    }
+
+    @Test
+    public void testInitialSplitGzipModeTxt() throws Exception {
+      PipelineOptions options = TestPipeline.testingPipelineOptions();
+      long desiredBundleSize = 1000;
+      File largeTxt = writeToFile(LARGE, tempFolder, "large.txt", UNCOMPRESSED);
+      // Sanity check: file is at least 2 bundles long.
+      assertThat(largeTxt.length(), greaterThan(2 * desiredBundleSize));
+
+      FileBasedSource<RecordWithMetadata> source =
+          ContextualTextIO.read().from(largeTxt.getPath()).withCompression(GZIP).getSource();
+      List<? extends FileBasedSource<RecordWithMetadata>> splits =
+          source.split(desiredBundleSize, options);
+
+      // Exactly 1 split, even though splittable text file, since using GZIP mode.
+      assertThat(splits, hasSize(equalTo(1)));
+      SourceTestUtils.assertSourcesEqualReferenceSource(source, splits, options);
+    }
+
+    @Test
+    @Category(NeedsRunner.class)
+    public void testReadFiles() throws IOException {
+      Path tempFolderPath = tempFolder.getRoot().toPath();
+      writeToFile(TINY, tempFolder, "readAllTiny1.zip", ZIP);
+      writeToFile(TINY, tempFolder, "readAllTiny2.txt", UNCOMPRESSED);
+      writeToFile(LARGE, tempFolder, "readAllLarge1.zip", ZIP);
+      writeToFile(LARGE, tempFolder, "readAllLarge2.txt", UNCOMPRESSED);
+      PCollection<String> lines =
+          p.apply(
+                  Create.of(
+                      tempFolderPath.resolve("readAllTiny*").toString(),
+                      tempFolderPath.resolve("readAllLarge*").toString()))
+              .apply(FileIO.matchAll())
+              .apply(FileIO.readMatches().withCompression(AUTO))
+              .apply(ContextualTextIO.readFiles().withDesiredBundleSizeBytes(10))
+              .apply(MapElements.into(strings()).via((RecordWithMetadata L) -> L.getValue()));
+      PAssert.that(lines).containsInAnyOrder(Iterables.concat(TINY, TINY, LARGE, LARGE));
+      p.run();
+    }
+
+    @Test
+    @Category({NeedsRunner.class, UsesUnboundedSplittableParDo.class})
+    public void testReadWatchForNewFiles() throws IOException, InterruptedException {
+      final Path basePath = tempFolder.getRoot().toPath().resolve("readWatch");
+      basePath.toFile().mkdir();
+
+      p.apply(GenerateSequence.from(0).to(10).withRate(1, Duration.millis(100)))
+          .apply(
+              Window.<Long>into(FixedWindows.of(Duration.millis(150)))
+                  .withAllowedLateness(Duration.ZERO)
+                  .triggering(Repeatedly.forever(AfterPane.elementCountAtLeast(1)))
+                  .discardingFiredPanes())
+          .apply(ToString.elements())
+          .apply(
+              TextIO.write()
+                  .to(basePath.resolve("data").toString())
+                  .withNumShards(1)
+                  .withWindowedWrites());
+
+      PCollection<String> lines =
+          p.apply(
+                  FileIO.match()
+                      .filepattern(basePath.resolve("*").toString())
+                      .continuously(
+                          Duration.millis(100),
+                          Watch.Growth.afterTimeSinceNewOutput(Duration.standardSeconds(3))))
+              .apply(FileIO.readMatches())
+              .apply(ContextualTextIO.readFiles())
+              .apply(MapElements.into(strings()).via((RecordWithMetadata L) -> L.getValue()));
+
+      PAssert.that(lines).containsInAnyOrder("0", "1", "2", "3", "4", "5", "6", "7", "8", "9");
+      p.run();
+    }
+  }
+}
diff --git a/settings.gradle b/settings.gradle
index 2e8ce8ab84ff..6f9a3d6c041f 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -97,6 +97,7 @@ include ":sdks:java:io:azure"
 include ":sdks:java:io:cassandra"
 include ":sdks:java:io:clickhouse"
 include ":sdks:java:io:common"
+include ":sdks:java:io:contextual-text-io"
 include ":sdks:java:io:elasticsearch"
 include ":sdks:java:io:elasticsearch-tests:elasticsearch-tests-2"
 include ":sdks:java:io:elasticsearch-tests:elasticsearch-tests-5"
