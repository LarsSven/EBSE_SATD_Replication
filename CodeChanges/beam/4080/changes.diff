diff --git a/pom.xml b/pom.xml
index c53da31f1a2b..c742edb0c52a 100644
--- a/pom.xml
+++ b/pom.xml
@@ -457,6 +457,12 @@
         <version>${project.version}</version>
       </dependency>
 
+      <dependency>
+        <groupId>org.apache.beam</groupId>
+        <artifactId>beam-sdks-java-io-amazon-web-services</artifactId>
+        <version>${project.version}</version>
+      </dependency>
+
       <dependency>
         <groupId>org.apache.beam</groupId>
         <artifactId>beam-sdks-java-io-amqp</artifactId>
diff --git a/sdks/java/io/amazon-web-services/pom.xml b/sdks/java/io/amazon-web-services/pom.xml
new file mode 100644
index 000000000000..eee47440e9e3
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/pom.xml
@@ -0,0 +1,171 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+    Licensed to the Apache Software Foundation (ASF) under one or more
+    contributor license agreements.  See the NOTICE file distributed with
+    this work for additional information regarding copyright ownership.
+    The ASF licenses this file to You under the Apache License, Version 2.0
+    (the "License"); you may not use this file except in compliance with
+    the License.  You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing, software
+    distributed under the License is distributed on an "AS IS" BASIS,
+    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+    See the License for the specific language governing permissions and
+    limitations under the License.
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.beam</groupId>
+    <artifactId>beam-sdks-java-io-parent</artifactId>
+    <version>2.3.0-SNAPSHOT</version>
+    <relativePath>../pom.xml</relativePath>
+  </parent>
+
+  <artifactId>beam-sdks-java-io-amazon-web-services</artifactId>
+  <name>Apache Beam :: SDKs :: Java :: IO :: Amazon Web Services</name>
+  <description>IO library to read and write Amazon Web Services services from Beam.</description>
+
+  <packaging>jar</packaging>
+
+  <properties>
+    <aws-java-sdk.version>1.11.255</aws-java-sdk.version>
+  </properties>
+
+  <build>
+    <plugins>
+      <plugin>
+        <groupId>org.apache.maven.plugins</groupId>
+        <artifactId>maven-surefire-plugin</artifactId>
+        <configuration>
+          <excludedGroups>
+            org.apache.beam.sdk.testing.NeedsRunner
+          </excludedGroups>
+          <systemPropertyVariables>
+            <beamUseDummyRunner>true</beamUseDummyRunner>
+          </systemPropertyVariables>
+        </configuration>
+      </plugin>
+
+      <!-- Coverage analysis for unit tests. -->
+      <plugin>
+        <groupId>org.jacoco</groupId>
+        <artifactId>jacoco-maven-plugin</artifactId>
+      </plugin>
+    </plugins>
+  </build>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.beam</groupId>
+      <artifactId>beam-sdks-java-core</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.amazonaws</groupId>
+      <artifactId>aws-java-sdk-core</artifactId>
+      <version>${aws-java-sdk.version}</version>
+    </dependency>
+
+    <dependency>
+      <groupId>com.amazonaws</groupId>
+      <artifactId>aws-java-sdk-s3</artifactId>
+      <version>${aws-java-sdk.version}</version>
+    </dependency>
+
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-core</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-annotations</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.fasterxml.jackson.core</groupId>
+      <artifactId>jackson-databind</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.google.code.findbugs</groupId>
+      <artifactId>jsr305</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>com.google.guava</groupId>
+      <artifactId>guava</artifactId>
+    </dependency>
+
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-api</artifactId>
+    </dependency>
+
+    <!-- runtime dependencies -->
+    <dependency>
+      <groupId>org.apache.httpcomponents</groupId>
+      <artifactId>httpclient</artifactId>
+      <version>4.5.2</version>
+      <scope>runtime</scope>
+    </dependency>
+
+    <!-- build dependencies -->
+    <dependency>
+      <groupId>com.google.auto.service</groupId>
+      <artifactId>auto-service</artifactId>
+      <optional>true</optional>
+    </dependency>
+
+    <dependency>
+      <groupId>com.google.auto.value</groupId>
+      <artifactId>auto-value</artifactId>
+      <scope>provided</scope>
+    </dependency>
+
+    <!-- test dependencies -->
+    <dependency>
+      <groupId>com.google.guava</groupId>
+      <artifactId>guava-testlib</artifactId>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.apache.beam</groupId>
+      <artifactId>beam-sdks-java-core</artifactId>
+      <classifier>tests</classifier>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.hamcrest</groupId>
+      <artifactId>hamcrest-all</artifactId>
+      <scope>provided</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.mockito</groupId>
+      <artifactId>mockito-all</artifactId>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+      <scope>test</scope>
+    </dependency>
+
+    <dependency>
+      <groupId>org.slf4j</groupId>
+      <artifactId>slf4j-jdk14</artifactId>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+</project>
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsModule.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsModule.java
new file mode 100644
index 000000000000..228ab5c8b85e
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsModule.java
@@ -0,0 +1,189 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.options;
+
+import com.amazonaws.auth.AWSCredentialsProvider;
+import com.amazonaws.auth.AWSStaticCredentialsProvider;
+import com.amazonaws.auth.BasicAWSCredentials;
+import com.amazonaws.auth.ClasspathPropertiesFileCredentialsProvider;
+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
+import com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper;
+import com.amazonaws.auth.EnvironmentVariableCredentialsProvider;
+import com.amazonaws.auth.PropertiesFileCredentialsProvider;
+import com.amazonaws.auth.SystemPropertiesCredentialsProvider;
+import com.amazonaws.auth.profile.ProfileCredentialsProvider;
+import com.fasterxml.jackson.annotation.JsonTypeInfo;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.core.JsonParser;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.DeserializationContext;
+import com.fasterxml.jackson.databind.JsonDeserializer;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.Module;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import com.fasterxml.jackson.databind.annotation.JsonDeserialize;
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import com.fasterxml.jackson.databind.jsontype.TypeDeserializer;
+import com.fasterxml.jackson.databind.jsontype.TypeSerializer;
+import com.fasterxml.jackson.databind.module.SimpleModule;
+import com.google.auto.service.AutoService;
+import com.google.common.collect.ImmutableSet;
+import java.io.IOException;
+import java.lang.reflect.Field;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * A Jackson {@link Module} that registers a {@link JsonSerializer} and {@link JsonDeserializer}
+ * for {@link AWSCredentialsProvider} and some subclasses. The serialized form is a JSON map.
+ */
+@AutoService(Module.class)
+public class AwsModule extends SimpleModule {
+
+  private static final String AWS_ACCESS_KEY_ID = "awsAccessKeyId";
+  private static final String AWS_SECRET_KEY = "awsSecretKey";
+  private static final String CREDENTIALS_FILE_PATH = "credentialsFilePath";
+
+  public AwsModule() {
+    super("AwsModule");
+    setMixInAnnotation(AWSCredentialsProvider.class, AWSCredentialsProviderMixin.class);
+  }
+
+  /**
+   * A mixin to add Jackson annotations to {@link AWSCredentialsProvider}.
+   */
+  @JsonDeserialize(using = AWSCredentialsProviderDeserializer.class)
+  @JsonSerialize(using = AWSCredentialsProviderSerializer.class)
+  @JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY)
+  private static class AWSCredentialsProviderMixin {
+
+  }
+
+  static class AWSCredentialsProviderDeserializer extends JsonDeserializer<AWSCredentialsProvider> {
+
+    @Override
+    public AWSCredentialsProvider deserialize(
+        JsonParser jsonParser, DeserializationContext context) throws IOException {
+      return context.readValue(jsonParser, AWSCredentialsProvider.class);
+    }
+
+    @Override
+    public AWSCredentialsProvider deserializeWithType(JsonParser jsonParser,
+        DeserializationContext context, TypeDeserializer typeDeserializer) throws IOException {
+      Map<String, String> asMap =
+          jsonParser.readValueAs(new TypeReference<Map<String, String>>() {
+          });
+
+      String typeNameKey = typeDeserializer.getPropertyName();
+      String typeName = asMap.get(typeNameKey);
+      if (typeName == null) {
+        throw new IOException(
+            String.format("AWS credentials provider type name key '%s' not found", typeNameKey));
+      }
+
+      if (typeName.equals(AWSStaticCredentialsProvider.class.getSimpleName())) {
+        return new AWSStaticCredentialsProvider(
+            new BasicAWSCredentials(asMap.get(AWS_ACCESS_KEY_ID), asMap.get(AWS_SECRET_KEY)));
+      } else if (typeName.equals(PropertiesFileCredentialsProvider.class.getSimpleName())) {
+        return new PropertiesFileCredentialsProvider(asMap.get(CREDENTIALS_FILE_PATH));
+      } else if (typeName
+          .equals(ClasspathPropertiesFileCredentialsProvider.class.getSimpleName())) {
+        return new ClasspathPropertiesFileCredentialsProvider(asMap.get(CREDENTIALS_FILE_PATH));
+      } else if (typeName.equals(DefaultAWSCredentialsProviderChain.class.getSimpleName())) {
+        return new DefaultAWSCredentialsProviderChain();
+      } else if (typeName.equals(EnvironmentVariableCredentialsProvider.class.getSimpleName())) {
+        return new EnvironmentVariableCredentialsProvider();
+      } else if (typeName.equals(SystemPropertiesCredentialsProvider.class.getSimpleName())) {
+        return new SystemPropertiesCredentialsProvider();
+      } else if (typeName.equals(ProfileCredentialsProvider.class.getSimpleName())) {
+        return new ProfileCredentialsProvider();
+      } else if (typeName.equals(EC2ContainerCredentialsProviderWrapper.class.getSimpleName())) {
+        return new EC2ContainerCredentialsProviderWrapper();
+      } else {
+        throw new IOException(
+            String.format("AWS credential provider type '%s' is not supported", typeName));
+      }
+    }
+  }
+
+  static class AWSCredentialsProviderSerializer extends JsonSerializer<AWSCredentialsProvider> {
+
+    // These providers are singletons, so don't require any serialization, other than type.
+    private static final Set<Object> SINGLETON_CREDENTIAL_PROVIDERS = ImmutableSet.of(
+        DefaultAWSCredentialsProviderChain.class,
+        EnvironmentVariableCredentialsProvider.class,
+        SystemPropertiesCredentialsProvider.class,
+        ProfileCredentialsProvider.class,
+        EC2ContainerCredentialsProviderWrapper.class
+    );
+
+    @Override
+    public void serialize(AWSCredentialsProvider credentialsProvider, JsonGenerator jsonGenerator,
+        SerializerProvider serializers) throws IOException {
+      serializers.defaultSerializeValue(credentialsProvider, jsonGenerator);
+    }
+
+    @Override
+    public void serializeWithType(AWSCredentialsProvider credentialsProvider,
+        JsonGenerator jsonGenerator, SerializerProvider serializers, TypeSerializer typeSerializer)
+        throws IOException {
+      typeSerializer.writeTypePrefixForObject(credentialsProvider, jsonGenerator);
+
+      if (credentialsProvider.getClass().equals(AWSStaticCredentialsProvider.class)) {
+        jsonGenerator.writeStringField(
+            AWS_ACCESS_KEY_ID, credentialsProvider.getCredentials().getAWSAccessKeyId());
+        jsonGenerator.writeStringField(
+            AWS_SECRET_KEY, credentialsProvider.getCredentials().getAWSSecretKey());
+
+      } else if (credentialsProvider.getClass().equals(PropertiesFileCredentialsProvider.class)) {
+        try {
+          PropertiesFileCredentialsProvider specificProvider =
+              (PropertiesFileCredentialsProvider) credentialsProvider;
+          Field field =
+              PropertiesFileCredentialsProvider.class.getDeclaredField(CREDENTIALS_FILE_PATH);
+          field.setAccessible(true);
+          String credentialsFilePath = ((String) field.get(specificProvider));
+          jsonGenerator.writeStringField(CREDENTIALS_FILE_PATH, credentialsFilePath);
+        } catch (NoSuchFieldException | IllegalAccessException e) {
+          throw new IOException("failed to access private field with reflection", e);
+        }
+
+      } else if (credentialsProvider.getClass()
+          .equals(ClasspathPropertiesFileCredentialsProvider.class)) {
+        try {
+          ClasspathPropertiesFileCredentialsProvider specificProvider =
+              (ClasspathPropertiesFileCredentialsProvider) credentialsProvider;
+          Field field =
+              ClasspathPropertiesFileCredentialsProvider.class
+                  .getDeclaredField(CREDENTIALS_FILE_PATH);
+          field.setAccessible(true);
+          String credentialsFilePath = ((String) field.get(specificProvider));
+          jsonGenerator.writeStringField(CREDENTIALS_FILE_PATH, credentialsFilePath);
+        } catch (NoSuchFieldException | IllegalAccessException e) {
+          throw new IOException("failed to access private field with reflection", e);
+        }
+
+      } else if (!SINGLETON_CREDENTIAL_PROVIDERS.contains(credentialsProvider.getClass())) {
+        throw new IllegalArgumentException(
+            "Unsupported AWS credentials provider type " + credentialsProvider.getClass());
+      }
+      typeSerializer.writeTypeSuffixForObject(credentialsProvider, jsonGenerator);
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsOptions.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsOptions.java
new file mode 100644
index 000000000000..8ed68e565b9b
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsOptions.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.options;
+
+import com.amazonaws.auth.AWSCredentialsProvider;
+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
+import org.apache.beam.sdk.options.Default;
+import org.apache.beam.sdk.options.DefaultValueFactory;
+import org.apache.beam.sdk.options.Description;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.options.Validation;
+
+/**
+ * Options used to configure Amazon Web Services specific options such as credentials and region.
+ */
+public interface AwsOptions extends PipelineOptions {
+
+  /**
+   * AWS region used by the AWS client.
+   */
+  @Description("AWS region used by the AWS client")
+  @Validation.Required
+  String getAwsRegion();
+  void setAwsRegion(String value);
+
+  /**
+   * The credential instance that should be used to authenticate against AWS services. Refer to
+   * {@link DefaultAWSCredentialsProviderChain} Javadoc for usage help.
+   */
+  @Description("The credential instance that should be used to authenticate against AWS services. "
+      + "Refer to DefaultAWSCredentialsProviderChain Javadoc for usage help.")
+  @Default.InstanceFactory(AwsUserCredentialsFactory.class)
+  AWSCredentialsProvider getAwsCredentialsProvider();
+  void setAwsCredentialsProvider(AWSCredentialsProvider value);
+
+  /**
+   * Attempts to load AWS credentials.
+   */
+  class AwsUserCredentialsFactory implements DefaultValueFactory<AWSCredentialsProvider> {
+
+    @Override
+    public AWSCredentialsProvider create(PipelineOptions options) {
+      return DefaultAWSCredentialsProviderChain.getInstance();
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsPipelineOptionsRegistrar.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsPipelineOptionsRegistrar.java
new file mode 100644
index 000000000000..5aa3da25fac4
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/AwsPipelineOptionsRegistrar.java
@@ -0,0 +1,38 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.options;
+
+import com.google.auto.service.AutoService;
+import com.google.common.collect.ImmutableList;
+import org.apache.beam.sdk.options.PipelineOptions;
+import org.apache.beam.sdk.options.PipelineOptionsRegistrar;
+
+/**
+ * A registrar containing the default AWS options.
+ */
+@AutoService(PipelineOptionsRegistrar.class)
+public class AwsPipelineOptionsRegistrar implements PipelineOptionsRegistrar {
+
+  @Override
+  public Iterable<Class<? extends PipelineOptions>> getPipelineOptions() {
+    return ImmutableList.<Class<? extends PipelineOptions>>builder()
+        .add(AwsOptions.class)
+        .add(S3Options.class)
+        .build();
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/S3Options.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/S3Options.java
new file mode 100644
index 000000000000..253272898a1a
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/S3Options.java
@@ -0,0 +1,45 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.options;
+
+import javax.annotation.Nullable;
+import org.apache.beam.sdk.options.Default;
+import org.apache.beam.sdk.options.Description;
+
+/**
+ * Options used to configure Amazon Web Services S3.
+ */
+public interface S3Options extends AwsOptions {
+
+  @Description("AWS S3 storage class used for creating S3 objects")
+  @Default.String("STANDARD")
+  String getS3StorageClass();
+  void setS3StorageClass(String value);
+
+  @Description(
+      "Size of S3 upload chunks; max upload object size is this value multiplied by 10000;"
+          + "default is 64MB, or 5MB in memory-constrained environments")
+  @Nullable
+  Integer getS3UploadBufferSizeBytes();
+  void setS3UploadBufferSizeBytes(Integer value);
+
+  @Description("Thread pool size, limiting max concurrent S3 operations")
+  @Default.Integer(50)
+  int getS3ThreadPoolSize();
+  void setS3ThreadPoolSize(int value);
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/package-info.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/package-info.java
new file mode 100644
index 000000000000..e0e54e5b8df1
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/options/package-info.java
@@ -0,0 +1,22 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+/**
+ * Defines {@link org.apache.beam.sdk.options.PipelineOptions} for
+ * configuring pipeline execution for Amazon Web Services components.
+ */
+package org.apache.beam.sdk.io.aws.options;
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystem.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystem.java
new file mode 100644
index 000000000000..b7bb4205e225
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystem.java
@@ -0,0 +1,704 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.AmazonS3ClientBuilder;
+import com.amazonaws.services.s3.model.AmazonS3Exception;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
+import com.amazonaws.services.s3.model.CopyPartRequest;
+import com.amazonaws.services.s3.model.CopyPartResult;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadResult;
+import com.amazonaws.services.s3.model.ListObjectsV2Request;
+import com.amazonaws.services.s3.model.ListObjectsV2Result;
+import com.amazonaws.services.s3.model.ObjectMetadata;
+import com.amazonaws.services.s3.model.PartETag;
+import com.amazonaws.services.s3.model.S3ObjectSummary;
+import com.google.auto.value.AutoValue;
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.base.Function;
+import com.google.common.base.Predicate;
+import com.google.common.base.Strings;
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.FluentIterable;
+import com.google.common.collect.ImmutableList;
+import com.google.common.collect.ImmutableSet;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Multimap;
+import com.google.common.util.concurrent.Futures;
+import com.google.common.util.concurrent.ListenableFuture;
+import com.google.common.util.concurrent.ListeningExecutorService;
+import com.google.common.util.concurrent.MoreExecutors;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.nio.channels.ReadableByteChannel;
+import java.nio.channels.WritableByteChannel;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.apache.beam.sdk.io.FileSystem;
+import org.apache.beam.sdk.io.aws.options.S3Options;
+import org.apache.beam.sdk.io.fs.CreateOptions;
+import org.apache.beam.sdk.io.fs.MatchResult;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+class S3FileSystem extends FileSystem<S3ResourceId> {
+
+  private static final Logger LOG = LoggerFactory.getLogger(S3FileSystem.class);
+
+  // Amazon S3 API docs: Each part must be at least 5 MB in size, except the last part.
+  private static final int MINIMUM_UPLOAD_BUFFER_SIZE_BYTES = 5 * 1024 * 1024;
+  private static final int DEFAULT_UPLOAD_BUFFER_SIZE_BYTES =
+      Runtime.getRuntime().maxMemory() < 512 * 1024 * 1024
+          ? MINIMUM_UPLOAD_BUFFER_SIZE_BYTES
+          : 64 * 1024 * 1024;
+
+  // S3 API, delete-objects: "You may specify up to 1000 keys."
+  private static final int MAX_DELETE_OBJECTS_PER_REQUEST = 1000;
+
+  private static final Set<String> NON_READ_SEEK_EFFICIENT_ENCODINGS = ImmutableSet.of("gzip");
+
+  // Non-final for testing.
+  private AmazonS3 amazonS3;
+  private final String storageClass;
+  private final int s3UploadBufferSizeBytes;
+  private final ListeningExecutorService executorService;
+
+  S3FileSystem(S3Options options) {
+    checkNotNull(options, "options");
+
+    if (Strings.isNullOrEmpty(options.getAwsRegion())) {
+      LOG.info(
+          "The AWS S3 Beam extension was included in this build, but the awsRegion flag "
+              + "was not specified. If you don't plan to use S3, then ignore this message.");
+    }
+
+    amazonS3 =
+        AmazonS3ClientBuilder.standard()
+            .withCredentials(options.getAwsCredentialsProvider())
+            .withRegion(options.getAwsRegion())
+            .build();
+
+    this.storageClass = checkNotNull(options.getS3StorageClass(), "storageClass");
+
+    int uploadBufferSizeBytes;
+    if (options.getS3UploadBufferSizeBytes() != null) {
+      uploadBufferSizeBytes = options.getS3UploadBufferSizeBytes();
+    } else {
+      uploadBufferSizeBytes = DEFAULT_UPLOAD_BUFFER_SIZE_BYTES;
+    }
+    this.s3UploadBufferSizeBytes =
+        Math.max(MINIMUM_UPLOAD_BUFFER_SIZE_BYTES, uploadBufferSizeBytes);
+
+    checkArgument(options.getS3ThreadPoolSize() > 0, "threadPoolSize");
+    executorService =
+        MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(
+            options.getS3ThreadPoolSize(), new ThreadFactoryBuilder().setDaemon(true).build()));
+
+  }
+
+  @Override
+  protected String getScheme() {
+    return S3ResourceId.SCHEME;
+  }
+
+  @VisibleForTesting
+  void setAmazonS3Client(AmazonS3 amazonS3) {
+    this.amazonS3 = amazonS3;
+  }
+
+  @VisibleForTesting
+  int getS3UploadBufferSizeBytes() {
+    return s3UploadBufferSizeBytes;
+  }
+
+  @Override
+  protected List<MatchResult> match(List<String> specs) throws IOException {
+    List<S3ResourceId> paths =
+        FluentIterable.from(specs)
+            .transform(
+                new Function<String, S3ResourceId>() {
+                  @Override
+                  public S3ResourceId apply(String spec) {
+                    return S3ResourceId.fromUri(spec);
+                  }
+                })
+            .toList();
+    List<S3ResourceId> globs = Lists.newArrayList();
+    List<S3ResourceId> nonGlobs = Lists.newArrayList();
+    List<Boolean> isGlobBooleans = Lists.newArrayList();
+
+    for (S3ResourceId path : paths) {
+      if (path.isWildcard()) {
+        globs.add(path);
+        isGlobBooleans.add(true);
+      } else {
+        nonGlobs.add(path);
+        isGlobBooleans.add(false);
+      }
+    }
+
+    Iterator<MatchResult> globMatches = matchGlobPaths(globs).iterator();
+    Iterator<MatchResult> nonGlobMatches = matchNonGlobPaths(nonGlobs).iterator();
+
+    ImmutableList.Builder<MatchResult> matchResults = ImmutableList.builder();
+    for (Boolean isGlob : isGlobBooleans) {
+      if (isGlob) {
+        checkState(globMatches.hasNext(), "Expect globMatches has next.");
+        matchResults.add(globMatches.next());
+      } else {
+        checkState(nonGlobMatches.hasNext(), "Expect nonGlobMatches has next.");
+        matchResults.add(nonGlobMatches.next());
+      }
+    }
+    checkState(!globMatches.hasNext(), "Expect no more elements in globMatches.");
+    checkState(!nonGlobMatches.hasNext(), "Expect no more elements in nonGlobMatches.");
+
+    return matchResults.build();
+  }
+
+  /**
+   * Gets {@link MatchResult} representing all objects that match wildcard-containing paths.
+   */
+  @VisibleForTesting
+  List<MatchResult> matchGlobPaths(Collection<S3ResourceId> globPaths) throws IOException {
+    List<Callable<ExpandedGlob>> expandTasks = new ArrayList<>(globPaths.size());
+    for (final S3ResourceId path : globPaths) {
+      expandTasks.add(
+          new Callable<ExpandedGlob>() {
+            @Override
+            public ExpandedGlob call() {
+              return expandGlob(path);
+            }
+          });
+    }
+
+    Map<S3ResourceId, ExpandedGlob> expandedGlobByGlobPath = new HashMap<>();
+    List<Callable<PathWithEncoding>> contentTypeTasks =
+        new ArrayList<>(globPaths.size());
+    for (ExpandedGlob expandedGlob : callTasks(expandTasks)) {
+      expandedGlobByGlobPath.put(expandedGlob.getGlobPath(), expandedGlob);
+      if (expandedGlob.getExpandedPaths() != null) {
+        for (final S3ResourceId path : expandedGlob.getExpandedPaths()) {
+          contentTypeTasks.add(
+              new Callable<PathWithEncoding>() {
+                @Override
+                public PathWithEncoding call() {
+                  return getPathContentEncoding(path);
+                }
+              }
+          );
+        }
+      }
+    }
+
+    Map<S3ResourceId, PathWithEncoding> exceptionByPath = new HashMap<>();
+    for (PathWithEncoding pathWithException : callTasks(contentTypeTasks)) {
+      exceptionByPath.put(pathWithException.getPath(), pathWithException);
+    }
+
+    List<MatchResult> results = new ArrayList<>(globPaths.size());
+    for (S3ResourceId globPath : globPaths) {
+      ExpandedGlob expandedGlob = expandedGlobByGlobPath.get(globPath);
+
+      if (expandedGlob.getException() != null) {
+        results.add(MatchResult.create(MatchResult.Status.ERROR, expandedGlob.getException()));
+
+      } else {
+        List<MatchResult.Metadata> metadatas = new ArrayList<>();
+        IOException exception = null;
+        for (S3ResourceId expandedPath : expandedGlob.getExpandedPaths()) {
+          PathWithEncoding pathWithEncoding = exceptionByPath.get(expandedPath);
+
+          if (pathWithEncoding.getException() != null) {
+            exception = pathWithEncoding.getException();
+            break;
+          } else {
+            metadatas.add(
+                createBeamMetadata(
+                    pathWithEncoding.getPath(), pathWithEncoding.getContentEncoding()));
+          }
+        }
+
+        if (exception != null) {
+          if (exception instanceof FileNotFoundException) {
+            results.add(MatchResult.create(MatchResult.Status.NOT_FOUND, exception));
+          } else {
+            results.add(MatchResult.create(MatchResult.Status.ERROR, exception));
+          }
+        } else {
+          results.add(MatchResult.create(MatchResult.Status.OK, metadatas));
+        }
+      }
+    }
+
+    return ImmutableList.copyOf(results);
+  }
+
+  @AutoValue
+  abstract static class ExpandedGlob {
+
+    abstract S3ResourceId getGlobPath();
+
+    @Nullable
+    abstract List<S3ResourceId> getExpandedPaths();
+
+    @Nullable
+    abstract IOException getException();
+
+    static ExpandedGlob create(S3ResourceId globPath, List<S3ResourceId> expandedPaths) {
+      checkNotNull(globPath, "globPath");
+      checkNotNull(expandedPaths, "expandedPaths");
+      return new AutoValue_S3FileSystem_ExpandedGlob(globPath, expandedPaths, null);
+    }
+
+    static ExpandedGlob create(S3ResourceId globPath, IOException exception) {
+      checkNotNull(globPath, "globPath");
+      checkNotNull(exception, "exception");
+      return new AutoValue_S3FileSystem_ExpandedGlob(globPath, null, exception);
+    }
+  }
+
+  @AutoValue
+  abstract static class PathWithEncoding {
+
+    abstract S3ResourceId getPath();
+
+    @Nullable
+    abstract String getContentEncoding();
+
+    @Nullable
+    abstract IOException getException();
+
+    static PathWithEncoding create(S3ResourceId path, String contentEncoding) {
+      checkNotNull(path, "path");
+      checkNotNull(contentEncoding, "contentEncoding");
+      return new AutoValue_S3FileSystem_PathWithEncoding(path, contentEncoding, null);
+    }
+
+    static PathWithEncoding create(S3ResourceId path, IOException exception) {
+      checkNotNull(path, "path");
+      checkNotNull(exception, "exception");
+      return new AutoValue_S3FileSystem_PathWithEncoding(path, null, exception);
+    }
+  }
+
+  private ExpandedGlob expandGlob(S3ResourceId glob) {
+    // The S3 API can list objects, filtered by prefix, but not by wildcard.
+    // Here, we find the longest prefix without wildcard "*",
+    // then filter the results with a regex.
+    checkArgument(glob.isWildcard(), "isWildcard");
+    String keyPrefix = glob.getKeyNonWildcardPrefix();
+    Pattern wildcardRegexp = Pattern.compile(wildcardToRegexp(glob.getKey()));
+
+    LOG.debug(
+        "expanding bucket {}, prefix {}, against pattern {}",
+        glob.getBucket(),
+        keyPrefix,
+        wildcardRegexp.toString());
+
+    ImmutableList.Builder<S3ResourceId> expandedPaths = ImmutableList.builder();
+    String continuationToken = null;
+
+    do {
+      ListObjectsV2Request request =
+          new ListObjectsV2Request()
+              .withBucketName(glob.getBucket())
+              .withPrefix(keyPrefix)
+              .withContinuationToken(continuationToken);
+      ListObjectsV2Result result;
+      try {
+        result = amazonS3.listObjectsV2(request);
+      } catch (AmazonClientException e) {
+        return ExpandedGlob.create(glob, new IOException(e));
+      }
+      continuationToken = result.getNextContinuationToken();
+
+      for (S3ObjectSummary objectSummary : result.getObjectSummaries()) {
+        // Filter against regex.
+        if (wildcardRegexp.matcher(objectSummary.getKey()).matches()) {
+          S3ResourceId expandedPath =
+              S3ResourceId.fromComponents(objectSummary.getBucketName(), objectSummary.getKey())
+                  .withSize(objectSummary.getSize());
+          LOG.debug("Expanded S3 object path {}", expandedPath);
+          expandedPaths.add(expandedPath);
+        }
+      }
+    } while (continuationToken != null);
+
+    return ExpandedGlob.create(glob, expandedPaths.build());
+  }
+
+  private PathWithEncoding getPathContentEncoding(S3ResourceId path) {
+    ObjectMetadata s3Metadata;
+    try {
+      s3Metadata = amazonS3.getObjectMetadata(path.getBucket(), path.getKey());
+    } catch (AmazonClientException e) {
+      if (e instanceof AmazonS3Exception && ((AmazonS3Exception) e).getStatusCode() == 404) {
+        return PathWithEncoding.create(path, new FileNotFoundException());
+      }
+      return PathWithEncoding.create(path, new IOException(e));
+    }
+    return PathWithEncoding.create(path, Strings.nullToEmpty(s3Metadata.getContentEncoding()));
+  }
+
+  private List<MatchResult> matchNonGlobPaths(Collection<S3ResourceId> paths) throws IOException {
+    List<Callable<MatchResult>> tasks = new ArrayList<>(paths.size());
+    for (final S3ResourceId path : paths) {
+      tasks.add(
+          new Callable<MatchResult>() {
+            @Override
+            public MatchResult call() {
+              return matchNonGlobPath(path);
+            }
+          });
+    }
+
+    return callTasks(tasks);
+  }
+
+  @VisibleForTesting
+  MatchResult matchNonGlobPath(S3ResourceId path) {
+    ObjectMetadata s3Metadata;
+    try {
+      s3Metadata = amazonS3.getObjectMetadata(path.getBucket(), path.getKey());
+    } catch (AmazonClientException e) {
+      if (e instanceof AmazonS3Exception && ((AmazonS3Exception) e).getStatusCode() == 404) {
+        return MatchResult.create(MatchResult.Status.NOT_FOUND, new FileNotFoundException());
+      }
+      return MatchResult.create(MatchResult.Status.ERROR, new IOException(e));
+    }
+
+    return MatchResult.create(
+        MatchResult.Status.OK,
+        ImmutableList.of(
+            createBeamMetadata(
+                path.withSize(s3Metadata.getContentLength()),
+                Strings.nullToEmpty(s3Metadata.getContentEncoding()))));
+  }
+
+  private static MatchResult.Metadata createBeamMetadata(
+      S3ResourceId path, String contentEncoding) {
+    checkArgument(path.getSize().isPresent(), "path has size");
+    checkNotNull(contentEncoding, "contentEncoding");
+    boolean isReadSeekEfficient = !NON_READ_SEEK_EFFICIENT_ENCODINGS.contains(contentEncoding);
+    return MatchResult.Metadata.builder()
+        .setIsReadSeekEfficient(isReadSeekEfficient)
+        .setResourceId(path)
+        .setSizeBytes(path.getSize().get())
+        .build();
+  }
+
+  /**
+   * Expands glob expressions to regular expressions.
+   *
+   * @param globExp the glob expression to expand
+   * @return a string with the regular expression this glob expands to
+   */
+  @VisibleForTesting
+  static String wildcardToRegexp(String globExp) {
+    StringBuilder dst = new StringBuilder();
+    char[] src = globExp.replace("**/*", "**").toCharArray();
+    int i = 0;
+    while (i < src.length) {
+      char c = src[i++];
+      switch (c) {
+        case '*':
+          // One char lookahead for **
+          if (i < src.length && src[i] == '*') {
+            dst.append(".*");
+            ++i;
+          } else {
+            dst.append("[^/]*");
+          }
+          break;
+        case '?':
+          dst.append("[^/]");
+          break;
+        case '.':
+        case '+':
+        case '{':
+        case '}':
+        case '(':
+        case ')':
+        case '|':
+        case '^':
+        case '$':
+          // These need to be escaped in regular expressions
+          dst.append('\\').append(c);
+          break;
+        case '\\':
+          i = doubleSlashes(dst, src, i);
+          break;
+        default:
+          dst.append(c);
+          break;
+      }
+    }
+    return dst.toString();
+  }
+
+  private static int doubleSlashes(StringBuilder dst, char[] src, int i) {
+    // Emit the next character without special interpretation
+    dst.append("\\\\");
+    if ((i - 1) != src.length) {
+      dst.append(src[i]);
+      i++;
+    } else {
+      // A backslash at the very end is treated like an escaped backslash
+      dst.append('\\');
+    }
+    return i;
+  }
+
+  @Override
+  protected WritableByteChannel create(S3ResourceId resourceId, CreateOptions createOptions)
+      throws IOException {
+    return new S3WritableByteChannel(
+        amazonS3, resourceId, createOptions.mimeType(), storageClass, s3UploadBufferSizeBytes);
+  }
+
+  @Override
+  protected ReadableByteChannel open(S3ResourceId resourceId) throws IOException {
+    return new S3ReadableSeekableByteChannel(amazonS3, resourceId);
+  }
+
+  @Override
+  protected void copy(
+      List<S3ResourceId> sourcePaths, List<S3ResourceId> destinationPaths)
+      throws IOException {
+    checkArgument(
+        sourcePaths.size() == destinationPaths.size(),
+        "sizes of sourcePaths and destinationPaths do not match");
+
+    List<Callable<Void>> tasks = new ArrayList<>(sourcePaths.size());
+
+    Iterator<S3ResourceId> sourcePathsIterator = sourcePaths.iterator();
+    Iterator<S3ResourceId> destinationPathsIterator = destinationPaths.iterator();
+    while (sourcePathsIterator.hasNext()) {
+      final S3ResourceId sourcePath = sourcePathsIterator.next();
+      final S3ResourceId destinationPath = destinationPathsIterator.next();
+
+      tasks.add(
+          new Callable<Void>() {
+            @Override
+            public Void call() throws IOException {
+              copy(sourcePath, destinationPath);
+              return null;
+            }
+          });
+    }
+
+    callTasks(tasks);
+  }
+
+  @VisibleForTesting
+  void copy(S3ResourceId sourcePath, S3ResourceId destinationPath) throws IOException {
+    String uploadId;
+    long objectSize;
+    try {
+      ObjectMetadata objectMetadata =
+          amazonS3.getObjectMetadata(sourcePath.getBucket(), sourcePath.getKey());
+      objectSize = objectMetadata.getContentLength();
+
+      InitiateMultipartUploadRequest initiateUploadRequest =
+          new InitiateMultipartUploadRequest(destinationPath.getBucket(), destinationPath.getKey())
+              .withStorageClass(storageClass)
+              .withObjectMetadata(objectMetadata);
+
+      InitiateMultipartUploadResult initiateUploadResult =
+          amazonS3.initiateMultipartUpload(initiateUploadRequest);
+      uploadId = initiateUploadResult.getUploadId();
+
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+
+    List<PartETag> eTags = new ArrayList<>();
+
+    long bytePosition = 0;
+
+    // Amazon parts are 1-indexed, not zero-indexed.
+    for (int partNumber = 1; bytePosition < objectSize; partNumber++) {
+      final CopyPartRequest copyPartRequest =
+          new CopyPartRequest()
+              .withSourceBucketName(sourcePath.getBucket())
+              .withSourceKey(sourcePath.getKey())
+              .withDestinationBucketName(destinationPath.getBucket())
+              .withDestinationKey(destinationPath.getKey())
+              .withUploadId(uploadId)
+              .withPartNumber(partNumber)
+              .withFirstByte(bytePosition)
+              .withLastByte(Math.min(objectSize - 1, bytePosition + s3UploadBufferSizeBytes - 1));
+
+      CopyPartResult copyPartResult;
+      try {
+        copyPartResult = amazonS3.copyPart(copyPartRequest);
+      } catch (AmazonClientException e) {
+        throw new IOException(e);
+      }
+      eTags.add(copyPartResult.getPartETag());
+
+      bytePosition += s3UploadBufferSizeBytes;
+    }
+
+    CompleteMultipartUploadRequest completeUploadRequest =
+        new CompleteMultipartUploadRequest()
+            .withBucketName(destinationPath.getBucket())
+            .withKey(destinationPath.getKey())
+            .withUploadId(uploadId)
+            .withPartETags(eTags);
+
+    try {
+      amazonS3.completeMultipartUpload(completeUploadRequest);
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  protected void rename(
+      List<S3ResourceId> sourceResourceIds, List<S3ResourceId> destinationResourceIds)
+      throws IOException {
+    copy(sourceResourceIds, destinationResourceIds);
+    delete(sourceResourceIds);
+  }
+
+  @Override
+  protected void delete(Collection<S3ResourceId> resourceIds) throws IOException {
+    List<S3ResourceId> nonDirectoryPaths = FluentIterable
+        .from(resourceIds)
+        .filter(new Predicate<S3ResourceId>() {
+          @Override
+          public boolean apply(S3ResourceId s3ResourceId) {
+            return !s3ResourceId.isDirectory();
+          }
+        })
+        .toList();
+    Multimap<String, String> keysByBucket = ArrayListMultimap.create();
+    for (S3ResourceId path : nonDirectoryPaths) {
+      keysByBucket.put(path.getBucket(), path.getKey());
+    }
+
+    List<Callable<Void>> tasks = new ArrayList<>();
+    for (final String bucket : keysByBucket.keySet()) {
+      for (final List<String> keysPartition
+          : Iterables.partition(keysByBucket.get(bucket), MAX_DELETE_OBJECTS_PER_REQUEST)) {
+        tasks.add(
+            new Callable<Void>() {
+              @Override
+              public Void call() throws IOException {
+                delete(bucket, keysPartition);
+                return null;
+              }
+            });
+      }
+    }
+
+    callTasks(tasks);
+  }
+
+  private void delete(String bucket, Collection<String> keys) throws IOException {
+    checkArgument(
+        keys.size() <= MAX_DELETE_OBJECTS_PER_REQUEST,
+        "only %d keys can be deleted per request, but got %d",
+        MAX_DELETE_OBJECTS_PER_REQUEST,
+        keys.size());
+    List<KeyVersion> deleteKeyVersions =
+        FluentIterable.from(keys)
+            .transform(
+                new Function<String, KeyVersion>() {
+                  @Override
+                  public KeyVersion apply(String key) {
+                    return new KeyVersion(key);
+                  }
+                })
+            .toList();
+    DeleteObjectsRequest request = new DeleteObjectsRequest(bucket).withKeys(deleteKeyVersions);
+    try {
+      amazonS3.deleteObjects(request);
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+  }
+
+  @Override
+  protected S3ResourceId matchNewResource(String singleResourceSpec, boolean isDirectory) {
+    if (isDirectory) {
+      if (!singleResourceSpec.endsWith("/")) {
+        singleResourceSpec += "/";
+      }
+    } else {
+      checkArgument(
+          !singleResourceSpec.endsWith("/"),
+          "Expected a file path, but [%s] ends with '/'. This is unsupported in S3FileSystem.",
+          singleResourceSpec);
+    }
+    return S3ResourceId.fromUri(singleResourceSpec);
+  }
+
+  /**
+   * Invokes tasks in a thread pool, then unwraps the resulting {@link Future Futures}.
+   *
+   * <p>Any task exception is wrapped in {@link IOException}.
+   */
+  private <T> List<T> callTasks(Collection<Callable<T>> tasks) throws IOException {
+
+    try {
+      List<ListenableFuture<T>> futures = new ArrayList<>(tasks.size());
+      for (Callable<T> task : tasks) {
+        futures.add(executorService.submit(task));
+      }
+      return Futures.allAsList(futures).get();
+
+    } catch (ExecutionException e) {
+      if (e.getCause() != null) {
+        if (e.getCause() instanceof IOException) {
+          throw ((IOException) e.getCause());
+        }
+        throw new IOException(e.getCause());
+      }
+      throw new IOException(e);
+
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new IOException("executor service was interrupted");
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemRegistrar.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemRegistrar.java
new file mode 100644
index 000000000000..ebfa7c40ec61
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemRegistrar.java
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkNotNull;
+
+import com.google.auto.service.AutoService;
+import com.google.common.collect.ImmutableList;
+import javax.annotation.Nonnull;
+import org.apache.beam.sdk.annotations.Experimental;
+import org.apache.beam.sdk.io.FileSystem;
+import org.apache.beam.sdk.io.FileSystemRegistrar;
+import org.apache.beam.sdk.io.aws.options.S3Options;
+import org.apache.beam.sdk.options.PipelineOptions;
+
+/**
+ * {@link AutoService} registrar for the {@link S3FileSystem}.
+ */
+@AutoService(FileSystemRegistrar.class)
+@Experimental(Experimental.Kind.FILESYSTEM)
+public class S3FileSystemRegistrar implements FileSystemRegistrar {
+
+  @Override
+  public Iterable<FileSystem> fromOptions(@Nonnull PipelineOptions options) {
+    checkNotNull(options, "Expect the runner have called FileSystems.setDefaultPipelineOptions().");
+    return ImmutableList.<FileSystem>of(new S3FileSystem(options.as(S3Options.class)));
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ReadableSeekableByteChannel.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ReadableSeekableByteChannel.java
new file mode 100644
index 000000000000..2c2813a0400f
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ReadableSeekableByteChannel.java
@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.GetObjectRequest;
+import com.amazonaws.services.s3.model.S3Object;
+import java.io.BufferedInputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.Channels;
+import java.nio.channels.ClosedChannelException;
+import java.nio.channels.NonWritableChannelException;
+import java.nio.channels.ReadableByteChannel;
+import java.nio.channels.SeekableByteChannel;
+
+/**
+ * A readable S3 object, as a {@link SeekableByteChannel}.
+ */
+class S3ReadableSeekableByteChannel implements SeekableByteChannel {
+
+  private final AmazonS3 amazonS3;
+  private final S3ResourceId path;
+  private final long contentLength;
+  private long position = 0;
+  private boolean open = true;
+  private S3Object s3Object;
+  private ReadableByteChannel s3ObjectContentChannel;
+
+  S3ReadableSeekableByteChannel(AmazonS3 amazonS3, S3ResourceId path) throws IOException {
+    this.amazonS3 = checkNotNull(amazonS3, "amazonS3");
+    checkNotNull(path, "path");
+
+    if (path.getSize().isPresent()) {
+      contentLength = path.getSize().get();
+      this.path = path;
+
+    } else {
+      try {
+        contentLength =
+            amazonS3.getObjectMetadata(path.getBucket(), path.getKey()).getContentLength();
+      } catch (AmazonClientException e) {
+        throw new IOException(e);
+      }
+      this.path = path.withSize(contentLength);
+    }
+  }
+
+  @Override
+  public int read(ByteBuffer destinationBuffer) throws IOException {
+    if (!isOpen()) {
+      throw new ClosedChannelException();
+    }
+    if (!destinationBuffer.hasRemaining()) {
+      return 0;
+    }
+    if (position == contentLength) {
+      return -1;
+    }
+
+    if (s3Object == null) {
+      GetObjectRequest request = new GetObjectRequest(path.getBucket(), path.getKey());
+      if (position > 0) {
+        request.setRange(position, contentLength);
+      }
+      try {
+        s3Object = amazonS3.getObject(request);
+      } catch (AmazonClientException e) {
+        throw new IOException(e);
+      }
+      s3ObjectContentChannel = Channels.newChannel(
+          new BufferedInputStream(s3Object.getObjectContent(), 1024 * 1024));
+    }
+
+    int totalBytesRead = 0;
+    int bytesRead = 0;
+
+    do {
+      totalBytesRead += bytesRead;
+      try {
+        bytesRead = s3ObjectContentChannel.read(destinationBuffer);
+      } catch (AmazonClientException e) {
+        // TODO replace all catch AmazonServiceException with client exception
+        throw new IOException(e);
+      }
+    } while (bytesRead > 0);
+
+    position += totalBytesRead;
+    return totalBytesRead;
+  }
+
+  @Override
+  public long position() throws ClosedChannelException {
+    if (!isOpen()) {
+      throw new ClosedChannelException();
+    }
+    return position;
+  }
+
+  @Override
+  public SeekableByteChannel position(long newPosition) throws IOException {
+    if (!isOpen()) {
+      throw new ClosedChannelException();
+    }
+    checkArgument(newPosition >= 0, "newPosition too low");
+    checkArgument(newPosition < contentLength, "new position too high");
+
+    if (newPosition == position) {
+      return this;
+    }
+
+    // The position has changed, so close the object to induce a re-open on the next call to read()
+    if (s3Object != null) {
+      s3Object.close();
+    }
+    position = newPosition;
+    return this;
+  }
+
+  @Override
+  public long size() throws ClosedChannelException {
+    if (!isOpen()) {
+      throw new ClosedChannelException();
+    }
+    return contentLength;
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (s3Object != null) {
+      s3Object.close();
+    }
+    open = false;
+  }
+
+  @Override
+  public boolean isOpen() {
+    return open;
+  }
+
+  @Override
+  public int write(ByteBuffer src) {
+    throw new NonWritableChannelException();
+  }
+
+  @Override
+  public SeekableByteChannel truncate(long size) {
+    throw new NonWritableChannelException();
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ResourceId.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ResourceId.java
new file mode 100644
index 000000000000..583689e1fb24
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3ResourceId.java
@@ -0,0 +1,194 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+import static com.google.common.base.Preconditions.checkState;
+
+import com.google.common.base.Optional;
+import com.google.common.base.Strings;
+import java.util.Objects;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import javax.annotation.Nullable;
+import org.apache.beam.sdk.io.fs.ResolveOptions;
+import org.apache.beam.sdk.io.fs.ResolveOptions.StandardResolveOptions;
+import org.apache.beam.sdk.io.fs.ResourceId;
+
+class S3ResourceId implements ResourceId {
+
+  static final String SCHEME = "s3";
+
+  private static final Pattern S3_URI =
+      Pattern.compile("(?<SCHEME>[^:]+)://(?<BUCKET>[^/]+)(/(?<KEY>.*))?");
+
+  /**
+   * Matches a glob containing a wildcard, capturing the portion before the first wildcard.
+   */
+  private static final Pattern GLOB_PREFIX = Pattern.compile("(?<PREFIX>[^\\[*?]*)[\\[*?].*");
+
+  private final String bucket;
+  private final String key;
+  private final Long size;
+
+  private S3ResourceId(
+      String bucket, String key, @Nullable Long size) {
+    checkArgument(!Strings.isNullOrEmpty(bucket), "bucket");
+    this.bucket = bucket;
+    this.key = checkNotNull(key, "key");
+    this.size = size;
+  }
+
+  static S3ResourceId fromComponents(String bucket, String key) {
+    if (!key.startsWith("/")) {
+      key = "/" + key;
+    }
+    return new S3ResourceId(bucket, key, null);
+  }
+
+  static S3ResourceId fromUri(String uri) {
+    Matcher m = S3_URI.matcher(uri);
+    checkArgument(m.matches(), "Invalid S3 URI: [%s]", uri);
+    checkArgument(m.group("SCHEME").equalsIgnoreCase(SCHEME), "Invalid S3 URI scheme: [%s]", uri);
+    String bucket = m.group("BUCKET");
+    String key = Strings.nullToEmpty(m.group("KEY"));
+    if (!key.startsWith("/")) {
+      key = "/" + key;
+    }
+    return fromComponents(bucket, key);
+  }
+
+  String getBucket() {
+    return bucket;
+  }
+
+  String getKey() {
+    // Skip leading slash
+    return key.substring(1);
+  }
+
+  Optional<Long> getSize() {
+    return Optional.fromNullable(size);
+  }
+
+  S3ResourceId withSize(long size) {
+    return new S3ResourceId(bucket, key, size);
+  }
+
+  @Override
+  public ResourceId resolve(String other, ResolveOptions resolveOptions) {
+    checkState(isDirectory(), "Expected this resource to be a directory, but was [%s]", toString());
+
+    if (resolveOptions == StandardResolveOptions.RESOLVE_DIRECTORY) {
+      if ("..".equals(other)) {
+        if ("/".equals(key)) {
+          return this;
+        }
+        int parentStopsAt = key.substring(0, key.length() - 1).lastIndexOf('/');
+        return fromComponents(bucket, key.substring(0, parentStopsAt + 1));
+      }
+
+      if ("".equals(other)) {
+        return this;
+      }
+
+      if (!other.endsWith("/")) {
+        other += "/";
+      }
+      if (S3_URI.matcher(other).matches()) {
+        return fromUri(other);
+      }
+      return fromComponents(bucket, key + other);
+    }
+
+    if (resolveOptions == StandardResolveOptions.RESOLVE_FILE) {
+      checkArgument(!other.endsWith("/"), "Cannot resolve a file with a directory path: [%s]",
+          other);
+      checkArgument(!"..".equals(other), "Cannot resolve parent as file: [%s]", other);
+      if (S3_URI.matcher(other).matches()) {
+        return fromUri(other);
+      }
+      return fromComponents(bucket, key + other);
+    }
+
+    throw new UnsupportedOperationException(
+        String.format("Unexpected StandardResolveOptions [%s]", resolveOptions));
+  }
+
+  @Override
+  public ResourceId getCurrentDirectory() {
+    if (isDirectory()) {
+      return this;
+    }
+    return fromComponents(getBucket(), key.substring(0, key.lastIndexOf('/') + 1));
+  }
+
+  @Override
+  public String getScheme() {
+    return SCHEME;
+  }
+
+  @Nullable
+  @Override
+  public String getFilename() {
+    if (!isDirectory()) {
+      return key.substring(key.lastIndexOf('/') + 1);
+    }
+    if ("/".equals(key)) {
+      return null;
+    }
+    String keyWithoutTrailingSlash = key.substring(0, key.length() - 1);
+    return keyWithoutTrailingSlash.substring(keyWithoutTrailingSlash.lastIndexOf('/') + 1);
+  }
+
+  @Override
+  public boolean isDirectory() {
+    return key.endsWith("/");
+  }
+
+  boolean isWildcard() {
+    return GLOB_PREFIX.matcher(getKey()).matches();
+  }
+
+  String getKeyNonWildcardPrefix() {
+    Matcher m = GLOB_PREFIX.matcher(getKey());
+    checkArgument(m.matches(), String.format("Glob expression: [%s] is not expandable.", getKey()));
+    return m.group("PREFIX");
+  }
+
+  @Override
+  public String toString() {
+    return String.format("%s://%s%s", SCHEME, bucket, key);
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (!(obj instanceof S3ResourceId)) {
+      return false;
+    }
+
+    return bucket.equals(((S3ResourceId) obj).bucket) && key.equals(((S3ResourceId) obj).key);
+  }
+
+  @Override
+  public int hashCode() {
+    return Objects.hash(bucket, key);
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannel.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannel.java
new file mode 100644
index 000000000000..17263d9c4156
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannel.java
@@ -0,0 +1,147 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+
+import com.amazonaws.AmazonClientException;
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadResult;
+import com.amazonaws.services.s3.model.ObjectMetadata;
+import com.amazonaws.services.s3.model.PartETag;
+import com.amazonaws.services.s3.model.UploadPartRequest;
+import com.amazonaws.services.s3.model.UploadPartResult;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.ClosedChannelException;
+import java.nio.channels.WritableByteChannel;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * A writable S3 object, as a {@link WritableByteChannel}.
+ */
+class S3WritableByteChannel implements WritableByteChannel {
+
+  private final AmazonS3 amazonS3;
+  private final S3ResourceId path;
+  private final String uploadId;
+  private final ByteBuffer uploadBuffer;
+  private final List<PartETag> eTags;
+
+  // AWS S3 parts are 1-indexed, not zero-indexed.
+  private int partNumber = 1;
+  private boolean open = true;
+
+  S3WritableByteChannel(AmazonS3 amazonS3, S3ResourceId path, String contentType,
+      String storageClass, int uploadBufferSizeBytes)
+      throws IOException {
+    this.amazonS3 = checkNotNull(amazonS3, "amazonS3");
+    this.path = checkNotNull(path, "path");
+    checkArgument(uploadBufferSizeBytes > 0, "uploadBufferSizeBytes");
+    this.uploadBuffer = ByteBuffer.allocate(uploadBufferSizeBytes);
+    eTags = new ArrayList<>();
+
+    ObjectMetadata objectMetadata = new ObjectMetadata();
+    objectMetadata.setContentType(contentType);
+    InitiateMultipartUploadRequest request =
+        new InitiateMultipartUploadRequest(path.getBucket(), path.getKey())
+            .withStorageClass(storageClass)
+            .withObjectMetadata(objectMetadata);
+    InitiateMultipartUploadResult result;
+    try {
+      result = amazonS3.initiateMultipartUpload(request);
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+    uploadId = result.getUploadId();
+  }
+
+  @Override
+  public int write(ByteBuffer sourceBuffer) throws IOException {
+    if (!isOpen()) {
+      throw new ClosedChannelException();
+    }
+
+    int totalBytesWritten = 0;
+    while (sourceBuffer.hasRemaining()) {
+      int bytesWritten = Math.min(sourceBuffer.remaining(), uploadBuffer.remaining());
+      totalBytesWritten += bytesWritten;
+
+      byte[] copyBuffer = new byte[bytesWritten];
+      sourceBuffer.get(copyBuffer);
+      uploadBuffer.put(copyBuffer);
+
+      if (!uploadBuffer.hasRemaining() || sourceBuffer.hasRemaining()) {
+        flush();
+      }
+    }
+
+    return totalBytesWritten;
+  }
+
+  private void flush() throws IOException {
+    uploadBuffer.flip();
+    ByteArrayInputStream inputStream = new ByteArrayInputStream(uploadBuffer.array());
+
+    UploadPartRequest request =
+        new UploadPartRequest()
+            .withBucketName(path.getBucket())
+            .withKey(path.getKey())
+            .withUploadId(uploadId)
+            .withPartNumber(partNumber++)
+            .withPartSize(uploadBuffer.remaining())
+            .withInputStream(inputStream);
+    UploadPartResult result;
+    try {
+      result = amazonS3.uploadPart(request);
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+    uploadBuffer.clear();
+    eTags.add(result.getPartETag());
+  }
+
+  @Override
+  public boolean isOpen() {
+    return open;
+  }
+
+  @Override
+  public void close() throws IOException {
+    open = false;
+    if (uploadBuffer.remaining() > 0) {
+      flush();
+    }
+    CompleteMultipartUploadRequest request =
+        new CompleteMultipartUploadRequest()
+            .withBucketName(path.getBucket())
+            .withKey(path.getKey())
+            .withUploadId(uploadId)
+            .withPartETags(eTags);
+    try {
+      amazonS3.completeMultipartUpload(request);
+    } catch (AmazonClientException e) {
+      throw new IOException(e);
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/package-info.java b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/package-info.java
new file mode 100644
index 000000000000..a3238202214b
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/main/java/org/apache/beam/sdk/io/aws/s3/package-info.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+/**
+ * Defines IO connectors for Amazon Web Services S3.
+ */
+package org.apache.beam.sdk.io.aws.s3;
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/options/AwsModuleTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/options/AwsModuleTest.java
new file mode 100644
index 000000000000..5047312f7d43
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/options/AwsModuleTest.java
@@ -0,0 +1,158 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.options;
+
+import static org.hamcrest.Matchers.hasItem;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertThat;
+
+import com.amazonaws.auth.AWSCredentialsProvider;
+import com.amazonaws.auth.AWSStaticCredentialsProvider;
+import com.amazonaws.auth.BasicAWSCredentials;
+import com.amazonaws.auth.ClasspathPropertiesFileCredentialsProvider;
+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;
+import com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper;
+import com.amazonaws.auth.EnvironmentVariableCredentialsProvider;
+import com.amazonaws.auth.PropertiesFileCredentialsProvider;
+import com.amazonaws.auth.SystemPropertiesCredentialsProvider;
+import com.amazonaws.auth.profile.ProfileCredentialsProvider;
+import com.fasterxml.jackson.databind.Module;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import java.lang.reflect.Field;
+import java.util.List;
+import org.apache.beam.sdk.util.common.ReflectHelpers;
+import org.hamcrest.Matchers;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+/**
+ * Tests {@link AwsModule}.
+ */
+@RunWith(JUnit4.class)
+public class AwsModuleTest {
+
+  private final ObjectMapper objectMapper = new ObjectMapper().registerModule(new AwsModule());
+
+  @Test
+  public void testObjectMapperIsAbleToFindModule() throws Exception {
+    List<Module> modules = ObjectMapper.findModules(ReflectHelpers.findClassLoader());
+    assertThat(modules, hasItem(Matchers.instanceOf(AwsModule.class)));
+  }
+
+  @Test
+  public void testAWSStaticCredentialsProviderSerializationDeserialization() throws Exception {
+
+    String awsKeyId = "key-id";
+    String awsSecretKey = "secret-key";
+
+    AWSStaticCredentialsProvider credentialsProvider =
+        new AWSStaticCredentialsProvider(new BasicAWSCredentials(awsKeyId, awsSecretKey));
+
+    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    AWSCredentialsProvider deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+    assertEquals(credentialsProvider.getCredentials().getAWSAccessKeyId(),
+        deserializedCredentialsProvider.getCredentials().getAWSAccessKeyId());
+    assertEquals(credentialsProvider.getCredentials().getAWSSecretKey(),
+        deserializedCredentialsProvider.getCredentials().getAWSSecretKey());
+  }
+
+  @Test
+  public void testPropertiesFileCredentialsProviderSerializationDeserialization() throws Exception {
+
+    String credentialsFilePath = "/path/to/file";
+
+    PropertiesFileCredentialsProvider credentialsProvider =
+        new PropertiesFileCredentialsProvider(credentialsFilePath);
+
+    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    AWSCredentialsProvider deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    Field field =
+        PropertiesFileCredentialsProvider.class.getDeclaredField("credentialsFilePath");
+    field.setAccessible(true);
+    String deserializedCredentialsFilePath = ((String) field.get(deserializedCredentialsProvider));
+    assertEquals(credentialsFilePath, deserializedCredentialsFilePath);
+  }
+
+  @Test
+  public void testClasspathPropertiesFileCredentialsProviderSerializationDeserialization()
+      throws Exception {
+
+    String credentialsFilePath = "/path/to/file";
+
+    ClasspathPropertiesFileCredentialsProvider credentialsProvider =
+        new ClasspathPropertiesFileCredentialsProvider(credentialsFilePath);
+
+    String serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    AWSCredentialsProvider deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    Field field =
+        ClasspathPropertiesFileCredentialsProvider.class.getDeclaredField("credentialsFilePath");
+    field.setAccessible(true);
+    String deserializedCredentialsFilePath = ((String) field.get(deserializedCredentialsProvider));
+    assertEquals(credentialsFilePath, deserializedCredentialsFilePath);
+  }
+
+  @Test
+  public void testSingletonAWSCredentialsProviderSerializationDeserialization() throws Exception {
+    AWSCredentialsProvider credentialsProvider;
+    String serializedCredentialsProvider;
+    AWSCredentialsProvider deserializedCredentialsProvider;
+
+    credentialsProvider = new DefaultAWSCredentialsProviderChain();
+    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    credentialsProvider = new EnvironmentVariableCredentialsProvider();
+    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    credentialsProvider = new SystemPropertiesCredentialsProvider();
+    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    credentialsProvider = new ProfileCredentialsProvider();
+    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+
+    credentialsProvider = new EC2ContainerCredentialsProviderWrapper();
+    serializedCredentialsProvider = objectMapper.writeValueAsString(credentialsProvider);
+    deserializedCredentialsProvider =
+        objectMapper.readValue(serializedCredentialsProvider, AWSCredentialsProvider.class);
+    assertEquals(credentialsProvider.getClass(), deserializedCredentialsProvider.getClass());
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/MatchResultMatcher.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/MatchResultMatcher.java
new file mode 100644
index 000000000000..04fdbc5f865b
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/MatchResultMatcher.java
@@ -0,0 +1,118 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkArgument;
+import static com.google.common.base.Preconditions.checkNotNull;
+
+import com.google.common.collect.ImmutableList;
+import java.io.IOException;
+import java.util.List;
+import org.apache.beam.sdk.io.fs.MatchResult;
+import org.apache.beam.sdk.io.fs.ResourceId;
+import org.hamcrest.BaseMatcher;
+import org.hamcrest.Description;
+import org.hamcrest.Matcher;
+
+/**
+ * Hamcrest {@link Matcher} to match {@link MatchResult}. Necessary because {@link
+ * MatchResult#metadata()} throws an exception under normal circumstances.
+ */
+public class MatchResultMatcher extends BaseMatcher<MatchResult> {
+
+  private final MatchResult.Status expectedStatus;
+  private final List<MatchResult.Metadata> expectedMetadata;
+  private final IOException expectedException;
+
+  private MatchResultMatcher(
+      MatchResult.Status expectedStatus,
+      List<MatchResult.Metadata> expectedMetadata,
+      IOException expectedException) {
+    this.expectedStatus = checkNotNull(expectedStatus);
+    checkArgument((expectedMetadata == null) ^ (expectedException == null));
+    this.expectedMetadata = expectedMetadata;
+    this.expectedException = expectedException;
+  }
+
+  static MatchResultMatcher create(List<MatchResult.Metadata> expectedMetadata) {
+    return new MatchResultMatcher(MatchResult.Status.OK, expectedMetadata, null);
+  }
+
+  static MatchResultMatcher create(MatchResult.Metadata expectedMetadata) {
+    return create(ImmutableList.of(expectedMetadata));
+  }
+
+  static MatchResultMatcher create(
+      long sizeBytes, ResourceId resourceId, boolean isReadSeekEfficient) {
+    return create(
+        MatchResult.Metadata.builder()
+            .setSizeBytes(sizeBytes)
+            .setResourceId(resourceId)
+            .setIsReadSeekEfficient(isReadSeekEfficient)
+            .build());
+  }
+
+  static MatchResultMatcher create(
+      MatchResult.Status expectedStatus, IOException expectedException) {
+    return new MatchResultMatcher(expectedStatus, null, expectedException);
+  }
+
+  static MatchResultMatcher create(MatchResult expected) {
+    MatchResult.Status expectedStatus = expected.status();
+    List<MatchResult.Metadata> expectedMetadata = null;
+    IOException expectedException = null;
+    try {
+      expectedMetadata = expected.metadata();
+    } catch (IOException e) {
+      expectedException = e;
+    }
+    return new MatchResultMatcher(expectedStatus, expectedMetadata, expectedException);
+  }
+
+  @Override
+  public boolean matches(Object actual) {
+    if (actual == null) {
+      return false;
+    }
+    if (!(actual instanceof MatchResult)) {
+      return false;
+    }
+    MatchResult actualResult = (MatchResult) actual;
+    if (!expectedStatus.equals(actualResult.status())) {
+      return false;
+    }
+
+    List<MatchResult.Metadata> actualMetadata;
+    try {
+      actualMetadata = actualResult.metadata();
+    } catch (IOException e) {
+      return expectedException != null && expectedException.toString().equals(e.toString());
+    }
+    return expectedMetadata != null && expectedMetadata.equals(actualMetadata);
+  }
+
+  @Override
+  public void describeTo(Description description) {
+    if (expectedMetadata != null) {
+      description.appendText(MatchResult.create(expectedStatus, expectedMetadata).toString());
+    } else {
+      description.appendText(MatchResult.create(expectedStatus, expectedException).toString());
+    }
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemTest.java
new file mode 100644
index 000000000000..931e9d0a7478
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3FileSystemTest.java
@@ -0,0 +1,499 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.s3;
+
+import static com.google.common.base.Preconditions.checkNotNull;
+import static org.hamcrest.Matchers.contains;
+import static org.hamcrest.Matchers.notNullValue;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertThat;
+import static org.mockito.Matchers.anyString;
+import static org.mockito.Matchers.argThat;
+import static org.mockito.Matchers.notNull;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.when;
+
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.AmazonS3Exception;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
+import com.amazonaws.services.s3.model.CopyPartRequest;
+import com.amazonaws.services.s3.model.CopyPartResult;
+import com.amazonaws.services.s3.model.DeleteObjectsRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadResult;
+import com.amazonaws.services.s3.model.ListObjectsV2Request;
+import com.amazonaws.services.s3.model.ListObjectsV2Result;
+import com.amazonaws.services.s3.model.ObjectMetadata;
+import com.amazonaws.services.s3.model.S3ObjectSummary;
+import com.google.common.collect.ImmutableList;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import org.apache.beam.sdk.io.aws.options.S3Options;
+import org.apache.beam.sdk.io.fs.MatchResult;
+import org.apache.beam.sdk.options.PipelineOptionsFactory;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+import org.mockito.ArgumentMatcher;
+import org.mockito.Mockito;
+
+/**
+ * Test case for {@link S3FileSystem}.
+ */
+@RunWith(JUnit4.class)
+public class S3FileSystemTest {
+
+  @Test
+  public void testGlobTranslation() {
+    assertEquals("foo", S3FileSystem.wildcardToRegexp("foo"));
+    assertEquals("fo[^/]*o", S3FileSystem.wildcardToRegexp("fo*o"));
+    assertEquals("f[^/]*o\\.[^/]", S3FileSystem.wildcardToRegexp("f*o.?"));
+    assertEquals("foo-[0-9][^/]*", S3FileSystem.wildcardToRegexp("foo-[0-9]*"));
+    assertEquals("foo-[0-9].*", S3FileSystem.wildcardToRegexp("foo-[0-9]**"));
+    assertEquals(".*foo", S3FileSystem.wildcardToRegexp("**/*foo"));
+    assertEquals(".*foo", S3FileSystem.wildcardToRegexp("**foo"));
+    assertEquals("foo/[^/]*", S3FileSystem.wildcardToRegexp("foo/*"));
+    assertEquals("foo[^/]*", S3FileSystem.wildcardToRegexp("foo*"));
+    assertEquals("foo/[^/]*/[^/]*/[^/]*", S3FileSystem.wildcardToRegexp("foo/*/*/*"));
+    assertEquals("foo/[^/]*/.*", S3FileSystem.wildcardToRegexp("foo/*/**"));
+    assertEquals("foo.*baz", S3FileSystem.wildcardToRegexp("foo**baz"));
+  }
+
+  private static S3Options s3Options() {
+    S3Options options = PipelineOptionsFactory.as(S3Options.class);
+    options.setAwsRegion("us-west-1");
+    return options;
+  }
+
+  @Test
+  public void testGetScheme() {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    assertEquals("s3", s3FileSystem.getScheme());
+  }
+
+  @Test
+  public void testCopyMultipleParts() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId sourcePath = S3ResourceId.fromUri("s3://bucket/from");
+    S3ResourceId destinationPath = S3ResourceId.fromUri("s3://bucket/to");
+
+    InitiateMultipartUploadResult initiateMultipartUploadResult =
+        new InitiateMultipartUploadResult();
+    initiateMultipartUploadResult.setUploadId("upload-id");
+    when(mockAmazonS3.initiateMultipartUpload(
+        argThat(notNullValue(InitiateMultipartUploadRequest.class))))
+        .thenReturn(initiateMultipartUploadResult);
+
+    ObjectMetadata sourceS3ObjectMetadata = new ObjectMetadata();
+    sourceS3ObjectMetadata
+        .setContentLength((long) (s3FileSystem.getS3UploadBufferSizeBytes() * 1.5));
+    sourceS3ObjectMetadata.setContentEncoding("read-seek-efficient");
+    when(mockAmazonS3.getObjectMetadata(sourcePath.getBucket(), sourcePath.getKey()))
+        .thenReturn(sourceS3ObjectMetadata);
+
+    CopyPartResult copyPartResult1 = new CopyPartResult();
+    copyPartResult1.setETag("etag-1");
+    CopyPartResult copyPartResult2 = new CopyPartResult();
+    copyPartResult1.setETag("etag-2");
+    when(mockAmazonS3.copyPart(argThat(notNullValue(CopyPartRequest.class))))
+        .thenReturn(copyPartResult1)
+        .thenReturn(copyPartResult2);
+
+    s3FileSystem.copy(sourcePath, destinationPath);
+
+    verify(mockAmazonS3, times(1))
+        .completeMultipartUpload(argThat(notNullValue(CompleteMultipartUploadRequest.class)));
+  }
+
+  @Test
+  public void deleteThousandsOfObjectsInMultipleBuckets() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    List<String> buckets = ImmutableList.of("bucket1", "bucket2");
+    List<String> keys = new ArrayList<>();
+    for (int i = 0; i < 2500; i++) {
+      keys.add(String.format("key-%d", i));
+    }
+    List<S3ResourceId> paths = new ArrayList<>();
+    for (String bucket : buckets) {
+      for (String key : keys) {
+        paths.add(S3ResourceId.fromComponents(bucket, key));
+      }
+    }
+
+    s3FileSystem.delete(paths);
+
+    // Should require 6 calls to delete 2500 objects in each of 2 buckets.
+    verify(mockAmazonS3, times(6)).deleteObjects(argThat(notNullValue(DeleteObjectsRequest.class)));
+  }
+
+
+  @Test
+  public void matchNonGlob() {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");
+    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();
+    s3ObjectMetadata.setContentLength(100);
+    s3ObjectMetadata.setContentEncoding("read-seek-efficient");
+    when(mockAmazonS3.getObjectMetadata(path.getBucket(), path.getKey()))
+        .thenReturn(s3ObjectMetadata);
+
+    MatchResult result = s3FileSystem.matchNonGlobPath(path);
+    assertThat(
+        result,
+        MatchResultMatcher.create(
+            ImmutableList.of(
+                MatchResult.Metadata.builder()
+                    .setSizeBytes(100)
+                    .setResourceId(path)
+                    .setIsReadSeekEfficient(true)
+                    .build())));
+  }
+
+  @Test
+  public void matchNonGlobNotReadSeekEfficient() {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");
+    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();
+    s3ObjectMetadata.setContentLength(100);
+    s3ObjectMetadata.setContentEncoding("gzip");
+    when(mockAmazonS3.getObjectMetadata(path.getBucket(), path.getKey()))
+        .thenReturn(s3ObjectMetadata);
+
+    MatchResult result = s3FileSystem.matchNonGlobPath(path);
+    assertThat(
+        result,
+        MatchResultMatcher.create(
+            ImmutableList.of(
+                MatchResult.Metadata.builder()
+                    .setSizeBytes(100)
+                    .setResourceId(path)
+                    .setIsReadSeekEfficient(false)
+                    .build())));
+  }
+
+  @Test
+  public void matchNonGlobNullContentEncoding() {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");
+    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();
+    s3ObjectMetadata.setContentLength(100);
+    s3ObjectMetadata.setContentEncoding(null);
+    when(mockAmazonS3.getObjectMetadata(path.getBucket(), path.getKey()))
+        .thenReturn(s3ObjectMetadata);
+
+    MatchResult result = s3FileSystem.matchNonGlobPath(path);
+    assertThat(
+        result,
+        MatchResultMatcher.create(
+            ImmutableList.of(
+                MatchResult.Metadata.builder()
+                    .setSizeBytes(100)
+                    .setResourceId(path)
+                    .setIsReadSeekEfficient(true)
+                    .build())));
+  }
+
+  @Test
+  public void matchNonGlobNotFound() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/nonexistentfile");
+    AmazonS3Exception exception = new AmazonS3Exception("mock exception");
+    exception.setStatusCode(404);
+    when(mockAmazonS3.getObjectMetadata(path.getBucket(), path.getKey())).thenThrow(exception);
+
+    MatchResult result = s3FileSystem.matchNonGlobPath(path);
+    assertThat(
+        result,
+        MatchResultMatcher.create(MatchResult.Status.NOT_FOUND, new FileNotFoundException()));
+  }
+
+  @Test
+  public void matchNonGlobForbidden() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    AmazonS3Exception exception = new AmazonS3Exception("mock exception");
+    exception.setStatusCode(403);
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/testdirectory/keyname");
+    when(mockAmazonS3.getObjectMetadata(path.getBucket(), path.getKey())).thenThrow(exception);
+
+    assertThat(
+        s3FileSystem.matchNonGlobPath(path),
+        MatchResultMatcher.create(MatchResult.Status.ERROR, new IOException(exception)));
+  }
+
+  static class ListObjectsV2RequestArgumentMatches extends ArgumentMatcher<ListObjectsV2Request> {
+
+    private final ListObjectsV2Request expected;
+
+    ListObjectsV2RequestArgumentMatches(ListObjectsV2Request expected) {
+      this.expected = checkNotNull(expected);
+    }
+
+    @Override
+    public boolean matches(Object argument) {
+      if (argument != null && argument instanceof ListObjectsV2Request) {
+        ListObjectsV2Request actual = (ListObjectsV2Request) argument;
+        return expected.getBucketName().equals(actual.getBucketName())
+            && expected.getPrefix().equals(actual.getPrefix())
+            && (expected.getContinuationToken() == null
+            ? actual.getContinuationToken() == null
+            : expected.getContinuationToken().equals(actual.getContinuationToken()));
+      }
+      return false;
+    }
+  }
+
+  @Test
+  public void matchGlob() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/foo/bar*baz");
+
+    ListObjectsV2Request firstRequest =
+        new ListObjectsV2Request()
+            .withBucketName(path.getBucket())
+            .withPrefix(path.getKeyNonWildcardPrefix())
+            .withContinuationToken(null);
+
+    // Expected to be returned; prefix and wildcard/regex match
+    S3ObjectSummary firstMatch = new S3ObjectSummary();
+    firstMatch.setBucketName(path.getBucket());
+    firstMatch.setKey("foo/bar0baz");
+    firstMatch.setSize(100);
+
+    // Expected to not be returned; prefix matches, but substring after wildcard does not
+    S3ObjectSummary secondMatch = new S3ObjectSummary();
+    secondMatch.setBucketName(path.getBucket());
+    secondMatch.setKey("foo/bar1qux");
+    secondMatch.setSize(200);
+
+    // Expected first request returns continuation token
+    ListObjectsV2Result firstResult = new ListObjectsV2Result();
+    firstResult.setNextContinuationToken("token");
+    firstResult.getObjectSummaries().add(firstMatch);
+    firstResult.getObjectSummaries().add(secondMatch);
+    when(mockAmazonS3.listObjectsV2(argThat(new ListObjectsV2RequestArgumentMatches(firstRequest))))
+        .thenReturn(firstResult);
+
+    // Expect second request with continuation token
+    ListObjectsV2Request secondRequest =
+        new ListObjectsV2Request()
+            .withBucketName(path.getBucket())
+            .withPrefix(path.getKeyNonWildcardPrefix())
+            .withContinuationToken("token");
+
+    // Expected to be returned; prefix and wildcard/regex match
+    S3ObjectSummary thirdMatch = new S3ObjectSummary();
+    thirdMatch.setBucketName(path.getBucket());
+    thirdMatch.setKey("foo/bar2baz");
+    thirdMatch.setSize(300);
+
+    // Expected second request returns third prefix match and no continuation token
+    ListObjectsV2Result secondResult = new ListObjectsV2Result();
+    secondResult.setNextContinuationToken(null);
+    secondResult.getObjectSummaries().add(thirdMatch);
+    when(mockAmazonS3.listObjectsV2(
+        argThat(new ListObjectsV2RequestArgumentMatches(secondRequest))))
+        .thenReturn(secondResult);
+
+    // Expect object metadata queries for content encoding
+    ObjectMetadata metadata = new ObjectMetadata();
+    metadata.setContentEncoding("");
+    when(mockAmazonS3.getObjectMetadata(anyString(), anyString())).thenReturn(metadata);
+
+    assertThat(
+        s3FileSystem.matchGlobPaths(ImmutableList.of(path)).get(0),
+        MatchResultMatcher.create(
+            ImmutableList.of(
+                MatchResult.Metadata.builder()
+                    .setIsReadSeekEfficient(true)
+                    .setResourceId(
+                        S3ResourceId
+                            .fromComponents(firstMatch.getBucketName(), firstMatch.getKey()))
+                    .setSizeBytes(firstMatch.getSize())
+                    .build(),
+                MatchResult.Metadata.builder()
+                    .setIsReadSeekEfficient(true)
+                    .setResourceId(
+                        S3ResourceId
+                            .fromComponents(thirdMatch.getBucketName(), thirdMatch.getKey()))
+                    .setSizeBytes(thirdMatch.getSize())
+                    .build())));
+  }
+
+  @Test
+  public void matchGlobWithSlashes() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://testbucket/foo/bar\\baz*");
+
+    ListObjectsV2Request request =
+        new ListObjectsV2Request()
+            .withBucketName(path.getBucket())
+            .withPrefix(path.getKeyNonWildcardPrefix())
+            .withContinuationToken(null);
+
+    // Expected to be returned; prefix and wildcard/regex match
+    S3ObjectSummary firstMatch = new S3ObjectSummary();
+    firstMatch.setBucketName(path.getBucket());
+    firstMatch.setKey("foo/bar\\baz0");
+    firstMatch.setSize(100);
+
+    // Expected to not be returned; prefix matches, but substring after wildcard does not
+    S3ObjectSummary secondMatch = new S3ObjectSummary();
+    secondMatch.setBucketName(path.getBucket());
+    secondMatch.setKey("foo/bar/baz1");
+    secondMatch.setSize(200);
+
+    // Expected first request returns continuation token
+    ListObjectsV2Result result = new ListObjectsV2Result();
+    result.getObjectSummaries().add(firstMatch);
+    result.getObjectSummaries().add(secondMatch);
+    when(mockAmazonS3.listObjectsV2(argThat(new ListObjectsV2RequestArgumentMatches(request))))
+        .thenReturn(result);
+
+    // Expect object metadata queries for content encoding
+    ObjectMetadata metadata = new ObjectMetadata();
+    metadata.setContentEncoding("");
+    when(mockAmazonS3.getObjectMetadata(anyString(), anyString())).thenReturn(metadata);
+
+    assertThat(
+        s3FileSystem.matchGlobPaths(ImmutableList.of(path)).get(0),
+        MatchResultMatcher.create(
+            ImmutableList.of(
+                MatchResult.Metadata.builder()
+                    .setIsReadSeekEfficient(true)
+                    .setResourceId(
+                        S3ResourceId
+                            .fromComponents(firstMatch.getBucketName(), firstMatch.getKey()))
+                    .setSizeBytes(firstMatch.getSize())
+                    .build())));
+  }
+
+  @Test
+  public void matchVariousInvokeThreadPool() throws IOException {
+    S3Options pipelineOptions = s3Options();
+    S3FileSystem s3FileSystem = new S3FileSystem(pipelineOptions);
+
+    AmazonS3 mockAmazonS3 = Mockito.mock(AmazonS3.class);
+    s3FileSystem.setAmazonS3Client(mockAmazonS3);
+
+    AmazonS3Exception notFoundException = new AmazonS3Exception("mock exception");
+    notFoundException.setStatusCode(404);
+    S3ResourceId pathNotExist = S3ResourceId
+        .fromUri("s3://testbucket/testdirectory/nonexistentfile");
+    when(mockAmazonS3.getObjectMetadata(pathNotExist.getBucket(), pathNotExist.getKey()))
+        .thenThrow(notFoundException);
+
+    AmazonS3Exception forbiddenException = new AmazonS3Exception("mock exception");
+    forbiddenException.setStatusCode(403);
+    S3ResourceId pathForbidden = S3ResourceId
+        .fromUri("s3://testbucket/testdirectory/forbiddenfile");
+    when(mockAmazonS3.getObjectMetadata(pathForbidden.getBucket(), pathForbidden.getKey()))
+        .thenThrow(forbiddenException);
+
+    S3ResourceId pathExist = S3ResourceId.fromUri("s3://testbucket/testdirectory/filethatexists");
+    ObjectMetadata s3ObjectMetadata = new ObjectMetadata();
+    s3ObjectMetadata.setContentLength(100);
+    s3ObjectMetadata.setContentEncoding("not-gzip");
+    when(mockAmazonS3.getObjectMetadata(pathExist.getBucket(), pathExist.getKey()))
+        .thenReturn(s3ObjectMetadata);
+
+    S3ResourceId pathGlob = S3ResourceId.fromUri("s3://testbucket/path/part*");
+
+    S3ObjectSummary foundListObject = new S3ObjectSummary();
+    foundListObject.setBucketName(pathGlob.getBucket());
+    foundListObject.setKey("path/part-0");
+    foundListObject.setSize(200);
+
+    ListObjectsV2Result listObjectsResult = new ListObjectsV2Result();
+    listObjectsResult.setNextContinuationToken(null);
+    listObjectsResult.getObjectSummaries().add(foundListObject);
+    when(mockAmazonS3.listObjectsV2(notNull(ListObjectsV2Request.class)))
+        .thenReturn(listObjectsResult);
+
+    ObjectMetadata metadata = new ObjectMetadata();
+    metadata.setContentEncoding("");
+    when(mockAmazonS3.getObjectMetadata(pathGlob.getBucket(), "path/part-0"))
+        .thenReturn(metadata);
+
+    assertThat(
+        s3FileSystem.match(ImmutableList
+            .of(pathNotExist.toString(),
+                pathForbidden.toString(),
+                pathExist.toString(),
+                pathGlob.toString())),
+        contains(
+            MatchResultMatcher.create(MatchResult.Status.NOT_FOUND, new FileNotFoundException()),
+            MatchResultMatcher.create(
+                MatchResult.Status.ERROR, new IOException(forbiddenException)),
+            MatchResultMatcher.create(100, pathExist, true),
+            MatchResultMatcher.create(
+                200,
+                S3ResourceId.fromComponents(pathGlob.getBucket(), foundListObject.getKey()),
+                true)));
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3ResourceIdTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3ResourceIdTest.java
new file mode 100644
index 000000000000..3884f0fdcddd
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3ResourceIdTest.java
@@ -0,0 +1,299 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.beam.sdk.io.aws.s3;
+
+import static org.apache.beam.sdk.io.fs.ResolveOptions.StandardResolveOptions.RESOLVE_DIRECTORY;
+import static org.apache.beam.sdk.io.fs.ResolveOptions.StandardResolveOptions.RESOLVE_FILE;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNotEquals;
+import static org.junit.Assert.assertTrue;
+
+import java.util.Arrays;
+import java.util.List;
+import org.apache.beam.sdk.io.FileSystems;
+import org.apache.beam.sdk.io.aws.options.S3Options;
+import org.apache.beam.sdk.io.fs.ResolveOptions.StandardResolveOptions;
+import org.apache.beam.sdk.io.fs.ResourceId;
+import org.apache.beam.sdk.io.fs.ResourceIdTester;
+import org.apache.beam.sdk.options.PipelineOptionsFactory;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+/**
+ * Tests {@link S3ResourceId}.
+ */
+@RunWith(JUnit4.class)
+public class S3ResourceIdTest {
+
+  @Rule
+  public ExpectedException thrown = ExpectedException.none();
+
+  static final class TestCase {
+
+    final String baseUri;
+    final String relativePath;
+    final StandardResolveOptions resolveOptions;
+    final String expectedResult;
+
+    TestCase(String baseUri, String relativePath, StandardResolveOptions resolveOptions,
+        String expectedResult) {
+      this.baseUri = baseUri;
+      this.relativePath = relativePath;
+      this.resolveOptions = resolveOptions;
+      this.expectedResult = expectedResult;
+    }
+  }
+
+  // Each test case is an expected URL, then the components used to build it.
+  // Empty components result in a double slash.
+  static final List<TestCase> PATH_TEST_CASES =
+      Arrays.asList(
+          new TestCase("s3://bucket/", "", RESOLVE_DIRECTORY,
+              "s3://bucket/"),
+          new TestCase("s3://bucket", "", RESOLVE_DIRECTORY, "s3://bucket/"),
+          new TestCase("s3://bucket", "path/to/dir", RESOLVE_DIRECTORY, "s3://bucket/path/to/dir/"),
+          new TestCase("s3://bucket", "path/to/object", RESOLVE_FILE, "s3://bucket/path/to/object"),
+          new TestCase("s3://bucket/path/to/dir/", "..", RESOLVE_DIRECTORY, "s3://bucket/path/to/")
+      );
+
+  @Test
+  public void testResolve() throws Exception {
+    for (TestCase testCase : PATH_TEST_CASES) {
+      ResourceId resourceId = S3ResourceId.fromUri(testCase.baseUri);
+      ResourceId resolved = resourceId.resolve(testCase.relativePath, testCase.resolveOptions);
+      assertEquals(testCase.expectedResult, resolved.toString());
+    }
+
+    // Tests for common gcs paths.
+    assertEquals(
+        S3ResourceId.fromUri("s3://bucket/tmp/aa"),
+        S3ResourceId.fromUri("s3://bucket/tmp/")
+            .resolve("aa", StandardResolveOptions.RESOLVE_FILE));
+    assertEquals(
+        S3ResourceId.fromUri("s3://bucket/tmp/aa/bb/cc/"),
+        S3ResourceId.fromUri("s3://bucket/tmp/")
+            .resolve("aa", StandardResolveOptions.RESOLVE_DIRECTORY)
+            .resolve("bb", StandardResolveOptions.RESOLVE_DIRECTORY)
+            .resolve("cc", StandardResolveOptions.RESOLVE_DIRECTORY));
+
+    // Tests absolute path.
+    assertEquals(
+        S3ResourceId.fromUri("s3://bucket/tmp/aa"),
+        S3ResourceId.fromUri("s3://bucket/tmp/bb/")
+            .resolve("s3://bucket/tmp/aa", StandardResolveOptions.RESOLVE_FILE));
+
+    // Tests bucket with no ending '/'.
+    assertEquals(
+        S3ResourceId.fromUri("s3://my-bucket/tmp"),
+        S3ResourceId.fromUri("s3://my-bucket")
+            .resolve("tmp", StandardResolveOptions.RESOLVE_FILE));
+
+    // Tests path with unicode
+    assertEquals(
+        S3ResourceId.fromUri("s3://bucket/输出 目录/输出 文件01.txt"),
+        S3ResourceId.fromUri("s3://bucket/输出 目录/")
+            .resolve("输出 文件01.txt", StandardResolveOptions.RESOLVE_FILE));
+  }
+
+  @Test
+  public void testResolveInvalidInputs() throws Exception {
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("Cannot resolve a file with a directory path: [tmp/]");
+    S3ResourceId.fromUri("s3://my_bucket/").resolve("tmp/", StandardResolveOptions.RESOLVE_FILE);
+  }
+
+  @Test
+  public void testResolveInvalidNotDirectory() throws Exception {
+    ResourceId tmpDir = S3ResourceId.fromUri("s3://my_bucket/")
+        .resolve("tmp dir", StandardResolveOptions.RESOLVE_FILE);
+
+    thrown.expect(IllegalStateException.class);
+    thrown.expectMessage(
+        "Expected this resource to be a directory, but was [s3://my_bucket/tmp dir]");
+    tmpDir.resolve("aa", StandardResolveOptions.RESOLVE_FILE);
+  }
+
+  @Test
+  public void testS3ResolveWithFileBase() {
+    ResourceId resourceId = S3ResourceId.fromUri("s3://bucket/path/to/file");
+    thrown.expect(IllegalStateException.class);
+    resourceId.resolve("child-path", RESOLVE_DIRECTORY); // resource is not a directory
+  }
+
+  @Test
+  public void testResolveParentToFile() {
+    ResourceId resourceId = S3ResourceId.fromUri("s3://bucket/path/to/dir/");
+    thrown.expect(IllegalArgumentException.class);
+    resourceId.resolve("..", RESOLVE_FILE); // '..' only resolves as dir, not as file
+  }
+
+  @Test
+  public void testGetCurrentDirectory() throws Exception {
+    // Tests gcs paths.
+    assertEquals(
+        S3ResourceId.fromUri("s3://my_bucket/tmp dir/"),
+        S3ResourceId.fromUri("s3://my_bucket/tmp dir/").getCurrentDirectory());
+
+    // Tests path with unicode.
+    assertEquals(
+        S3ResourceId.fromUri("s3://my_bucket/输出 目录/"),
+        S3ResourceId.fromUri("s3://my_bucket/输出 目录/文件01.txt").getCurrentDirectory());
+
+    // Tests bucket with no ending '/'.
+    assertEquals(
+        S3ResourceId.fromUri("s3://my_bucket/"),
+        S3ResourceId.fromUri("s3://my_bucket").getCurrentDirectory());
+    assertEquals(
+        S3ResourceId.fromUri("s3://my_bucket/"),
+        S3ResourceId.fromUri("s3://my_bucket/not-directory").getCurrentDirectory());
+  }
+
+  @Test
+  public void testIsDirectory() throws Exception {
+    assertTrue(S3ResourceId.fromUri("s3://my_bucket/tmp dir/").isDirectory());
+    assertTrue(S3ResourceId.fromUri("s3://my_bucket/").isDirectory());
+    assertTrue(S3ResourceId.fromUri("s3://my_bucket").isDirectory());
+    assertFalse(S3ResourceId.fromUri("s3://my_bucket/file").isDirectory());
+  }
+
+  @Test
+  public void testInvalidPathNoBucket() throws Exception {
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("Invalid S3 URI: [s3://]");
+    S3ResourceId.fromUri("s3://");
+  }
+
+  @Test
+  public void testInvalidPathNoBucketAndSlash() throws Exception {
+    thrown.expect(IllegalArgumentException.class);
+    thrown.expectMessage("Invalid S3 URI: [s3:///]");
+    S3ResourceId.fromUri("s3:///");
+  }
+
+  @Test
+  public void testGetScheme() throws Exception {
+    // Tests gcs paths.
+    assertEquals("s3", S3ResourceId.fromUri("s3://my_bucket/tmp dir/").getScheme());
+
+    // Tests bucket with no ending '/'.
+    assertEquals("s3", S3ResourceId.fromUri("s3://my_bucket").getScheme());
+  }
+
+  @Test
+  public void testGetFilename() throws Exception {
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/").getFilename(), null);
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/abc").getFilename(), "abc");
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/abc/").getFilename(), "abc");
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/abc/def").getFilename(), "def");
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/abc/def/").getFilename(), "def");
+    assertEquals(S3ResourceId.fromUri("s3://my_bucket/abc/xyz.txt").getFilename(), "xyz.txt");
+  }
+
+  @Test
+  public void testParentRelationship() throws Exception {
+    S3ResourceId path = S3ResourceId.fromUri("s3://bucket/dir/subdir/object");
+    assertEquals("bucket", path.getBucket());
+    assertEquals("dir/subdir/object", path.getKey());
+
+    // s3://bucket/dir/
+    path = S3ResourceId.fromUri("s3://bucket/dir/subdir/");
+    S3ResourceId parent = (S3ResourceId) path.resolve("..", RESOLVE_DIRECTORY);
+    assertEquals("bucket", parent.getBucket());
+    assertEquals("dir/", parent.getKey());
+    assertNotEquals(path, parent);
+    assertTrue(path.getKey().startsWith(parent.getKey()));
+    assertFalse(parent.getKey().startsWith(path.getKey()));
+
+    // s3://bucket/
+    S3ResourceId grandParent = ((S3ResourceId) parent.resolve("..", RESOLVE_DIRECTORY));
+    assertEquals("bucket", grandParent.getBucket());
+    assertEquals("", grandParent.getKey());
+  }
+
+  @Test
+  public void testBucketParsing() throws Exception {
+    S3ResourceId path = S3ResourceId.fromUri("s3://bucket");
+    S3ResourceId path2 = S3ResourceId.fromUri("s3://bucket/");
+
+    assertEquals(path, path2);
+    assertEquals(path.toString(), path2.toString());
+  }
+
+  @Test
+  public void testS3ResourceIdToString() throws Exception {
+    String filename = "s3://some-bucket/some/file.txt";
+    S3ResourceId path = S3ResourceId.fromUri(filename);
+    assertEquals(filename, path.toString());
+
+    filename = "s3://some-bucket/some/";
+    path = S3ResourceId.fromUri(filename);
+    assertEquals(filename, path.toString());
+
+    filename = "s3://some-bucket/";
+    path = S3ResourceId.fromUri(filename);
+    assertEquals(filename, path.toString());
+  }
+
+  @Test
+  public void testEquals() {
+    S3ResourceId a = S3ResourceId.fromComponents("bucket", "a/b/c");
+    S3ResourceId b = S3ResourceId.fromComponents("bucket", "a/b/c");
+    assertEquals(a, a);
+    assertEquals(a, b);
+
+    b = S3ResourceId.fromComponents(a.getBucket(), "a/b/c/");
+    assertNotEquals(a, b);
+    assertNotEquals(b, a);
+
+    b = S3ResourceId.fromComponents(a.getBucket(), "x/y/z");
+    assertNotEquals(a, b);
+    assertNotEquals(b, a);
+
+    b = S3ResourceId.fromComponents("other-bucket", a.getKey());
+    assertNotEquals(a, b);
+    assertNotEquals(b, a);
+  }
+
+  public void testInvalidS3ResourceId() {
+    thrown.expect(IllegalArgumentException.class);
+    S3ResourceId.fromUri("file://invalid/s3/path");
+  }
+
+  public void testInvalidBucket() {
+    thrown.expect(IllegalArgumentException.class);
+    S3ResourceId.fromComponents("invalid/", "");
+  }
+
+  public void testInvalidBucketWithUnderscore() {
+    thrown.expect(IllegalArgumentException.class);
+    S3ResourceId.fromComponents("invalid_bucket", "");
+  }
+
+  @Test
+  public void testResourceIdTester() throws Exception {
+    S3Options options = PipelineOptionsFactory.create().as(S3Options.class);
+    options.setAwsRegion("us-west-1");
+    FileSystems.setDefaultPipelineOptions(options);
+    ResourceIdTester.runResourceIdBattery(S3ResourceId.fromUri("s3://bucket/foo/"));
+  }
+}
diff --git a/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannelTest.java b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannelTest.java
new file mode 100644
index 000000000000..cfa4672b0da3
--- /dev/null
+++ b/sdks/java/io/amazon-web-services/src/test/java/org/apache/beam/sdk/io/aws/s3/S3WritableByteChannelTest.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.beam.sdk.io.aws.s3;
+
+import static org.hamcrest.Matchers.notNullValue;
+import static org.junit.Assert.assertEquals;
+import static org.mockito.Matchers.argThat;
+import static org.mockito.Matchers.notNull;
+import static org.mockito.Mockito.RETURNS_SMART_NULLS;
+import static org.mockito.Mockito.mock;
+import static org.mockito.Mockito.times;
+import static org.mockito.Mockito.verify;
+import static org.mockito.Mockito.verifyNoMoreInteractions;
+import static org.mockito.Mockito.when;
+import static org.mockito.Mockito.withSettings;
+
+import com.amazonaws.services.s3.AmazonS3;
+import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
+import com.amazonaws.services.s3.model.InitiateMultipartUploadResult;
+import com.amazonaws.services.s3.model.UploadPartRequest;
+import com.amazonaws.services.s3.model.UploadPartResult;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+/**
+ * Tests {@link S3WritableByteChannel}.
+ */
+@RunWith(JUnit4.class)
+public class S3WritableByteChannelTest {
+
+  @Test
+  public void write() throws IOException {
+    AmazonS3 mockAmazonS3 = mock(AmazonS3.class, withSettings().defaultAnswer(RETURNS_SMART_NULLS));
+
+    InitiateMultipartUploadResult initiateMultipartUploadResult =
+        new InitiateMultipartUploadResult();
+    initiateMultipartUploadResult.setUploadId("upload-id");
+    when(mockAmazonS3.initiateMultipartUpload(
+        argThat(notNullValue(InitiateMultipartUploadRequest.class))))
+        .thenReturn(initiateMultipartUploadResult);
+    UploadPartResult result = new UploadPartResult();
+    result.setETag("etag");
+    when(mockAmazonS3.uploadPart(argThat(notNullValue(UploadPartRequest.class))))
+        .thenReturn(result);
+
+    S3ResourceId path = S3ResourceId.fromUri("s3://bucket/dir/file");
+    int uploadBufferSize = 10;
+
+    S3WritableByteChannel channel =
+        new S3WritableByteChannel(mockAmazonS3, path, "text/plain", "STANDARD", uploadBufferSize);
+    int contentSize = 65;
+    ByteBuffer uploadContent = ByteBuffer.allocate((int) (contentSize * 2.5));
+    for (byte i = 0; i < contentSize; i++) {
+      uploadContent.put(i);
+    }
+    uploadContent.flip();
+
+    int uploadedSize = channel.write(uploadContent);
+    assertEquals(contentSize, uploadedSize);
+
+    channel.close();
+
+    verify(mockAmazonS3, times(1))
+        .initiateMultipartUpload(notNull(InitiateMultipartUploadRequest.class));
+    int partQuantity = (int) Math.ceil((double) contentSize / uploadBufferSize);
+    verify(mockAmazonS3, times(partQuantity)).uploadPart(notNull(UploadPartRequest.class));
+    verify(mockAmazonS3, times(1))
+        .completeMultipartUpload(notNull(CompleteMultipartUploadRequest.class));
+    verifyNoMoreInteractions(mockAmazonS3);
+  }
+}
diff --git a/sdks/java/io/pom.xml b/sdks/java/io/pom.xml
index c29fa0f70d55..b321b7a0a831 100644
--- a/sdks/java/io/pom.xml
+++ b/sdks/java/io/pom.xml
@@ -42,6 +42,7 @@
   </properties>
 
   <modules>
+    <module>amazon-web-services</module>
     <module>amqp</module>
     <module>cassandra</module>
     <module>common</module>
