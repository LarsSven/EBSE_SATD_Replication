diff --git a/sdks/python/apache_beam/coders/coder_impl.py b/sdks/python/apache_beam/coders/coder_impl.py
index fb76875feb17..8f78b9514850 100644
--- a/sdks/python/apache_beam/coders/coder_impl.py
+++ b/sdks/python/apache_beam/coders/coder_impl.py
@@ -909,7 +909,8 @@ def encode_to_stream(self, value, out, nested):
           buffer = create_OutputStream()
           if (self._write_state is not None
               and out.size() - start_size > self._write_state_threshold):
-            tail = (value_iter[index + 1:] if isinstance(value, (list, tuple))
+            tail = (value_iter[index + 1:]
+                    if isinstance(value_iter, (list, tuple))
                     else value_iter)
             state_token = self._write_state(tail, self._elem_coder)
             out.write_var_int64(-1)
diff --git a/sdks/python/apache_beam/coders/row_coder.py b/sdks/python/apache_beam/coders/row_coder.py
index 73886c1b1599..bc5fd696a928 100644
--- a/sdks/python/apache_beam/coders/row_coder.py
+++ b/sdks/python/apache_beam/coders/row_coder.py
@@ -70,7 +70,7 @@ def to_type_hint(self):
   def as_cloud_object(self, coders_context=None):
     raise NotImplementedError("as_cloud_object not supported for RowCoder")
 
-  __hash__ = None
+  __hash__ = None  # type: ignore[assignment]
 
   def __eq__(self, other):
     return type(self) == type(other) and self.schema == other.schema
diff --git a/sdks/python/apache_beam/examples/complete/game/game_stats.py b/sdks/python/apache_beam/examples/complete/game/game_stats.py
index 13be705c04cd..f0ab8fc517dd 100644
--- a/sdks/python/apache_beam/examples/complete/game/game_stats.py
+++ b/sdks/python/apache_beam/examples/complete/game/game_stats.py
@@ -186,11 +186,6 @@ def get_schema(self):
     return ', '.join(
         '%s:%s' % (col, self.schema[col]) for col in self.schema)
 
-  def get_schema(self):
-    """Build the output table schema."""
-    return ', '.join(
-        '%s:%s' % (col, self.schema[col]) for col in self.schema)
-
   def expand(self, pcoll):
     return (
         pcoll
diff --git a/sdks/python/apache_beam/io/fileio_test.py b/sdks/python/apache_beam/io/fileio_test.py
index 946fac0ec12b..f8803d15156f 100644
--- a/sdks/python/apache_beam/io/fileio_test.py
+++ b/sdks/python/apache_beam/io/fileio_test.py
@@ -95,7 +95,7 @@ def test_match_all_two_directories(self):
 
       assert_that(files_pc, equal_to(files))
 
-  def test_match_files_one_directory_failure(self):
+  def test_match_files_one_directory_failure1(self):
     directories = [
         '%s%s' % (self._new_tempdir(), os.sep),
         '%s%s' % (self._new_tempdir(), os.sep)]
@@ -114,7 +114,7 @@ def test_match_files_one_directory_failure(self):
 
         assert_that(files_pc, equal_to(files))
 
-  def test_match_files_one_directory_failure(self):
+  def test_match_files_one_directory_failure2(self):
     directories = [
         '%s%s' % (self._new_tempdir(), os.sep),
         '%s%s' % (self._new_tempdir(), os.sep)]
diff --git a/sdks/python/apache_beam/io/filesystems_test.py b/sdks/python/apache_beam/io/filesystems_test.py
index d2133b0d250c..298644ffe472 100644
--- a/sdks/python/apache_beam/io/filesystems_test.py
+++ b/sdks/python/apache_beam/io/filesystems_test.py
@@ -26,6 +26,7 @@
 import logging
 import os
 import shutil
+import sys
 import tempfile
 import unittest
 
@@ -49,6 +50,12 @@ def _join(first_path, *paths):
 
 class FileSystemsTest(unittest.TestCase):
 
+  @classmethod
+  def setUpClass(cls):
+    # Method has been renamed in Python 3
+    if sys.version_info[0] < 3:
+      cls.assertCountEqual = cls.assertItemsEqual
+
   def setUp(self):
     self.tmpdir = tempfile.mkdtemp()
 
@@ -132,7 +139,7 @@ def test_match_file_exception(self):
       FileSystems.match([None])
     self.assertEqual(list(error.exception.exception_details), [None])
 
-  def test_match_directory(self):
+  def test_match_directory_with_files(self):
     path1 = os.path.join(self.tmpdir, 'f1')
     path2 = os.path.join(self.tmpdir, 'f2')
     open(path1, 'a').close()
@@ -142,7 +149,7 @@ def test_match_directory(self):
     path = os.path.join(self.tmpdir, '*')
     result = FileSystems.match([path])[0]
     files = [f.path for f in result.metadata_list]
-    self.assertEqual(files, [path1, path2])
+    self.assertCountEqual(files, [path1, path2])
 
   def test_match_directory(self):
     result = FileSystems.match([self.tmpdir])[0]
diff --git a/sdks/python/apache_beam/io/iobase.py b/sdks/python/apache_beam/io/iobase.py
index affb6c11dbca..fdb6c00f1075 100644
--- a/sdks/python/apache_beam/io/iobase.py
+++ b/sdks/python/apache_beam/io/iobase.py
@@ -1246,6 +1246,7 @@ class ThreadsafeRestrictionTracker(object):
   """
 
   def __init__(self, restriction_tracker):
+    # type: (RestrictionTracker) -> None
     if not isinstance(restriction_tracker, RestrictionTracker):
       raise ValueError(
           'Initialize ThreadsafeRestrictionTracker requires'
@@ -1379,6 +1380,7 @@ def __repr__(self):
 
   @property
   def completed_work(self):
+    # type: () -> float
     if self._completed:
       return self._completed
     elif self._remaining and self._fraction:
@@ -1386,6 +1388,7 @@ def completed_work(self):
 
   @property
   def remaining_work(self):
+    # type: () -> float
     if self._remaining:
       return self._remaining
     elif self._completed:
@@ -1393,10 +1396,12 @@ def remaining_work(self):
 
   @property
   def total_work(self):
+    # type: () -> float
     return self.completed_work + self.remaining_work
 
   @property
   def fraction_completed(self):
+    # type: () -> float
     if self._fraction is not None:
       return self._fraction
     else:
@@ -1404,6 +1409,7 @@ def fraction_completed(self):
 
   @property
   def fraction_remaining(self):
+    # type: () -> float
     if self._fraction is not None:
       return 1 - self._fraction
     else:
diff --git a/sdks/python/apache_beam/io/restriction_trackers_test.py b/sdks/python/apache_beam/io/restriction_trackers_test.py
index aaebb0930384..8c2c7f7b9d99 100644
--- a/sdks/python/apache_beam/io/restriction_trackers_test.py
+++ b/sdks/python/apache_beam/io/restriction_trackers_test.py
@@ -141,13 +141,6 @@ def test_check_done_after_try_claim_past_end_of_range(self):
     self.assertFalse(tracker.try_claim(220))
     tracker.check_done()
 
-  def test_check_done_after_try_claim_past_end_of_range(self):
-    tracker = OffsetRestrictionTracker(OffsetRange(100, 200))
-    self.assertTrue(tracker.try_claim(150))
-    self.assertTrue(tracker.try_claim(175))
-    self.assertFalse(tracker.try_claim(200))
-    tracker.check_done()
-
   def test_check_done_after_try_claim_right_before_end_of_range(self):
     tracker = OffsetRestrictionTracker(OffsetRange(100, 200))
     self.assertTrue(tracker.try_claim(150))
diff --git a/sdks/python/apache_beam/io/tfrecordio_test.py b/sdks/python/apache_beam/io/tfrecordio_test.py
index 9ed525bdae1a..90f484a67d29 100644
--- a/sdks/python/apache_beam/io/tfrecordio_test.py
+++ b/sdks/python/apache_beam/io/tfrecordio_test.py
@@ -290,7 +290,7 @@ def test_process_deflate(self):
                       validate=True))
         assert_that(result, equal_to([b'foo', b'bar']))
 
-  def test_process_gzip(self):
+  def test_process_gzip_with_coder(self):
     with TempDir() as temp_dir:
       path = temp_dir.create_temp_file('result')
       _write_file_gzip(path, FOO_BAR_RECORD_BASE64)
@@ -303,27 +303,28 @@ def test_process_gzip(self):
                       validate=True))
         assert_that(result, equal_to([b'foo', b'bar']))
 
-  def test_process_auto(self):
+  def test_process_gzip_without_coder(self):
     with TempDir() as temp_dir:
-      path = temp_dir.create_temp_file('result.gz')
+      path = temp_dir.create_temp_file('result')
       _write_file_gzip(path, FOO_BAR_RECORD_BASE64)
       with TestPipeline() as p:
         result = (p
                   | ReadFromTFRecord(
                       path,
-                      coder=coders.BytesCoder(),
-                      compression_type=CompressionTypes.AUTO,
-                      validate=True))
+                      compression_type=CompressionTypes.GZIP))
         assert_that(result, equal_to([b'foo', b'bar']))
 
-  def test_process_gzip(self):
+  def test_process_auto(self):
     with TempDir() as temp_dir:
-      path = temp_dir.create_temp_file('result')
+      path = temp_dir.create_temp_file('result.gz')
       _write_file_gzip(path, FOO_BAR_RECORD_BASE64)
       with TestPipeline() as p:
         result = (p
                   | ReadFromTFRecord(
-                      path, compression_type=CompressionTypes.GZIP))
+                      path,
+                      coder=coders.BytesCoder(),
+                      compression_type=CompressionTypes.AUTO,
+                      validate=True))
         assert_that(result, equal_to([b'foo', b'bar']))
 
   def test_process_gzip_auto(self):
diff --git a/sdks/python/apache_beam/io/vcfio.py b/sdks/python/apache_beam/io/vcfio.py
index c6e550237c28..7cff80ae8bd4 100644
--- a/sdks/python/apache_beam/io/vcfio.py
+++ b/sdks/python/apache_beam/io/vcfio.py
@@ -181,9 +181,6 @@ def __le__(self, other):
 
     return self < other or self == other
 
-  def __ne__(self, other):
-    return not self == other
-
   def __gt__(self, other):
     if not isinstance(other, Variant):
       return NotImplemented
diff --git a/sdks/python/apache_beam/metrics/cells.py b/sdks/python/apache_beam/metrics/cells.py
index 0b6caa6df8ab..8f9128856052 100644
--- a/sdks/python/apache_beam/metrics/cells.py
+++ b/sdks/python/apache_beam/metrics/cells.py
@@ -29,6 +29,7 @@
 import threading
 import time
 from builtins import object
+from typing import Optional
 
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import metrics_pb2
@@ -86,6 +87,7 @@ def reset(self):
     self.value = CounterAggregator.identity_element()
 
   def combine(self, other):
+    # type: (CounterCell) -> CounterCell
     result = CounterCell()
     result.inc(self.value + other.value)
     return result
@@ -106,6 +108,7 @@ def update(self, value):
         self.value += value
 
   def get_cumulative(self):
+    # type: () -> int
     with self._lock:
       return self.value
 
@@ -144,6 +147,7 @@ def reset(self):
     self.data = DistributionAggregator.identity_element()
 
   def combine(self, other):
+    # type: (DistributionCell) -> DistributionCell
     result = DistributionCell()
     result.data = self.data.combine(other.data)
     return result
@@ -169,6 +173,7 @@ def _update(self, value):
       self.data.max = ivalue
 
   def get_cumulative(self):
+    # type: () -> DistributionData
     with self._lock:
       return self.data.get_cumulative()
 
@@ -204,6 +209,7 @@ def reset(self):
     self.data = GaugeAggregator.identity_element()
 
   def combine(self, other):
+    # type: (GaugeCell) -> GaugeCell
     result = GaugeCell()
     result.data = self.data.combine(other.data)
     return result
@@ -220,6 +226,7 @@ def update(self, value):
       self.data.timestamp = time.time()
 
   def get_cumulative(self):
+    # type: () -> GaugeData
     with self._lock:
       return self.data.get_cumulative()
 
@@ -239,6 +246,7 @@ def to_runner_api_monitoring_info(self, name, transform_id):
 class DistributionResult(object):
   """The result of a Distribution metric."""
   def __init__(self, data):
+    # type: (DistributionData) -> None
     self.data = data
 
   def __eq__(self, other):
@@ -290,6 +298,7 @@ def mean(self):
 
 class GaugeResult(object):
   def __init__(self, data):
+    # type: (GaugeData) -> None
     self.data = data
 
   def __eq__(self, other):
@@ -349,9 +358,11 @@ def __repr__(self):
         self.timestamp)
 
   def get_cumulative(self):
+    # type: () -> GaugeData
     return GaugeData(self.value, timestamp=self.timestamp)
 
   def combine(self, other):
+    # type: (Optional[GaugeData]) -> GaugeData
     if other is None:
       return self
 
@@ -362,6 +373,7 @@ def combine(self, other):
 
   @staticmethod
   def singleton(value, timestamp=None):
+    # type: (...) -> GaugeData
     return GaugeData(value, timestamp=timestamp)
 
   def to_runner_api(self):
@@ -427,9 +439,11 @@ def __repr__(self):
         self.max)
 
   def get_cumulative(self):
+    # type: () -> DistributionData
     return DistributionData(self.sum, self.count, self.min, self.max)
 
   def combine(self, other):
+    # type: (Optional[DistributionData]) -> DistributionData
     if other is None:
       return self
 
@@ -474,7 +488,7 @@ def identity_element(self):
     """
     raise NotImplementedError
 
-  def combine(self, updates):
+  def combine(self, x, y):
     raise NotImplementedError
 
   def result(self, x):
@@ -490,12 +504,15 @@ class CounterAggregator(MetricAggregator):
   """
   @staticmethod
   def identity_element():
+    # type: () -> int
     return 0
 
   def combine(self, x, y):
+    # type: (...) -> int
     return int(x) + int(y)
 
   def result(self, x):
+    # type: (...) -> int
     return int(x)
 
 
@@ -508,12 +525,15 @@ class DistributionAggregator(MetricAggregator):
   """
   @staticmethod
   def identity_element():
+    # type: () -> DistributionData
     return DistributionData(0, 0, 2**63 - 1, -2**63)
 
   def combine(self, x, y):
+    # type: (DistributionData, DistributionData) -> DistributionData
     return x.combine(y)
 
   def result(self, x):
+    # type: (DistributionData) -> DistributionResult
     return DistributionResult(x.get_cumulative())
 
 
@@ -526,11 +546,14 @@ class GaugeAggregator(MetricAggregator):
   """
   @staticmethod
   def identity_element():
+    # type: () -> GaugeData
     return GaugeData(None, timestamp=0)
 
   def combine(self, x, y):
+    # type: (GaugeData, GaugeData) -> GaugeData
     result = x.combine(y)
     return result
 
   def result(self, x):
+    # type: (GaugeData) -> GaugeResult
     return GaugeResult(x.get_cumulative())
diff --git a/sdks/python/apache_beam/pipeline.py b/sdks/python/apache_beam/pipeline.py
index 1dbafd95ec1c..7b1be3988a76 100644
--- a/sdks/python/apache_beam/pipeline.py
+++ b/sdks/python/apache_beam/pipeline.py
@@ -51,7 +51,6 @@
 import abc
 import logging
 import os
-import re
 import shutil
 import tempfile
 from builtins import object
@@ -80,7 +79,9 @@
 from apache_beam.portability import common_urns
 from apache_beam.runners import PipelineRunner
 from apache_beam.runners import create_runner
+from apache_beam.transforms import ParDo
 from apache_beam.transforms import ptransform
+from apache_beam.transforms.sideinputs import get_sideinput_index
 #from apache_beam.transforms import external
 from apache_beam.typehints import TypeCheckError
 from apache_beam.typehints import typehints
@@ -91,6 +92,7 @@
   from apache_beam.portability.api import beam_runner_api_pb2
   from apache_beam.runners.pipeline_context import PipelineContext
   from apache_beam.runners.runner import PipelineResult
+  from apache_beam.transforms import environments
 
 __all__ = ['Pipeline', 'PTransformOverride']
 
@@ -679,7 +681,7 @@ def to_runner_api(self,
                     return_context=False,
                     context=None,  # type: Optional[PipelineContext]
                     use_fake_coders=False,
-                    default_environment=None  # type: Optional[beam_runner_api_pb2.Environment]
+                    default_environment=None  # type: Optional[environments.Environment]
                    ):
     # type: (...) -> beam_runner_api_pb2.Pipeline
     """For internal use only; no backwards-compatibility guarantees."""
@@ -821,7 +823,7 @@ class AppliedPTransform(object):
 
   def __init__(self,
                parent,
-               transform,  # type: ptransform.PTransform
+               transform,  # type: Optional[ptransform.PTransform]
                full_label,  # type: str
                inputs,  # type: Optional[Sequence[Union[pvalue.PBegin, pvalue.PCollection]]]
                environment_id=None  # type: Optional[str]
@@ -909,17 +911,18 @@ def visit(self,
     # type: (...) -> None
     """Visits all nodes reachable from the current node."""
 
-    for pval in self.inputs:
-      if pval not in visited and not isinstance(pval, pvalue.PBegin):
-        if pval.producer is not None:
-          pval.producer.visit(visitor, pipeline, visited)
+    for in_pval in self.inputs:
+      if in_pval not in visited and not isinstance(in_pval, pvalue.PBegin):
+        if in_pval.producer is not None:
+          in_pval.producer.visit(visitor, pipeline, visited)
           # The value should be visited now since we visit outputs too.
-          assert pval in visited, pval
+          assert in_pval in visited, in_pval
 
     # Visit side inputs.
-    for pval in self.side_inputs:
-      if isinstance(pval, pvalue.AsSideInput) and pval.pvalue not in visited:
-        pval = pval.pvalue  # Unpack marker-object-wrapped pvalue.
+    for side_input in self.side_inputs:
+      if isinstance(side_input, pvalue.AsSideInput) \
+          and side_input.pvalue not in visited:
+        pval = side_input.pvalue  # Unpack marker-object-wrapped pvalue.
         if pval.producer is not None:
           pval.producer.visit(visitor, pipeline, visited)
           # The value should be visited now since we visit outputs too.
@@ -944,11 +947,11 @@ def visit(self,
     # output of such a transform is the containing DoOutputsTuple, not the
     # PCollection inside it. Without the code below a tagged PCollection will
     # not be marked as visited while visiting its producer.
-    for pval in self.outputs.values():
-      if isinstance(pval, pvalue.DoOutputsTuple):
-        pvals = (v for v in pval)
+    for out_pval in self.outputs.values():
+      if isinstance(out_pval, pvalue.DoOutputsTuple):
+        pvals = (v for v in out_pval)
       else:
-        pvals = (pval,)
+        pvals = (out_pval,)
       for v in pvals:
         if v not in visited:
           visited.add(v)
@@ -1019,15 +1022,17 @@ def is_side_input(tag):
     main_inputs = [context.pcollections.get_by_id(id)
                    for tag, id in proto.inputs.items()
                    if not is_side_input(tag)]
+
     # Ordering is important here.
-    indexed_side_inputs = [(int(re.match('side([0-9]+)(-.*)?$', tag).group(1)),
+    indexed_side_inputs = [(get_sideinput_index(tag),
                             context.pcollections.get_by_id(id))
                            for tag, id in proto.inputs.items()
                            if is_side_input(tag)]
     side_inputs = [si for _, si in sorted(indexed_side_inputs)]
+    transform = ptransform.PTransform.from_runner_api(proto.spec, context)
     result = AppliedPTransform(
         parent=None,
-        transform=ptransform.PTransform.from_runner_api(proto.spec, context),
+        transform=transform,
         full_label=proto.unique_name,
         inputs=main_inputs,
         environment_id=proto.environment_id)
@@ -1045,6 +1050,7 @@ def is_side_input(tag):
         for tag, id in proto.outputs.items()}
     # This annotation is expected by some runners.
     if proto.spec.urn == common_urns.primitives.PAR_DO.urn:
+      assert isinstance(result.transform, ParDo)
       result.transform.output_tags = set(proto.outputs.keys()).difference(
           {'None'})
     if not result.parts:
diff --git a/sdks/python/apache_beam/portability/common_urns.py b/sdks/python/apache_beam/portability/common_urns.py
index 9c43570da11b..6f2943e09016 100644
--- a/sdks/python/apache_beam/portability/common_urns.py
+++ b/sdks/python/apache_beam/portability/common_urns.py
@@ -21,70 +21,36 @@
 
 from __future__ import absolute_import
 
-from builtins import object
-
-from apache_beam.portability.api import beam_runner_api_pb2
-from apache_beam.portability.api import metrics_pb2
-from apache_beam.portability.api import standard_window_fns_pb2
-
-
-class PropertiesFromEnumValue(object):
-  def __init__(self, value_descriptor):
-    self.urn = (value_descriptor.GetOptions().Extensions[
-        beam_runner_api_pb2.beam_urn])
-    self.constant = (value_descriptor.GetOptions().Extensions[
-        beam_runner_api_pb2.beam_constant])
-    self.spec = (value_descriptor.GetOptions().Extensions[
-        metrics_pb2.monitoring_info_spec])
-    self.label_props = (value_descriptor.GetOptions().Extensions[
-        metrics_pb2.label_props])
-
-
-class PropertiesFromEnumType(object):
-  def __init__(self, enum_type):
-    for v in enum_type.DESCRIPTOR.values:
-      setattr(self, v.name, PropertiesFromEnumValue(v))
-
-
-primitives = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardPTransforms.Primitives)
-deprecated_primitives = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardPTransforms.DeprecatedPrimitives)
-composites = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardPTransforms.Composites)
-combine_components = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardPTransforms.CombineComponents)
-sdf_components = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardPTransforms.SplittableParDoComponents)
-
-side_inputs = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardSideInputTypes.Enum)
-
-coders = PropertiesFromEnumType(beam_runner_api_pb2.StandardCoders.Enum)
-
-constants = PropertiesFromEnumType(
-    beam_runner_api_pb2.BeamConstants.Constants)
-
-environments = PropertiesFromEnumType(
-    beam_runner_api_pb2.StandardEnvironments.Environments)
-
-
-def PropertiesFromPayloadType(payload_type):
-  return PropertiesFromEnumType(payload_type.Enum).PROPERTIES
-
-
-global_windows = PropertiesFromPayloadType(
-    standard_window_fns_pb2.GlobalWindowsPayload)
-fixed_windows = PropertiesFromPayloadType(
-    standard_window_fns_pb2.FixedWindowsPayload)
-sliding_windows = PropertiesFromPayloadType(
-    standard_window_fns_pb2.SlidingWindowsPayload)
-session_windows = PropertiesFromPayloadType(
-    standard_window_fns_pb2.SessionsPayload)
-
-monitoring_info_specs = PropertiesFromEnumType(
-    metrics_pb2.MonitoringInfoSpecs.Enum)
-monitoring_info_types = PropertiesFromEnumType(
-    metrics_pb2.MonitoringInfoTypeUrns.Enum)
-monitoring_info_labels = PropertiesFromEnumType(
-    metrics_pb2.MonitoringInfo.MonitoringInfoLabels)
+from apache_beam.portability.api.beam_runner_api_pb2_urns import BeamConstants
+from apache_beam.portability.api.beam_runner_api_pb2_urns import StandardCoders
+from apache_beam.portability.api.beam_runner_api_pb2_urns import StandardEnvironments
+from apache_beam.portability.api.beam_runner_api_pb2_urns import StandardPTransforms
+from apache_beam.portability.api.beam_runner_api_pb2_urns import StandardSideInputTypes
+from apache_beam.portability.api.metrics_pb2_urns import MonitoringInfo
+from apache_beam.portability.api.metrics_pb2_urns import MonitoringInfoSpecs
+from apache_beam.portability.api.metrics_pb2_urns import MonitoringInfoTypeUrns
+from apache_beam.portability.api.standard_window_fns_pb2_urns import FixedWindowsPayload
+from apache_beam.portability.api.standard_window_fns_pb2_urns import GlobalWindowsPayload
+from apache_beam.portability.api.standard_window_fns_pb2_urns import SessionsPayload
+from apache_beam.portability.api.standard_window_fns_pb2_urns import SlidingWindowsPayload
+
+primitives = StandardPTransforms.Primitives
+deprecated_primitives = StandardPTransforms.DeprecatedPrimitives
+composites = StandardPTransforms.Composites
+combine_components = StandardPTransforms.CombineComponents
+sdf_components = StandardPTransforms.SplittableParDoComponents
+
+side_inputs = StandardSideInputTypes.Enum
+coders = StandardCoders.Enum
+constants = BeamConstants.Constants
+
+environments = StandardEnvironments.Environments
+
+global_windows = GlobalWindowsPayload.Enum.PROPERTIES
+fixed_windows = FixedWindowsPayload.Enum.PROPERTIES
+sliding_windows = SlidingWindowsPayload.Enum.PROPERTIES
+session_windows = SessionsPayload.Enum.PROPERTIES
+
+monitoring_info_specs = MonitoringInfoSpecs.Enum
+monitoring_info_types = MonitoringInfoTypeUrns.Enum
+monitoring_info_labels = MonitoringInfo.MonitoringInfoLabels
diff --git a/sdks/python/apache_beam/portability/utils.py b/sdks/python/apache_beam/portability/utils.py
new file mode 100644
index 000000000000..7c6059830a31
--- /dev/null
+++ b/sdks/python/apache_beam/portability/utils.py
@@ -0,0 +1,34 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+"""For internal use only; no backwards-compatibility guarantees."""
+from __future__ import absolute_import
+
+from typing import TYPE_CHECKING
+from typing import NamedTuple
+
+if TYPE_CHECKING:
+  from apache_beam.portability.api import metrics_pb2
+
+
+PropertiesFromEnumValue = NamedTuple(
+    'PropertiesFromEnumValue', [
+        ('urn', str),
+        ('constant', str),
+        ('spec', 'metrics_pb2.MonitoringInfoSpec'),
+        ('label_props', 'metrics_pb2.MonitoringInfoLabelProps'),
+    ])
diff --git a/sdks/python/apache_beam/pvalue.py b/sdks/python/apache_beam/pvalue.py
index 7e5e7c5c7c03..bf1f761f21b1 100644
--- a/sdks/python/apache_beam/pvalue.py
+++ b/sdks/python/apache_beam/pvalue.py
@@ -160,6 +160,7 @@ def __hash__(self):
   def windowing(self):
     # type: () -> Windowing
     if not hasattr(self, '_windowing'):
+      assert self.producer is not None and self.producer.transform is not None
       self._windowing = self.producer.transform.get_windowing(
           self.producer.inputs)
     return self._windowing
@@ -201,10 +202,14 @@ def _unique_name(self):
   @staticmethod
   def from_runner_api(proto, context):
     # type: (beam_runner_api_pb2.PCollection, PipelineContext) -> PCollection
-    # Producer and tag will be filled in later, the key point is that the
-    # same object is returned for the same pcollection id.
+    # Producer and tag will be filled in later, the key point is that the same
+    # object is returned for the same pcollection id.
+    # We pass None for the PCollection's Pipeline to avoid a cycle during
+    # deserialization.  It will be populated soon after this call, in
+    # Pipeline.from_runner_api(). This brief period is the only time that
+    # PCollection.pipeline is allowed to be None.
     return PCollection(
-        None,
+        None,  # type: ignore[arg-type]
         element_type=context.element_type_from_coder_id(proto.coder_id),
         windowing=context.windowing_strategies.get_by_id(
             proto.windowing_strategy_id),
diff --git a/sdks/python/apache_beam/runners/common.py b/sdks/python/apache_beam/runners/common.py
index 220e3d371d5c..429fdd771f0f 100644
--- a/sdks/python/apache_beam/runners/common.py
+++ b/sdks/python/apache_beam/runners/common.py
@@ -400,6 +400,8 @@ def create_invoker(
     if use_simple_invoker:
       return SimpleInvoker(output_processor, signature)
     else:
+      if context is None:
+        raise TypeError("Must provide context when not using SimpleInvoker")
       return PerWindowInvoker(
           output_processor,
           signature, context, side_inputs, input_args, input_kwargs,
@@ -419,7 +421,7 @@ def invoke_process(self,
       windowed_value: a WindowedValue object that gives the element for which
                       process() method should be invoked along with the window
                       the element belongs to.
-      output_procesor: if provided given OutputProcessor will be used.
+      output_processor: if provided given OutputProcessor will be used.
       additional_args: additional arguments to be passed to the current
                       `DoFn.process()` invocation, usually as side inputs.
       additional_kwargs: additional keyword arguments to be passed to the
@@ -437,14 +439,16 @@ def invoke_start_bundle(self):
     # type: () -> None
     """Invokes the DoFn.start_bundle() method.
     """
-    self.output_processor.start_bundle_outputs(
+    # self.output_processor is Optional, but in practice it won't be None here
+    self.output_processor.start_bundle_outputs(  # type: ignore[union-attr]
         self.signature.start_bundle_method.method_value())
 
   def invoke_finish_bundle(self):
     # type: () -> None
     """Invokes the DoFn.finish_bundle() method.
     """
-    self.output_processor.finish_bundle_outputs(
+    # self.output_processor is Optional, but in practice it won't be None here
+    self.output_processor.finish_bundle_outputs(  # type: ignore[union-attr]
         self.signature.finish_bundle_method.method_value())
 
   def invoke_teardown(self):
@@ -454,7 +458,8 @@ def invoke_teardown(self):
     self.signature.teardown_lifecycle_method.method_value()
 
   def invoke_user_timer(self, timer_spec, key, window, timestamp):
-    self.output_processor.process_outputs(
+    # self.output_processor is Optional, but in practice it won't be None here
+    self.output_processor.process_outputs(  # type: ignore[union-attr]
         WindowedValue(None, timestamp, (window,)),
         self.signature.timer_methods[timer_spec].invoke_timer_callback(
             self.user_state_context, key, window, timestamp))
@@ -493,7 +498,8 @@ def invoke_process(self,
     # type: (...) -> None
     if not output_processor:
       output_processor = self.output_processor
-    output_processor.process_outputs(
+    # self.output_processor is Optional, but in practice it won't be None here
+    output_processor.process_outputs(  # type: ignore[union-attr]
         windowed_value, self.process_method(windowed_value.value))
 
 
@@ -525,8 +531,8 @@ def __init__(self,
     self.watermark_estimator_param = (
         self.signature.process_method.watermark_estimator_arg_name
         if self.watermark_estimator else None)
-    self.threadsafe_restriction_tracker = None
-    self.current_windowed_value = None
+    self.threadsafe_restriction_tracker = None  # type: Optional[iobase.ThreadsafeRestrictionTracker]
+    self.current_windowed_value = None  # type: Optional[WindowedValue]
     self.bundle_finalizer_param = bundle_finalizer_param
     self.is_key_param_required = False
 
@@ -619,6 +625,7 @@ def invoke_process(self,
       additional_kwargs = {}
 
     if not output_processor:
+      assert self.output_processor is not None
       output_processor = self.output_processor
     self.context.set_element(windowed_value)
     # Call for the process function for each window if has windowed side inputs
@@ -669,6 +676,7 @@ def invoke_process(self,
     else:
       self._invoke_process_per_window(
           windowed_value, additional_args, additional_kwargs, output_processor)
+    return None
 
   def _invoke_process_per_window(self,
                                  windowed_value,  # type: WindowedValue
@@ -725,9 +733,11 @@ def _invoke_process_per_window(self,
       elif core.DoFn.PaneInfoParam == p:
         args_for_process[i] = windowed_value.pane_info
       elif isinstance(p, core.DoFn.StateParam):
+        assert self.user_state_context is not None
         args_for_process[i] = (
             self.user_state_context.get_state(p.state_spec, key, window))
       elif isinstance(p, core.DoFn.TimerParam):
+        assert self.user_state_context is not None
         args_for_process[i] = (
             self.user_state_context.get_timer(p.timer_spec, key, window))
       elif core.DoFn.BundleFinalizerParam == p:
@@ -749,6 +759,7 @@ def _invoke_process_per_window(self,
           windowed_value, self.process_method(*args_for_process))
 
     if self.is_splittable:
+      assert self.threadsafe_restriction_tracker is not None
       # TODO: Consider calling check_done right after SDF.Process() finishing.
       # In order to do this, we need to know that current invoking dofn is
       # ProcessSizedElementAndRestriction.
@@ -765,11 +776,11 @@ def _invoke_process_per_window(self,
         return ((
             windowed_value.with_value(((element, deferred_restriction), size)),
             output_watermark), deferred_watermark)
+    return None
 
   def try_split(self, fraction):
-    restriction_tracker = self.threadsafe_restriction_tracker
-    current_windowed_value = self.current_windowed_value
-    if restriction_tracker and current_windowed_value:
+    # type: (...) -> Optional[Tuple[SplitResultType, SplitResultType]]
+    if self.threadsafe_restriction_tracker and self.current_windowed_value:
       # Temporary workaround for [BEAM-7473]: get current_watermark before
       # split, in case watermark gets advanced before getting split results.
       # In worst case, current_watermark is always stale, which is ok.
@@ -777,7 +788,7 @@ def try_split(self, fraction):
         current_watermark = self.watermark_estimator.current_watermark()
       else:
         current_watermark = None
-      split = restriction_tracker.try_split(fraction)
+      split = self.threadsafe_restriction_tracker.try_split(fraction)
       if split:
         primary, residual = split
         element = self.current_windowed_value.value
@@ -795,6 +806,8 @@ def current_element_progress(self):
     restriction_tracker = self.threadsafe_restriction_tracker
     if restriction_tracker:
       return restriction_tracker.current_progress()
+    else:
+      return None
 
 
 class DoFnRunner(Receiver):
@@ -809,7 +822,7 @@ def __init__(self,
                kwargs,
                side_inputs,  # type: Iterable[sideinputs.SideInputMap]
                windowing,
-               tagged_receivers=None,  # type: Mapping[Optional[str], Receiver]
+               tagged_receivers,  # type: Mapping[Optional[str], Receiver]
                step_name=None,  # type: Optional[str]
                logging_context=None,
                state=None,
@@ -881,6 +894,7 @@ def process(self, windowed_value):
       return self.do_fn_invoker.invoke_process(windowed_value)
     except BaseException as exn:
       self._reraise_augmented(exn)
+      return None
 
   def process_with_sized_restriction(self, windowed_value):
     # type: (WindowedValue) -> Optional[Tuple[WindowedValue, Timestamp]]
@@ -895,6 +909,7 @@ def try_split(self, fraction):
 
   def current_element_progress(self):
     # type: () -> Optional[iobase.RestrictionProgress]
+    assert isinstance(self.do_fn_invoker, PerWindowInvoker)
     return self.do_fn_invoker.current_element_progress()
 
   def process_user_timer(self, timer_spec, key, window, timestamp):
diff --git a/sdks/python/apache_beam/runners/direct/bundle_factory.py b/sdks/python/apache_beam/runners/direct/bundle_factory.py
index c7677f484b31..6958fb95dd32 100644
--- a/sdks/python/apache_beam/runners/direct/bundle_factory.py
+++ b/sdks/python/apache_beam/runners/direct/bundle_factory.py
@@ -26,6 +26,7 @@
 from typing import Iterator
 from typing import List
 from typing import Union
+from typing import cast
 
 from apache_beam import pvalue
 from apache_beam.runners import common
@@ -145,9 +146,11 @@ def get_elements_iterable(self, make_copy=False):
       or as a list of copied WindowedValues.
     """
     if not self._stacked:
+      # we can safely assume self._elements contains only WindowedValues
+      elements = cast('List[WindowedValue]', self._elements)
       if self._committed and not make_copy:
-        return self._elements
-      return list(self._elements)
+        return elements
+      return list(elements)
 
     def iterable_stacked_or_elements(elements):
       for e in elements:
diff --git a/sdks/python/apache_beam/runners/direct/transform_evaluator.py b/sdks/python/apache_beam/runners/direct/transform_evaluator.py
index 0f47b1c07bcd..71fd1339c272 100644
--- a/sdks/python/apache_beam/runners/direct/transform_evaluator.py
+++ b/sdks/python/apache_beam/runners/direct/transform_evaluator.py
@@ -586,6 +586,7 @@ def finish_bundle(self):
       bundles = [bundle]
     else:
       bundles = []
+    assert self._applied_ptransform.transform is not None
     if self._applied_ptransform.inputs:
       input_pvalue = self._applied_ptransform.inputs[0]  # type: Union[pvalue.PBegin, pvalue.PCollection]
     else:
diff --git a/sdks/python/apache_beam/runners/pipeline_context.py b/sdks/python/apache_beam/runners/pipeline_context.py
index b3c4d5b3efba..6b6c3a0cc953 100644
--- a/sdks/python/apache_beam/runners/pipeline_context.py
+++ b/sdks/python/apache_beam/runners/pipeline_context.py
@@ -136,14 +136,6 @@ class PipelineContext(object):
   Used for accessing and constructing the referenced objects of a Pipeline.
   """
 
-  _COMPONENT_TYPES = {
-      'transforms': pipeline.AppliedPTransform,
-      'pcollections': pvalue.PCollection,
-      'coders': coders.Coder,
-      'windowing_strategies': core.Windowing,
-      'environments': environments.Environment,
-  }
-
   def __init__(self,
                proto=None,  # type: Optional[Union[beam_runner_api_pb2.Components, beam_fn_api_pb2.ProcessBundleDescriptor]]
                default_environment=None,  # type: Optional[environments.Environment]
@@ -158,13 +150,26 @@ def __init__(self,
           coders=dict(proto.coders.items()),
           windowing_strategies=dict(proto.windowing_strategies.items()),
           environments=dict(proto.environments.items()))
-    for name, cls in self._COMPONENT_TYPES.items():
-      setattr(
-          self, name, _PipelineContextMap(
-              self, cls, namespace, getattr(proto, name, None)))
+
+    self.transforms = _PipelineContextMap(
+        self, pipeline.AppliedPTransform, namespace,
+        proto.transforms if proto is not None else None)
+    self.pcollections = _PipelineContextMap(
+        self, pvalue.PCollection, namespace,
+        proto.pcollections if proto is not None else None)
+    self.coders = _PipelineContextMap(
+        self, coders.Coder, namespace,
+        proto.coders if proto is not None else None)
+    self.windowing_strategies = _PipelineContextMap(
+        self, core.Windowing, namespace,
+        proto.windowing_strategies if proto is not None else None)
+    self.environments = _PipelineContextMap(
+        self, environments.Environment, namespace,
+        proto.environments if proto is not None else None)
+
     if default_environment:
       self._default_environment_id = self.environments.get_id(
-          default_environment, label='default_environment')
+          default_environment, label='default_environment')  # type: Optional[str]
     else:
       self._default_environment_id = None
     self.use_fake_coders = use_fake_coders
@@ -179,7 +184,7 @@ def __init__(self,
   def coder_id_from_element_type(self, element_type):
     # type: (Any) -> str
     if self.use_fake_coders:
-      return pickler.dumps(element_type)
+      return pickler.dumps(element_type).decode('ascii')
     else:
       return self.coders.get_id(coders.registry.get_coder(element_type))
 
@@ -199,8 +204,13 @@ def from_runner_api(proto):
   def to_runner_api(self):
     # type: () -> beam_runner_api_pb2.Components
     context_proto = beam_runner_api_pb2.Components()
-    for name in self._COMPONENT_TYPES:
-      getattr(self, name).populate_map(getattr(context_proto, name))
+
+    self.transforms.populate_map(context_proto.transforms)
+    self.pcollections.populate_map(context_proto.pcollections)
+    self.coders.populate_map(context_proto.coders)
+    self.windowing_strategies.populate_map(context_proto.windowing_strategies)
+    self.environments.populate_map(context_proto.environments)
+
     return context_proto
 
   def default_environment_id(self):
diff --git a/sdks/python/apache_beam/runners/portability/abstract_job_service.py b/sdks/python/apache_beam/runners/portability/abstract_job_service.py
index 50532f224992..8a97b7370413 100644
--- a/sdks/python/apache_beam/runners/portability/abstract_job_service.py
+++ b/sdks/python/apache_beam/runners/portability/abstract_job_service.py
@@ -31,6 +31,7 @@
 from typing import Dict
 from typing import Iterator
 from typing import Optional
+from typing import Tuple
 from typing import Union
 
 import grpc
@@ -50,6 +51,7 @@
 
 _LOGGER = logging.getLogger(__name__)
 
+StateEvent = Tuple[int, Union[timestamp_pb2.Timestamp, Timestamp]]
 
 def make_state_event(state, timestamp):
   if isinstance(timestamp, Timestamp):
@@ -129,7 +131,7 @@ def GetState(self,
                request,  # type: beam_job_api_pb2.GetJobStateRequest
                context=None
               ):
-    # type: (...) -> beam_job_api_pb2.GetJobStateResponse
+    # type: (...) -> beam_job_api_pb2.JobStateEvent
     return beam_job_api_pb2.JobStateEvent(
         state=self._jobs[request.job_id].get_state())
 
@@ -153,7 +155,7 @@ def Cancel(self,
         state=self._jobs[request.job_id].get_state())
 
   def GetStateStream(self, request, context=None, timeout=None):
-    # type: (...) -> Iterator[beam_job_api_pb2.GetJobStateResponse]
+    # type: (...) -> Iterator[beam_job_api_pb2.JobStateEvent]
     """Yields state transitions since the stream started.
       """
     if request.job_id not in self._jobs:
@@ -218,11 +220,11 @@ def artifact_staging_endpoint(self):
     raise NotImplementedError(self)
 
   def get_state_stream(self):
-    # type: () -> Iterator[Optional[beam_job_api_pb2.JobState.Enum]]
+    # type: () -> Iterator[StateEvent]
     raise NotImplementedError(self)
 
   def get_message_stream(self):
-    # type: () -> Iterator[Union[int, Optional[beam_job_api_pb2.JobMessage]]]
+    # type: () -> Iterator[Union[StateEvent, Optional[beam_job_api_pb2.JobMessage]]]
     raise NotImplementedError(self)
 
 
diff --git a/sdks/python/apache_beam/runners/portability/artifact_service.py b/sdks/python/apache_beam/runners/portability/artifact_service.py
index 17f9b70e6288..708aceb45ee8 100644
--- a/sdks/python/apache_beam/runners/portability/artifact_service.py
+++ b/sdks/python/apache_beam/runners/portability/artifact_service.py
@@ -150,7 +150,7 @@ def GetArtifact(self,
         with self._open(artifact.uri, 'r') as fin:
           # This value is not emitted, but lets us yield a single empty
           # chunk on an empty file.
-          chunk = True
+          chunk = b'1'
           while chunk:
             chunk = fin.read(self._chunk_size)
             yield beam_artifact_api_pb2.ArtifactChunk(data=chunk)
diff --git a/sdks/python/apache_beam/runners/portability/fn_api_runner.py b/sdks/python/apache_beam/runners/portability/fn_api_runner.py
index 6100efaa7301..c5e1dac33e54 100644
--- a/sdks/python/apache_beam/runners/portability/fn_api_runner.py
+++ b/sdks/python/apache_beam/runners/portability/fn_api_runner.py
@@ -50,6 +50,7 @@
 from typing import Type
 from typing import TypeVar
 from typing import Union
+from typing import overload
 
 import grpc
 
@@ -152,8 +153,17 @@ def _read(self):
     for data in self._input:
       self._futures_by_id.pop(data.instruction_id).set(data)
 
+  @overload
+  def push(self, req):
+    # type: (BeamFnControlServicer.DoneMarker) -> None
+    pass
+
+  @overload
+  def push(self, req):
+    # type: (beam_fn_api_pb2.InstructionRequest) -> ControlFuture
+    pass
+
   def push(self, req):
-    # type: (...) -> Optional[ControlFuture]
     if req == BeamFnControlServicer._DONE_MARKER:
       self._push_queue.put(req)
       return None
@@ -195,7 +205,10 @@ class BeamFnControlServicer(beam_fn_api_pb2_grpc.BeamFnControlServicer):
   STARTED_STATE = 'started'
   DONE_STATE = 'done'
 
-  _DONE_MARKER = object()
+  class DoneMarker(object):
+    pass
+
+  _DONE_MARKER = DoneMarker()
 
   def __init__(self):
     self._lock = threading.Lock()
@@ -367,8 +380,9 @@ def append(self, elements_data):
     # type: (bytes) -> None
     input_stream = create_InputStream(elements_data)
     while input_stream.size() > 0:
-      windowed_value = self._windowed_value_coder.get_impl(
-          ).decode_from_stream(input_stream, True)
+      windowed_val_coder_impl = self._windowed_value_coder.get_impl()  # type: WindowedValueCoderImpl
+      windowed_value = windowed_val_coder_impl.decode_from_stream(
+          input_stream, True)
       key, value = self._kv_extractor(windowed_value.value)
       for window in windowed_value.windows:
         self._values_by_window[key, window].append(value)
@@ -390,7 +404,7 @@ class FnApiRunner(runner.PipelineRunner):
 
   def __init__(
       self,
-      default_environment=None,  # type: Optional[beam_runner_api_pb2.Environment]
+      default_environment=None,  # type: Optional[environments.Environment]
       bundle_repeat=0,
       use_state_iterables=False,
       provision_info=None,  # type: Optional[ExtendedProvisionInfo]
@@ -819,9 +833,10 @@ def iterable_state_write(values, element_coder_impl):
             pipeline_components.windowing_strategies.items()),
         environments=dict(pipeline_components.environments.items()))
 
-    if worker_handler.state_api_service_descriptor():
+    state_api_service_descriptor = worker_handler.state_api_service_descriptor()
+    if state_api_service_descriptor:
       process_bundle_descriptor.state_api_service_descriptor.url = (
-          worker_handler.state_api_service_descriptor().url)
+          state_api_service_descriptor.url)
 
     # Store the required side inputs into state so it is accessible for the
     # worker when it runs this bundle.
@@ -930,7 +945,8 @@ def input_for(transform_id, input_id):
         # merged results. Without residual_roots, pipeline stops earlier and we
         # may miss some data.
         bundle_manager._num_workers = 1
-        bundle_manager._skip_registration = True
+        # TODO(BEAM-8486): this should be changed to _registered
+        bundle_manager._skip_registration = True  # type: ignore[attr-defined]
         last_result, splits = bundle_manager.process_bundle(
             deferred_inputs, data_output)
         last_sent = deferred_inputs
@@ -1007,7 +1023,8 @@ def _extract_endpoints(stage,  # type: fn_api_runner_transforms.Stage
 
   # These classes are used to interact with the worker.
 
-  class StateServicer(beam_fn_api_pb2_grpc.BeamFnStateServicer):
+  class StateServicer(beam_fn_api_pb2_grpc.BeamFnStateServicer,
+                      sdk_worker.StateHandler):
 
     class CopyOnWriteState(object):
       def __init__(self, underlying):
@@ -1089,11 +1106,11 @@ def get_raw(self,
           else:
             token_base, index = continuation_token.split(':')
             ix = int(index)
-            full_state = self._continuations[token_base]
-            if ix == len(full_state):
+            full_state_cont = self._continuations[token_base]
+            if ix == len(full_state_cont):
               return b'', None
             else:
-              return full_state[ix], '%s:%d' % (token_base, ix + 1)
+              return full_state_cont[ix], '%s:%d' % (token_base, ix + 1)
         else:
           assert not continuation_token
           return b''.join(full_state), None
@@ -1163,16 +1180,16 @@ class SingletonStateHandlerFactory(sdk_worker.StateHandlerFactory):
     """A singleton cache for a StateServicer."""
 
     def __init__(self, state_handler):
-      # type: (sdk_worker.StateHandler) -> None
+      # type: (sdk_worker.CachingStateHandler) -> None
       self._state_handler = state_handler
 
     def create_state_handler(self, api_service_descriptor):
-      # type: (endpoints_pb2.ApiServiceDescriptor) -> sdk_worker.StateHandler
+      # type: (endpoints_pb2.ApiServiceDescriptor) -> sdk_worker.CachingStateHandler
       """Returns the singleton state handler."""
       return self._state_handler
 
     def close(self):
-      # type: (...) -> None
+      # type: () -> None
       """Does nothing."""
       pass
 
@@ -1229,6 +1246,9 @@ class WorkerHandler(object):
   _worker_id_counter = -1
   _lock = threading.Lock()
 
+  control_conn = None  # type: ControlConnection
+  data_conn = None  # type: data_plane._GrpcDataChannel
+
   def __init__(self,
                control_handler,
                data_plane_handler,
@@ -1310,7 +1330,7 @@ class EmbeddedWorkerHandler(WorkerHandler):
 
   def __init__(self,
                unused_payload,  # type: None
-               state,
+               state,  # type: sdk_worker.StateHandler
                provision_info,  # type: Optional[ExtendedProvisionInfo]
                unused_grpc_server=None
               ):
@@ -1444,10 +1464,10 @@ def __init__(self,
     # If we have provision info, serve these off the control port as well.
     if self.provision_info:
       if self.provision_info.provision_info:
-        provision_info = self.provision_info.provision_info
-        if not provision_info.worker_id:
-          provision_info = copy.copy(provision_info)
-          provision_info.worker_id = str(uuid.uuid4())
+        provision_info_proto = self.provision_info.provision_info
+        if not provision_info_proto.worker_id:
+          provision_info_proto = copy.copy(provision_info_proto)
+          provision_info_proto.worker_id = str(uuid.uuid4())
         beam_provision_api_pb2_grpc.add_ProvisionServiceServicer_to_server(
             BasicProvisionService(self.provision_info.provision_info),
             self.control_server)
@@ -1872,6 +1892,7 @@ def _send_input_to_worker(self,
                             read_transform_id,  # type: str
                             byte_streams
                            ):
+    assert self._worker_handler is not None
     data_out = self._worker_handler.data_conn.output_stream(
         process_bundle_id, read_transform_id)
     for byte_stream in byte_streams:
@@ -1883,6 +1904,7 @@ def _register_bundle_descriptor(self):
     if self._registered:
       registration_future = None
     else:
+      assert self._worker_handler is not None
       process_bundle_registration = beam_fn_api_pb2.InstructionRequest(
           register=beam_fn_api_pb2.RegisterRequest(
               process_bundle_descriptor=[self._bundle_descriptor]))
@@ -1930,6 +1952,8 @@ def _generate_splits_for_testing(self,
     self._send_input_to_worker(
         process_bundle_id, read_transform_id, [byte_stream])
 
+    assert self._worker_handler is not None
+
     # Execute the requested splits.
     while not done:
       if split_fraction is None:
@@ -2067,13 +2091,18 @@ def process_bundle(self,
 
     merged_result = None  # type: Optional[beam_fn_api_pb2.InstructionResponse]
     split_result_list = []  # type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]
-    with UnboundedThreadPoolExecutor() as executor:
-      for result, split_result in executor.map(lambda part: BundleManager(
+
+    def execute(part_map):
+      # type: (...) -> BundleProcessResult
+      bundle_manager = BundleManager(
           self._worker_handler_list, self._get_buffer,
           self._get_input_coder_impl, self._bundle_descriptor,
           self._progress_frequency, self._registered,
-          cache_token_generator=self._cache_token_generator).process_bundle(
-              part, expected_outputs), part_inputs):
+          cache_token_generator=self._cache_token_generator)
+      return bundle_manager.process_bundle(part_map, expected_outputs)
+
+    with UnboundedThreadPoolExecutor() as executor:
+      for result, split_result in executor.map(execute, part_inputs):
 
         split_result_list += split_result
         if merged_result is None:
@@ -2086,6 +2115,7 @@ def process_bundle(self,
                           result.process_bundle.monitoring_infos,
                           merged_result.process_bundle.monitoring_infos))),
               error=result.error or merged_result.error)
+    assert merged_result is not None
 
     return merged_result, split_result_list
 
diff --git a/sdks/python/apache_beam/runners/portability/fn_api_runner_transforms.py b/sdks/python/apache_beam/runners/portability/fn_api_runner_transforms.py
index 97c6cfab46ad..d206122e39ca 100644
--- a/sdks/python/apache_beam/runners/portability/fn_api_runner_transforms.py
+++ b/sdks/python/apache_beam/runners/portability/fn_api_runner_transforms.py
@@ -277,7 +277,7 @@ def executable_stage_transform(self,
           stage_components.transforms[side.transform_id].inputs[side.local_name]
           for side in side_inputs
       }, main_input=main_input_id)
-      payload = beam_runner_api_pb2.ExecutableStagePayload(
+      exec_payload = beam_runner_api_pb2.ExecutableStagePayload(
           environment=components.environments[self.environment],
           input=main_input_id,
           outputs=external_outputs,
@@ -291,7 +291,7 @@ def executable_stage_transform(self,
           unique_name=unique_name(None, self.name),
           spec=beam_runner_api_pb2.FunctionSpec(
               urn='beam:runner:executable_stage:v1',
-              payload=payload.SerializeToString()),
+              payload=exec_payload.SerializeToString()),
           inputs=named_inputs,
           outputs={'output_%d' % ix: pcoll
                    for ix, pcoll in enumerate(external_outputs)},)
@@ -317,7 +317,8 @@ def wrapper(self, *args):
 class TransformContext(object):
 
   _KNOWN_CODER_URNS = set(
-      value.urn for value in common_urns.coders.__dict__.values())
+      value.urn for key, value in common_urns.coders.__dict__.items()
+      if not key.startswith('_'))
 
   def __init__(self,
                components,  # type: beam_runner_api_pb2.Components
@@ -580,15 +581,18 @@ def annotate_downstream_side_inputs(stages, pipeline_context):
   This representation is also amenable to simple recomputation on fusion.
   """
   consumers = collections.defaultdict(list)  # type: DefaultDict[str, List[Stage]]
+  def get_all_side_inputs():
+    # type: () -> Set[str]
+    all_side_inputs = set()  # type: Set[str]
+    for stage in stages:
+      for transform in stage.transforms:
+        for input in transform.inputs.values():
+          consumers[input].append(stage)
+      for si in stage.side_inputs():
+        all_side_inputs.add(si)
+    return all_side_inputs
 
-  all_side_inputs = set()
-  for stage in stages:
-    for transform in stage.transforms:
-      for input in transform.inputs.values():
-        consumers[input].append(stage)
-    for si in stage.side_inputs():
-      all_side_inputs.add(si)
-  all_side_inputs = frozenset(all_side_inputs)
+  all_side_inputs = frozenset(get_all_side_inputs())
 
   downstream_side_inputs_by_stage = {}  # type: Dict[Stage, FrozenSet[str]]
 
@@ -1440,4 +1444,5 @@ def create_buffer_id(name, kind='materialize'):
 def split_buffer_id(buffer_id):
   # type: (bytes) -> Tuple[str, str]
   """A buffer id is "kind:pcollection_id". Split into (kind, pcoll_id). """
-  return buffer_id.decode('utf-8').split(':', 1)
+  kind, pcoll_id = buffer_id.decode('utf-8').split(':', 1)
+  return kind, pcoll_id
diff --git a/sdks/python/apache_beam/runners/portability/portable_runner.py b/sdks/python/apache_beam/runners/portability/portable_runner.py
index 071531136bc9..b5dd1d40b6af 100644
--- a/sdks/python/apache_beam/runners/portability/portable_runner.py
+++ b/sdks/python/apache_beam/runners/portability/portable_runner.py
@@ -88,7 +88,7 @@ def __init__(self):
 
   @staticmethod
   def _create_environment(options):
-    # type: (PipelineOptions) -> beam_runner_api_pb2.Environment
+    # type: (PipelineOptions) -> environments.Environment
     portable_options = options.view_as(PortableOptions)
     # Do not set a Runner. Otherwise this can cause problems in Java's
     # PipelineOptions, i.e. ClassNotFoundException, if the corresponding Runner
diff --git a/sdks/python/apache_beam/runners/worker/bundle_processor.py b/sdks/python/apache_beam/runners/worker/bundle_processor.py
index 052582c22f4f..0612e5e9a740 100644
--- a/sdks/python/apache_beam/runners/worker/bundle_processor.py
+++ b/sdks/python/apache_beam/runners/worker/bundle_processor.py
@@ -28,7 +28,6 @@
 import json
 import logging
 import random
-import re
 import threading
 from builtins import next
 from builtins import object
@@ -160,7 +159,7 @@ class DataInputOperation(RunnerIOOperation):
   """A source-like operation that gathers input from the runner."""
 
   def __init__(self,
-               operation_name,  # type: str
+               operation_name,  # type: Union[str, common.NameContext]
                step_name,
                consumers,  # type: Mapping[Any, Iterable[operations.Operation]]
                counter_factory,
@@ -246,6 +245,7 @@ def try_split(self, fraction_of_remainder, total_buffer_size):
       if stop_index < self.stop:
         self.stop = stop_index
         return self.stop - 1, None, None, self.stop
+    return None
 
   def progress_metrics(self):
     # type: () -> beam_fn_api_pb2.Metrics.PTransform
@@ -265,7 +265,7 @@ def finish(self):
 
 class _StateBackedIterable(object):
   def __init__(self,
-               state_handler,
+               state_handler,  # type: sdk_worker.CachingStateHandler
                state_key,  # type: beam_fn_api_pb2.StateKey
                coder_or_impl,  # type: Union[coders.Coder, coder_impl.CoderImpl]
                is_cached=False
@@ -294,7 +294,7 @@ def __reduce__(self):
 
 class StateBackedSideInputMap(object):
   def __init__(self,
-               state_handler,
+               state_handler,  # type: sdk_worker.CachingStateHandler
                transform_id,  # type: str
                tag,  # type: Optional[str]
                side_input_data,  # type: pvalue.SideInputData
@@ -434,7 +434,7 @@ def __iter__(self):
 class SynchronousBagRuntimeState(userstate.BagRuntimeState):
 
   def __init__(self,
-               state_handler,
+               state_handler,  # type: sdk_worker.CachingStateHandler
                state_key,  # type: beam_fn_api_pb2.StateKey
                value_coder  # type: coders.Coder
               ):
@@ -481,7 +481,7 @@ def commit(self):
 class SynchronousSetRuntimeState(userstate.SetRuntimeState):
 
   def __init__(self,
-               state_handler,
+               state_handler,  # type: sdk_worker.CachingStateHandler
                state_key,  # type: beam_fn_api_pb2.StateKey
                value_coder  # type: coders.Coder
               ):
@@ -578,7 +578,7 @@ class FnApiUserStateContext(userstate.UserStateContext):
   """Interface for state and timers from SDK to Fn API servicer of state.."""
 
   def __init__(self,
-               state_handler,
+               state_handler,  # type: sdk_worker.CachingStateHandler
                transform_id,  # type: str
                key_coder,  # type: coders.Coder
                window_coder,  # type: coders.Coder
@@ -616,6 +616,7 @@ def get_timer(self,
                 window  # type: windowed_value.BoundedWindow
                ):
     # type: (...) -> OutputTimer
+    assert self._timer_receivers is not None
     return OutputTimer(
         key, window, self._timer_receivers[timer_spec.name])
 
@@ -695,7 +696,7 @@ class BundleProcessor(object):
 
   def __init__(self,
                process_bundle_descriptor,  # type: beam_fn_api_pb2.ProcessBundleDescriptor
-               state_handler,  # type: Union[FnApiRunner.StateServicer, GrpcStateHandler]
+               state_handler,  # type: sdk_worker.CachingStateHandler
                data_channel_factory  # type: data_plane.DataChannelFactory
               ):
     # type: (...) -> None
@@ -868,6 +869,7 @@ def delayed_bundle_application(self,
                                  deferred_remainder  # type: Tuple[windowed_value.WindowedValue, Timestamp]
                                 ):
     # type: (...) -> beam_fn_api_pb2.DelayedBundleApplication
+    assert op.input_info is not None
     # TODO(SDF): For non-root nodes, need main_input_coder + residual_coder.
     ((element_and_restriction, output_watermark),
      deferred_watermark) = deferred_remainder
@@ -1011,7 +1013,7 @@ def __init__(self,
                data_channel_factory,  # type: data_plane.DataChannelFactory
                counter_factory,
                state_sampler,  # type: statesampler.StateSampler
-               state_handler
+               state_handler  # type: sdk_worker.CachingStateHandler
               ):
     self.descriptor = descriptor
     self.data_channel_factory = data_channel_factory
@@ -1130,19 +1132,26 @@ def process(self, windowed_value):
 
 @BeamTransformFactory.register_urn(
     DATA_INPUT_URN, beam_fn_api_pb2.RemoteGrpcPort)
-def create(factory, transform_id, transform_proto, grpc_port, consumers):
+def create_source_runner(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    grpc_port,  # type: beam_fn_api_pb2.RemoteGrpcPort
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> DataInputOperation
   # Timers are the one special case where we don't want to call the
   # (unlabeled) operation.process() method, which we detect here.
   # TODO(robertwb): Consider generalizing if there are any more cases.
   output_pcoll = only_element(transform_proto.outputs.values())
   output_consumers = only_element(consumers.values())
-  if (len(output_consumers) == 1
-      and isinstance(only_element(output_consumers), operations.DoOperation)):
+  if len(output_consumers) == 1:
     do_op = only_element(output_consumers)
-    for tag, pcoll_id in do_op.timer_inputs.items():
-      if pcoll_id == output_pcoll:
-        output_consumers[:] = [TimerConsumer(tag, do_op)]
-        break
+    if isinstance(do_op, operations.DoOperation):
+      for tag, pcoll_id in do_op.timer_inputs.items():
+        if pcoll_id == output_pcoll:
+          output_consumers[:] = [TimerConsumer(tag, do_op)]
+          break
 
   if grpc_port.coder_id:
     output_coder = factory.get_coder(grpc_port.coder_id)
@@ -1165,7 +1174,14 @@ def create(factory, transform_id, transform_proto, grpc_port, consumers):
 
 @BeamTransformFactory.register_urn(
     DATA_OUTPUT_URN, beam_fn_api_pb2.RemoteGrpcPort)
-def create(factory, transform_id, transform_proto, grpc_port, consumers):
+def create_sink_runner(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    grpc_port,  # type: beam_fn_api_pb2.RemoteGrpcPort
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> DataOutputOperation
   if grpc_port.coder_id:
     output_coder = factory.get_coder(grpc_port.coder_id)
   else:
@@ -1186,7 +1202,14 @@ def create(factory, transform_id, transform_proto, grpc_port, consumers):
 
 
 @BeamTransformFactory.register_urn(OLD_DATAFLOW_RUNNER_HARNESS_READ_URN, None)
-def create(factory, transform_id, transform_proto, parameter, consumers):
+def create_source_java(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.ReadOperation
   # The Dataflow runner harness strips the base64 encoding.
   source = pickler.loads(base64.b64encode(parameter))
   spec = operation_specs.WorkerRead(
@@ -1228,7 +1251,14 @@ def create_deprecated_read(
 
 @BeamTransformFactory.register_urn(
     python_urns.IMPULSE_READ_TRANSFORM, beam_runner_api_pb2.ReadPayload)
-def create(factory, transform_id, transform_proto, parameter, consumers):
+def create_read_from_impulse_python(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,  # type: beam_runner_api_pb2.ReadPayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.ImpulseReadOperation
   return operations.ImpulseReadOperation(
       common.NameContext(transform_proto.unique_name, transform_id),
       factory.counter_factory,
@@ -1240,7 +1270,13 @@ def create(factory, transform_id, transform_proto, parameter, consumers):
 
 
 @BeamTransformFactory.register_urn(OLD_DATAFLOW_RUNNER_HARNESS_PARDO_URN, None)
-def create(factory, transform_id, transform_proto, serialized_fn, consumers):
+def create_dofn_javasdk(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    serialized_fn,
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   return _create_pardo_operation(
       factory, transform_id, transform_proto, consumers, serialized_fn)
 
@@ -1248,7 +1284,7 @@ def create(factory, transform_id, transform_proto, serialized_fn, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.sdf_components.PAIR_WITH_RESTRICTION.urn,
     beam_runner_api_pb2.ParDoPayload)
-def create(*args):
+def create_pair_with_restriction(*args):
 
   class PairWithRestriction(beam.DoFn):
     def __init__(self, fn, restriction_provider):
@@ -1269,7 +1305,7 @@ def process(
 @BeamTransformFactory.register_urn(
     common_urns.sdf_components.SPLIT_AND_SIZE_RESTRICTIONS.urn,
     beam_runner_api_pb2.ParDoPayload)
-def create(*args):
+def create_split_and_size_restrictions(*args):
 
   class SplitAndSizeRestrictions(beam.DoFn):
     def __init__(self, fn, restriction_provider):
@@ -1287,7 +1323,13 @@ def process(self, element_restriction, *args, **kwargs):
 @BeamTransformFactory.register_urn(
     common_urns.sdf_components.PROCESS_SIZED_ELEMENTS_AND_RESTRICTIONS.urn,
     beam_runner_api_pb2.ParDoPayload)
-def create(factory, transform_id, transform_proto, parameter, consumers):
+def create_process_sized_elements_and_restrictions(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,  # type: beam_runner_api_pb2.ParDoPayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   assert parameter.do_fn.urn == python_urns.PICKLED_DOFN_INFO
   serialized_fn = parameter.do_fn.payload
   return _create_pardo_operation(
@@ -1312,7 +1354,14 @@ def _create_sdf_operation(
 
 @BeamTransformFactory.register_urn(
     common_urns.primitives.PAR_DO.urn, beam_runner_api_pb2.ParDoPayload)
-def create(factory, transform_id, transform_proto, parameter, consumers):
+def create_par_do(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,  # type: beam_runner_api_pb2.ParDoPayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.DoOperation
   assert parameter.do_fn.urn == python_urns.PICKLED_DOFN_INFO
   serialized_fn = parameter.do_fn.payload
   return _create_pardo_operation(
@@ -1336,9 +1385,7 @@ def _create_pardo_operation(
         (tag, beam.pvalue.SideInputData.from_runner_api(si, factory.context))
         for tag, si in pardo_proto.side_inputs.items()]
     tagged_side_inputs.sort(
-        key=lambda tag_si: int(re.match('side([0-9]+)(-.*)?$',
-                                        tag_si[0],
-                                        re.DOTALL).group(1)))
+        key=lambda tag_si: sideinputs.get_sideinput_index(tag_si[0]))
     side_input_maps = [
         StateBackedSideInputMap(
             factory.state_handler,
@@ -1447,7 +1494,13 @@ def _create_simple_pardo_operation(factory,  # type: BeamTransformFactory
 @BeamTransformFactory.register_urn(
     common_urns.primitives.ASSIGN_WINDOWS.urn,
     beam_runner_api_pb2.WindowingStrategy)
-def create(factory, transform_id, transform_proto, parameter, consumers):
+def create_assign_windows(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,  # type: beam_runner_api_pb2.WindowingStrategy
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   class WindowIntoDoFn(beam.DoFn):
     def __init__(self, windowing):
       self.windowing = windowing
@@ -1466,7 +1519,14 @@ def process(self, element, timestamp=beam.DoFn.TimestampParam,
 
 
 @BeamTransformFactory.register_urn(IDENTITY_DOFN_URN, None)
-def create(factory, transform_id, transform_proto, unused_parameter, consumers):
+def create_identity_dofn(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    parameter,
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.FlattenOperation
   return factory.augment_oldstyle_op(
       operations.FlattenOperation(
           common.NameContext(transform_proto.unique_name, transform_id),
@@ -1481,7 +1541,14 @@ def create(factory, transform_id, transform_proto, unused_parameter, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.combine_components.COMBINE_PER_KEY_PRECOMBINE.urn,
     beam_runner_api_pb2.CombinePayload)
-def create(factory, transform_id, transform_proto, payload, consumers):
+def create_combine_per_key_precombine(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    payload,  # type: beam_runner_api_pb2.CombinePayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.PGBKCVOperation
   serialized_combine_fn = pickler.dumps(
       (beam.CombineFn.from_runner_api(payload.combine_fn, factory.context),
        [], {}))
@@ -1502,7 +1569,13 @@ def create(factory, transform_id, transform_proto, payload, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.combine_components.COMBINE_PER_KEY_MERGE_ACCUMULATORS.urn,
     beam_runner_api_pb2.CombinePayload)
-def create(factory, transform_id, transform_proto, payload, consumers):
+def create_combbine_per_key_merge_accumulators(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    payload,  # type: beam_runner_api_pb2.CombinePayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   return _create_combine_phase_operation(
       factory, transform_id, transform_proto, payload, consumers, 'merge')
 
@@ -1510,7 +1583,13 @@ def create(factory, transform_id, transform_proto, payload, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.combine_components.COMBINE_PER_KEY_EXTRACT_OUTPUTS.urn,
     beam_runner_api_pb2.CombinePayload)
-def create(factory, transform_id, transform_proto, payload, consumers):
+def create_combine_per_key_extract_outputs(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    payload,  # type: beam_runner_api_pb2.CombinePayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   return _create_combine_phase_operation(
       factory, transform_id, transform_proto, payload, consumers, 'extract')
 
@@ -1518,7 +1597,13 @@ def create(factory, transform_id, transform_proto, payload, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.combine_components.COMBINE_GROUPED_VALUES.urn,
     beam_runner_api_pb2.CombinePayload)
-def create(factory, transform_id, transform_proto, payload, consumers):
+def create_combine_grouped_values(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    payload,  # type: beam_runner_api_pb2.CombinePayload
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   return _create_combine_phase_operation(
       factory, transform_id, transform_proto, payload, consumers, 'all')
 
@@ -1544,7 +1629,14 @@ def _create_combine_phase_operation(
 
 
 @BeamTransformFactory.register_urn(common_urns.primitives.FLATTEN.urn, None)
-def create(factory, transform_id, transform_proto, unused_parameter, consumers):
+def create_flatten(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    payload,
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
+  # type: (...) -> operations.FlattenOperation
   return factory.augment_oldstyle_op(
       operations.FlattenOperation(
           common.NameContext(transform_proto.unique_name, transform_id),
@@ -1560,7 +1652,13 @@ def create(factory, transform_id, transform_proto, unused_parameter, consumers):
 @BeamTransformFactory.register_urn(
     common_urns.primitives.MAP_WINDOWS.urn,
     beam_runner_api_pb2.FunctionSpec)
-def create(factory, transform_id, transform_proto, mapping_fn_spec, consumers):
+def create_map_windows(
+    factory,  # type: BeamTransformFactory
+    transform_id,  # type: str
+    transform_proto,  # type: beam_runner_api_pb2.PTransform
+    mapping_fn_spec,  # type: beam_runner_api_pb2.SdkFunctionSpec
+    consumers  # type: Dict[str, List[operations.Operation]]
+):
   assert mapping_fn_spec.urn == python_urns.PICKLED_WINDOW_MAPPING_FN
   window_mapping_fn = pickler.loads(mapping_fn_spec.payload)
 
diff --git a/sdks/python/apache_beam/runners/worker/operations.py b/sdks/python/apache_beam/runners/worker/operations.py
index 6bb8bf1c6874..aa950571fde0 100644
--- a/sdks/python/apache_beam/runners/worker/operations.py
+++ b/sdks/python/apache_beam/runners/worker/operations.py
@@ -657,6 +657,7 @@ def process(self, o):
     with self.scoped_process_state:
       delayed_application = self.dofn_receiver.receive(o)
       if delayed_application:
+        assert self.execution_context is not None
         self.execution_context.delayed_applications.append(
             (self, delayed_application))
 
@@ -748,6 +749,7 @@ def __init__(self, *args, **kwargs):
 
   def process(self, o):
     # type: (WindowedValue) -> None
+    assert self.tagged_receivers is not None
     with self.scoped_process_state:
       try:
         with self.lock:
@@ -758,6 +760,7 @@ def process(self, o):
         # the lock.
         delayed_application = self.dofn_runner.process_with_sized_restriction(o)
         if delayed_application:
+          assert self.execution_context is not None
           self.execution_context.delayed_applications.append(
               (self, delayed_application))
       finally:
@@ -784,6 +787,7 @@ def progress_metrics(self):
       metrics = super(SdfProcessSizedElements, self).progress_metrics()
       current_element_progress = self.current_element_progress()
     if current_element_progress:
+      assert self.input_info is not None
       metrics.active_elements.measured.input_element_counts[
           self.input_info[1]] = 1
       metrics.active_elements.fraction_remaining = (
diff --git a/sdks/python/apache_beam/runners/worker/sdk_worker.py b/sdks/python/apache_beam/runners/worker/sdk_worker.py
index 66724a7e85e8..193ac1c1e14c 100644
--- a/sdks/python/apache_beam/runners/worker/sdk_worker.py
+++ b/sdks/python/apache_beam/runners/worker/sdk_worker.py
@@ -33,9 +33,11 @@
 from builtins import object
 from concurrent import futures
 from typing import TYPE_CHECKING
+from typing import Any
 from typing import Callable
 from typing import DefaultDict
 from typing import Dict
+from typing import Iterable
 from typing import Iterator
 from typing import List
 from typing import Optional
@@ -491,16 +493,43 @@ def maybe_profile(self, instruction_id):
       yield
 
 
-class StateHandlerFactory(with_metaclass(abc.ABCMeta, object)):
+class StateHandler(with_metaclass(abc.ABCMeta, object)):  # type: ignore[misc]
+  """An abstract object representing a ``StateHandler``."""
+
+  @abc.abstractmethod
+  def get_raw(self,
+              state_key,  # type: beam_fn_api_pb2.StateKey
+              continuation_token=None  # type: Optional[bytes]
+             ):
+    # type: (...) -> Tuple[bytes, Optional[bytes]]
+    raise NotImplementedError(type(self))
+
+  @abc.abstractmethod
+  def append_raw(self,
+                 state_key,  # type: beam_fn_api_pb2.StateKey
+                 data  # type: bytes
+                ):
+    # type: (...) -> _Future
+    raise NotImplementedError(type(self))
+
+  @abc.abstractmethod
+  def clear(self, state_key):
+    # type: (beam_fn_api_pb2.StateKey) -> _Future
+    raise NotImplementedError(type(self))
+
+
+class StateHandlerFactory(with_metaclass(abc.ABCMeta, object)):  # type: ignore[misc]
   """An abstract factory for creating ``DataChannel``."""
 
   @abc.abstractmethod
   def create_state_handler(self, api_service_descriptor):
+    # type: (endpoints_pb2.ApiServiceDescriptor) -> CachingStateHandler
     """Returns a ``StateHandler`` from the given ApiServiceDescriptor."""
     raise NotImplementedError(type(self))
 
   @abc.abstractmethod
   def close(self):
+    # type: () -> None
     """Close all channels that this factory owns."""
     raise NotImplementedError(type(self))
 
@@ -512,14 +541,14 @@ class GrpcStateHandlerFactory(StateHandlerFactory):
   """
 
   def __init__(self, state_cache, credentials=None):
-    self._state_handler_cache = {}  # type: Dict[str, GrpcStateHandler]
+    self._state_handler_cache = {}  # type: Dict[str, CachingStateHandler]
     self._lock = threading.Lock()
     self._throwing_state_handler = ThrowingStateHandler()
     self._credentials = credentials
     self._state_cache = state_cache
 
   def create_state_handler(self, api_service_descriptor):
-    # type: (endpoints_pb2.ApiServiceDescriptor) -> GrpcStateHandler
+    # type: (endpoints_pb2.ApiServiceDescriptor) -> CachingStateHandler
     if not api_service_descriptor:
       return self._throwing_state_handler
     url = api_service_descriptor.url
@@ -550,6 +579,7 @@ def create_state_handler(self, api_service_descriptor):
     return self._state_handler_cache[url]
 
   def close(self):
+    # type: () -> None
     _LOGGER.info('Closing all cached gRPC state handlers.')
     for _, state_handler in self._state_handler_cache.items():
       state_handler.done()
@@ -557,15 +587,15 @@ def close(self):
     self._state_cache.evict_all()
 
 
-class ThrowingStateHandler(object):
+class ThrowingStateHandler(StateHandler):
   """A state handler that errors on any requests."""
 
-  def blocking_get(self, state_key, coder):
+  def get_raw(self, state_key, coder):
     raise RuntimeError(
         'Unable to handle state requests for ProcessBundleDescriptor without '
         'state ApiServiceDescriptor for state key %s.' % state_key)
 
-  def append(self, state_key, coder, elements):
+  def append_raw(self, state_key, coder, elements):
     raise RuntimeError(
         'Unable to handle state requests for ProcessBundleDescriptor without '
         'state ApiServiceDescriptor for state key %s.' % state_key)
@@ -576,7 +606,7 @@ def clear(self, state_key):
         'state ApiServiceDescriptor for state key %s.' % state_key)
 
 
-class GrpcStateHandler(object):
+class GrpcStateHandler(StateHandler):
 
   _DONE = object()
 
@@ -674,6 +704,7 @@ def _request(self, request):
     return future
 
   def _blocking_request(self, request):
+    # type: (beam_fn_api_pb2.StateRequest) -> beam_fn_api_pb2.StateResponse
     req_future = self._request(request)
     while not req_future.wait(timeout=1):
       if self._exc_info:
@@ -702,7 +733,10 @@ def _next_id(self):
 class CachingStateHandler(object):
   """ A State handler which retrieves and caches state. """
 
-  def __init__(self, global_state_cache, underlying_state):
+  def __init__(self,
+               global_state_cache,  # type: StateCache
+               underlying_state  # type: StateHandler
+              ):
     self._underlying = underlying_state
     self._state_cache = global_state_cache
     self._context = threading.local()
@@ -728,7 +762,12 @@ def process_instruction_id(self, bundle_id, cache_tokens):
     finally:
       self._context.cache_token = None
 
-  def blocking_get(self, state_key, coder, is_cached=False):
+  def blocking_get(self,
+                   state_key,  # type: beam_fn_api_pb2.StateKey
+                   coder,  # type: coder_impl.CoderImpl
+                   is_cached=False
+                  ):
+    # type: (...) -> Iterator[Any]
     if not self._should_be_cached(is_cached):
       # Cache disabled / no cache token. Can't do a lookup/store in the cache.
       # Fall back to lazily materializing the state, one element at a time.
@@ -769,6 +808,7 @@ def extend(self,
     return self._underlying.append_raw(state_key, out.get())
 
   def clear(self, state_key, is_cached=False):
+    # type: (beam_fn_api_pb2.StateKey, bool) -> _Future
     if self._should_be_cached(is_cached):
       cache_key = self._convert_to_cache_key(state_key)
       self._state_cache.clear(cache_key, self._context.cache_token)
@@ -778,7 +818,11 @@ def done(self):
     # type: () -> None
     self._underlying.done()
 
-  def _materialize_iter(self, state_key, coder):
+  def _materialize_iter(self,
+                        state_key,  # type: beam_fn_api_pb2.StateKey
+                        coder  # type: coder_impl.CoderImpl
+                       ):
+    # type: (...) -> Iterator[Any]
     """Materializes the state lazily, one element at a time.
        :return A generator which returns the next element if advanced.
     """
diff --git a/sdks/python/apache_beam/runners/worker/statecache.py b/sdks/python/apache_beam/runners/worker/statecache.py
index e8fd2aebcbdf..1e304c459755 100644
--- a/sdks/python/apache_beam/runners/worker/statecache.py
+++ b/sdks/python/apache_beam/runners/worker/statecache.py
@@ -23,17 +23,24 @@
 import collections
 import logging
 import threading
+from typing import Callable
+from typing import DefaultDict
+from typing import Hashable
+from typing import Set
+from typing import TypeVar
 
 from apache_beam.metrics import monitoring_infos
 
 _LOGGER = logging.getLogger(__name__)
 
+CallableT = TypeVar('CallableT', bound='Callable')
+
 
 class Metrics(object):
   """Metrics container for state cache metrics."""
 
   # A set of all registered metrics
-  ALL_METRICS = set()
+  ALL_METRICS = set()  # type: Set[Hashable]
   PREFIX = "beam:metric:statecache:"
 
   def __init__(self):
@@ -44,12 +51,14 @@ def initialize(self):
     """
     if hasattr(self._context, 'metrics'):
       return # Already initialized
-    self._context.metrics = collections.defaultdict(int)
+    self._context.metrics = collections.defaultdict(int)  # type: DefaultDict[Hashable, int]
 
   def count(self, name):
+    # type: (str) -> None
     self._context.metrics[name] += 1
 
   def hit_miss(self, total_name, hit_miss_name):
+    # type: (str, str) -> None
     self._context.metrics[total_name] += 1
     self._context.metrics[hit_miss_name] += 1
 
@@ -80,6 +89,7 @@ def get_monitoring_infos(self, cache_size, cache_capacity):
 
   @staticmethod
   def counter_hit_miss(total_name, hit_name, miss_name):
+    # type: (str, str, str) -> Callable[[CallableT], CallableT]
     """Decorator for counting function calls and whether
        the return value equals None (=miss) or not (=hit)."""
     Metrics.ALL_METRICS.update([total_name, hit_name, miss_name])
@@ -100,6 +110,7 @@ def reporter(self, *args, **kwargs):
 
   @staticmethod
   def counter(metric_name):
+    # type: (str) -> Callable[[CallableT], CallableT]
     """Decorator for counting function calls."""
     Metrics.ALL_METRICS.add(metric_name)
 
diff --git a/sdks/python/apache_beam/runners/worker/statesampler.py b/sdks/python/apache_beam/runners/worker/statesampler.py
index a9de8b1a89a3..36a656803a9f 100644
--- a/sdks/python/apache_beam/runners/worker/statesampler.py
+++ b/sdks/python/apache_beam/runners/worker/statesampler.py
@@ -23,9 +23,9 @@
 
 import contextlib
 import threading
-from collections import namedtuple
 from typing import TYPE_CHECKING
 from typing import Dict
+from typing import NamedTuple
 from typing import Optional
 from typing import Union
 
@@ -82,12 +82,12 @@ def for_test():
   return get_current_tracker()
 
 
-StateSamplerInfo = namedtuple(
+StateSamplerInfo = NamedTuple(
     'StateSamplerInfo',
-    ['state_name',
-     'transition_count',
-     'time_since_transition',
-     'tracked_thread'])
+    [('state_name', CounterName),
+     ('transition_count', int),
+     ('time_since_transition', int),
+     ('tracked_thread', Optional[threading.Thread])])
 
 
 # Default period for sampling current state of pipeline execution.
diff --git a/sdks/python/apache_beam/runners/worker/statesampler_slow.py b/sdks/python/apache_beam/runners/worker/statesampler_slow.py
index fb3cbf633063..0034046938ae 100644
--- a/sdks/python/apache_beam/runners/worker/statesampler_slow.py
+++ b/sdks/python/apache_beam/runners/worker/statesampler_slow.py
@@ -80,8 +80,7 @@ def stop(self):
 
   def reset(self):
     # type: () -> None
-    for state in self._states_by_name.values():
-      state.nsecs = 0
+    pass
 
 
 class ScopedState(object):
diff --git a/sdks/python/apache_beam/testing/util_test.py b/sdks/python/apache_beam/testing/util_test.py
index 6716b0588cd6..c9385ee95e71 100644
--- a/sdks/python/apache_beam/testing/util_test.py
+++ b/sdks/python/apache_beam/testing/util_test.py
@@ -140,7 +140,7 @@ def test_assert_that_passes_is_not_empty(self):
     with TestPipeline() as p:
       assert_that(p | Create([1, 2, 3]), is_not_empty())
 
-  def test_assert_that_fails_on_empty_expected(self):
+  def test_assert_that_fails_on_is_not_empty_expected(self):
     with self.assertRaises(BeamAssertException):
       with TestPipeline() as p:
         assert_that(p | Create([]), is_not_empty())
diff --git a/sdks/python/apache_beam/transforms/combiners.py b/sdks/python/apache_beam/transforms/combiners.py
index 0aedaf770850..80bbdfccc326 100644
--- a/sdks/python/apache_beam/transforms/combiners.py
+++ b/sdks/python/apache_beam/transforms/combiners.py
@@ -405,33 +405,44 @@ def __init__(self, n, less_than, key):
 
   def process(self, key_and_bundles):
     _, bundles = key_and_bundles
-    heap = []
-    for bundle in bundles:
-      if not heap:
-        if self._less_than or self._key:
-          heap = [
+
+    def push(hp, e):
+      if len(hp) < self._n:
+        heapq.heappush(hp, e)
+        return False
+      elif e < hp[0]:
+        # Because _TopPerBundle returns sorted lists, all other elements
+        # will also be smaller.
+        return True
+      else:
+        heapq.heappushpop(hp, e)
+        return False
+
+    if self._less_than or self._key:
+      heapc = []  # type: List[cy_combiners.ComparableValue]
+      for bundle in bundles:
+        if not heapc:
+          heapc = [
               cy_combiners.ComparableValue(element, self._less_than, self._key)
               for element in bundle]
-        else:
-          heap = bundle
-        continue
-      for element in reversed(bundle):
-        if self._less_than or self._key:
-          element = cy_combiners.ComparableValue(
-              element, self._less_than, self._key)
-        if len(heap) < self._n:
-          heapq.heappush(heap, element)
-        elif element < heap[0]:
-          # Because _TopPerBundle returns sorted lists, all other elements
-          # will also be smaller.
-          break
-        else:
-          heapq.heappushpop(heap, element)
+          continue
+        for element in reversed(bundle):
+          if push(heapc, cy_combiners.ComparableValue(
+              element, self._less_than, self._key)):
+            break
+      heapc.sort()
+      yield [wrapper.value for wrapper in reversed(heapc)]
 
-    heap.sort()
-    if self._less_than or self._key:
-      yield [wrapper.value for wrapper in reversed(heap)]
     else:
+      heap = []  # type: List[T]
+      for bundle in bundles:
+        if not heap:
+          heap = bundle
+          continue
+        for element in reversed(bundle):
+          if push(heap, element):
+            break
+      heap.sort()
       yield heap[::-1]
 
 
diff --git a/sdks/python/apache_beam/transforms/core.py b/sdks/python/apache_beam/transforms/core.py
index 5410b5bdee07..1d13f26f4878 100644
--- a/sdks/python/apache_beam/transforms/core.py
+++ b/sdks/python/apache_beam/transforms/core.py
@@ -25,7 +25,6 @@
 import inspect
 import logging
 import random
-import re
 import types
 import typing
 from builtins import map
@@ -50,6 +49,7 @@
 from apache_beam.transforms.display import HasDisplayData
 from apache_beam.transforms.ptransform import PTransform
 from apache_beam.transforms.ptransform import PTransformWithSideInputs
+from apache_beam.transforms.sideinputs import get_sideinput_index
 from apache_beam.transforms.userstate import StateSpec
 from apache_beam.transforms.userstate import TimerSpec
 from apache_beam.transforms.window import GlobalWindows
@@ -421,9 +421,10 @@ class WatermarkEstimator(object):
   TODO(BEAM-8537): Create WatermarkEstimatorProvider to support different types.
   """
   def __init__(self):
-    self._watermark = None
+    self._watermark = None  # type: typing.Optional[timestamp.Timestamp]
 
   def set_watermark(self, watermark):
+    # type: (timestamp.Timestamp) -> None
     """Update tracking output_watermark with latest output_watermark.
     This function is called inside an SDF.Process() to track the watermark of
     output element.
@@ -439,6 +440,7 @@ def set_watermark(self, watermark):
       self._watermark = min(self._watermark, watermark)
 
   def current_watermark(self):
+    # type: () -> typing.Optional[timestamp.Timestamp]
     """Get current output_watermark. This function is called by system."""
     return self._watermark
 
@@ -1338,7 +1340,7 @@ def from_runner_api_parameter(pardo_payload, context):
     # This is an ordered list stored as a dict (see the comments in
     # to_runner_api_parameter above).
     indexed_side_inputs = [
-        (int(re.match('side([0-9]+)(-.*)?$', tag).group(1)),
+        (get_sideinput_index(tag),
          pvalue.AsSideInput.from_runner_api(si, context))
         for tag, si in pardo_payload.side_inputs.items()]
     result.side_inputs = [si for _, si in sorted(indexed_side_inputs)]
diff --git a/sdks/python/apache_beam/transforms/display.py b/sdks/python/apache_beam/transforms/display.py
index 0f9fa530635c..21e9d32ac5c9 100644
--- a/sdks/python/apache_beam/transforms/display.py
+++ b/sdks/python/apache_beam/transforms/display.py
@@ -132,15 +132,18 @@ def _populate_items(self, display_data_dict):
 
   @classmethod
   def create_from_options(cls, pipeline_options):
-    """ Creates :class:`DisplayData` from a
+    """ Creates :class:`~apache_beam.transforms.display.DisplayData` from a
     :class:`~apache_beam.options.pipeline_options.PipelineOptions` instance.
 
-    When creating :class:`DisplayData`, this method will convert the value of
-    any item of a non-supported type to its string representation.
+    When creating :class:`~apache_beam.transforms.display.DisplayData`, this
+    method will convert the value of any item of a non-supported type to its
+    string representation.
     The normal :meth:`.create_from()` method rejects those items.
 
     Returns:
-      DisplayData: A :class:`DisplayData` instance with populated items.
+      ~apache_beam.transforms.display.DisplayData:
+        A :class:`~apache_beam.transforms.display.DisplayData` instance with
+        populated items.
 
     Raises:
       ~exceptions.ValueError: If the **has_display_data** argument is
@@ -160,10 +163,13 @@ def create_from_options(cls, pipeline_options):
 
   @classmethod
   def create_from(cls, has_display_data):
-    """ Creates :class:`DisplayData` from a :class:`HasDisplayData` instance.
+    """ Creates :class:`~apache_beam.transforms.display.DisplayData` from a
+    :class:`HasDisplayData` instance.
 
     Returns:
-      DisplayData: A :class:`DisplayData` instance with populated items.
+      ~apache_beam.transforms.display.DisplayData:
+        A :class:`~apache_beam.transforms.display.DisplayData` instance with
+        populated items.
 
     Raises:
       ~exceptions.ValueError: If the **has_display_data** argument is
diff --git a/sdks/python/apache_beam/transforms/environments.py b/sdks/python/apache_beam/transforms/environments.py
index f3b3c22a123c..9a0c7e29409a 100644
--- a/sdks/python/apache_beam/transforms/environments.py
+++ b/sdks/python/apache_beam/transforms/environments.py
@@ -26,6 +26,16 @@
 import json
 import logging
 import sys
+from typing import TYPE_CHECKING
+from typing import Any
+from typing import Callable
+from typing import Dict
+from typing import Optional
+from typing import Tuple
+from typing import Type
+from typing import TypeVar
+from typing import Union
+from typing import overload
 
 from google.protobuf import message
 
@@ -35,11 +45,20 @@
 from apache_beam.portability.api import endpoints_pb2
 from apache_beam.utils import proto_utils
 
+if TYPE_CHECKING:
+  from apache_beam.options.pipeline_options import PipelineOptions
+  from apache_beam.runners.pipeline_context import PipelineContext
+
 __all__ = ['Environment',
            'DockerEnvironment', 'ProcessEnvironment', 'ExternalEnvironment',
            'EmbeddedPythonEnvironment', 'EmbeddedPythonGrpcEnvironment',
            'SubprocessSDKEnvironment', 'RunnerAPIEnvironmentHolder']
 
+T = TypeVar('T')
+EnvironmentT = TypeVar('EnvironmentT', bound='Environment')
+ConstructorFn = Callable[
+    [Optional[Any], 'PipelineContext'],
+    Any]
 
 def looks_like_json(s):
   import re
@@ -55,12 +74,52 @@ class Environment(object):
   For internal use only. No backwards compatibility guarantees.
   """
 
-  _known_urns = {}
-  _urn_to_env_cls = {}
+  _known_urns = {}  # type: Dict[str, Tuple[Optional[type], ConstructorFn]]
+  _urn_to_env_cls = {}  # type: Dict[str, type]
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, Optional[Union[message.Message, bytes, str]]]
     raise NotImplementedError
 
+
+  @classmethod
+  @overload
+  def register_urn(cls,
+                   urn,  # type: str
+                   parameter_type,  # type: Type[T]
+                  ):
+    # type: (...) -> Callable[[Union[type, Callable[[T, PipelineContext], Any]]], Callable[[T, PipelineContext], Any]]
+    pass
+
+  @classmethod
+  @overload
+  def register_urn(cls,
+                   urn,  # type: str
+                   parameter_type,  # type: None
+                  ):
+    # type: (...) -> Callable[[Union[type, Callable[[bytes, PipelineContext], Any]]], Callable[[bytes, PipelineContext], Any]]
+    pass
+
+  @classmethod
+  @overload
+  def register_urn(cls,
+                   urn,  # type: str
+                   parameter_type,  # type: Type[T]
+                   constructor  # type: Callable[[T, PipelineContext], Any]
+                  ):
+    # type: (...) -> None
+    pass
+
+  @classmethod
+  @overload
+  def register_urn(cls,
+                   urn,  # type: str
+                   parameter_type,  # type: None
+                   constructor  # type: Callable[[bytes, PipelineContext], Any]
+                  ):
+    # type: (...) -> None
+    pass
+
   @classmethod
   def register_urn(cls, urn, parameter_type, constructor=None):
 
@@ -88,6 +147,7 @@ def get_env_cls_from_urn(cls, urn):
     return cls._urn_to_env_cls[urn]
 
   def to_runner_api(self, context):
+    # type: (PipelineContext) -> beam_runner_api_pb2.Environment
     urn, typed_param = self.to_runner_api_parameter(context)
     return beam_runner_api_pb2.Environment(
         urn=urn,
@@ -99,7 +159,11 @@ def to_runner_api(self, context):
     )
 
   @classmethod
-  def from_runner_api(cls, proto, context):
+  def from_runner_api(cls,
+                      proto,  # type: Optional[beam_runner_api_pb2.FunctionSpec]
+                      context  # type: PipelineContext
+                     ):
+    # type: (...) -> Optional[Environment]
     if proto is None or not proto.urn:
       return None
     parameter_type, constructor = cls._known_urns[proto.urn]
@@ -115,6 +179,7 @@ def from_runner_api(cls, proto, context):
 
   @classmethod
   def from_options(cls, options):
+    # type: (Type[EnvironmentT], PipelineOptions) -> EnvironmentT
     """Creates an Environment object from PipelineOptions.
 
     Args:
@@ -148,6 +213,7 @@ def __repr__(self):
     return 'DockerEnvironment(container_image=%s)' % self.container_image
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, beam_runner_api_pb2.DockerPayload]
     return (common_urns.environments.DOCKER.urn,
             beam_runner_api_pb2.DockerPayload(
                 container_image=self.container_image))
@@ -158,6 +224,7 @@ def from_runner_api_parameter(payload, context):
 
   @classmethod
   def from_options(cls, options):
+    # type: (PipelineOptions) -> DockerEnvironment
     return cls(container_image=options.environment_config)
 
   @staticmethod
@@ -212,6 +279,7 @@ def __repr__(self):
     return 'ProcessEnvironment(%s)' % ','.join(repr_parts)
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, beam_runner_api_pb2.ProcessPayload]
     return (common_urns.environments.PROCESS.urn,
             beam_runner_api_pb2.ProcessPayload(
                 os=self.os,
@@ -257,6 +325,7 @@ def __repr__(self):
     return 'ExternalEnvironment(url=%s,params=%s)' % (self.url, self.params)
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, beam_runner_api_pb2.ExternalPayload]
     return (common_urns.environments.EXTERNAL.urn,
             beam_runner_api_pb2.ExternalPayload(
                 endpoint=endpoints_pb2.ApiServiceDescriptor(url=self.url),
@@ -297,6 +366,7 @@ def __hash__(self):
     return hash(self.__class__)
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, None]
     return python_urns.EMBEDDED_PYTHON, None
 
   @staticmethod
@@ -338,6 +408,7 @@ def __repr__(self):
     return 'EmbeddedPythonGrpcEnvironment(%s)' % ','.join(repr_parts)
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, bytes]
     params = {}
     if self.state_cache_size is not None:
       params['state_cache_size'] = self.state_cache_size
@@ -404,6 +475,7 @@ def __repr__(self):
     return 'SubprocessSDKEnvironment(command_string=%s)' % self.command_string
 
   def to_runner_api_parameter(self, context):
+    # type: (PipelineContext) -> Tuple[str, bytes]
     return python_urns.SUBPROCESS_SDK, self.command_string.encode('utf-8')
 
   @staticmethod
diff --git a/sdks/python/apache_beam/transforms/sideinputs.py b/sdks/python/apache_beam/transforms/sideinputs.py
index 8e57ede337da..79b82904cead 100644
--- a/sdks/python/apache_beam/transforms/sideinputs.py
+++ b/sdks/python/apache_beam/transforms/sideinputs.py
@@ -28,6 +28,7 @@
 
 from __future__ import absolute_import
 
+import re
 from builtins import object
 from typing import TYPE_CHECKING
 from typing import Any
@@ -60,6 +61,16 @@ def map_via_end(source_window):
   return map_via_end
 
 
+def get_sideinput_index(tag):
+  # type: (str) -> int
+  match = re.match('side([0-9]+)(-.*)?$', tag,
+                   re.DOTALL)
+  if match:
+    return int(match.group(1))
+  else:
+    raise RuntimeError("Invalid tag %r" % tag)
+
+
 class SideInputMap(object):
   """Represents a mapping of windows to side input values."""
 
diff --git a/sdks/python/apache_beam/transforms/userstate.py b/sdks/python/apache_beam/transforms/userstate.py
index dd2b2964dbfa..c9b203813290 100644
--- a/sdks/python/apache_beam/transforms/userstate.py
+++ b/sdks/python/apache_beam/transforms/userstate.py
@@ -50,8 +50,14 @@
 class StateSpec(object):
   """Specification for a user DoFn state cell."""
 
-  def __init__(self):
-    raise NotImplementedError
+  def __init__(self, name, coder):
+    # type: (str, Coder) -> None
+    if not isinstance(name, str):
+      raise TypeError("name is not a string")
+    if not isinstance(coder, Coder):
+      raise TypeError("coder is not of type Coder")
+    self.name = name
+    self.coder = coder
 
   def __repr__(self):
     return '%s(%s)' % (self.__class__.__name__, self.name)
@@ -63,13 +69,6 @@ def to_runner_api(self, context):
 class BagStateSpec(StateSpec):
   """Specification for a user DoFn bag state cell."""
 
-  def __init__(self, name, coder):
-    # type: (str, Coder) -> None
-    assert isinstance(name, str)
-    assert isinstance(coder, Coder)
-    self.name = name
-    self.coder = coder
-
   def to_runner_api(self, context):
     # type: (PipelineContext) -> beam_runner_api_pb2.StateSpec
     return beam_runner_api_pb2.StateSpec(
@@ -80,15 +79,6 @@ def to_runner_api(self, context):
 class SetStateSpec(StateSpec):
   """Specification for a user DoFn Set State cell"""
 
-  def __init__(self, name, coder):
-    # type: (str, Coder) -> None
-    if not isinstance(name, str):
-      raise TypeError("SetState name is not a string")
-    if not isinstance(coder, Coder):
-      raise TypeError("SetState coder is not of type Coder")
-    self.name = name
-    self.coder = coder
-
   def to_runner_api(self, context):
     return beam_runner_api_pb2.StateSpec(
         set_spec=beam_runner_api_pb2.SetStateSpec(
@@ -128,14 +118,11 @@ def __init__(self, name, coder=None, combine_fn=None):
       else:
         coder, combine_fn = None, coder
     self.combine_fn = CombineFn.maybe_from_callable(combine_fn)
+    # The coder here should be for the accumulator type of the given CombineFn.
     if coder is None:
       coder = self.combine_fn.get_accumulator_coder()
 
-    assert isinstance(name, str)
-    assert isinstance(coder, Coder)
-    self.name = name
-    # The coder here should be for the accumulator type of the given CombineFn.
-    self.coder = coder
+    super(CombiningValueStateSpec, self).__init__(name, coder)
 
   def to_runner_api(self, context):
     # type: (PipelineContext) -> beam_runner_api_pb2.StateSpec
diff --git a/sdks/python/apache_beam/transforms/userstate_test.py b/sdks/python/apache_beam/transforms/userstate_test.py
index 8c1ace064dd7..9c79551609be 100644
--- a/sdks/python/apache_beam/transforms/userstate_test.py
+++ b/sdks/python/apache_beam/transforms/userstate_test.py
@@ -113,10 +113,10 @@ def test_validate_dofn(self, unused_mock):
 
   def test_spec_construction(self):
     BagStateSpec('statename', VarIntCoder())
-    with self.assertRaises(AssertionError):
+    with self.assertRaises(TypeError):
       BagStateSpec(123, VarIntCoder())
     CombiningValueStateSpec('statename', VarIntCoder(), TopCombineFn(10))
-    with self.assertRaises(AssertionError):
+    with self.assertRaises(TypeError):
       CombiningValueStateSpec(123, VarIntCoder(), TopCombineFn(10))
     with self.assertRaises(TypeError):
       CombiningValueStateSpec('statename', VarIntCoder(), object())
diff --git a/sdks/python/apache_beam/typehints/decorators_test_py3.py b/sdks/python/apache_beam/typehints/decorators_test_py3.py
index 647a4fae871c..a9e2aad947d8 100644
--- a/sdks/python/apache_beam/typehints/decorators_test_py3.py
+++ b/sdks/python/apache_beam/typehints/decorators_test_py3.py
@@ -43,7 +43,7 @@
 class IOTypeHintsTest(unittest.TestCase):
 
   def test_from_callable(self):
-    def fn(a: int, b: str = None, *args: Tuple[T], foo: List[int],
+    def fn(a: int, b: str = '', *args: Tuple[T], foo: List[int],
            **kwargs: Dict[str, str]) -> Tuple[Any, ...]:
       return a, b, args, foo, kwargs
     th = decorators.IOTypeHints.from_callable(fn)
@@ -96,7 +96,7 @@ def fn(a: typing.List[int],
     self.assertEqual(th.output_types, ((Tuple[Any, ...],), {}))
 
   def test_getcallargs_forhints(self):
-    def fn(a: int, b: str = None, *args: Tuple[T], foo: List[int],
+    def fn(a: int, b: str = '', *args: Tuple[T], foo: List[int],
            **kwargs: Dict[str, str]) -> Tuple[Any, ...]:
       return a, b, args, foo, kwargs
     callargs = decorators.getcallargs_forhints(fn, float, foo=List[str])
diff --git a/sdks/python/apache_beam/utils/profiler.py b/sdks/python/apache_beam/utils/profiler.py
index 1b421043da88..ab1da2f1bd63 100644
--- a/sdks/python/apache_beam/utils/profiler.py
+++ b/sdks/python/apache_beam/utils/profiler.py
@@ -107,6 +107,7 @@ def create_profiler(profile_id, **kwargs):
         if random.random() < options.profile_sample_rate:
           return Profile(profile_id, options.profile_location, **kwargs)
       return create_profiler
+    return None
 
 
 class MemoryReporter(object):
diff --git a/sdks/python/apache_beam/utils/thread_pool_executor.py b/sdks/python/apache_beam/utils/thread_pool_executor.py
index 903d9f7084d9..a71a1138888c 100644
--- a/sdks/python/apache_beam/utils/thread_pool_executor.py
+++ b/sdks/python/apache_beam/utils/thread_pool_executor.py
@@ -27,7 +27,7 @@
 try:  # Python3
   import queue
 except Exception:  # Python2
-  import Queue as queue
+  import Queue as queue  # type: ignore[no-redef]
 
 
 class _WorkItem(object):
diff --git a/sdks/python/gen_protos.py b/sdks/python/gen_protos.py
index 5105ad5cf137..3a7b23b9a7c0 100644
--- a/sdks/python/gen_protos.py
+++ b/sdks/python/gen_protos.py
@@ -19,11 +19,14 @@
 from __future__ import absolute_import
 from __future__ import print_function
 
+import contextlib
 import glob
+import inspect
 import logging
 import multiprocessing
 import os
 import platform
+import re
 import shutil
 import subprocess
 import sys
@@ -47,6 +50,172 @@
 ]
 
 
+def generate_urn_files(log, out_dir):
+  """
+  Create python files with statically defined URN constants.
+
+  Creates a <proto>_pb2_urn.py file for each <proto>_pb2.py file that contains
+  an enum type.
+
+  This works by importing each api.<proto>_pb2 module created by `protoc`,
+  inspecting the module's contents, and generating a new side-car urn module.
+  This is executed at build time rather than dynamically on import to ensure
+  that it is compatible with static type checkers like mypy.
+  """
+  import google.protobuf.message as message
+  import google.protobuf.pyext._message as pyext_message
+
+  class Context(object):
+    INDENT = '  '
+    CAP_SPLIT = re.compile('([A-Z][^A-Z]*|^[a-z]+)')
+
+    def __init__(self, indent=0):
+      self.lines = []
+      self.imports = set()
+      self.empty_types = set()
+      self._indent = indent
+
+    @contextlib.contextmanager
+    def indent(self):
+      self._indent += 1
+      yield
+      self._indent -= 1
+
+    def prepend(self, s):
+      if s:
+        self.lines.insert(0, (self.INDENT * self._indent) + s + '\n')
+      else:
+        self.lines.insert(0, '\n')
+
+    def line(self, s):
+      if s:
+        self.lines.append((self.INDENT * self._indent) + s + '\n')
+      else:
+        self.lines.append('\n')
+
+    def import_type(self, typ):
+      modname = typ.__module__
+      if modname in ('__builtin__', 'builtin'):
+        return typ.__name__
+      else:
+        self.imports.add(modname)
+        return modname + '.' + typ.__name__
+
+    @staticmethod
+    def is_message_type(obj):
+      return isinstance(obj, type) and \
+             issubclass(obj, message.Message)
+
+    @staticmethod
+    def is_enum_type(obj):
+      return type(obj).__name__ == 'EnumTypeWrapper'
+
+    def python_repr(self, obj):
+      if isinstance(obj, message.Message):
+        return self.message_repr(obj)
+      elif isinstance(obj, (list,
+                            pyext_message.RepeatedCompositeContainer,  # pylint: disable=c-extension-no-member
+                            pyext_message.RepeatedScalarContainer)):  # pylint: disable=c-extension-no-member
+        return '[%s]' % ', '.join(self.python_repr(x) for x in obj)
+      else:
+        return repr(obj)
+
+    def empty_type(self, typ):
+      name = ('EMPTY_' +
+              '_'.join(x.upper()
+                       for x in self.CAP_SPLIT.findall(typ.__name__)))
+      self.empty_types.add('%s = %s()' % (name, self.import_type(typ)))
+      return name
+
+    def message_repr(self, msg):
+      parts = []
+      for field, value in msg.ListFields():
+        parts.append('%s=%s' % (field.name, self.python_repr(value)))
+      if parts:
+        return '%s(%s)' % (self.import_type(type(msg)), ', '.join(parts))
+      else:
+        return self.empty_type(type(msg))
+
+    def write_enum(self, enum_name, enum, indent):
+      ctx = Context(indent=indent)
+
+      with ctx.indent():
+        for v in enum.DESCRIPTOR.values:
+          extensions = v.GetOptions().Extensions
+
+          prop = (
+              extensions[beam_runner_api_pb2.beam_urn],
+              extensions[beam_runner_api_pb2.beam_constant],
+              extensions[metrics_pb2.monitoring_info_spec],
+              extensions[metrics_pb2.label_props],
+          )
+          reprs = [self.python_repr(x) for x in prop]
+          if all(x == "''" or x.startswith('EMPTY_') for x in reprs):
+            continue
+          ctx.line('%s = PropertiesFromEnumValue(%s)' %
+                   (v.name, ', '.join(self.python_repr(x) for x in prop)))
+
+      if ctx.lines:
+        ctx.prepend('class %s(object):' % enum_name)
+        ctx.prepend('')
+        ctx.line('')
+      return ctx.lines
+
+    def write_message(self, message_name, message, indent=0):
+      ctx = Context(indent=indent)
+
+      with ctx.indent():
+        for obj_name, obj in inspect.getmembers(message):
+          if self.is_message_type(obj):
+            ctx.lines += self.write_message(obj_name, obj, ctx._indent)
+          elif self.is_enum_type(obj):
+            ctx.lines += self.write_enum(obj_name, obj, ctx._indent)
+
+      if ctx.lines:
+        ctx.prepend('class %s(object):' % message_name)
+        ctx.prepend('')
+      return ctx.lines
+
+  pb2_files = [path for path in glob.glob(os.path.join(out_dir, '*_pb2.py'))]
+  api_path = os.path.dirname(pb2_files[0])
+  sys.path.insert(0, os.path.dirname(api_path))
+
+  def _import(m):
+    # TODO: replace with importlib when we drop support for python2.
+    return __import__('api.%s' % m, fromlist=[None])
+
+  try:
+    beam_runner_api_pb2 = _import('beam_runner_api_pb2')
+    metrics_pb2 = _import('metrics_pb2')
+
+    for pb2_file in pb2_files:
+      modname = os.path.splitext(pb2_file)[0]
+      out_file = modname + '_urns.py'
+      modname = os.path.basename(modname)
+      mod = _import(modname)
+
+      ctx = Context()
+      for obj_name, obj in inspect.getmembers(mod):
+        if ctx.is_message_type(obj):
+          ctx.lines += ctx.write_message(obj_name, obj)
+
+      if ctx.lines:
+        for line in reversed(sorted(ctx.empty_types)):
+          ctx.prepend(line)
+
+        for modname in reversed(sorted(ctx.imports)):
+          ctx.prepend('from . import %s' % modname)
+
+        ctx.prepend('from ..utils import PropertiesFromEnumValue')
+
+        log.info("Writing urn stubs: %s" % out_file)
+        with open(out_file, 'w') as f:
+          f.writelines(ctx.lines)
+
+  finally:
+    sys.path.pop(0)
+
+
 def generate_proto_files(force=False, log=None):
 
   try:
@@ -114,7 +283,8 @@ def generate_proto_files(force=False, log=None):
       # Note that this requires a separate module from setup.py for Windows:
       # https://docs.python.org/2/library/multiprocessing.html#windows
       p = multiprocessing.Process(
-          target=_install_grpcio_tools_and_generate_proto_files)
+          target=_install_grpcio_tools_and_generate_proto_files,
+          kwargs={'force': force})
       p.start()
       p.join()
       if p.exitcode:
@@ -151,6 +321,11 @@ def generate_proto_files(force=False, log=None):
         raise RuntimeError(
             'Error applying futurize to generated protobuf python files.')
 
+      generate_urn_files(log, out_dir)
+
+  else:
+    log.info('Skipping proto regeneration: all files up to date')
+
 
 # Though wheels are available for grpcio-tools, setup_requires uses
 # easy_install which doesn't understand them.  This means that it is
@@ -158,7 +333,7 @@ def generate_proto_files(force=False, log=None):
 # protoc compiler).  Instead, we attempt to install a wheel in a temporary
 # directory and add it to the path as needed.
 # See https://github.com/pypa/setuptools/issues/377
-def _install_grpcio_tools_and_generate_proto_files():
+def _install_grpcio_tools_and_generate_proto_files(force=False):
   py_sdk_root = os.path.dirname(os.path.abspath(__file__))
   install_path = os.path.join(py_sdk_root, '.eggs', 'grpcio-wheels')
   build_path = install_path + '-build'
@@ -179,10 +354,11 @@ def _install_grpcio_tools_and_generate_proto_files():
     shutil.rmtree(build_path, ignore_errors=True)
   sys.path.append(install_path)
   try:
-    generate_proto_files()
+    generate_proto_files(force=force)
   finally:
     sys.stderr.flush()
 
 
 if __name__ == '__main__':
+  logging.getLogger().setLevel(logging.INFO)
   generate_proto_files(force=True)
