diff --git a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
index e5b47da0c458..cb8722bb94bb 100644
--- a/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
+++ b/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java
@@ -67,11 +67,20 @@
     private boolean transactionCommitted;
     private boolean transactionAborted;
     private boolean producerFenced;
-    private boolean producerFencedOnCommitTxn;
     private boolean sentOffsets;
     private long commitCount = 0L;
     private Map<MetricName, Metric> mockMetrics;
 
+    public RuntimeException initTransactionException = null;
+    public RuntimeException beginTransactionException = null;
+    public RuntimeException sendOffsetsToTransactionException = null;
+    public RuntimeException commitTransactionException = null;
+    public RuntimeException abortTransactionException = null;
+    public RuntimeException sendException = null;
+    public RuntimeException flushException = null;
+    public RuntimeException partitionsForException = null;
+    public RuntimeException closeException = null;
+
     /**
      * Create a mock producer
      *
@@ -141,13 +150,29 @@ public void initTransactions() {
         if (this.transactionInitialized) {
             throw new IllegalStateException("MockProducer has already been initialized for transactions.");
         }
+        if (this.initTransactionException != null) {
+            throw this.initTransactionException;
+        }
         this.transactionInitialized = true;
+        this.transactionInFlight = false;
+        this.transactionCommitted = false;
+        this.transactionAborted = false;
+        this.sentOffsets = false;
     }
 
     @Override
     public void beginTransaction() throws ProducerFencedException {
         verifyProducerState();
         verifyTransactionsInitialized();
+
+        if (this.beginTransactionException != null) {
+            throw this.beginTransactionException;
+        }
+
+        if (transactionInFlight) {
+            throw new IllegalStateException("Transaction already started");
+        }
+
         this.transactionInFlight = true;
         this.transactionCommitted = false;
         this.transactionAborted = false;
@@ -160,15 +185,17 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs
         Objects.requireNonNull(consumerGroupId);
         verifyProducerState();
         verifyTransactionsInitialized();
-        verifyNoTransactionInFlight();
+        verifyTransactionInFlight();
+
+        if (this.sendOffsetsToTransactionException != null) {
+            throw this.sendOffsetsToTransactionException;
+        }
+
         if (offsets.size() == 0) {
             return;
         }
-        Map<TopicPartition, OffsetAndMetadata> uncommittedOffsets = this.uncommittedConsumerGroupOffsets.get(consumerGroupId);
-        if (uncommittedOffsets == null) {
-            uncommittedOffsets = new HashMap<>();
-            this.uncommittedConsumerGroupOffsets.put(consumerGroupId, uncommittedOffsets);
-        }
+        Map<TopicPartition, OffsetAndMetadata> uncommittedOffsets =
+            this.uncommittedConsumerGroupOffsets.computeIfAbsent(consumerGroupId, k -> new HashMap<>());
         uncommittedOffsets.putAll(offsets);
         this.sentOffsets = true;
     }
@@ -182,13 +209,13 @@ public void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offs
 
     @Override
     public void commitTransaction() throws ProducerFencedException {
-        if (producerFencedOnCommitTxn) {
-            throw new ProducerFencedException("Producer is fenced");
-        }
-
         verifyProducerState();
         verifyTransactionsInitialized();
-        verifyNoTransactionInFlight();
+        verifyTransactionInFlight();
+
+        if (this.commitTransactionException != null) {
+            throw this.commitTransactionException;
+        }
 
         flush();
 
@@ -209,7 +236,12 @@ public void commitTransaction() throws ProducerFencedException {
     public void abortTransaction() throws ProducerFencedException {
         verifyProducerState();
         verifyTransactionsInitialized();
-        verifyNoTransactionInFlight();
+        verifyTransactionInFlight();
+
+        if (this.abortTransactionException != null) {
+            throw this.abortTransactionException;
+        }
+
         flush();
         this.uncommittedSends.clear();
         this.uncommittedConsumerGroupOffsets.clear();
@@ -233,7 +265,7 @@ private void verifyTransactionsInitialized() {
         }
     }
 
-    private void verifyNoTransactionInFlight() {
+    private void verifyTransactionInFlight() {
         if (!this.transactionInFlight) {
             throw new IllegalStateException("There is no open transaction.");
         }
@@ -259,9 +291,14 @@ public synchronized Future<RecordMetadata> send(ProducerRecord<K, V> record, Cal
         if (this.closed) {
             throw new IllegalStateException("MockProducer is already closed.");
         }
+
         if (this.producerFenced) {
             throw new KafkaException("MockProducer is fenced.", new ProducerFencedException("Fenced"));
         }
+        if (this.sendException != null) {
+            throw this.sendException;
+        }
+
         int partition = 0;
         if (!this.cluster.partitionsForTopic(record.topic()).isEmpty())
             partition = partition(record, this.cluster);
@@ -303,11 +340,20 @@ private long nextOffset(TopicPartition tp) {
 
     public synchronized void flush() {
         verifyProducerState();
+
+        if (this.flushException != null) {
+            throw this.flushException;
+        }
+
         while (!this.completions.isEmpty())
             completeNext();
     }
 
     public List<PartitionInfo> partitionsFor(String topic) {
+        if (this.partitionsForException != null) {
+            throw this.partitionsForException;
+        }
+
         return this.cluster.partitionsForTopic(topic);
     }
 
@@ -329,6 +375,10 @@ public void close() {
 
     @Override
     public void close(Duration timeout) {
+        if (this.closeException != null) {
+            throw this.closeException;
+        }
+
         this.closed = true;
     }
 
@@ -342,12 +392,6 @@ public synchronized void fenceProducer() {
         this.producerFenced = true;
     }
 
-    public void fenceProducerOnCommitTxn() {
-        verifyProducerState();
-        verifyTransactionsInitialized();
-        this.producerFencedOnCommitTxn = true;
-    }
-
     public boolean transactionInitialized() {
         return this.transactionInitialized;
     }
@@ -383,27 +427,32 @@ public synchronized List<ProducerRecord<K, V>> history() {
         return new ArrayList<>(this.sent);
     }
 
+    public synchronized List<ProducerRecord<K, V>> uncommittedRecords() {
+        return new ArrayList<>(this.uncommittedSends);
+    }
+
     /**
+     *
      * Get the list of committed consumer group offsets since the last call to {@link #clear()}
      */
     public synchronized List<Map<String, Map<TopicPartition, OffsetAndMetadata>>> consumerGroupOffsetsHistory() {
         return new ArrayList<>(this.consumerGroupOffsets);
     }
+
+    public synchronized Map<String, Map<TopicPartition, OffsetAndMetadata>> uncommittedOffsets() {
+        return this.uncommittedConsumerGroupOffsets;
+    }
+
     /**
-     *
-     * Clear the stored history of sent records, consumer group offsets, and transactional state
+     * Clear the stored history of sent records, consumer group offsets
      */
     public synchronized void clear() {
         this.sent.clear();
         this.uncommittedSends.clear();
+        this.sentOffsets = false;
         this.completions.clear();
         this.consumerGroupOffsets.clear();
         this.uncommittedConsumerGroupOffsets.clear();
-        this.transactionInitialized = false;
-        this.transactionInFlight = false;
-        this.transactionCommitted = false;
-        this.transactionAborted = false;
-        this.producerFenced = false;
     }
 
     /**
diff --git a/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java b/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java
index c1dabc035364..a72d57cc14b2 100644
--- a/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java
+++ b/clients/src/test/java/org/apache/kafka/clients/producer/MockProducerTest.java
@@ -150,6 +150,14 @@ public void shouldBeginTransactions() {
         assertTrue(producer.transactionInFlight());
     }
 
+    @Test(expected = IllegalStateException.class)
+    public void shouldThrowOnBeginTransactionsIfTransactionInflight() {
+        buildMockProducer(true);
+        producer.initTransactions();
+        producer.beginTransaction();
+        producer.beginTransaction();
+    }
+
     @Test(expected = IllegalStateException.class)
     public void shouldThrowOnSendOffsetsToTransactionIfTransactionsNotInitialized() {
         buildMockProducer(true);
diff --git a/streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java b/streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java
index 4a758b404737..172a975ee068 100644
--- a/streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java
+++ b/streams/src/main/java/org/apache/kafka/streams/errors/TaskMigratedException.java
@@ -27,6 +27,6 @@ public class TaskMigratedException extends StreamsException {
     private final static long serialVersionUID = 1L;
 
     public TaskMigratedException(final String message, final Throwable throwable) {
-        super(message + "; It means all tasks belonging to this thread should be migrated", throwable);
+        super(message + "; it means all tasks belonging to this thread should be migrated.", throwable);
     }
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java
index 950412dfc1a9..4fef0f3d0d7b 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java
@@ -20,7 +20,6 @@
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
 import org.apache.kafka.clients.producer.Producer;
-import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.clients.producer.ProducerRecord;
 import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.PartitionInfo;
@@ -35,13 +34,11 @@
 import org.apache.kafka.common.errors.SecurityDisabledException;
 import org.apache.kafka.common.errors.SerializationException;
 import org.apache.kafka.common.errors.TimeoutException;
-import org.apache.kafka.common.errors.UnknownProducerIdException;
 import org.apache.kafka.common.errors.UnknownServerException;
 import org.apache.kafka.common.header.Headers;
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.common.serialization.Serializer;
 import org.apache.kafka.common.utils.LogContext;
-import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.errors.ProductionExceptionHandler;
 import org.apache.kafka.streams.errors.ProductionExceptionHandler.ProductionExceptionHandlerResponse;
 import org.apache.kafka.streams.errors.StreamsException;
@@ -62,179 +59,43 @@ public class RecordCollectorImpl implements RecordCollector {
 
     private final Logger log;
     private final TaskId taskId;
-    private final boolean eosEnabled;
-    private final String applicationId;
+    private final Consumer<byte[], byte[]> mainConsumer;
+    private final StreamsProducer streamsProducer;
+    private final ProductionExceptionHandler productionExceptionHandler;
     private final Sensor droppedRecordsSensor;
+    private final boolean eosEnabled;
     private final Map<TopicPartition, Long> offsets;
-    private final Consumer<byte[], byte[]> consumer;
-    private final ProductionExceptionHandler productionExceptionHandler;
 
-    // used when eosEnabled is true only
-    private boolean transactionInFlight = false;
-    private boolean transactionInitialized = false;
-    private Producer<byte[], byte[]> producer;
     private volatile KafkaException sendException;
 
     /**
      * @throws StreamsException fatal error that should cause the thread to die (from producer.initTxn)
      */
-    public RecordCollectorImpl(final TaskId taskId,
-                               final StreamsConfig config,
-                               final LogContext logContext,
-                               final StreamsMetricsImpl streamsMetrics,
-                               final Consumer<byte[], byte[]> consumer,
-                               final StreamThread.ProducerSupplier producerSupplier) {
-        this.taskId = taskId;
-        this.consumer = consumer;
-        this.offsets = new HashMap<>();
+    public RecordCollectorImpl(final LogContext logContext,
+                               final TaskId taskId,
+                               final Consumer<byte[], byte[]> mainConsumer,
+                               final StreamsProducer streamsProducer,
+                               final ProductionExceptionHandler productionExceptionHandler,
+                               final boolean eosEnabled,
+                               final StreamsMetricsImpl streamsMetrics) {
         this.log = logContext.logger(getClass());
-
-        this.applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);
-        this.productionExceptionHandler = config.defaultProductionExceptionHandler();
-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));
+        this.taskId = taskId;
+        this.mainConsumer = mainConsumer;
+        this.streamsProducer = streamsProducer;
+        this.productionExceptionHandler = productionExceptionHandler;
+        this.eosEnabled = eosEnabled;
 
         final String threadId = Thread.currentThread().getName();
         this.droppedRecordsSensor = TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor(threadId, taskId.toString(), streamsMetrics);
 
-        producer = producerSupplier.get(taskId);
+        this.offsets = new HashMap<>();
     }
 
     @Override
     public void initialize() {
-        maybeInitTxns();
-    }
-
-    private void maybeInitTxns() {
-        if (eosEnabled && !transactionInitialized) {
-            // initialize transactions if eos is turned on, which will block if the previous transaction has not
-            // completed yet; do not start the first transaction until the topology has been initialized later
-            try {
-                producer.initTransactions();
-
-                transactionInitialized = true;
-            } catch (final TimeoutException exception) {
-                log.warn("Timeout exception caught when initializing transactions for task {}. " +
-                    "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " +
-                    "or the connection to broker was interrupted sending the request or receiving the response. " +
-                    "Would retry initializing the task in the next loop." +
-                    "\nConsider overwriting producer config {} to a larger value to avoid timeout errors",
-                    ProducerConfig.MAX_BLOCK_MS_CONFIG, taskId);
-
-                throw exception;
-            } catch (final KafkaException exception) {
-                throw new StreamsException("Error encountered while initializing transactions for task " + taskId, exception);
-            }
-        }
-    }
-
-    private void maybeBeginTxn() {
-        if (eosEnabled && !transactionInFlight) {
-            try {
-                producer.beginTransaction();
-            } catch (final ProducerFencedException error) {
-                throw new TaskMigratedException("Producer get fenced trying to begin a new transaction", error);
-            } catch (final KafkaException error) {
-                throw new StreamsException("Producer encounter unexpected error trying to begin a new transaction", error);
-            }
-            transactionInFlight = true;
-        }
-    }
-
-    private void maybeAbortTxn() {
-        if (eosEnabled && transactionInFlight) {
-            try {
-                producer.abortTransaction();
-            } catch (final ProducerFencedException ignore) {
-                /* TODO
-                 * this should actually never happen atm as we guard the call to #abortTransaction
-                 * -> the reason for the guard is a "bug" in the Producer -- it throws IllegalStateException
-                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got
-                 * fixed and fall-back to this catch-and-swallow code
-                 */
-
-                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens
-            } catch (final KafkaException error) {
-                throw new StreamsException("Producer encounter unexpected error trying to abort the transaction", error);
-            }
-            transactionInFlight = false;
-        }
-    }
-
-    public void commit(final Map<TopicPartition, OffsetAndMetadata> offsets) {
         if (eosEnabled) {
-            maybeBeginTxn();
-
-            try {
-                producer.sendOffsetsToTransaction(offsets, applicationId);
-                producer.commitTransaction();
-                transactionInFlight = false;
-            } catch (final ProducerFencedException error) {
-                throw new TaskMigratedException("Producer get fenced trying to commit a transaction", error);
-            } catch (final TimeoutException error) {
-                // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level
-                throw new StreamsException("Timed out while committing transaction via producer for task " + taskId, error);
-            } catch (final KafkaException error) {
-                throw new StreamsException("Error encountered sending offsets and committing transaction " +
-                    "via producer for task " + taskId, error);
-            }
-        } else {
-            try {
-                consumer.commitSync(offsets);
-            } catch (final CommitFailedException error) {
-                throw new TaskMigratedException("Consumer committing offsets failed, " +
-                    "indicating the corresponding thread is no longer part of the group.", error);
-            } catch (final TimeoutException error) {
-                // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level
-                throw new StreamsException("Timed out while committing offsets via consumer for task " + taskId, error);
-            } catch (final KafkaException error) {
-                throw new StreamsException("Error encountered committing offsets via consumer for task " + taskId, error);
-            }
+            streamsProducer.initTransaction();
         }
-
-    }
-
-    private boolean productionExceptionIsFatal(final Exception exception) {
-        final boolean securityException = exception instanceof AuthenticationException ||
-            exception instanceof AuthorizationException ||
-            exception instanceof SecurityDisabledException;
-
-        final boolean communicationException = exception instanceof InvalidTopicException ||
-            exception instanceof UnknownServerException ||
-            exception instanceof SerializationException ||
-            exception instanceof OffsetMetadataTooLarge ||
-            exception instanceof IllegalStateException;
-
-        return securityException || communicationException;
-    }
-
-    private void recordSendError(final String topic, final Exception exception, final ProducerRecord<byte[], byte[]> serializedRecord) {
-        String errorMessage = String.format(SEND_EXCEPTION_MESSAGE, topic, taskId, exception.toString());
-
-        if (productionExceptionIsFatal(exception)) {
-            errorMessage += "\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.";
-            sendException = new StreamsException(errorMessage, exception);
-        } else if (exception instanceof ProducerFencedException || exception instanceof OutOfOrderSequenceException) {
-            errorMessage += "\nWritten offsets would not be recorded and no more records would be sent since the producer is fenced, " +
-                "indicating the task may be migrated out.";
-            sendException = new TaskMigratedException(errorMessage, exception);
-        } else {
-            if (exception instanceof RetriableException) {
-                errorMessage += "\nThe broker is either slow or in bad state (like not having enough replicas) in responding the request, " +
-                    "or the connection to broker was interrupted sending the request or receiving the response. " +
-                    "\nConsider overwriting `max.block.ms` and /or " +
-                    "`delivery.timeout.ms` to a larger value to wait longer for such scenarios and avoid timeout errors";
-            }
-
-            if (productionExceptionHandler.handle(serializedRecord, exception) == ProductionExceptionHandlerResponse.FAIL) {
-                errorMessage += "\nException handler choose to FAIL the processing, no more records would be sent.";
-                sendException = new StreamsException(errorMessage, exception);
-            } else {
-                errorMessage += "\nException handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded.";
-                droppedRecordsSensor.record();
-            }
-        }
-
-        log.error(errorMessage);
     }
 
     /**
@@ -255,7 +116,7 @@ public <K, V> void send(final String topic,
         if (partitioner != null) {
             final List<PartitionInfo> partitions;
             try {
-                partitions = producer.partitionsFor(topic);
+                partitions = streamsProducer.partitionsFor(topic);
             } catch (final KafkaException e) {
                 // here we cannot drop the message on the floor even if it is a transient timeout exception,
                 // so we treat everything the same as a fatal exception
@@ -265,7 +126,7 @@ public <K, V> void send(final String topic,
             if (partitions.size() > 0) {
                 partition = partitioner.partition(topic, key, value, partitions.size());
             } else {
-                throw new StreamsException("Could not get partition information for topic '" + topic + "'  for task " + taskId +
+                throw new StreamsException("Could not get partition information for topic " + topic + " for task " + taskId +
                     ". This can happen if the topic does not exist.");
             }
         } else {
@@ -286,52 +147,97 @@ public <K, V> void send(final String topic,
                             final Serializer<V> valueSerializer) {
         checkForException();
 
-        maybeBeginTxn();
-
+        final byte[] keyBytes;
+        final byte[] valBytes;
         try {
-            final byte[] keyBytes = keySerializer.serialize(topic, headers, key);
-            final byte[] valBytes = valueSerializer.serialize(topic, headers, value);
-
-            final ProducerRecord<byte[], byte[]> serializedRecord = new ProducerRecord<>(topic, partition, timestamp, keyBytes, valBytes, headers);
-
-            producer.send(serializedRecord, (metadata, exception) -> {
-                // if there's already an exception record, skip logging offsets or new exceptions
-                if (sendException != null) {
-                    return;
-                }
-
-                if (exception == null) {
-                    final TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());
-                    offsets.put(tp, metadata.offset());
-                } else {
-                    recordSendError(topic, exception, serializedRecord);
-
-                    // KAFKA-7510 only put message key and value in TRACE level log so we don't leak data by default
-                    log.trace("Failed record: (key {} value {} timestamp {}) topic=[{}] partition=[{}]", key, value, timestamp, topic, partition);
-                }
-            });
-        } catch (final RuntimeException uncaughtException) {
-            if (isRecoverable(uncaughtException)) {
-                // producer.send() call may throw a KafkaException which wraps a FencedException,
-                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException
-                throw new TaskMigratedException("Producer cannot send records anymore since it got fenced", uncaughtException.getCause());
+            keyBytes = keySerializer.serialize(topic, headers, key);
+            valBytes = valueSerializer.serialize(topic, headers, value);
+        } catch (final RuntimeException exception) {
+            final String errorMessage = String.format(SEND_EXCEPTION_MESSAGE, topic, taskId, exception.toString());
+            throw new StreamsException(errorMessage, exception);
+        }
+
+        final ProducerRecord<byte[], byte[]> serializedRecord = new ProducerRecord<>(topic, partition, timestamp, keyBytes, valBytes, headers);
+
+        streamsProducer.send(serializedRecord, (metadata, exception) -> {
+            // if there's already an exception record, skip logging offsets or new exceptions
+            if (sendException != null) {
+                return;
+            }
+
+            if (exception == null) {
+                final TopicPartition tp = new TopicPartition(metadata.topic(), metadata.partition());
+                offsets.put(tp, metadata.offset());
             } else {
-                final String errorMessage = String.format(SEND_EXCEPTION_MESSAGE, topic, taskId, uncaughtException.toString());
-                throw new StreamsException(errorMessage, uncaughtException);
+                recordSendError(topic, exception, serializedRecord);
+
+                // KAFKA-7510 only put message key and value in TRACE level log so we don't leak data by default
+                log.trace("Failed record: (key {} value {} timestamp {}) topic=[{}] partition=[{}]", key, value, timestamp, topic, partition);
+            }
+        });
+    }
+
+    private void recordSendError(final String topic, final Exception exception, final ProducerRecord<byte[], byte[]> serializedRecord) {
+        String errorMessage = String.format(SEND_EXCEPTION_MESSAGE, topic, taskId, exception.toString());
+
+        if (isFatalException(exception)) {
+            errorMessage += "\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error.";
+            sendException = new StreamsException(errorMessage, exception);
+        } else if (exception instanceof ProducerFencedException || exception instanceof OutOfOrderSequenceException) {
+            errorMessage += "\nWritten offsets would not be recorded and no more records would be sent since the producer is fenced, " +
+                "indicating the task may be migrated out";
+            sendException = new TaskMigratedException(errorMessage, exception);
+        } else {
+            if (exception instanceof RetriableException) {
+                errorMessage += "\nThe broker is either slow or in bad state (like not having enough replicas) in responding the request, " +
+                    "or the connection to broker was interrupted sending the request or receiving the response. " +
+                    "\nConsider overwriting `max.block.ms` and /or " +
+                    "`delivery.timeout.ms` to a larger value to wait longer for such scenarios and avoid timeout errors";
+            }
+
+            if (productionExceptionHandler.handle(serializedRecord, exception) == ProductionExceptionHandlerResponse.FAIL) {
+                errorMessage += "\nException handler choose to FAIL the processing, no more records would be sent.";
+                sendException = new StreamsException(errorMessage, exception);
+            } else {
+                errorMessage += "\nException handler choose to CONTINUE processing in spite of this error but written offsets would not be recorded.";
+                droppedRecordsSensor.record();
             }
         }
+
+        log.error(errorMessage);
     }
 
-    private static boolean isRecoverable(final RuntimeException uncaughtException) {
-        return uncaughtException instanceof KafkaException && (
-            uncaughtException.getCause() instanceof ProducerFencedException ||
-                uncaughtException.getCause() instanceof UnknownProducerIdException);
+    private boolean isFatalException(final Exception exception) {
+        final boolean securityException = exception instanceof AuthenticationException ||
+            exception instanceof AuthorizationException ||
+            exception instanceof SecurityDisabledException;
+
+        final boolean communicationException = exception instanceof InvalidTopicException ||
+            exception instanceof UnknownServerException ||
+            exception instanceof SerializationException ||
+            exception instanceof OffsetMetadataTooLarge ||
+            exception instanceof IllegalStateException;
+
+        return securityException || communicationException;
     }
 
-    private void checkForException() {
-        if (sendException != null) {
-            throw sendException;
+    public void commit(final Map<TopicPartition, OffsetAndMetadata> offsets) {
+        if (eosEnabled) {
+            streamsProducer.commitTransaction(offsets);
+        } else {
+            try {
+                mainConsumer.commitSync(offsets);
+            } catch (final CommitFailedException error) {
+                throw new TaskMigratedException("Consumer committing offsets failed, " +
+                    "indicating the corresponding thread is no longer part of the group", error);
+            } catch (final TimeoutException error) {
+                // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level
+                throw new StreamsException("Timed out while committing offsets via consumer for task " + taskId, error);
+            } catch (final KafkaException error) {
+                throw new StreamsException("Error encountered committing offsets via consumer for task " + taskId, error);
+            }
         }
+
     }
 
     /**
@@ -341,9 +247,7 @@ private void checkForException() {
     @Override
     public void flush() {
         log.debug("Flushing record collector");
-
-        producer.flush();
-
+        streamsProducer.flush();
         checkForException();
     }
 
@@ -354,15 +258,11 @@ public void flush() {
     @Override
     public void close() {
         log.debug("Closing record collector");
-        maybeAbortTxn();
 
         if (eosEnabled) {
-            try {
-                producer.close();
-            } catch (final KafkaException e) {
-                throw new StreamsException("Caught a recoverable exception while closing", e);
-            }
+            streamsProducer.abortTransaction();
         }
+        streamsProducer.close();
 
         checkForException();
     }
@@ -372,9 +272,14 @@ public Map<TopicPartition, Long> offsets() {
         return Collections.unmodifiableMap(new HashMap<>(offsets));
     }
 
+    private void checkForException() {
+        if (sendException != null) {
+            throw sendException;
+        }
+    }
+
     // for testing only
     Producer<byte[], byte[]> producer() {
-        return producer;
+        return streamsProducer.kafkaProducer();
     }
-
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
index e2e8ddb182a4..fc6dd9cbb1c9 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java
@@ -51,7 +51,7 @@
  * ChangelogReader is created and maintained by the stream thread and used for both updating standby tasks and
  * restoring active tasks. It manages the restore consumer, including its assigned partitions, when to pause / resume
  * these partitions, etc.
- *
+ * <p>
  * The reader also maintains the source of truth for restoration state: only active tasks restoring changelog could
  * be completed, while standby tasks updating changelog would always be in restoring state after being initialized.
  */
@@ -76,8 +76,10 @@ enum ChangelogState {
         }
     }
 
-    // NOTE we assume that the changelog read is used only for either 1) restoring active task
-    // or 2) updating standby task at a given time, but never doing both
+    // NOTE we assume that the changelog reader is used only for either
+    //   1) restoring active task or
+    //   2) updating standby task at a given time,
+    // but never doing both
     enum ChangelogReaderState {
         ACTIVE_RESTORING("ACTIVE_RESTORING"),
 
@@ -108,6 +110,7 @@ static class ChangelogMetadata {
         //
         // the log-end-offset only needs to be updated once and only need to be for active tasks since for standby
         // tasks it would never "complete" based on the end-offset;
+        //
         // the committed-offset needs to be updated periodically for those standby tasks
         //
         // NOTE we do not book keep the current offset since we leverage state manager as its source of truth
@@ -136,10 +139,11 @@ private void clear() {
         }
 
         private void transitTo(final ChangelogState newState) {
-            if (newState.prevStates.contains(changelogState.ordinal()))
+            if (newState.prevStates.contains(changelogState.ordinal())) {
                 changelogState = newState;
-            else
+            } else {
                 throw new IllegalStateException("Invalid transition from " + changelogState + " to " + newState);
+            }
         }
 
         @Override
@@ -171,7 +175,7 @@ int bufferedLimitIndex() {
         }
     }
 
-    private final static long DEFAULT_OFFSET_UPDATE_MS = 5 * 60 * 1000; // five minutes
+    private final static long DEFAULT_OFFSET_UPDATE_MS = Duration.ofMinutes(5L).toMillis();
 
     private ChangelogReaderState state;
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
index f50ab008c0d6..32fb47bcd65e 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java
@@ -74,7 +74,7 @@ public class StreamTask extends AbstractTask implements ProcessorNodePunctuator,
     private final Time time;
     private final Logger log;
     private final String logPrefix;
-    private final Consumer<byte[], byte[]> consumer;
+    private final Consumer<byte[], byte[]> mainConsumer;
 
     // we want to abstract eos logic out of StreamTask, however
     // there's still an optimization that requires this info to be
@@ -104,7 +104,7 @@ public class StreamTask extends AbstractTask implements ProcessorNodePunctuator,
     public StreamTask(final TaskId id,
                       final Set<TopicPartition> partitions,
                       final ProcessorTopology topology,
-                      final Consumer<byte[], byte[]> consumer,
+                      final Consumer<byte[], byte[]> mainConsumer,
                       final StreamsConfig config,
                       final StreamsMetricsImpl streamsMetrics,
                       final StateDirectory stateDirectory,
@@ -113,7 +113,7 @@ public StreamTask(final TaskId id,
                       final ProcessorStateManager stateMgr,
                       final RecordCollector recordCollector) {
         super(id, topology, stateDirectory, stateMgr, partitions);
-        this.consumer = consumer;
+        this.mainConsumer = mainConsumer;
 
         final String threadIdPrefix = format("stream-thread [%s] ", Thread.currentThread().getName());
         logPrefix = threadIdPrefix + format("%s [%s] ", "task", id);
@@ -351,7 +351,7 @@ private void commitState() {
             Long offset = partitionGroup.headRecordOffset(partition);
             if (offset == null) {
                 try {
-                    offset = consumer.position(partition);
+                    offset = mainConsumer.position(partition);
                 } catch (final TimeoutException error) {
                     // the `consumer.position()` call should never block, because we know that we did process data
                     // for the requested partition and thus the consumer should have a valid local position
@@ -539,7 +539,7 @@ public boolean process(final long wallClockTime) {
             // after processing this record, if its partition queue's buffered size has been
             // decreased to the threshold, we can then resume the consumption on this partition
             if (recordInfo.queue().size() == maxBufferedSize) {
-                consumer.resume(singleton(partition));
+                mainConsumer.resume(singleton(partition));
             }
         } catch (final StreamsException e) {
             throw e;
@@ -625,13 +625,14 @@ private Map<TopicPartition, Long> checkpointableOffsets() {
 
     private void initializeMetadata() {
         try {
-            final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = consumer.committed(partitions).entrySet().stream()
+            final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mainConsumer.committed(partitions).entrySet().stream()
                 .filter(e -> e.getValue() != null)
                 .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));
             initializeTaskTime(offsetsAndMetadata);
         } catch (final TimeoutException e) {
             log.warn("Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop." +
                 "\nConsider overwriting consumer config {} to a larger value to avoid timeout errors",
+                e.toString(),
                 ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);
 
             throw e;
@@ -740,7 +741,7 @@ public void addRecords(final TopicPartition partition, final Iterable<ConsumerRe
         // if after adding these records, its partition queue's buffered size has been
         // increased beyond the threshold, we can then pause the consumption for this partition
         if (newQueueSize > maxBufferedSize) {
-            consumer.pause(singleton(partition));
+            mainConsumer.pause(singleton(partition));
         }
     }
 
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
index 0df38fdbf350..823cb9911b4a 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java
@@ -53,6 +53,7 @@
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
 import java.util.List;
@@ -61,6 +62,8 @@
 import java.util.UUID;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;
+
 public class StreamThread extends Thread {
 
     private final Admin adminClient;
@@ -248,11 +251,6 @@ void setRebalanceException(final Throwable rebalanceException) {
         this.rebalanceException = rebalanceException;
     }
 
-    // public for testing purposes
-    public interface ProducerSupplier {
-        Producer<byte[], byte[]> get(final TaskId id);
-    }
-
     static abstract class AbstractTaskCreator<T extends Task> {
         final String applicationId;
         final InternalTopologyBuilder builder;
@@ -314,7 +312,7 @@ static class TaskCreator extends AbstractTaskCreator<StreamTask> {
         private final ThreadCache cache;
         private final Producer<byte[], byte[]> threadProducer;
         private final KafkaClientSupplier clientSupplier;
-        private final ProducerSupplier producerSupplier;
+        final Map<TaskId, Producer<byte[], byte[]>> taskProducers;
         private final Sensor createTaskSensor;
 
         TaskCreator(final InternalTopologyBuilder builder,
@@ -325,6 +323,7 @@ static class TaskCreator extends AbstractTaskCreator<StreamTask> {
                     final ThreadCache cache,
                     final Time time,
                     final KafkaClientSupplier clientSupplier,
+                    final Map<TaskId, Producer<byte[], byte[]>> taskProducers,
                     final String threadId,
                     final Logger log) {
             super(
@@ -336,25 +335,25 @@ static class TaskCreator extends AbstractTaskCreator<StreamTask> {
                 time,
                 log);
 
-            final boolean eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));
+            final boolean eosEnabled = EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));
             if (!eosEnabled) {
                 final Map<String, Object> producerConfigs = config.getProducerConfigs(getThreadProducerClientId(threadId));
                 log.info("Creating thread producer client");
-                threadProducer = clientSupplier.getProducer(producerConfigs);
+                this.threadProducer = clientSupplier.getProducer(producerConfigs);
             } else {
-                threadProducer = null;
+                this.threadProducer = null;
             }
-
+            this.taskProducers = taskProducers;
 
             this.cache = cache;
             this.threadId = threadId;
             this.clientSupplier = clientSupplier;
-            this.producerSupplier = new TaskProducerSupplier();
+
             this.createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);
         }
 
         @Override
-        StreamTask createTask(final Consumer<byte[], byte[]> consumer,
+        StreamTask createTask(final Consumer<byte[], byte[]> mainConsumer,
                               final TaskId taskId,
                               final Set<TopicPartition> partitions) {
             createTaskSensor.record();
@@ -374,19 +373,30 @@ StreamTask createTask(final Consumer<byte[], byte[]> consumer,
                 storeChangelogReader,
                 logContext);
 
+            if (threadProducer == null) {
+                // create one producer per task for EOS
+                // TODO: after KIP-447 this would be removed
+                final Map<String, Object> producerConfigs = config.getProducerConfigs(getTaskProducerClientId(threadId, taskId));
+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + "-" + taskId);
+                log.info("Creating producer client for task {}", taskId);
+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));
+            }
             final RecordCollector recordCollector = new RecordCollectorImpl(
-                taskId,
-                config,
                 logContext,
-                streamsMetrics,
-                consumer,
-                producerSupplier);
+                taskId,
+                mainConsumer,
+                threadProducer != null ?
+                    new StreamsProducer(logContext, threadProducer) :
+                    new StreamsProducer(logContext, taskProducers.get(taskId), applicationId, taskId),
+                config.defaultProductionExceptionHandler(),
+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),
+                streamsMetrics);
 
             return new StreamTask(
                 taskId,
                 partitions,
                 topology,
-                consumer,
+                mainConsumer,
                 config,
                 streamsMetrics,
                 stateDirectory,
@@ -396,23 +406,6 @@ StreamTask createTask(final Consumer<byte[], byte[]> consumer,
                 recordCollector);
         }
 
-        private class TaskProducerSupplier implements ProducerSupplier {
-            @Override
-            public Producer<byte[], byte[]> get(final TaskId id) {
-                if (threadProducer == null) {
-                    // create one producer per task for EOS
-                    // TODO: after KIP-447 this would be removed
-                    final Map<String, Object> producerConfigs = config.getProducerConfigs(getTaskProducerClientId(threadId, id));
-                    producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + "-" + id);
-                    log.info("Creating producer client for task {}", id);
-
-                    return clientSupplier.getProducer(producerConfigs);
-                } else {
-                    return threadProducer;
-                }
-            }
-        }
-
         public void close() {
             if (threadProducer != null) {
                 try {
@@ -518,9 +511,10 @@ StandbyTask createTask(final Consumer<byte[], byte[]> consumer,
 
     // package-private for testing
     final ConsumerRebalanceListener rebalanceListener;
+    final Consumer<byte[], byte[]> mainConsumer;
     final Consumer<byte[], byte[]> restoreConsumer;
-    final Consumer<byte[], byte[]> consumer;
-    final Producer<byte[], byte[]> producer;
+    final Producer<byte[], byte[]> threadProducer;
+    final Map<TaskId, Producer<byte[], byte[]>> taskProducers;
     final InternalTopologyBuilder builder;
 
     public static StreamThread create(final InternalTopologyBuilder builder,
@@ -546,10 +540,19 @@ public static StreamThread create(final InternalTopologyBuilder builder,
         final Map<String, Object> restoreConsumerConfigs = config.getRestoreConsumerConfigs(getRestoreConsumerClientId(threadId));
         final Consumer<byte[], byte[]> restoreConsumer = clientSupplier.getRestoreConsumer(restoreConsumerConfigs);
 
-        final StoreChangelogReader changelogReader = new StoreChangelogReader(time, config, logContext, restoreConsumer, userStateRestoreListener);
+        final StoreChangelogReader changelogReader = new StoreChangelogReader(
+            time,
+            config,
+            logContext,
+            restoreConsumer,
+            userStateRestoreListener);
 
         final ThreadCache cache = new ThreadCache(logContext, cacheSizeBytes, streamsMetrics);
 
+        final Map<TaskId, Producer<byte[], byte[]>> taskProducers = new HashMap<>();
+
+        // TODO: refactor `TaskCreator` into `TaskManager`;
+        //  this will allow to reduce the surface area of `taskProducers` that is passed to many classes atm
         final TaskCreator activeTaskCreator = new TaskCreator(
             builder,
             config,
@@ -559,6 +562,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             cache,
             time,
             clientSupplier,
+            taskProducers,
             threadId,
             log);
         final StandbyTaskCreator standbyTaskCreator = new StandbyTaskCreator(
@@ -577,6 +581,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             streamsMetrics,
             activeTaskCreator,
             standbyTaskCreator,
+            taskProducers,
             builder,
             adminClient
         );
@@ -595,17 +600,18 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "none");
         }
 
-        final Consumer<byte[], byte[]> consumer = clientSupplier.getConsumer(consumerConfigs);
-        changelogReader.setMainConsumer(consumer);
-        taskManager.setConsumer(consumer);
+        final Consumer<byte[], byte[]> mainConsumer = clientSupplier.getConsumer(consumerConfigs);
+        changelogReader.setMainConsumer(mainConsumer);
+        taskManager.setMainConsumer(mainConsumer);
 
         final StreamThread streamThread = new StreamThread(
             time,
             config,
             activeTaskCreator.threadProducer,
+            taskProducers,
             adminClient,
+            mainConsumer,
             restoreConsumer,
-            consumer,
             changelogReader,
             originalReset,
             taskManager,
@@ -620,10 +626,11 @@ public static StreamThread create(final InternalTopologyBuilder builder,
 
     public StreamThread(final Time time,
                         final StreamsConfig config,
-                        final Producer<byte[], byte[]> producer,
+                        final Producer<byte[], byte[]> threadProducer,
+                        final Map<TaskId, Producer<byte[], byte[]>> taskProducers,
                         final Admin adminClient,
+                        final Consumer<byte[], byte[]> mainConsumer,
                         final Consumer<byte[], byte[]> restoreConsumer,
-                        final Consumer<byte[], byte[]> consumer,
                         final ChangelogReader changelogReader,
                         final String originalReset,
                         final TaskManager taskManager,
@@ -662,8 +669,9 @@ public StreamThread(final Time time,
         this.rebalanceListener = new StreamsRebalanceListener(time, taskManager, this, this.log);
         this.taskManager = taskManager;
         this.restoreConsumer = restoreConsumer;
-        this.consumer = consumer;
-        this.producer = producer;
+        this.mainConsumer = mainConsumer;
+        this.threadProducer = threadProducer;
+        this.taskProducers = taskProducers;
         this.changelogReader = changelogReader;
         this.originalReset = originalReset;
         this.assignmentErrorCode = assignmentErrorCode;
@@ -767,15 +775,15 @@ private void runLoop() {
     }
 
     private void enforceRebalance() {
-        consumer.unsubscribe();
+        mainConsumer.unsubscribe();
         subscribeConsumer();
     }
 
     private void subscribeConsumer() {
         if (builder.usesPatternSubscription()) {
-            consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);
+            mainConsumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);
         } else {
-            consumer.subscribe(builder.sourceTopicCollection(), rebalanceListener);
+            mainConsumer.subscribe(builder.sourceTopicCollection(), rebalanceListener);
         }
     }
 
@@ -914,7 +922,7 @@ private ConsumerRecords<byte[], byte[]> pollRequests(final Duration pollTime) {
         lastPollMs = now;
 
         try {
-            records = consumer.poll(pollTime);
+            records = mainConsumer.poll(pollTime);
         } catch (final InvalidOffsetException e) {
             resetInvalidOffsets(e);
         }
@@ -951,17 +959,17 @@ private void resetInvalidOffsets(final InvalidOffsetException e) {
 
                 if (originalReset.equals("earliest")) {
                     addToResetList(partition, seekToBeginning, "No custom setting defined for topic '{}' using original config '{}' for offset reset", "earliest", loggedTopics);
-                } else {
+                } else { // can only be "latest"
                     addToResetList(partition, seekToEnd, "No custom setting defined for topic '{}' using original config '{}' for offset reset", "latest", loggedTopics);
                 }
             }
         }
 
         if (!seekToBeginning.isEmpty()) {
-            consumer.seekToBeginning(seekToBeginning);
+            mainConsumer.seekToBeginning(seekToBeginning);
         }
         if (!seekToEnd.isEmpty()) {
-            consumer.seekToEnd(seekToEnd);
+            mainConsumer.seekToEnd(seekToEnd);
         }
     }
 
@@ -1099,7 +1107,7 @@ private void completeShutdown(final boolean cleanRun) {
             log.error("Failed to close changelog reader due to the following error:", e);
         }
         try {
-            consumer.close();
+            mainConsumer.close();
         } catch (final Throwable e) {
             log.error("Failed to close consumer due to the following error:", e);
         }
@@ -1131,7 +1139,9 @@ StreamThread updateThreadMetadata(final String adminClientId) {
             this.state().name(),
             getConsumerClientId(this.getName()),
             getRestoreConsumerClientId(this.getName()),
-            producer == null ? Collections.emptySet() : Collections.singleton(getThreadProducerClientId(this.getName())),
+            threadProducer == null ?
+                Collections.emptySet() :
+                Collections.singleton(getThreadProducerClientId(this.getName())),
             adminClientId,
             Collections.emptySet(),
             Collections.emptySet());
@@ -1158,7 +1168,9 @@ private void updateThreadMetadata(final Map<TaskId, Task> activeTasks,
             this.state().name(),
             getConsumerClientId(this.getName()),
             getRestoreConsumerClientId(this.getName()),
-            producer == null ? producerClientIds : Collections.singleton(getThreadProducerClientId(this.getName())),
+            threadProducer == null ?
+                producerClientIds :
+                Collections.singleton(getThreadProducerClientId(this.getName())),
             adminClientId,
             activeTasksMetadata,
             standbyTasksMetadata);
@@ -1199,8 +1211,8 @@ public String toString(final String indent) {
 
     public Map<MetricName, Metric> producerMetrics() {
         final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();
-        if (producer != null) {
-            final Map<MetricName, ? extends Metric> producerMetrics = producer.metrics();
+        if (threadProducer != null) {
+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();
             if (producerMetrics != null) {
                 result.putAll(producerMetrics);
             }
@@ -1209,7 +1221,7 @@ public Map<MetricName, Metric> producerMetrics() {
             // and the producer object passed in here will be null. We would then iterate through
             // all the active tasks and add their metrics to the output metrics map.
             for (final StreamTask task : taskManager.fixmeStreamTasks().values()) {
-                final Map<MetricName, ? extends Metric> taskProducerMetrics = ((RecordCollectorImpl) task.recordCollector()).producer().metrics();
+                final Map<MetricName, ? extends Metric> taskProducerMetrics = taskProducers.get(task.id).metrics();
                 result.putAll(taskProducerMetrics);
             }
         }
@@ -1217,7 +1229,7 @@ public Map<MetricName, Metric> producerMetrics() {
     }
 
     public Map<MetricName, Metric> consumerMetrics() {
-        final Map<MetricName, ? extends Metric> consumerMetrics = consumer.metrics();
+        final Map<MetricName, ? extends Metric> consumerMetrics = mainConsumer.metrics();
         final Map<MetricName, ? extends Metric> restoreConsumerMetrics = restoreConsumer.metrics();
         final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();
         result.putAll(consumerMetrics);
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java
new file mode 100644
index 000000000000..6a72015c4165
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java
@@ -0,0 +1,227 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.clients.consumer.OffsetAndMetadata;
+import org.apache.kafka.clients.producer.Callback;
+import org.apache.kafka.clients.producer.Producer;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.clients.producer.RecordMetadata;
+import org.apache.kafka.common.KafkaException;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.ProducerFencedException;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.errors.UnknownProducerIdException;
+import org.apache.kafka.common.utils.LogContext;
+import org.apache.kafka.streams.errors.StreamsException;
+import org.apache.kafka.streams.errors.TaskMigratedException;
+import org.apache.kafka.streams.processor.TaskId;
+import org.slf4j.Logger;
+
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.Future;
+
+/**
+ * {@code StreamsProducer} manages the producers within a Kafka Streams application.
+ * <p>
+ * If EOS is enabled, it is responsible to init and begin transactions if necessary.
+ * It also tracks the transaction status, ie, if a transaction is in-fight.
+ * <p>
+ * For non-EOS, the user should not call transaction related methods.
+ */
+public class StreamsProducer {
+    private final Logger log;
+
+    private final Producer<byte[], byte[]> producer;
+    private final String applicationId;
+    private final TaskId taskId;
+    private final String logMessage;
+    private final boolean eosEnabled;
+
+    private boolean transactionInFlight = false;
+    private boolean transactionInitialized = false;
+
+    public StreamsProducer(final LogContext logContext,
+                           final Producer<byte[], byte[]> producer) {
+        this(logContext, producer, null, null);
+    }
+
+    public StreamsProducer(final LogContext logContext,
+                           final Producer<byte[], byte[]> producer,
+                           final String applicationId,
+                           final TaskId taskId) {
+        if ((applicationId != null && taskId == null) ||
+            (applicationId == null && taskId != null)) {
+            throw new IllegalArgumentException("applicationId and taskId must either be both null or both be not null");
+        }
+
+        this.log = logContext.logger(getClass());
+
+        this.producer = Objects.requireNonNull(producer, "producer cannot be null");
+        this.applicationId = applicationId;
+        this.taskId = taskId;
+        if (taskId != null) {
+            logMessage = "task " + taskId.toString();
+            eosEnabled = true;
+        } else {
+            logMessage = "all owned active tasks";
+            eosEnabled = false;
+        }
+    }
+
+    /**
+     * @throws IllegalStateException if EOS is disabled
+     */
+    public void initTransaction() {
+        if (!eosEnabled) {
+            throw new IllegalStateException("EOS is disabled");
+        }
+        if (!transactionInitialized) {
+            // initialize transactions if eos is turned on, which will block if the previous transaction has not
+            // completed yet; do not start the first transaction until the topology has been initialized later
+            try {
+                producer.initTransactions();
+                transactionInitialized = true;
+            } catch (final TimeoutException exception) {
+                log.warn("Timeout exception caught when initializing transactions for {}. " +
+                    "\nThe broker is either slow or in bad state (like not having enough replicas) in responding to the request, " +
+                    "or the connection to broker was interrupted sending the request or receiving the response. " +
+                    "Will retry initializing the task in the next loop. " +
+                    "\nConsider overwriting {} to a larger value to avoid timeout errors",
+                    logMessage,
+                    ProducerConfig.MAX_BLOCK_MS_CONFIG);
+
+                throw exception;
+            } catch (final KafkaException exception) {
+                throw new StreamsException("Error encountered while initializing transactions for " + logMessage, exception);
+            }
+        }
+    }
+
+    private void maybeBeginTransaction() throws ProducerFencedException {
+        if (eosEnabled && !transactionInFlight) {
+            try {
+                producer.beginTransaction();
+                transactionInFlight = true;
+            } catch (final ProducerFencedException error) {
+                throw new TaskMigratedException("Producer get fenced trying to begin a new transaction", error);
+            } catch (final KafkaException error) {
+                throw new StreamsException("Producer encounter unexpected error trying to begin a new transaction for " + logMessage, error);
+            }
+        }
+    }
+
+    public Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record,
+                                       final Callback callback) {
+        maybeBeginTransaction();
+        try {
+            return producer.send(record, callback);
+        } catch (final KafkaException uncaughtException) {
+            if (isRecoverable(uncaughtException)) {
+                // producer.send() call may throw a KafkaException which wraps a FencedException,
+                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException
+                throw new TaskMigratedException("Producer cannot send records anymore since it got fenced", uncaughtException.getCause());
+            } else {
+                final String errorMessage = String.format(
+                    "Error encountered sending record to topic %s%s due to:%n%s",
+                    record.topic(),
+                    taskId == null ? "" : " " + logMessage,
+                    uncaughtException.toString());
+                throw new StreamsException(errorMessage, uncaughtException);
+            }
+        }
+    }
+
+    private static boolean isRecoverable(final KafkaException uncaughtException) {
+        return uncaughtException.getCause() instanceof ProducerFencedException ||
+            uncaughtException.getCause() instanceof UnknownProducerIdException;
+    }
+
+    /**
+     * @throws IllegalStateException if EOS is disabled
+     * @throws TaskMigratedException
+     */
+    public void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets) throws ProducerFencedException {
+        if (!eosEnabled) {
+            throw new IllegalStateException("EOS is disabled");
+        }
+        maybeBeginTransaction();
+        try {
+            producer.sendOffsetsToTransaction(offsets, applicationId);
+            producer.commitTransaction();
+            transactionInFlight = false;
+        } catch (final ProducerFencedException error) {
+            throw new TaskMigratedException("Producer get fenced trying to commit a transaction", error);
+        } catch (final TimeoutException error) {
+            // TODO KIP-447: we can consider treating it as non-fatal and retry on the thread level
+            throw new StreamsException("Timed out while committing a transaction for " + logMessage, error);
+        } catch (final KafkaException error) {
+            throw new StreamsException("Producer encounter unexpected error trying to commit a transaction for " + logMessage, error);
+        }
+    }
+
+    /**
+     * @throws IllegalStateException if EOS is disabled
+     */
+    public void abortTransaction() throws ProducerFencedException {
+        if (!eosEnabled) {
+            throw new IllegalStateException("EOS is disabled");
+        }
+        if (transactionInFlight) {
+            try {
+                producer.abortTransaction();
+            } catch (final ProducerFencedException ignore) {
+                /* TODO
+                 * this should actually never happen atm as we guard the call to #abortTransaction
+                 * -> the reason for the guard is a "bug" in the Producer -- it throws IllegalStateException
+                 * instead of ProducerFencedException atm. We can remove the isZombie flag after KAFKA-5604 got
+                 * fixed and fall-back to this catch-and-swallow code
+                 */
+
+                // can be ignored: transaction got already aborted by brokers/transactional-coordinator if this happens
+            } catch (final KafkaException error) {
+                throw new StreamsException("Producer encounter unexpected error trying to abort a transaction for " + logMessage, error);
+            }
+            transactionInFlight = false;
+        }
+    }
+
+    public List<PartitionInfo> partitionsFor(final String topic) throws TimeoutException {
+        return producer.partitionsFor(topic);
+    }
+
+    public void flush() {
+        producer.flush();
+    }
+
+    public void close() {
+        try {
+            producer.close();
+        } catch (final KafkaException error) {
+            throw new StreamsException("Producer encounter unexpected error trying to close" + (taskId == null ? "" : " " + logMessage), error);
+        }
+    }
+
+    // for testing only
+    Producer<byte[], byte[]> kafkaProducer() {
+        return producer;
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
index 48747e931ae2..657e34c26d53 100644
--- a/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
+++ b/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java
@@ -20,6 +20,7 @@
 import org.apache.kafka.clients.admin.DeleteRecordsResult;
 import org.apache.kafka.clients.admin.RecordsToDelete;
 import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.clients.producer.Producer;
 import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.common.errors.TimeoutException;
@@ -56,20 +57,22 @@ public class TaskManager {
     // activeTasks needs to be concurrent as it can be accessed
     // by QueryableState
     private final Logger log;
+    private final ChangelogReader changelogReader;
     private final UUID processId;
     private final String logPrefix;
-    private final InternalTopologyBuilder builder;
-    private final ChangelogReader changelogReader;
     private final StreamsMetricsImpl streamsMetrics;
     private final StreamThread.AbstractTaskCreator<? extends Task> activeTaskCreator;
     private final StreamThread.AbstractTaskCreator<? extends Task> standbyTaskCreator;
+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;
+    private final InternalTopologyBuilder builder;
+    private final Admin adminClient;
 
     private final Map<TaskId, Task> tasks = new TreeMap<>();
     // materializing this relationship because the lookup is on the hot path
     private final Map<TopicPartition, Task> partitionToTask = new HashMap<>();
 
-    private final Admin adminClient;
-    private Consumer<byte[], byte[]> consumer;
+    private Consumer<byte[], byte[]> mainConsumer;
+
     private DeleteRecordsResult deleteRecordsResult;
 
     private boolean rebalanceInProgress = false;  // if we are in the middle of a rebalance, it is not safe to commit
@@ -80,23 +83,25 @@ public class TaskManager {
                 final StreamsMetricsImpl streamsMetrics,
                 final StreamThread.AbstractTaskCreator<? extends Task> activeTaskCreator,
                 final StreamThread.AbstractTaskCreator<? extends Task> standbyTaskCreator,
+                final Map<TaskId, Producer<byte[], byte[]>> taskProducers,
                 final InternalTopologyBuilder builder,
                 final Admin adminClient) {
-        this.builder = builder;
+        this.changelogReader = changelogReader;
         this.processId = processId;
         this.logPrefix = logPrefix;
-        this.adminClient = adminClient;
         this.streamsMetrics = streamsMetrics;
-        this.changelogReader = changelogReader;
         this.activeTaskCreator = activeTaskCreator;
         this.standbyTaskCreator = standbyTaskCreator;
+        this.taskProducers = taskProducers;
+        this.builder = builder;
+        this.adminClient = adminClient;
 
         final LogContext logContext = new LogContext(logPrefix);
         this.log = logContext.logger(getClass());
     }
 
-    void setConsumer(final Consumer<byte[], byte[]> consumer) {
-        this.consumer = consumer;
+    void setMainConsumer(final Consumer<byte[], byte[]> mainConsumer) {
+        this.mainConsumer = mainConsumer;
     }
 
     public UUID processId() {
@@ -116,7 +121,7 @@ void handleRebalanceStart(final Set<String> subscribedTopics) {
     void handleRebalanceComplete() {
         // we should pause consumer only within the listener since
         // before then the assignment has not been updated yet.
-        consumer.pause(consumer.assignment());
+        mainConsumer.pause(mainConsumer.assignment());
 
         rebalanceInProgress = false;
     }
@@ -184,6 +189,8 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,
                     // We've already recorded the exception (which is the point of clean).
                     // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.
                     task.closeDirty();
+                } finally {
+                    taskProducers.remove(task.id());
                 }
 
                 iterator.remove();
@@ -200,11 +207,11 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,
         }
 
         if (!activeTasksToCreate.isEmpty()) {
-            activeTaskCreator.createTasks(consumer, activeTasksToCreate).forEach(this::addNewTask);
+            activeTaskCreator.createTasks(mainConsumer, activeTasksToCreate).forEach(this::addNewTask);
         }
 
         if (!standbyTasksToCreate.isEmpty()) {
-            standbyTaskCreator.createTasks(consumer, standbyTasksToCreate).forEach(this::addNewTask);
+            standbyTaskCreator.createTasks(mainConsumer, standbyTasksToCreate).forEach(this::addNewTask);
         }
 
         builder.addSubscribedTopicsFromAssignment(
@@ -231,6 +238,7 @@ private void addNewTask(final Task task) {
      *
      * @throws IllegalStateException If store gets registered after initialized is already finished
      * @throws StreamsException if the store's change log does not contain the partition
+     * @return {@code true} if all tasks are fully restored
      */
     boolean tryToCompleteRestoration() {
         boolean allRunning = true;
@@ -275,7 +283,7 @@ boolean tryToCompleteRestoration() {
 
         if (allRunning) {
             // we can call resume multiple times since it is idempotent.
-            consumer.resume(consumer.assignment());
+            mainConsumer.resume(mainConsumer.assignment());
         }
 
         return allRunning;
@@ -317,12 +325,18 @@ void handleLostAll() {
         final Iterator<Task> iterator = tasks.values().iterator();
         while (iterator.hasNext()) {
             final Task task = iterator.next();
+            final Set<TopicPartition> inputPartitions = task.inputPartitions();
             // Even though we've apparently dropped out of the group, we can continue safely to maintain our
             // standby tasks while we rejoin.
             if (task.isActive()) {
                 cleanupTask(task);
                 task.closeDirty();
                 iterator.remove();
+                taskProducers.remove(task.id());
+            }
+
+            for (final TopicPartition inputPartition : inputPartitions) {
+                partitionToTask.remove(inputPartition);
             }
         }
     }
diff --git a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java
index 5d495f64af37..2787ffa9913f 100644
--- a/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java
@@ -246,7 +246,7 @@ private void prepareStreams() throws Exception {
         globalStreamThread.shutdown();
         EasyMock.expectLastCall().andAnswer(() -> {
             supplier.restoreConsumer.close();
-            for (final MockProducer producer : supplier.producers) {
+            for (final MockProducer<byte[], byte[]> producer : supplier.producers) {
                 producer.close();
             }
             globalThreadState.set(GlobalStreamThread.State.DEAD);
@@ -302,7 +302,7 @@ private void prepareStreamThread(final StreamThread thread, final boolean termin
         EasyMock.expectLastCall().andAnswer(() -> {
             supplier.consumer.close();
             supplier.restoreConsumer.close();
-            for (final MockProducer producer : supplier.producers) {
+            for (final MockProducer<byte[], byte[]> producer : supplier.producers) {
                 producer.close();
             }
             state.set(StreamThread.State.DEAD);
@@ -478,7 +478,7 @@ public void shouldCleanupResourcesOnCloseWithoutPreviousStart() throws Exception
 
         assertTrue(supplier.consumer.closed());
         assertTrue(supplier.restoreConsumer.closed());
-        for (final MockProducer p : supplier.producers) {
+        for (final MockProducer<byte[], byte[]> p : supplier.producers) {
             assertTrue(p.closed());
         }
     }
@@ -920,7 +920,7 @@ public void process(final String key, final String value) {
             Serdes.String().deserializer(),
             globalTopicName,
             globalTopicName + "-processor",
-            new MockProcessorSupplier());
+            new MockProcessorSupplier<byte[], byte[]>());
         return topology;
     }
 
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
index b63f166a0fad..624f9dc3c8be 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java
@@ -556,7 +556,7 @@ public void close() {
         };
         stateManager.registerStore(stateStore, stateStore.stateRestoreCallback);
 
-        final ProcessorStateException thrown = assertThrows(ProcessorStateException.class, () -> stateManager.close());
+        final ProcessorStateException thrown = assertThrows(ProcessorStateException.class, stateManager::close);
         assertEquals(exception, thrown.getCause());
     }
 
@@ -572,7 +572,7 @@ public void close() {
         };
         stateManager.registerStore(stateStore, stateStore.stateRestoreCallback);
 
-        final StreamsException thrown = assertThrows(StreamsException.class, () -> stateManager.close());
+        final StreamsException thrown = assertThrows(StreamsException.class, stateManager::close);
         assertEquals(exception, thrown);
     }
 
@@ -724,7 +724,7 @@ private MockKeyValueStore getConverterStore() {
         return new ConverterStore(persistentStoreName, true);
     }
 
-    private class ConverterStore extends MockKeyValueStore implements TimestampedBytesStore {
+    private static class ConverterStore extends MockKeyValueStore implements TimestampedBytesStore {
         ConverterStore(final String name, final boolean persistent) {
             super(name, persistent);
         }
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java
index 7d4f1320a2df..116a8420bd42 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java
@@ -17,6 +17,7 @@
 package org.apache.kafka.streams.processor.internals;
 
 import org.apache.kafka.clients.consumer.CommitFailedException;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.clients.consumer.MockConsumer;
 import org.apache.kafka.clients.consumer.OffsetAndMetadata;
 import org.apache.kafka.clients.consumer.OffsetResetStrategy;
@@ -35,7 +36,6 @@
 import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.ProducerFencedException;
 import org.apache.kafka.common.errors.TimeoutException;
-import org.apache.kafka.common.errors.UnknownProducerIdException;
 import org.apache.kafka.common.header.Header;
 import org.apache.kafka.common.header.Headers;
 import org.apache.kafka.common.header.internals.RecordHeader;
@@ -45,15 +45,15 @@
 import org.apache.kafka.common.serialization.StringSerializer;
 import org.apache.kafka.common.utils.LogContext;
 import org.apache.kafka.common.utils.Utils;
-import org.apache.kafka.streams.StreamsConfig;
 import org.apache.kafka.streams.errors.AlwaysContinueProductionExceptionHandler;
+import org.apache.kafka.streams.errors.DefaultProductionExceptionHandler;
+import org.apache.kafka.streams.errors.ProductionExceptionHandler;
 import org.apache.kafka.streams.errors.StreamsException;
 import org.apache.kafka.streams.errors.TaskMigratedException;
 import org.apache.kafka.streams.processor.StreamPartitioner;
 import org.apache.kafka.streams.processor.TaskId;
 import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
 import org.apache.kafka.streams.processor.internals.testutil.LogCaptureAppender;
-import org.apache.kafka.test.StreamsTestUtils;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -62,10 +62,15 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
-import java.util.Properties;
 import java.util.concurrent.Future;
 import java.util.concurrent.atomic.AtomicBoolean;
 
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.mock;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.core.IsEqual.equalTo;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertThrows;
@@ -73,10 +78,10 @@
 
 public class RecordCollectorTest {
 
-    private final TaskId taskId = new TaskId(0, 0);
     private final LogContext logContext = new LogContext("test ");
+    private final TaskId taskId = new TaskId(0, 0);
+    private final ProductionExceptionHandler productionExceptionHandler = new DefaultProductionExceptionHandler();
     private final StreamsMetricsImpl streamsMetrics = new MockStreamsMetrics(new Metrics());
-    private final StreamsConfig streamsConfig = new StreamsConfig(StreamsTestUtils.getStreamsConfig("test"));
 
     private final String topic = "topic";
     private final Cluster cluster = new Cluster(
@@ -93,15 +98,26 @@ public class RecordCollectorTest {
 
     private final StringSerializer stringSerializer = new StringSerializer();
     private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();
-    private final MockProducer<byte[], byte[]> producer = new MockProducer<>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);
-    private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
+
     private final StreamPartitioner<String, Object> streamPartitioner = (topic, key, value, numPartitions) -> Integer.parseInt(key) % numPartitions;
 
+    private final MockConsumer<byte[], byte[]> mockConsumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(
+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);
+    private final StreamsProducer streamsProducer = new StreamsProducer(logContext, mockProducer);
+
     private RecordCollectorImpl collector;
 
     @Before
     public void setup() {
-        collector = new RecordCollectorImpl(taskId, streamsConfig, logContext, streamsMetrics, consumer, id -> producer);
+        collector = new RecordCollectorImpl(
+            logContext,
+            taskId,
+            mockConsumer,
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics);
     }
 
     @After
@@ -125,7 +141,7 @@ public void shouldSendToSpecificPartition() {
         assertEquals(2L, (long) offsets.get(new TopicPartition(topic, 0)));
         assertEquals(1L, (long) offsets.get(new TopicPartition(topic, 1)));
         assertEquals(0L, (long) offsets.get(new TopicPartition(topic, 2)));
-        assertEquals(6, producer.history().size());
+        assertEquals(6, mockProducer.history().size());
 
         collector.send(topic, "999", "0", null, 0, null, stringSerializer, stringSerializer);
         collector.send(topic, "999", "0", null, 1, null, stringSerializer, stringSerializer);
@@ -136,7 +152,7 @@ public void shouldSendToSpecificPartition() {
         assertEquals(3L, (long) offsets.get(new TopicPartition(topic, 0)));
         assertEquals(2L, (long) offsets.get(new TopicPartition(topic, 1)));
         assertEquals(1L, (long) offsets.get(new TopicPartition(topic, 2)));
-        assertEquals(9, producer.history().size());
+        assertEquals(9, mockProducer.history().size());
     }
 
     @Test
@@ -158,7 +174,7 @@ public void shouldSendWithPartitioner() {
         assertEquals(4L, (long) offsets.get(new TopicPartition(topic, 0)));
         assertEquals(2L, (long) offsets.get(new TopicPartition(topic, 1)));
         assertEquals(0L, (long) offsets.get(new TopicPartition(topic, 2)));
-        assertEquals(9, producer.history().size());
+        assertEquals(9, mockProducer.history().size());
 
         // returned offsets should not be modified
         final TopicPartition topicPartition = new TopicPartition(topic, 0);
@@ -185,27 +201,18 @@ public void shouldSendWithNoPartition() {
         assertEquals(3L, (long) offsets.get(new TopicPartition(topic, 0)));
         assertEquals(2L, (long) offsets.get(new TopicPartition(topic, 1)));
         assertEquals(1L, (long) offsets.get(new TopicPartition(topic, 2)));
-        assertEquals(9, producer.history().size());
+        assertEquals(9, mockProducer.history().size());
     }
 
     @Test
     public void shouldUpdateOffsetsUponCompletion() {
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
-            logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<>(cluster, false, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer)
-        );
-
         Map<TopicPartition, Long> offsets = collector.offsets();
 
         collector.send(topic, "999", "0", null, 0, null, stringSerializer, stringSerializer);
         collector.send(topic, "999", "0", null, 1, null, stringSerializer, stringSerializer);
         collector.send(topic, "999", "0", null, 2, null, stringSerializer, stringSerializer);
 
-        assertEquals(Collections.emptyMap(), offsets);
+        assertEquals(Collections.<TopicPartition, Long>emptyMap(), offsets);
 
         collector.flush();
 
@@ -216,194 +223,235 @@ public void shouldUpdateOffsetsUponCompletion() {
     }
 
     @Test
-    public void shouldThrowStreamsExceptionOnSendFatalException() {
-        final KafkaException exception = new KafkaException();
+    public void shouldPassThroughRecordHeaderToSerializer() {
+        final CustomStringSerializer keySerializer = new CustomStringSerializer();
+        final CustomStringSerializer valueSerializer = new CustomStringSerializer();
+        keySerializer.configure(Collections.emptyMap(), true);
+
+        collector.send(topic, "3", "0", new RecordHeaders(), null, keySerializer, valueSerializer, streamPartitioner);
+
+        final List<ProducerRecord<byte[], byte[]>> recordHistory = mockProducer.history();
+        for (final ProducerRecord<byte[], byte[]> sentRecord : recordHistory) {
+            final Headers headers = sentRecord.headers();
+            assertEquals(2, headers.toArray().length);
+            assertEquals(new RecordHeader("key", "key".getBytes()), headers.lastHeader("key"));
+            assertEquals(new RecordHeader("value", "value".getBytes()), headers.lastHeader("value"));
+        }
+    }
+
+    @Test
+    public void shouldCommitViaConsumerIfEosDisabled() {
+        final KafkaConsumer<byte[], byte[]> consumer = mock(KafkaConsumer.class);
+        consumer.commitSync((Map<TopicPartition, OffsetAndMetadata>) null);
+        expectLastCall();
+        replay(consumer);
+
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
+            taskId,
             consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {
-                    throw exception;
-                }
-            }
-        );
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics);
+
+        collector.commit(null);
+
+        verify(consumer);
 
-        final StreamsException thrown = assertThrows(StreamsException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
-        );
-        assertEquals(exception, thrown.getCause());
     }
 
     @Test
-    public void shouldThrowTaskMigratedExceptionOnProducerFencedException() {
-        final KafkaException exception = new ProducerFencedException("KABOOM!");
+    public void shouldCommitViaProducerIfEosEnabled() {
+        final StreamsProducer streamsProducer = mock(StreamsProducer.class);
+        streamsProducer.commitTransaction(null);
+        expectLastCall();
+        replay(streamsProducer);
+
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
-                    throw new KafkaException(exception);
-                }
-            }
-        );
+            taskId,
+            mockConsumer,
+            streamsProducer,
+            productionExceptionHandler,
+            true,
+            streamsMetrics);
 
-        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
-        );
-        assertEquals(exception, thrown.getCause());
+        collector.commit(null);
+
+        verify(streamsProducer);
     }
 
     @Test
-    public void shouldThrowTaskMigratedExceptionOnUnknownProducerIdException() {
-        final KafkaException exception = new UnknownProducerIdException("KABOOM!");
+    public void shouldForwardFlushToTransactionManager() {
+        final StreamsProducer streamsProducer = mock(StreamsProducer.class);
+        streamsProducer.flush();
+        expectLastCall();
+        replay(streamsProducer);
+
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
-                    throw new KafkaException(exception);
-                }
-            }
-        );
+            taskId,
+            mockConsumer,
+            streamsProducer,
+            productionExceptionHandler,
+            true,
+            streamsMetrics);
 
-        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
-        );
-        assertEquals(exception, thrown.getCause());
+        collector.flush();
+
+        verify(streamsProducer);
     }
 
     @Test
-    public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerFencedInCallback() {
-        final KafkaException exception = new ProducerFencedException("KABOOM!");
+    public void shouldForwardCloseToTransactionManager() {
+        final StreamsProducer streamsProducer = mock(StreamsProducer.class);
+        streamsProducer.close();
+        expectLastCall();
+        replay(streamsProducer);
+
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {
-                    callback.onCompletion(null, exception);
-                    return null;
-                }
-            }
-        );
+            taskId,
+            mockConsumer,
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics);
 
-        collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
+        collector.close();
 
-        TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
-        );
-        assertEquals(exception, thrown.getCause());
+        verify(streamsProducer);
+    }
 
-        thrown = assertThrows(TaskMigratedException.class, collector::flush);
-        assertEquals(exception, thrown.getCause());
+    @Test
+    public void shouldAbortTxIfEosEnabled() {
+        final StreamsProducer streamsProducer = mock(StreamsProducer.class);
+        streamsProducer.abortTransaction();
+        streamsProducer.close();
+        expectLastCall();
+        replay(streamsProducer);
 
-        thrown = assertThrows(TaskMigratedException.class, collector::close);
-        assertEquals(exception, thrown.getCause());
+        final RecordCollector collector = new RecordCollectorImpl(
+            logContext,
+            taskId,
+            mockConsumer,
+            streamsProducer,
+            productionExceptionHandler,
+            true,
+            streamsMetrics);
+
+        collector.close();
+
+        verify(streamsProducer);
     }
 
     @Test
-    public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerUnknownInCallback() {
-        final KafkaException exception = new UnknownProducerIdException("KABOOM!");
+    public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerFencedInCallback() {
+        final KafkaException exception = new ProducerFencedException("KABOOM!");
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
-                    callback.onCompletion(null, exception);
-                    return null;
-                }
-            });
+            taskId,
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
+                        callback.onCompletion(null, exception);
+                        return null;
+                    }
+                },
+                "appId",
+                taskId),
+            productionExceptionHandler,
+            true,
+            streamsMetrics
+        );
+        collector.initialize();
 
         collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
 
-        TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->
+        TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class, () ->
             collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
         );
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.ProducerFencedException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since the producer is fenced, indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated."));
 
         thrown = assertThrows(TaskMigratedException.class, collector::flush);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.ProducerFencedException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since the producer is fenced, indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated."));
 
         thrown = assertThrows(TaskMigratedException.class, collector::close);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.ProducerFencedException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since the producer is fenced, indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated."));
     }
 
     @Test
     public void shouldThrowStreamsExceptionOnSubsequentCallIfASendFailsWithDefaultExceptionHandler() {
         final KafkaException exception = new KafkaException("KABOOM!");
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {
-                    callback.onCompletion(null, exception);
-                    return null;
-                }
-            }
+            taskId,
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
+                        callback.onCompletion(null, exception);
+                        return null;
+                    }
+                }),
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
 
         collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
 
-        StreamsException thrown = assertThrows(StreamsException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
+        StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
         );
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.KafkaException: KABOOM!\nException handler choose to FAIL the processing, no more records would be sent."));
 
         thrown = assertThrows(StreamsException.class, collector::flush);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.KafkaException: KABOOM!\nException handler choose to FAIL the processing, no more records would be sent."));
 
         thrown = assertThrows(StreamsException.class, collector::close);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.KafkaException: KABOOM!\nException handler choose to FAIL the processing, no more records would be sent."));
     }
 
     @Test
     public void shouldNotThrowStreamsExceptionOnSubsequentCallIfASendFailsWithContinueExceptionHandler() {
-        final Metrics metrics = new Metrics();
         final LogCaptureAppender logCaptureAppender = LogCaptureAppender.createAndRegister();
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, AlwaysContinueProductionExceptionHandler.class.getName());
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            new MockStreamsMetrics(metrics),
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {
-                    callback.onCompletion(null, new Exception());
-                    return null;
-                }
-            }
+            taskId,
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
+                        callback.onCompletion(null, new Exception());
+                        return null;
+                    }
+                }),
+            new AlwaysContinueProductionExceptionHandler(),
+            false,
+            streamsMetrics
         );
 
         collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
         collector.flush();
 
-        final Metric metric = metrics.metrics().get(new MetricName(
+        final Metric metric = streamsMetrics.metrics().get(new MetricName(
             "dropped-records-total",
             "stream-task-metrics",
             "The total number of dropped records",
@@ -422,482 +470,189 @@ public synchronized Future<RecordMetadata> send(final ProducerRecord record, fin
     @Test
     public void shouldThrowStreamsExceptionOnSubsequentCallIfFatalEvenWithContinueExceptionHandler() {
         final KafkaException exception = new AuthenticationException("KABOOM!");
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, AlwaysContinueProductionExceptionHandler.class.getName());
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized Future<RecordMetadata> send(final ProducerRecord record, final Callback callback) {
-                    callback.onCompletion(null, exception);
-                    return null;
-                }
-            }
+            taskId,
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public synchronized Future<RecordMetadata> send(final ProducerRecord<byte[], byte[]> record, final Callback callback) {
+                        callback.onCompletion(null, exception);
+                        return null;
+                    }
+                }),
+            new AlwaysContinueProductionExceptionHandler(),
+            false,
+            streamsMetrics
         );
 
         collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
 
-        StreamsException thrown = assertThrows(StreamsException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
+        StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
         );
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error."));
 
         thrown = assertThrows(StreamsException.class, collector::flush);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error."));
 
         thrown = assertThrows(StreamsException.class, collector::close);
         assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic for task 0_0 due to:\norg.apache.kafka.common.errors.AuthenticationException: KABOOM!\nWritten offsets would not be recorded and no more records would be sent since this is a fatal error."));
     }
 
     @Test
-    public void shouldRethrowOnEOSInitializeTimeout() {
-        final KafkaException exception = new TimeoutException("KABOOM!");
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-
-        final RecordCollector recordCollector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void initTransactions() {
-                    throw exception;
-                }
-            }
-        );
-
-        final TimeoutException thrown = assertThrows(TimeoutException.class, recordCollector::initialize);
-        assertEquals(exception, thrown);
-    }
-
-    @Test
-    public void shouldThrowStreamsExceptionOnEOSInitializeError() {
-        final KafkaException exception = new KafkaException("KABOOM!");
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-
-        final RecordCollector recordCollector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void initTransactions() {
-                    throw exception;
-                }
-            }
-        );
-
-        final StreamsException thrown = assertThrows(StreamsException.class, recordCollector::initialize);
-        assertEquals(exception, thrown.getCause());
-    }
-
-    @Test
-    public void shouldThrowMigrateExceptionOnEOSFirstSendProducerFenced() {
-        final KafkaException exception = new ProducerFencedException("KABOOM!");
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
+    public void shouldThrowTaskMigratedExceptionOnCommitFailed() {
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized void beginTransaction() {
-                    throw exception;
-                }
-            }
-        );
-
-        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner));
-        assertEquals(exception, thrown.getCause());
-    }
-
-    @Test
-    public void shouldThrowMigrateExceptionOnEOSFirstSendFatal() {
-        final KafkaException exception = new KafkaException("KABOOM!");
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public synchronized void beginTransaction() {
-                    throw exception;
-                }
-            }
-        );
-
-        final StreamsException thrown = assertThrows(StreamsException.class, () ->
-            collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner));
-        assertEquals(exception, thrown.getCause());
-    }
-
-    @Test
-    public void shouldFailWithMigrateExceptionOnCommitFailed() {
-        final RecordCollector collector = new RecordCollectorImpl(
             taskId,
-            streamsConfig,
-            logContext,
-            streamsMetrics,
             new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {
                 @Override
                 public void commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets) {
                     throw new CommitFailedException();
                 }
             },
-            id -> new MockProducer<>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer)
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
 
-        assertThrows(TaskMigratedException.class, () -> collector.commit(null));
+        final TaskMigratedException thrown = assertThrows(TaskMigratedException.class, () -> collector.commit(null));
+
+        assertThat(thrown.getMessage(), equalTo("Consumer committing offsets failed, indicating the corresponding thread is no longer part of the group; it means all tasks belonging to this thread should be migrated."));
     }
 
     @Test
-    public void shouldFailWithMigrateExceptionOnCommitUnexpected() {
+    public void shouldThrowStreamsExceptionOnCommitTimeout() {
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            streamsMetrics,
+            taskId,
             new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {
                 @Override
                 public void commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets) {
-                    throw new KafkaException();
+                    throw new TimeoutException();
                 }
             },
-            id -> new MockProducer<>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer)
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
 
-        assertThrows(StreamsException.class, () -> collector.commit(null));
-    }
-
-    @Test
-    public void shouldFailWithMigrateExceptionOnEOSBeginTxnFenced() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void beginTransaction() {
-                    throw new ProducerFencedException("KABOOM!");
-                }
-            }
-        );
+        final StreamsException thrown = assertThrows(StreamsException.class, () -> collector.commit(null));
 
-        assertThrows(TaskMigratedException.class, () -> collector.commit(null));
+        assertThat(thrown.getMessage(), equalTo("Timed out while committing offsets via consumer for task 0_0"));
     }
 
     @Test
-    public void shouldFailWithMigrateExceptionOnEOSSendOffsetFenced() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
+    public void shouldStreamsExceptionOnCommitError() {
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void sendOffsetsToTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets, final String consumerGroupId) {
-                    throw new ProducerFencedException("KABOOM!");
-                }
-            }
-        );
-        collector.initialize();
-
-        assertThrows(TaskMigratedException.class, () -> collector.commit(null));
-    }
-
-    @Test
-    public void shouldFailWithMigrateExceptionOnEOSCommitFenced() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
             taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+            new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {
                 @Override
-                public void commitTransaction() {
-                    throw new ProducerFencedException("KABOOM!");
+                public void commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets) {
+                    throw new KafkaException();
                 }
-            }
+            },
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
         collector.initialize();
 
-        assertThrows(TaskMigratedException.class, () -> collector.commit(Collections.emptyMap()));
-    }
-
-    @Test
-    public void shouldFailWithMigrateExceptionOnEOSBeginTxnUnexpected() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void beginTransaction() {
-                    throw new KafkaException("KABOOM!");
-                }
-            }
-        );
+        final StreamsException thrown = assertThrows(StreamsException.class, () -> collector.commit(null));
 
-        assertThrows(StreamsException.class, () -> collector.commit(null));
+        assertThat(thrown.getMessage(), equalTo("Error encountered committing offsets via consumer for task 0_0"));
     }
 
     @Test
-    public void shouldFailWithStreamsExceptionOnEOSSendOffsetUnexpected() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
+    public void shouldFailOnCommitFatal() {
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void sendOffsetsToTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets, final String consumerGroupId) {
-                    throw new KafkaException("KABOOM!");
-                }
-            }
-        );
-        collector.initialize();
-
-        assertThrows(StreamsException.class, () -> collector.commit(null));
-    }
-
-    @Test
-    public void shouldFailWithMigrateExceptionOnEOSCommitUnexpected() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
             taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+            new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {
                 @Override
-                public void commitTransaction() {
-                    throw new KafkaException("KABOOM!");
+                public void commitSync(final Map<TopicPartition, OffsetAndMetadata> offsets) {
+                    throw new RuntimeException("KABOOM!");
                 }
-            }
+            },
+            streamsProducer,
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
         collector.initialize();
 
-        assertThrows(StreamsException.class, () -> collector.commit(Collections.emptyMap()));
-    }
-
-    @Test
-    public void shouldThrowStreamsExceptionOnEOSCloseFatalException() {
-        final KafkaException exception = new KafkaException();
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void close() {
-                    throw exception;
-                }
-            }
-        );
+        final RuntimeException thrown = assertThrows(RuntimeException.class, () -> collector.commit(null));
 
-        final StreamsException thrown = assertThrows(StreamsException.class, collector::close);
-        assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
     }
 
     @Test
     public void shouldNotAbortTxnOnEOSCloseIfNothingSent() {
         final AtomicBoolean functionCalled = new AtomicBoolean(false);
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void abortTransaction() {
-                    functionCalled.set(true);
-                    super.abortTransaction();
-                }
-            }
-        );
-
-        collector.close();
-
-        assertFalse(functionCalled.get());
-    }
-
-    @Test
-    public void shouldNotAbortTxnOnEOSCloseIfCommitted() {
-        final AtomicBoolean functionCalled = new AtomicBoolean(false);
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
             taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void abortTransaction() {
-                    functionCalled.set(true);
-                    super.abortTransaction();
-                }
-            }
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public void abortTransaction() {
+                        functionCalled.set(true);
+                    }
+                },
+                "appId",
+                taskId),
+            productionExceptionHandler,
+            true,
+            streamsMetrics
         );
-        collector.initialize();
-        collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
-        collector.commit(Collections.emptyMap());
 
         collector.close();
-
         assertFalse(functionCalled.get());
     }
 
     @Test
-    public void shouldThrowStreamsExceptionOnEOSAbortTxnFatalException() {
-        final KafkaException exception = new KafkaException();
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
+    public void shouldThrowIfTopicIsUnknownOnSendWithPartitioner() {
         final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            new StreamsConfig(props),
             logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void abortTransaction() {
-                    throw exception;
-                }
-            }
-        );
-        collector.initialize();
-        collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
-
-        final StreamsException thrown = assertThrows(StreamsException.class, collector::close);
-        assertEquals(exception, thrown.getCause());
-    }
-
-    @Test
-    public void shouldSwallowOnEOSAbortTxnFatalException() {
-        final Properties props = StreamsTestUtils.getStreamsConfig("test");
-        props.setProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
-        final RecordCollector collector = new RecordCollectorImpl(
             taskId,
-            new StreamsConfig(props),
-            logContext,
-            streamsMetrics,
-            consumer,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public void abortTransaction() {
-                    throw new ProducerFencedException("KABOOM!");
-                }
-            }
+            mockConsumer,
+            new StreamsProducer(
+                logContext,
+                new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
+                    @Override
+                    public List<PartitionInfo> partitionsFor(final String topic) {
+                        return Collections.emptyList();
+                    }
+                }),
+            productionExceptionHandler,
+            false,
+            streamsMetrics
         );
         collector.initialize();
 
-        // this call is to begin an inflight txn
-        collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner);
-
-        collector.close();
-    }
-
-    @Test
-    public void shouldThrowIfTopicIsUnknownOnSendWithPartitioner() {
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
-            logContext,
-            streamsMetrics,
-            null,
-            id -> new MockProducer<byte[], byte[]>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer) {
-                @Override
-                public List<PartitionInfo> partitionsFor(final String topic) {
-                    return Collections.emptyList();
-                }
-            }
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner)
         );
-
-        final StreamsException thrown = assertThrows(StreamsException.class, () -> collector.send(topic, "3", "0", null, null, stringSerializer, stringSerializer, streamPartitioner));
-        assertTrue(thrown.getMessage().startsWith("Could not get partition information for topic"));
+        assertThat(thrown.getMessage(), equalTo("Could not get partition information for topic topic for task 0_0. This can happen if the topic does not exist."));
     }
 
-    @Test
-    public void shouldPassThroughRecordHeaderToSerializer() {
-        final CustomStringSerializer keySerializer = new CustomStringSerializer();
-        final CustomStringSerializer valueSerializer = new CustomStringSerializer();
-        keySerializer.configure(Collections.emptyMap(), true);
-
-        final MockProducer<byte[], byte[]> mockProducer =
-            new MockProducer<>(cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);
-        final RecordCollector collector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
-            logContext,
-            streamsMetrics,
-            null,
-            id -> mockProducer
-        );
-
-        collector.send(topic, "3", "0", new RecordHeaders(), null, keySerializer, valueSerializer, streamPartitioner);
-
-        final List<ProducerRecord<byte[], byte[]>> recordHistory = mockProducer.history();
-        for (final ProducerRecord<byte[], byte[]> sentRecord : recordHistory) {
-            final Headers headers = sentRecord.headers();
-            assertEquals(2, headers.toArray().length);
-            assertEquals(new RecordHeader("key", "key".getBytes()), headers.lastHeader("key"));
-            assertEquals(new RecordHeader("value", "value".getBytes()), headers.lastHeader("value"));
-        }
-    }
 
     private static class CustomStringSerializer extends StringSerializer {
-
         private boolean isKey;
 
-        private CustomStringSerializer() {
-        }
-
         @Override
         public void configure(final Map<String, ?> configs, final boolean isKey) {
             this.isKey = isKey;
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
index 2570d4a3d86c..f3a16a7d720f 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java
@@ -32,6 +32,7 @@
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.ProducerFencedException;
 import org.apache.kafka.common.metrics.JmxReporter;
 import org.apache.kafka.common.metrics.KafkaMetric;
 import org.apache.kafka.common.metrics.Measurable;
@@ -249,7 +250,7 @@ public void shouldChangeStateInRebalanceListener() {
         // assign single partition
         assignedPartitions = Collections.singletonList(t1p1);
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -425,6 +426,7 @@ public void shouldNotCommitBeforeTheCommitInterval() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -447,10 +449,10 @@ public void shouldNotCommitBeforeTheCommitInterval() {
 
     @Test
     public void shouldRespectNumIterationsInMainLoop() {
-        final MockProcessor mockProcessor = new MockProcessor(PunctuationType.WALL_CLOCK_TIME, 10L);
+        final MockProcessor<byte[], byte[]> mockProcessor = new MockProcessor<>(PunctuationType.WALL_CLOCK_TIME, 10L);
         internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);
         internalTopologyBuilder.addProcessor("processor1", () -> mockProcessor, "source1");
-        internalTopologyBuilder.addProcessor("processor2", () -> new MockProcessor(PunctuationType.STREAM_TIME, 10L), "source1");
+        internalTopologyBuilder.addProcessor("processor2", () -> new MockProcessor<byte[], byte[]>(PunctuationType.STREAM_TIME, 10L), "source1");
 
         final Properties properties = new Properties();
         properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);
@@ -472,7 +474,7 @@ public void shouldRespectNumIterationsInMainLoop() {
             Collections.emptyMap()
         );
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(Collections.singleton(t1p1));
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -554,6 +556,7 @@ public void shouldNotCauseExceptionIfNothingCommitted() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -591,6 +594,7 @@ public void shouldCommitAfterTheCommitInterval() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             changelogReader,
@@ -633,7 +637,7 @@ public void shouldInjectSharedProducerForAllTasksUsingClientSupplierOnCreateIfEo
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         final Map<TopicPartition, Long> beginOffsets = new HashMap<>();
         beginOffsets.put(t1p1, 0L);
@@ -642,11 +646,11 @@ public void shouldInjectSharedProducerForAllTasksUsingClientSupplierOnCreateIfEo
         thread.rebalanceListener.onPartitionsAssigned(new HashSet<>(assignedPartitions));
 
         assertEquals(1, clientSupplier.producers.size());
-        final Producer globalProducer = clientSupplier.producers.get(0);
+        final Producer<byte[], byte[]> globalProducer = clientSupplier.producers.get(0);
         for (final Task task : thread.activeTasks()) {
             assertSame(globalProducer, ((RecordCollectorImpl) ((StreamTask) task).recordCollector()).producer());
         }
-        assertSame(clientSupplier.consumer, thread.consumer);
+        assertSame(clientSupplier.consumer, thread.mainConsumer);
         assertSame(clientSupplier.restoreConsumer, thread.restoreConsumer);
     }
 
@@ -670,7 +674,7 @@ public void shouldInjectProducerPerTaskUsingClientSupplierOnCreateIfEosEnable()
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         final Map<TopicPartition, Long> beginOffsets = new HashMap<>();
         beginOffsets.put(t1p1, 0L);
@@ -681,7 +685,7 @@ public void shouldInjectProducerPerTaskUsingClientSupplierOnCreateIfEosEnable()
         thread.runOnce();
 
         assertEquals(thread.activeTasks().size(), clientSupplier.producers.size());
-        assertSame(clientSupplier.consumer, thread.consumer);
+        assertSame(clientSupplier.consumer, thread.mainConsumer);
         assertSame(clientSupplier.restoreConsumer, thread.restoreConsumer);
     }
 
@@ -718,7 +722,7 @@ public void shouldCloseAllTaskProducersOnCloseIfEosEnabled() throws InterruptedE
             "Thread never shut down.");
 
         for (final Task task : thread.activeTasks()) {
-            assertTrue(((MockProducer) ((RecordCollectorImpl) ((StreamTask) task).recordCollector()).producer()).closed());
+            assertTrue(((MockProducer<byte[], byte[]>) ((RecordCollectorImpl) ((StreamTask) task).recordCollector()).producer()).closed());
         }
     }
 
@@ -737,6 +741,7 @@ public void shouldShutdownTaskManagerOnClose() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -773,6 +778,7 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -803,6 +809,7 @@ public void shouldOnlyShutdownOnce() {
             config,
             null,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -863,14 +870,14 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerWasFencedWhilePr
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
 
         thread.runOnce();
         assertThat(thread.activeTasks().size(), equalTo(1));
-        final MockProducer producer = clientSupplier.producers.get(0);
+        final MockProducer<byte[], byte[]> producer = clientSupplier.producers.get(0);
 
         // change consumer subscription from "pattern" to "manual" to be able to call .addRecords()
         consumer.updateBeginningOffsets(Collections.singletonMap(assignedPartitions.iterator().next(), 0L));
@@ -921,7 +928,7 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -930,7 +937,7 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi
 
         assertThat(thread.activeTasks().size(), equalTo(1));
 
-        clientSupplier.producers.get(0).fenceProducerOnCommitTxn();
+        clientSupplier.producers.get(0).commitTransactionException = new ProducerFencedException("Producer is fenced");
         thread.rebalanceListener.onPartitionsRevoked(assignedPartitions);
         assertTrue(thread.rebalanceException() instanceof TaskMigratedException);
         assertFalse(clientSupplier.producers.get(0).transactionCommitted());
@@ -961,16 +968,16 @@ public void shouldNotCloseTaskAndRemoveFromTaskManagerIfProducerGotFencedInCommi
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
 
         thread.runOnce();
         assertThat(thread.activeTasks().size(), equalTo(1));
-        final MockProducer producer = clientSupplier.producers.get(0);
+        final MockProducer<byte[], byte[]> producer = clientSupplier.producers.get(0);
 
-        producer.fenceProducerOnCommitTxn();
+        producer.commitTransactionException = new ProducerFencedException("Producer is fenced");
         mockTime.sleep(config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG) + 1L);
         consumer.addRecord(new ConsumerRecord<>(topic1, 1, 1, new byte[0], new byte[0]));
         try {
@@ -1009,7 +1016,7 @@ public void shouldNotCloseTaskProducerWhenSuspending() {
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -1042,7 +1049,7 @@ public void shouldReturnActiveTaskMetadataWhileRunningState() {
 
         thread.taskManager().handleAssignment(activeTasks, Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(assignedPartitions);
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -1210,7 +1217,8 @@ public void shouldNotCreateStandbyTaskWithoutStateStores() {
     @Test
     public void shouldNotCreateStandbyTaskIfStateStoresHaveLoggingDisabled() {
         setupInternalTopologyWithoutState();
-        final StoreBuilder storeBuilder = new MockKeyValueStoreBuilder("myStore", true);
+        final StoreBuilder<KeyValueStore<Object, Object>> storeBuilder =
+            new MockKeyValueStoreBuilder("myStore", true);
         storeBuilder.withLoggingDisabled();
         internalTopologyBuilder.addStateStore(storeBuilder, "processor1");
 
@@ -1371,7 +1379,7 @@ public void shouldRecoverFromInvalidOffsetExceptionOnRestoreAndFinishRestore() t
         internalStreamsBuilder.buildAndOptimizeTopology();
 
         final StreamThread thread = createStreamThread("clientId", config, false);
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         final MockConsumer<byte[], byte[]> mockRestoreConsumer = (MockConsumer<byte[], byte[]>) thread.restoreConsumer;
 
         final TopicPartition topicPartition = new TopicPartition("topic", 0);
@@ -1506,7 +1514,7 @@ private void shouldLogAndRecordSkippedMetricForDeserializationException(final St
             Collections.singletonMap(task1, assignedPartitions),
             Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(Collections.singleton(t1p1));
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -1595,7 +1603,7 @@ private void shouldLogAndRecordSkippedRecordsForInvalidTimestamps(final String b
                 assignedPartitions),
             Collections.emptyMap());
 
-        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.consumer;
+        final MockConsumer<byte[], byte[]> mockConsumer = (MockConsumer<byte[], byte[]>) thread.mainConsumer;
         mockConsumer.assign(Collections.singleton(t1p1));
         mockConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));
         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);
@@ -1699,6 +1707,7 @@ public void shouldConstructProducerMetricsWithoutEOS() {
             config,
             producer,
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -1735,6 +1744,7 @@ public void shouldConstructProducerMetricsWithEOS() {
             new StreamsConfig(configProps(true)),
             null,       // with EOS the thread producer should be null
             null,
+            null,
             consumer,
             consumer,
             null,
@@ -1757,7 +1767,7 @@ public void shouldConstructProducerMetricsWithEOS() {
         // without creating tasks the metrics should be empty
         producer.setMockMetrics(testMetricName, testMetric);
         final Map<MetricName, Metric> producerMetrics = thread.producerMetrics();
-        assertEquals(Collections.emptyMap(), producerMetrics);
+        assertEquals(Collections.<MetricName, Metric>emptyMap(), producerMetrics);
     }
 
     @Test
@@ -1777,6 +1787,7 @@ public void shouldConstructAdminMetrics() {
             mockTime,
             config,
             producer,
+            null,
             adminClient,
             consumer,
             consumer,
@@ -1815,7 +1826,7 @@ private TaskManager mockTaskManagerCommit(final Consumer<byte[], byte[]> consume
     }
 
     private void setupInternalTopologyWithoutState() {
-        final MockProcessor mockProcessor = new MockProcessor();
+        final MockProcessor<byte[], byte[]> mockProcessor = new MockProcessor<>();
         internalTopologyBuilder.addSource(null, "source1", null, null, null, topic1);
         internalTopologyBuilder.addProcessor("processor1", () -> mockProcessor, "source1");
     }
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java
new file mode 100644
index 000000000000..d3c81c7a834c
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java
@@ -0,0 +1,638 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.processor.internals;
+
+import org.apache.kafka.clients.consumer.OffsetAndMetadata;
+import org.apache.kafka.clients.producer.MockProducer;
+import org.apache.kafka.clients.producer.Producer;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.clients.producer.internals.DefaultPartitioner;
+import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.KafkaException;
+import org.apache.kafka.common.Node;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.errors.ProducerFencedException;
+import org.apache.kafka.common.errors.TimeoutException;
+import org.apache.kafka.common.errors.UnknownProducerIdException;
+import org.apache.kafka.common.header.internals.RecordHeaders;
+import org.apache.kafka.common.serialization.ByteArraySerializer;
+import org.apache.kafka.common.utils.LogContext;
+import org.apache.kafka.streams.errors.StreamsException;
+import org.apache.kafka.streams.errors.TaskMigratedException;
+import org.apache.kafka.streams.processor.TaskId;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+
+import static org.apache.kafka.common.utils.Utils.mkEntry;
+import static org.apache.kafka.common.utils.Utils.mkMap;
+import static org.easymock.EasyMock.expect;
+import static org.easymock.EasyMock.expectLastCall;
+import static org.easymock.EasyMock.mock;
+import static org.easymock.EasyMock.replay;
+import static org.easymock.EasyMock.verify;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.core.IsEqual.equalTo;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertThrows;
+import static org.junit.Assert.assertTrue;
+
+public class StreamsProducerTest {
+
+    private final LogContext logContext = new LogContext("test ");
+    private final String topic = "topic";
+    private final Cluster cluster = new Cluster(
+        "cluster",
+        Collections.singletonList(Node.noNode()),
+        Collections.singletonList(new PartitionInfo(topic, 0, Node.noNode(), new Node[0], new Node[0])),
+        Collections.emptySet(),
+        Collections.emptySet()
+    );
+    private final TaskId taskId = new TaskId(0, 0);
+    private final ByteArraySerializer byteArraySerializer = new ByteArraySerializer();
+    private final Map<TopicPartition, OffsetAndMetadata> offsetsAndMetadata = mkMap(
+        mkEntry(new TopicPartition(topic, 0), new OffsetAndMetadata(0L, null))
+    );
+
+    private final MockProducer<byte[], byte[]> mockProducer = new MockProducer<>(
+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);
+    private final StreamsProducer streamsProducer = new StreamsProducer(logContext, mockProducer);
+
+    private final MockProducer<byte[], byte[]> eosMockProducer = new MockProducer<>(
+        cluster, true, new DefaultPartitioner(), byteArraySerializer, byteArraySerializer);
+    private final StreamsProducer eosStreamsProducer = new StreamsProducer(logContext, eosMockProducer, "appId", taskId);
+
+    private final ProducerRecord<byte[], byte[]> record =
+        new ProducerRecord<>(topic, 0, 0L, new byte[0], new byte[0], new RecordHeaders());
+
+    @Before
+    public void before() {
+        eosStreamsProducer.initTransaction();
+    }
+
+    @Test
+    public void shouldFailIfProducerIsNull() {
+        {
+            final NullPointerException thrown = assertThrows(
+                NullPointerException.class,
+                () -> new StreamsProducer(logContext, null)
+            );
+
+            assertThat(thrown.getMessage(), equalTo("producer cannot be null"));
+        }
+
+        {
+            final NullPointerException thrown = assertThrows(
+                NullPointerException.class,
+                () -> new StreamsProducer(logContext, null, "appId", taskId)
+            );
+
+            assertThat(thrown.getMessage(), equalTo("producer cannot be null"));
+        }
+    }
+
+    @Test
+    public void shouldThrowIfIncorrectlyInitialized() {
+        {
+            final IllegalArgumentException thrown = assertThrows(
+                IllegalArgumentException.class,
+                () -> new StreamsProducer(logContext, mockProducer, null, taskId)
+            );
+            assertThat(thrown.getMessage(), equalTo("applicationId and taskId must either be both null or both be not null"));
+        }
+
+        {
+            final IllegalArgumentException thrown = assertThrows(
+                IllegalArgumentException.class,
+                () -> new StreamsProducer(logContext, mockProducer, "appId", null)
+            );
+            assertThat(thrown.getMessage(), equalTo("applicationId and taskId must either be both null or both be not null"));
+        }
+    }
+
+    // non-eos tests
+
+    // functional tests
+
+    @Test
+    public void shouldNotInitTxIfEosDisable() {
+        assertFalse(mockProducer.transactionInitialized());
+    }
+
+    @Test
+    public void shouldNotBeginTxOnSendIfEosDisable() {
+        streamsProducer.send(record, null);
+        assertFalse(mockProducer.transactionInFlight());
+    }
+
+    @Test
+    public void shouldForwardRecordOnSend() {
+        streamsProducer.send(record, null);
+        assertThat(mockProducer.history().size(), equalTo(1));
+        assertThat(mockProducer.history().get(0), equalTo(record));
+    }
+
+    @Test
+    public void shouldForwardCallToPartitionsFor() {
+        final Producer<byte[], byte[]> producer = mock(Producer.class);
+
+        final List<PartitionInfo> expectedPartitionInfo = Collections.emptyList();
+        expect(producer.partitionsFor("topic")).andReturn(expectedPartitionInfo);
+        replay(producer);
+
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, producer);
+
+        final List<PartitionInfo> partitionInfo = streamsProducer.partitionsFor(topic);
+
+        assertSame(expectedPartitionInfo, partitionInfo);
+        verify(producer);
+    }
+
+    @Test
+    public void shouldForwardCallToFlush() {
+        final Producer<byte[], byte[]> producer = mock(Producer.class);
+
+        producer.flush();
+        expectLastCall();
+        replay(producer);
+
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, producer);
+
+        streamsProducer.flush();
+
+        verify(producer);
+    }
+
+    @Test
+    public void shouldCloseProducerOnClose() {
+        streamsProducer.close();
+        assertTrue(mockProducer.closed());
+    }
+
+    // error handling tests
+
+    @Test
+    public void shouldFailOnInitTxIfEosDisabled() {
+        final IllegalStateException thrown = assertThrows(
+            IllegalStateException.class,
+            streamsProducer::initTransaction
+        );
+
+        assertThat(thrown.getMessage(), equalTo("EOS is disabled"));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnSendError() {
+        mockProducer.sendException  = new KafkaException("KABOOM!");
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> streamsProducer.send(record, null)
+        );
+
+        assertEquals(mockProducer.sendException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered sending record to topic topic due to:\norg.apache.kafka.common.KafkaException: KABOOM!"));
+    }
+
+    @Test
+    public void shouldFailOnSendFatal() {
+        mockProducer.sendException = new RuntimeException("KABOOM!");
+
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            () -> streamsProducer.send(record, null)
+        );
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldFailOnCommitIfEosDisabled() {
+        final IllegalStateException thrown = assertThrows(
+            IllegalStateException.class,
+            () -> streamsProducer.commitTransaction(null)
+        );
+
+        assertThat(thrown.getMessage(), equalTo("EOS is disabled"));
+    }
+
+    @Test
+    public void shouldFailOnAbortIfEosDisabled() {
+        final IllegalStateException thrown = assertThrows(
+            IllegalStateException.class,
+            streamsProducer::abortTransaction
+        );
+
+        assertThat(thrown.getMessage(), equalTo("EOS is disabled"));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnCloseError() {
+        mockProducer.closeException = new KafkaException("KABOOM!");
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            streamsProducer::close
+        );
+
+        assertEquals(mockProducer.closeException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer encounter unexpected error trying to close"));
+    }
+
+    @Test
+    public void shouldFailOnCloseFatal() {
+        mockProducer.closeException = new RuntimeException("KABOOM!");
+
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            streamsProducer::close
+        );
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    // EOS tests
+
+    // functional tests
+
+    @Test
+    public void shouldInitTxOnEos() {
+        assertTrue(eosMockProducer.transactionInitialized());
+    }
+
+    @Test
+    public void shouldBeginTxOnEosSend() {
+        eosStreamsProducer.send(record, null);
+        assertTrue(eosMockProducer.transactionInFlight());
+    }
+
+    @Test
+    public void shouldContinueTxnSecondEosSend() {
+        eosStreamsProducer.send(record, null);
+        eosStreamsProducer.send(record, null);
+        assertTrue(eosMockProducer.transactionInFlight());
+        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(2));
+    }
+
+    @Test
+    public void shouldForwardRecordButNotCommitOnEosSend() {
+        eosStreamsProducer.send(record, null);
+        assertTrue(eosMockProducer.transactionInFlight());
+        assertTrue(eosMockProducer.history().isEmpty());
+        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(1));
+        assertThat(eosMockProducer.uncommittedRecords().get(0), equalTo(record));
+    }
+
+    @Test
+    public void shouldBeginTxOnEosCommit() {
+        final Producer<byte[], byte[]> producer = mock(Producer.class);
+
+        producer.initTransactions();
+        producer.beginTransaction();
+        producer.sendOffsetsToTransaction(offsetsAndMetadata, "appId");
+        producer.commitTransaction();
+        expectLastCall();
+        replay(producer);
+
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, producer, "appId", taskId);
+        streamsProducer.initTransaction();
+
+        streamsProducer.commitTransaction(offsetsAndMetadata);
+
+        verify(producer);
+    }
+
+    @Test
+    public void shouldSendOffsetToTxOnEosCommit() {
+        eosStreamsProducer.commitTransaction(offsetsAndMetadata);
+        assertTrue(eosMockProducer.sentOffsets());
+    }
+
+    @Test
+    public void shouldCommitTxOnEosCommit() {
+        eosStreamsProducer.send(record, null);
+        assertTrue(eosMockProducer.transactionInFlight());
+
+        eosStreamsProducer.commitTransaction(offsetsAndMetadata);
+
+        assertFalse(eosMockProducer.transactionInFlight());
+        assertTrue(eosMockProducer.uncommittedRecords().isEmpty());
+        assertTrue(eosMockProducer.uncommittedOffsets().isEmpty());
+        assertThat(eosMockProducer.history().size(), equalTo(1));
+        assertThat(eosMockProducer.history().get(0), equalTo(record));
+        assertThat(eosMockProducer.consumerGroupOffsetsHistory().size(), equalTo(1));
+        assertThat(eosMockProducer.consumerGroupOffsetsHistory().get(0).get("appId"), equalTo(offsetsAndMetadata));
+    }
+
+    @Test
+    public void shouldAbortTxOnEosAbort() {
+        // call `send()` to start a transaction
+        eosStreamsProducer.send(record, null);
+        assertTrue(eosMockProducer.transactionInFlight());
+        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(1));
+        assertThat(eosMockProducer.uncommittedRecords().get(0), equalTo(record));
+
+        eosStreamsProducer.abortTransaction();
+
+        assertFalse(eosMockProducer.transactionInFlight());
+        assertTrue(eosMockProducer.uncommittedRecords().isEmpty());
+        assertTrue(eosMockProducer.uncommittedOffsets().isEmpty());
+        assertTrue(eosMockProducer.history().isEmpty());
+        assertTrue(eosMockProducer.consumerGroupOffsetsHistory().isEmpty());
+    }
+
+    @Test
+    public void shouldSkipAbortTxOnEosAbortIfNotTxInFlight() {
+        final Producer<byte[], byte[]> producer = mock(Producer.class);
+
+        producer.initTransactions();
+        expectLastCall();
+        replay(producer);
+
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, producer, "appId", taskId);
+        streamsProducer.initTransaction();
+
+        streamsProducer.abortTransaction();
+
+        verify(producer);
+    }
+
+    // error handling tests
+
+    @Test
+    public void shouldThrowTimeoutExceptionOnEosInitTxTimeout() {
+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init
+        mockProducer.initTransactionException = new TimeoutException("KABOOM!");
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, mockProducer, "appId", taskId);
+
+        final TimeoutException thrown = assertThrows(
+            TimeoutException.class,
+            streamsProducer::initTransaction
+        );
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnEosInitError() {
+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init
+        mockProducer.initTransactionException = new KafkaException("KABOOM!");
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, mockProducer, "appId", taskId);
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            streamsProducer::initTransaction
+        );
+
+        assertEquals(mockProducer.initTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Error encountered while initializing transactions for task 0_0"));
+    }
+
+    @Test
+    public void shouldFailOnEosInitFatal() {
+        // use `mockProducer` instead of `eosMockProducer` to avoid double Tx-Init
+        mockProducer.initTransactionException = new RuntimeException("KABOOM!");
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, mockProducer, "appId", taskId);
+
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            streamsProducer::initTransaction
+        );
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldThrowTaskMigrateExceptionOnEosBeginTxnFenced() {
+        eosMockProducer.fenceProducer();
+
+        final TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class,
+            () -> eosStreamsProducer.send(null, null)
+        );
+
+        assertThat(thrown.getMessage(), equalTo("Producer get fenced trying to begin a new transaction; it means all tasks belonging to this thread should be migrated."));
+    }
+
+    @Test
+    public void shouldThrowTaskMigrateExceptionOnEosBeginTxnError() {
+        eosMockProducer.beginTransactionException = new KafkaException("KABOOM!");
+
+        // calling `send()` implicitly starts a new transaction
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> eosStreamsProducer.send(null, null));
+
+        assertEquals(eosMockProducer.beginTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer encounter unexpected error trying to begin a new transaction for task 0_0"));
+    }
+
+    @Test
+    public void shouldFailOnEosBeginTxnFatal() {
+        eosMockProducer.beginTransactionException = new RuntimeException("KABOOM!");
+
+        // calling `send()` implicitly starts a new transaction
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            () -> eosStreamsProducer.send(null, null));
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldThrowTaskMigratedExceptionOnEosSendFenced() {
+        // cannot use `eosMockProducer.fenceProducer()` because this would already trigger in `beginTransaction()`
+        final ProducerFencedException exception = new ProducerFencedException("KABOOM!");
+        // we need to mimic that `send()` always wraps error in a KafkaException
+        eosMockProducer.sendException = new KafkaException(exception);
+
+        final TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class,
+            () -> eosStreamsProducer.send(record, null)
+        );
+
+        assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer cannot send records anymore since it got fenced; it means all tasks belonging to this thread should be migrated."));
+    }
+
+    @Test
+    public void shouldThrowTaskMigratedExceptionOnEosSendUnknownPid() {
+        final UnknownProducerIdException exception = new UnknownProducerIdException("KABOOM!");
+        // we need to mimic that `send()` always wraps error in a KafkaException
+        eosMockProducer.sendException = new KafkaException(exception);
+
+        final TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class,
+            () -> eosStreamsProducer.send(record, null)
+        );
+
+        assertEquals(exception, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer cannot send records anymore since it got fenced; it means all tasks belonging to this thread should be migrated."));
+    }
+
+    @Test
+    public void shouldThrowTaskMigrateExceptionOnEosSendOffsetFenced() {
+        // cannot use `eosMockProducer.fenceProducer()` because this would already trigger in `beginTransaction()`
+        eosMockProducer.sendOffsetsToTransactionException = new ProducerFencedException("KABOOM!");
+
+        final TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class,
+            // we pass in `null` to verify that `sendOffsetsToTransaction()` fails instead of `commitTransaction()`
+            // `sendOffsetsToTransaction()` would throw an NPE on `null` offsets
+            () -> eosStreamsProducer.commitTransaction(null)
+        );
+
+        assertEquals(eosMockProducer.sendOffsetsToTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated."));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnEosSendOffsetError() {
+        eosMockProducer.sendOffsetsToTransactionException = new KafkaException("KABOOM!");
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            // we pass in `null` to verify that `sendOffsetsToTransaction()` fails instead of `commitTransaction()`
+            // `sendOffsetsToTransaction()` would throw an NPE on `null` offsets
+            () -> eosStreamsProducer.commitTransaction(null)
+        );
+
+        assertEquals(eosMockProducer.sendOffsetsToTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer encounter unexpected error trying to commit a transaction for task 0_0"));
+    }
+
+    @Test
+    public void shouldFailOnEosSendOffsetFatal() {
+        eosMockProducer.sendOffsetsToTransactionException = new RuntimeException("KABOOM!");
+
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            // we pass in `null` to verify that `sendOffsetsToTransaction()` fails instead of `commitTransaction()`
+            // `sendOffsetsToTransaction()` would throw an NPE on `null` offsets
+            () -> eosStreamsProducer.commitTransaction(null)
+        );
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldThrowTaskMigrateExceptionOnEosCommitTxFenced() {
+        // cannot use `eosMockProducer.fenceProducer()` because this would already trigger in `beginTransaction()`
+        eosMockProducer.commitTransactionException = new ProducerFencedException("KABOOM!");
+
+        final TaskMigratedException thrown = assertThrows(
+            TaskMigratedException.class,
+            () -> eosStreamsProducer.commitTransaction(offsetsAndMetadata)
+        );
+
+        assertTrue(eosMockProducer.sentOffsets());
+        assertEquals(eosMockProducer.commitTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated."));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnEosCommitTxTimeout() {
+        // cannot use `eosMockProducer.fenceProducer()` because this would already trigger in `beginTransaction()`
+        eosMockProducer.commitTransactionException = new TimeoutException("KABOOM!");
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> eosStreamsProducer.commitTransaction(offsetsAndMetadata)
+        );
+
+        assertTrue(eosMockProducer.sentOffsets());
+        assertEquals(eosMockProducer.commitTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Timed out while committing a transaction for task " + taskId));
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnEosCommitTxError() {
+        eosMockProducer.commitTransactionException = new KafkaException("KABOOM!");
+
+        final StreamsException thrown = assertThrows(
+            StreamsException.class,
+            () -> eosStreamsProducer.commitTransaction(offsetsAndMetadata)
+        );
+
+        assertTrue(eosMockProducer.sentOffsets());
+        assertEquals(eosMockProducer.commitTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer encounter unexpected error trying to commit a transaction for task 0_0"));
+    }
+
+    @Test
+    public void shouldFailOnEosCommitTxFatal() {
+        eosMockProducer.commitTransactionException = new RuntimeException("KABOOM!");
+
+        final RuntimeException thrown = assertThrows(
+            RuntimeException.class,
+            () -> eosStreamsProducer.commitTransaction(offsetsAndMetadata)
+        );
+
+        assertTrue(eosMockProducer.sentOffsets());
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+
+    @Test
+    public void shouldSwallowExceptionOnEosAbortTxFenced() {
+        final Producer<byte[], byte[]> producer = mock(Producer.class);
+
+        producer.initTransactions();
+        producer.beginTransaction();
+        expect(producer.send(record, null)).andReturn(null);
+        producer.abortTransaction();
+        expectLastCall().andThrow(new ProducerFencedException("KABOOM!"));
+        replay(producer);
+
+        final StreamsProducer streamsProducer = new StreamsProducer(logContext, producer, "appId", taskId);
+        streamsProducer.initTransaction();
+        // call `send()` to start a transaction
+        streamsProducer.send(record, null);
+
+        streamsProducer.abortTransaction();
+
+        verify(producer);
+    }
+
+    @Test
+    public void shouldThrowStreamsExceptionOnEosAbortTxError() {
+        eosMockProducer.abortTransactionException = new KafkaException("KABOOM!");
+        // call `send()` to start a transaction
+        eosStreamsProducer.send(record, null);
+
+        final StreamsException thrown = assertThrows(StreamsException.class, eosStreamsProducer::abortTransaction);
+
+        assertEquals(eosMockProducer.abortTransactionException, thrown.getCause());
+        assertThat(thrown.getMessage(), equalTo("Producer encounter unexpected error trying to abort a transaction for task 0_0"));
+    }
+
+    @Test
+    public void shouldFailOnEosAbortTxFatal() {
+        eosMockProducer.abortTransactionException = new RuntimeException("KABOOM!");
+        // call `send()` to start a transaction
+        eosStreamsProducer.send(record, null);
+
+        final RuntimeException thrown = assertThrows(RuntimeException.class, eosStreamsProducer::abortTransaction);
+
+        assertThat(thrown.getMessage(), equalTo("KABOOM!"));
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
index 575e98ad8512..92093b55daed 100644
--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java
@@ -125,9 +125,10 @@ public void setUp() {
                                       streamsMetrics,
                                       activeTaskCreator,
                                       standbyTaskCreator,
+                                      new HashMap<>(),
                                       topologyBuilder,
                                       adminClient);
-        taskManager.setConsumer(consumer);
+        taskManager.setMainConsumer(consumer);
     }
 
     @Test
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java b/streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java
index 587e944285ab..5e6a3e49d0ab 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java
@@ -30,6 +30,7 @@
 import org.apache.kafka.common.utils.LogContext;
 import org.apache.kafka.streams.KeyValue;
 import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.errors.DefaultProductionExceptionHandler;
 import org.apache.kafka.streams.processor.ProcessorContext;
 import org.apache.kafka.streams.processor.StateRestoreCallback;
 import org.apache.kafka.streams.processor.StateStore;
@@ -38,12 +39,12 @@
 import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
 import org.apache.kafka.streams.processor.internals.RecordCollector;
 import org.apache.kafka.streams.processor.internals.RecordCollectorImpl;
+import org.apache.kafka.streams.processor.internals.StreamsProducer;
 import org.apache.kafka.streams.state.internals.MeteredKeyValueStore;
 import org.apache.kafka.streams.state.internals.RocksDBKeyValueStoreTest;
 import org.apache.kafka.streams.state.internals.ThreadCache;
 import org.apache.kafka.test.InternalMockProcessorContext;
 import org.apache.kafka.test.MockTimestampExtractor;
-import org.apache.kafka.test.StreamsTestUtils;
 import org.apache.kafka.test.TestUtils;
 
 import java.io.File;
@@ -195,13 +196,15 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {
         final Producer<byte[], byte[]> producer = new MockProducer<>(true, rawSerializer, rawSerializer);
         final Consumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
 
+        final LogContext logContext = new LogContext("KeyValueStoreTestDriver ");
         final RecordCollector recordCollector = new RecordCollectorImpl(
+            logContext,
             new TaskId(0, 0),
-            new StreamsConfig(StreamsTestUtils.getStreamsConfig("test")),
-            new LogContext("KeyValueStoreTestDriver "),
-            new MockStreamsMetrics(new Metrics()),
             consumer,
-            id -> producer
+            new StreamsProducer(logContext, producer),
+            new DefaultProductionExceptionHandler(),
+            false,
+            new MockStreamsMetrics(new Metrics())
         ) {
             @Override
             public <K1, V1> void send(final String topic,
diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
index 69a759b3b309..aa14a2e3372b 100644
--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProviderTest.java
@@ -29,6 +29,7 @@
 import org.apache.kafka.streams.TopologyWrapper;
 import org.apache.kafka.streams.errors.InvalidStateStoreException;
 import org.apache.kafka.streams.processor.TaskId;
+import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;
 import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
 import org.apache.kafka.streams.processor.internals.ProcessorStateManager;
 import org.apache.kafka.streams.processor.internals.ProcessorTopology;
@@ -36,9 +37,9 @@
 import org.apache.kafka.streams.processor.internals.RecordCollectorImpl;
 import org.apache.kafka.streams.processor.internals.StateDirectory;
 import org.apache.kafka.streams.processor.internals.StoreChangelogReader;
-import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;
 import org.apache.kafka.streams.processor.internals.StreamTask;
 import org.apache.kafka.streams.processor.internals.StreamThread;
+import org.apache.kafka.streams.processor.internals.StreamsProducer;
 import org.apache.kafka.streams.processor.internals.Task;
 import org.apache.kafka.streams.state.QueryableStoreTypes;
 import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;
@@ -59,6 +60,7 @@
 import java.io.File;
 import java.io.IOException;
 import java.time.Duration;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
@@ -66,7 +68,6 @@
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.stream.Collectors;
 
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.instanceOf;
@@ -324,13 +325,21 @@ private StreamTask createStreamsTask(final StreamsConfig streamsConfig,
                 clientSupplier.restoreConsumer,
                 new MockStateRestoreListener()),
             logContext);
+        final boolean eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(streamsConfig.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));
         final RecordCollector recordCollector = new RecordCollectorImpl(
-            taskId,
-            streamsConfig,
             logContext,
-            new MockStreamsMetrics(metrics),
+            taskId,
             clientSupplier.consumer,
-            id -> clientSupplier.getProducer(new HashMap<>()));
+            eosEnabled ?
+                new StreamsProducer(
+                    logContext,
+                    clientSupplier.getProducer(new HashMap<>()),
+                    streamsConfig.getString(StreamsConfig.APPLICATION_ID_CONFIG),
+                    taskId) :
+                new StreamsProducer(logContext, clientSupplier.getProducer(new HashMap<>())),
+            streamsConfig.defaultProductionExceptionHandler(),
+            eosEnabled,
+            new MockStreamsMetrics(metrics));
         return new StreamTask(
             taskId,
             partitions,
@@ -349,7 +358,7 @@ private void mockThread(final boolean initialized) {
         EasyMock.expect(threadMock.isRunning()).andReturn(initialized);
         EasyMock.expect(threadMock.allTasks()).andStubReturn(tasks);
         EasyMock.expect(threadMock.activeTaskMap()).andStubReturn(tasks);
-        EasyMock.expect(threadMock.activeTasks()).andStubReturn(tasks.values().stream().collect(Collectors.toList()));
+        EasyMock.expect(threadMock.activeTasks()).andStubReturn(new ArrayList<>(tasks.values()));
         EasyMock.expect(threadMock.state()).andReturn(
             initialized ? StreamThread.State.RUNNING : StreamThread.State.PARTITIONS_ASSIGNED
         ).anyTimes();
diff --git a/streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java b/streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java
index 746dbd101a1a..bf956c018c17 100644
--- a/streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java
+++ b/streams/src/test/java/org/apache/kafka/test/MockClientSupplier.java
@@ -43,7 +43,7 @@ public class MockClientSupplier implements KafkaClientSupplier {
 
     private String applicationId;
 
-    public final List<MockProducer> producers = new LinkedList<>();
+    public final List<MockProducer<byte[], byte[]>> producers = new LinkedList<>();
 
     public final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
     public final MockConsumer<byte[], byte[]> restoreConsumer = new MockConsumer<>(OffsetResetStrategy.LATEST);
diff --git a/streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java b/streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java
index 3cc4ee25f1b8..7cb376f370c1 100644
--- a/streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java
+++ b/streams/src/test/java/org/apache/kafka/test/MockKeyValueStore.java
@@ -18,6 +18,7 @@
 
 import org.apache.kafka.common.serialization.Deserializer;
 import org.apache.kafka.common.serialization.IntegerDeserializer;
+import org.apache.kafka.streams.KeyValue;
 import org.apache.kafka.streams.processor.ProcessorContext;
 import org.apache.kafka.streams.processor.StateRestoreCallback;
 import org.apache.kafka.streams.processor.StateStore;
@@ -28,7 +29,7 @@
 import java.util.List;
 import java.util.concurrent.atomic.AtomicInteger;
 
-public class MockKeyValueStore implements KeyValueStore {
+public class MockKeyValueStore implements KeyValueStore<Object, Object> {
     // keep a global counter of flushes and a local reference to which store had which
     // flush, so we can reason about the order in which stores get flushed.
     private static final AtomicInteger GLOBAL_FLUSH_COUNTER = new AtomicInteger(0);
@@ -111,9 +112,7 @@ public Object delete(final Object key) {
     }
 
     @Override
-    public void putAll(final List entries) {
-
-    }
+    public void putAll(final List<KeyValue<Object, Object>> entries) {}
 
     @Override
     public Object get(final Object key) {
@@ -121,12 +120,12 @@ public Object get(final Object key) {
     }
 
     @Override
-    public KeyValueIterator range(final Object from, final Object to) {
+    public KeyValueIterator<Object, Object> range(final Object from, final Object to) {
         return null;
     }
 
     @Override
-    public KeyValueIterator all() {
+    public KeyValueIterator<Object, Object> all() {
         return null;
     }
 
diff --git a/streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java b/streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java
index 70b4e022bf08..11c2f3d8d930 100644
--- a/streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java
+++ b/streams/src/test/java/org/apache/kafka/test/MockKeyValueStoreBuilder.java
@@ -21,7 +21,7 @@
 import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.internals.AbstractStoreBuilder;
 
-public class MockKeyValueStoreBuilder extends AbstractStoreBuilder<Integer, byte[], KeyValueStore> {
+public class MockKeyValueStoreBuilder extends AbstractStoreBuilder<Integer, byte[], KeyValueStore<Object, Object>> {
 
     private final boolean persistent;
 
@@ -32,7 +32,7 @@ public MockKeyValueStoreBuilder(final String storeName, final boolean persistent
     }
 
     @Override
-    public KeyValueStore build() {
+    public KeyValueStore<Object, Object> build() {
         return new MockKeyValueStore(name, persistent);
     }
 }
diff --git a/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java b/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java
index 98e059e24858..526af9dfcae6 100644
--- a/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java
+++ b/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java
@@ -65,6 +65,7 @@
 import org.apache.kafka.streams.processor.internals.StoreChangelogReader;
 import org.apache.kafka.streams.processor.internals.StreamTask;
 import org.apache.kafka.streams.processor.internals.Task;
+import org.apache.kafka.streams.processor.internals.StreamsProducer;
 import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;
 import org.apache.kafka.streams.processor.internals.metrics.TaskMetrics;
 import org.apache.kafka.streams.state.KeyValueStore;
@@ -98,12 +99,13 @@
 import java.util.Objects;
 import java.util.Properties;
 import java.util.Queue;
-import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.function.Supplier;
 import java.util.regex.Pattern;
 
+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;
+
 /**
  * This class makes it easier to write tests to verify the behavior of topologies created with {@link Topology} or
  * {@link StreamsBuilder}.
@@ -196,23 +198,23 @@ public class TopologyTestDriver implements Closeable {
 
     private static final Logger log = LoggerFactory.getLogger(TopologyTestDriver.class);
 
+    private final LogContext logContext;
     private final Time mockWallClockTime;
-    private final InternalTopologyBuilder internalTopologyBuilder;
+    private InternalTopologyBuilder internalTopologyBuilder;
 
     private final static int PARTITION_ID = 0;
     private final static TaskId TASK_ID = new TaskId(0, PARTITION_ID);
-    final StreamTask task;
-    private final GlobalStateUpdateTask globalStateTask;
-    private final GlobalStateManager globalStateManager;
+    StreamTask task;
+    private GlobalStateUpdateTask globalStateTask;
+    private GlobalStateManager globalStateManager;
 
-    private final StateDirectory stateDirectory;
-    private final Metrics metrics;
-    final ProcessorTopology processorTopology;
-    final ProcessorTopology globalTopology;
+    private StateDirectory stateDirectory;
+    private Metrics metrics;
+    ProcessorTopology processorTopology;
+    ProcessorTopology globalTopology;
 
     private final MockProducer<byte[], byte[]> producer;
 
-    private final Set<String> internalTopics = new HashSet<>();
     private final Map<String, TopicPartition> partitionsByInputTopic = new HashMap<>();
     private final Map<String, TopicPartition> globalPartitionsByInputTopic = new HashMap<>();
     private final Map<TopicPartition, AtomicLong> offsetsByTopicOrPatternPartition = new HashMap<>();
@@ -275,7 +277,6 @@ public TopologyTestDriver(final Topology topology,
             initialWallClockTime == null ? System.currentTimeMillis() : initialWallClockTime.toEpochMilli());
     }
 
-
     /**
      * Create a new test diver instance.
      *
@@ -288,15 +289,18 @@ private TopologyTestDriver(final InternalTopologyBuilder builder,
                                final long initialWallClockTimeMs) {
         final StreamsConfig streamsConfig = new QuietStreamsConfig(config);
         logIfTaskIdleEnabled(streamsConfig);
+
+        logContext = new LogContext("topology-test-driver ");
         mockWallClockTime = new MockTime(initialWallClockTimeMs);
+        eosEnabled = EXACTLY_ONCE.equals(streamsConfig.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));
 
-        internalTopologyBuilder = builder;
-        internalTopologyBuilder.rewriteTopology(streamsConfig);
+        final StreamsMetricsImpl streamsMetrics = setupMetrics(streamsConfig);
+        setupTopology(builder, streamsConfig);
 
-        processorTopology = internalTopologyBuilder.build();
-        globalTopology = internalTopologyBuilder.buildGlobalStateTopology();
-        final boolean createStateDirectory = processorTopology.hasPersistentLocalStore() ||
-            (globalTopology != null && globalTopology.hasPersistentGlobalStore());
+        final ThreadCache cache = new ThreadCache(
+            logContext,
+            Math.max(0, streamsConfig.getLong(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG)),
+            streamsMetrics);
 
         final Serializer<byte[]> bytesSerializer = new ByteArraySerializer();
         producer = new MockProducer<byte[], byte[]>(true, bytesSerializer, bytesSerializer) {
@@ -306,17 +310,33 @@ public List<PartitionInfo> partitionsFor(final String topic) {
             }
         };
 
-        final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
-        stateDirectory = new StateDirectory(streamsConfig, mockWallClockTime, createStateDirectory);
+        setupGlobalTask(streamsConfig, streamsMetrics, cache);
+        setupTask(streamsConfig, streamsMetrics, cache);
+    }
+
+    private static void logIfTaskIdleEnabled(final StreamsConfig streamsConfig) {
+        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);
+        if (taskIdleTime > 0) {
+            log.info("Detected {} config in use with TopologyTestDriver (set to {}ms)." +
+                         " This means you might need to use TopologyTestDriver#advanceWallClockTime()" +
+                         " or enqueue records on all partitions to allow Steams to make progress." +
+                         " TopologyTestDriver will log a message each time it cannot process enqueued" +
+                         " records due to {}.",
+                     StreamsConfig.MAX_TASK_IDLE_MS_CONFIG,
+                     taskIdleTime,
+                     StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);
+        }
+    }
+
+    private StreamsMetricsImpl setupMetrics(final StreamsConfig streamsConfig) {
+        final String threadId = Thread.currentThread().getName();
 
         final MetricConfig metricConfig = new MetricConfig()
             .samples(streamsConfig.getInt(StreamsConfig.METRICS_NUM_SAMPLES_CONFIG))
             .recordLevel(Sensor.RecordingLevel.forName(streamsConfig.getString(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG)))
             .timeWindow(streamsConfig.getLong(StreamsConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS);
-
         metrics = new Metrics(metricConfig, mockWallClockTime);
 
-        final String threadId = Thread.currentThread().getName();
         final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(
             metrics,
             "test-client",
@@ -324,27 +344,32 @@ public List<PartitionInfo> partitionsFor(final String topic) {
         );
         streamsMetrics.setRocksDBMetricsRecordingTrigger(new RocksDBMetricsRecordingTrigger());
         TaskMetrics.droppedRecordsSensorOrSkippedRecordsSensor(threadId, TASK_ID.toString(), streamsMetrics);
-        final ThreadCache cache = new ThreadCache(
-            new LogContext("topology-test-driver "),
-            Math.max(0, streamsConfig.getLong(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG)),
-            streamsMetrics);
 
-        for (final InternalTopologyBuilder.TopicsInfo topicsInfo : internalTopologyBuilder.topicGroups().values()) {
-            internalTopics.addAll(topicsInfo.repartitionSourceTopics.keySet());
-        }
+        return streamsMetrics;
+    }
+
+    private void setupTopology(final InternalTopologyBuilder builder,
+                               final StreamsConfig streamsConfig) {
+        internalTopologyBuilder = builder;
+        internalTopologyBuilder.rewriteTopology(streamsConfig);
+
+        processorTopology = internalTopologyBuilder.build();
+        globalTopology = internalTopologyBuilder.buildGlobalStateTopology();
 
         for (final String topic : processorTopology.sourceTopics()) {
             final TopicPartition tp = new TopicPartition(topic, PARTITION_ID);
             partitionsByInputTopic.put(topic, tp);
             offsetsByTopicOrPatternPartition.put(tp, new AtomicLong());
         }
-        consumer.assign(partitionsByInputTopic.values());
-        final Map<TopicPartition, Long> startOffsets = new HashMap<>();
-        for (final TopicPartition topicPartition : partitionsByInputTopic.values()) {
-            startOffsets.put(topicPartition, 0L);
-        }
-        consumer.updateBeginningOffsets(startOffsets);
 
+        final boolean createStateDirectory = processorTopology.hasPersistentLocalStore() ||
+            (globalTopology != null && globalTopology.hasPersistentGlobalStore());
+        stateDirectory = new StateDirectory(streamsConfig, mockWallClockTime, createStateDirectory);
+    }
+
+    private void setupGlobalTask(final StreamsConfig streamsConfig,
+                                 final StreamsMetricsImpl streamsMetrics,
+                                 final ThreadCache cache) {
         if (globalTopology != null) {
             final MockConsumer<byte[], byte[]> globalConsumer = new MockConsumer<>(OffsetResetStrategy.NONE);
             for (final String topicName : globalTopology.sourceTopics()) {
@@ -358,7 +383,7 @@ public List<PartitionInfo> partitionsFor(final String topic) {
             }
 
             globalStateManager = new GlobalStateManagerImpl(
-                new LogContext("mock "),
+                logContext,
                 globalTopology,
                 globalConsumer,
                 stateDirectory,
@@ -374,7 +399,7 @@ public List<PartitionInfo> partitionsFor(final String topic) {
                 globalProcessorContext,
                 globalStateManager,
                 new LogAndContinueExceptionHandler(),
-                new LogContext()
+                logContext
             );
             globalStateTask.initialize();
             globalProcessorContext.setRecordContext(new ProcessorRecordContext(
@@ -387,9 +412,20 @@ public List<PartitionInfo> partitionsFor(final String topic) {
             globalStateManager = null;
             globalStateTask = null;
         }
+    }
 
+    private void setupTask(final StreamsConfig streamsConfig,
+                           final StreamsMetricsImpl streamsMetrics,
+                           final ThreadCache cache) {
         if (!partitionsByInputTopic.isEmpty()) {
-            final LogContext logContext = new LogContext("topology-test-driver ");
+            final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);
+            consumer.assign(partitionsByInputTopic.values());
+            final Map<TopicPartition, Long> startOffsets = new HashMap<>();
+            for (final TopicPartition topicPartition : partitionsByInputTopic.values()) {
+                startOffsets.put(topicPartition, 0L);
+            }
+            consumer.updateBeginningOffsets(startOffsets);
+
             final ProcessorStateManager stateManager = new ProcessorStateManager(
                 TASK_ID,
                 new HashSet<>(partitionsByInputTopic.values()),
@@ -404,12 +440,17 @@ public List<PartitionInfo> partitionsFor(final String topic) {
                     stateRestoreListener),
                 logContext);
             final RecordCollector recordCollector = new RecordCollectorImpl(
-                TASK_ID,
-                streamsConfig,
                 logContext,
-                streamsMetrics,
+                TASK_ID,
                 consumer,
-                taskId -> producer);
+                new StreamsProducer(
+                    logContext,
+                    producer,
+                    eosEnabled ? streamsConfig.getString(StreamsConfig.APPLICATION_ID_CONFIG) : null,
+                    eosEnabled ? TASK_ID : null),
+                streamsConfig.defaultProductionExceptionHandler(),
+                eosEnabled,
+                streamsMetrics);
             task = new StreamTask(
                 TASK_ID,
                 new HashSet<>(partitionsByInputTopic.values()),
@@ -433,21 +474,6 @@ public List<PartitionInfo> partitionsFor(final String topic) {
         } else {
             task = null;
         }
-        eosEnabled = streamsConfig.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG).equals(StreamsConfig.EXACTLY_ONCE);
-    }
-
-    private static void logIfTaskIdleEnabled(final StreamsConfig streamsConfig) {
-        final Long taskIdleTime = streamsConfig.getLong(StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);
-        if (taskIdleTime > 0) {
-            log.info("Detected {} config in use with TopologyTestDriver (set to {}ms)." +
-                         " This means you might need to use TopologyTestDriver#advanceWallClockTime()" +
-                         " or enqueue records on all partitions to allow Steams to make progress." +
-                         " TopologyTestDriver will log a message each time it cannot process enqueued" +
-                         " records due to {}.",
-                     StreamsConfig.MAX_TASK_IDLE_MS_CONFIG,
-                     taskIdleTime,
-                     StreamsConfig.MAX_TASK_IDLE_MS_CONFIG);
-        }
     }
 
     /**
@@ -597,10 +623,7 @@ private void captureOutputsAndReEnqueueInternalResults() {
         // Capture all the records sent to the producer ...
         final List<ProducerRecord<byte[], byte[]>> output = producer.history();
         producer.clear();
-        if (eosEnabled && !producer.closed()) {
-            producer.initTransactions();
-            producer.beginTransaction();
-        }
+
         for (final ProducerRecord<byte[], byte[]> record : output) {
             outputRecordsByTopic.computeIfAbsent(record.topic(), k -> new LinkedList<>()).add(record);
 
