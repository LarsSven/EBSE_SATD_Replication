diff --git a/docker/install/perl.sh b/docker/install/perl.sh
index a981746bc18d..af49952f97d6 100755
--- a/docker/install/perl.sh
+++ b/docker/install/perl.sh
@@ -19,4 +19,4 @@
 
 # install libraries for mxnet's perl package on ubuntu
 apt-get update && apt-get install -y libmouse-perl pdl cpanminus swig libgraphviz-perl
-cpanm -q Function::Parameters
+cpanm -q Function::Parameters Hash::Ordered
diff --git a/docs/api/perl/index.md b/docs/api/perl/index.md
index 0ec631924bf5..aadff6f9d2aa 100644
--- a/docs/api/perl/index.md
+++ b/docs/api/perl/index.md
@@ -12,6 +12,12 @@ that's all that is needed there.
 In addition please refer to [excellent metacpan doc interface](https://metacpan.org/release/AI-MXNet) and to very detailed
 [MXNet Python API Documentation](http://mxnet.io/api/python/index.html).
 
+AI::MXNet supports new imperative PyTorch like Gluon MXNet interface. Please get acquainted with this new interface
+at [Deep Learning - The Straight Dope](http://gluon.mxnet.io/).
+
+For specific Perl Gluon usage please refer to Perl examples and tests directories on github, but be assured that the Python and Perl usage
+are extremely close in order to make the use of the Python Gluon docs and examples as easy as possible.
+
 AI::MXNet is seamlessly glued with PDL, the C++ level state can be easily initialized from PDL and the results can be
 transferred to PDL objects in order to allow you to use all the glory and power of the PDL!
 
diff --git a/docs/install/build_from_source.md b/docs/install/build_from_source.md
index 9bf397bc9f14..82baa1bb02ef 100644
--- a/docs/install/build_from_source.md
+++ b/docs/install/build_from_source.md
@@ -436,7 +436,7 @@ Run the following command from the MXNet source root directory to build the MXNe
 
 ```bash
     sudo apt-get install libmouse-perl pdl cpanminus swig libgraphviz-perl
-    cpanm -q -L "${HOME}/perl5" Function::Parameters
+    cpanm -q -L "${HOME}/perl5" Function::Parameters Hash::Ordered
 
     MXNET_HOME=${PWD}
     export LD_LIBRARY_PATH=${MXNET_HOME}/lib
diff --git a/docs/install/osx_setup.md b/docs/install/osx_setup.md
index a52a39878359..a009123fa0af 100644
--- a/docs/install/osx_setup.md
+++ b/docs/install/osx_setup.md
@@ -188,7 +188,7 @@ After you build the shared library, run the following command from the MXNet sou
 ```bash
     brew install swig
     sudo sh -c 'curl -L https://cpanmin.us | perl - App::cpanminus'
-    sudo cpanm -q -n PDL Mouse Function::Parameters
+    sudo cpanm -q -n PDL Mouse Function::Parameters Hash::Ordered
 
     MXNET_HOME=${PWD}
     export PERL5LIB=${HOME}/perl5/lib/perl5
diff --git a/docs/install/ubuntu_setup.md b/docs/install/ubuntu_setup.md
index b7130bef4967..15d06fc6ae98 100644
--- a/docs/install/ubuntu_setup.md
+++ b/docs/install/ubuntu_setup.md
@@ -238,7 +238,7 @@ Before you build MXNet for Perl from source code, you must complete [building th
 
 ```bash
     sudo apt-get install libmouse-perl pdl cpanminus swig libgraphviz-perl
-    cpanm -q -L "${HOME}/perl5" Function::Parameters
+    cpanm -q -L "${HOME}/perl5" Function::Parameters Hash::Ordered
 
     MXNET_HOME=${PWD}
     export LD_LIBRARY_PATH=${MXNET_HOME}/lib
diff --git a/perl-package/AI-MXNet/Changes b/perl-package/AI-MXNet/Changes
index f8ecc7509737..e9fd9f73041c 100644
--- a/perl-package/AI-MXNet/Changes
+++ b/perl-package/AI-MXNet/Changes
@@ -1,5 +1,10 @@
 Revision history for Perl extension AI::MXNet
 
+1.1     Sun Oct  1 10:19:08 PDT 2017
+        - Major update, added support for new imperative MXNet interface Gluon and realtime GPU kernels callable from Perl space.
+        - Bugfixes for distributed training.
+        - Miscellaneous fixes and performance improvements.
+
 1.0102 Sun Aug  6 16:55:08 PDT 2017
         - bugfixes in Image.pm, updated tests, added PearsonCorrelation metric, added Convolutional RNN modules.
 
diff --git a/perl-package/AI-MXNet/MANIFEST b/perl-package/AI-MXNet/MANIFEST
index 48cb31dd6b8e..abbd285e2878 100644
--- a/perl-package/AI-MXNet/MANIFEST
+++ b/perl-package/AI-MXNet/MANIFEST
@@ -1,77 +1,105 @@
-META.yml
-MANIFEST
+Changes
 examples/calculator.pl
-examples/plot_network.pl
 examples/char_lstm.pl
+examples/cudnn_lstm_bucketing.pl
 examples/get_ptb_data.sh
+examples/gluon/dcgan.pl
+examples/gluon/mnist.pl
 examples/lstm_bucketing.pl
 examples/mnist.pl
-examples/cudnn_lstm_bucketing.pl
-Makefile.PL
-Changes
-META.json
-t/test_recordio.t
-t/test_random.t
-t/test_init.t
-t/test_model_parallel.t
-t/test_optimizers.t
-t/test_multi_device_exec.t
-t/test_ndarray.t
-t/test_io.t
-t/AI-MXNet.t
-t/test_kvstore.t
-t/test_attr.t
-t/test_module.t
-t/test_symbol.t
-t/test_conv.t
-t/test_viz.t
-t/test_rnn.t
-t/test_io_image.t
-t/test_executor.t
-t/test_infer_shape.t
+examples/plot_network.pl
 lib/AI/MXNet.pm
-lib/AI/MXNet/Random.pm
+lib/AI/MXNet/AutoGrad.pm
+lib/AI/MXNet/Base.pm
 lib/AI/MXNet/CachedOp.pm
+lib/AI/MXNet/Callback.pm
 lib/AI/MXNet/Context.pm
-lib/AI/MXNet/Contrib/AutoGrad.pm
-lib/AI/MXNet/Contrib/Symbol.pm
+lib/AI/MXNet/Contrib.pm
 lib/AI/MXNet/Contrib/NDArray.pm
-lib/AI/MXNet/Profiler.pm
-lib/AI/MXNet/Module.pm
-lib/AI/MXNet/Monitor.pm
-lib/AI/MXNet/Function/Parameters.pm
-lib/AI/MXNet/Initializer.pm
-lib/AI/MXNet/Types.pm
-lib/AI/MXNet/Util/Printable.pm
-lib/AI/MXNet/Rtc.pm
-lib/AI/MXNet/RNN.pm
+lib/AI/MXNet/Contrib/Symbol.pm
+lib/AI/MXNet/CudaModule.pm
 lib/AI/MXNet/Executor.pm
-lib/AI/MXNet/Visualization.pm
-lib/AI/MXNet/Optimizer.pm
-lib/AI/MXNet/Contrib.pm
+lib/AI/MXNet/Executor/Group.pm
+lib/AI/MXNet/Function/Parameters.pm
+lib/AI/MXNet/Gluon.pm
+lib/AI/MXNet/Gluon/Block.pm
+lib/AI/MXNet/Gluon/Data.pm
+lib/AI/MXNet/Gluon/Data/Loader.pm
+lib/AI/MXNet/Gluon/Data/Sampler.pm
+lib/AI/MXNet/Gluon/Data/Set.pm
+lib/AI/MXNet/Gluon/Data/Vision.pm
+lib/AI/MXNet/Gluon/Loss.pm
+lib/AI/MXNet/Gluon/Mouse.pm
+lib/AI/MXNet/Gluon/NN.pm
+lib/AI/MXNet/Gluon/NN/BasicLayers.pm
+lib/AI/MXNet/Gluon/NN/ConvLayers.pm
+lib/AI/MXNet/Gluon/Parameter.pm
+lib/AI/MXNet/Gluon/RNN.pm
+lib/AI/MXNet/Gluon/RNN/Cell.pm
+lib/AI/MXNet/Gluon/RNN/Layer.pm
+lib/AI/MXNet/Gluon/Trainer.pm
+lib/AI/MXNet/Gluon/Utils.pm
 lib/AI/MXNet/Image.pm
-lib/AI/MXNet/Symbol/AttrScope.pm
-lib/AI/MXNet/Symbol/Doc.pm
-lib/AI/MXNet/Symbol/Base.pm
-lib/AI/MXNet/Symbol/NameManager.pm
-lib/AI/MXNet/KVStoreServer.pm
+lib/AI/MXNet/Initializer.pm
+lib/AI/MXNet/IO.pm
 lib/AI/MXNet/KVStore.pm
-lib/AI/MXNet/RecordIO.pm
-lib/AI/MXNet/Base.pm
-lib/AI/MXNet/NDArray/Slice.pm
-lib/AI/MXNet/NDArray/Doc.pm
-lib/AI/MXNet/NDArray/Base.pm
-lib/AI/MXNet/Symbol.pm
+lib/AI/MXNet/KVStoreServer.pm
+lib/AI/MXNet/Logging.pm
+lib/AI/MXNet/LRScheduler.pm
 lib/AI/MXNet/Metric.pm
-lib/AI/MXNet/Executor/Group.pm
+lib/AI/MXNet/Module.pm
+lib/AI/MXNet/Module/Base.pm
+lib/AI/MXNet/Module/Bucketing.pm
+lib/AI/MXNet/Monitor.pm
 lib/AI/MXNet/NDArray.pm
+lib/AI/MXNet/NDArray/Base.pm
+lib/AI/MXNet/NDArray/Doc.pm
+lib/AI/MXNet/NDArray/Slice.pm
+lib/AI/MXNet/Optimizer.pm
+lib/AI/MXNet/Profiler.pm
+lib/AI/MXNet/Random.pm
+lib/AI/MXNet/RecordIO.pm
+lib/AI/MXNet/RNN.pm
 lib/AI/MXNet/RNN/Cell.pm
 lib/AI/MXNet/RNN/IO.pm
-lib/AI/MXNet/LRScheduler.pm
-lib/AI/MXNet/Callback.pm
-lib/AI/MXNet/IO.pm
-lib/AI/MXNet/Module/Bucketing.pm
-lib/AI/MXNet/Module/Base.pm
+lib/AI/MXNet/Symbol.pm
+lib/AI/MXNet/Symbol/AttrScope.pm
+lib/AI/MXNet/Symbol/Base.pm
+lib/AI/MXNet/Symbol/Doc.pm
+lib/AI/MXNet/Symbol/NameManager.pm
+lib/AI/MXNet/Symbol/Random.pm
 lib/AI/MXNet/TestUtils.pm
-lib/AI/MXNet/Logging.pm
+lib/AI/MXNet/Types.pm
+lib/AI/MXNet/Util/Printable.pm
+lib/AI/MXNet/Visualization.pm
+Makefile.PL
+MANIFEST			This list of files
+META.json
+META.yml
 README
+t/AI-MXNet.t
+t/test_attr.t
+t/test_autograd.t
+t/test_conv.t
+t/test_cuda_module.t
+t/test_executor.t
+t/test_gluon.t
+t/test_gluon_data.t
+t/test_gluon_rnn.t
+t/test_infer_shape.t
+t/test_init.t
+t/test_io.t
+t/test_io_image.t
+t/test_kvstore.t
+t/test_loss.t
+t/test_metric.t
+t/test_model_parallel.t
+t/test_module.t
+t/test_multi_device_exec.t
+t/test_ndarray.t
+t/test_optimizers.t
+t/test_random.t
+t/test_recordio.t
+t/test_rnn.t
+t/test_symbol.t
+t/test_viz.t
diff --git a/perl-package/AI-MXNet/META.json b/perl-package/AI-MXNet/META.json
index 692f1ddaae39..5ab77257ab8b 100644
--- a/perl-package/AI-MXNet/META.json
+++ b/perl-package/AI-MXNet/META.json
@@ -30,9 +30,10 @@
       },
       "runtime" : {
          "requires" : {
-            "AI::MXNetCAPI" : "1.0102",
-            "AI::NNVMCAPI" : "1.01",
+            "AI::MXNetCAPI" : "1.1",
+            "AI::NNVMCAPI" : "1.1",
             "Function::Parameters" : "1.0705",
+            "Hash::Ordered" : "0.012",
             "GraphViz" : "2.14",
             "Mouse" : "v2.1.0",
             "PDL" : "2.007"
@@ -43,5 +44,5 @@
       }
    },
    "release_status" : "stable",
-   "version" : "1.0102"
+   "version" : "1.1"
 }
diff --git a/perl-package/AI-MXNet/META.yml b/perl-package/AI-MXNet/META.yml
index 5b920182f159..1d1d76006f9e 100644
--- a/perl-package/AI-MXNet/META.yml
+++ b/perl-package/AI-MXNet/META.yml
@@ -17,10 +17,11 @@ no_index:
     - t
     - inc
 requires:
-  AI::MXNetCAPI: '1.0102'
-  AI::NNVMCAPI: '1.01'
+  AI::MXNetCAPI: '1.1'
+  AI::NNVMCAPI: '1.1'
   Function::Parameters: '1.0705'
+  Hash::Ordered: '0.012'
   GraphViz: '2.14'
   Mouse: v2.1.0
   PDL: '2.007'
-version: '1.0102'
+version: '1.1'
diff --git a/perl-package/AI-MXNet/Makefile.PL b/perl-package/AI-MXNet/Makefile.PL
index 2c9bda83330c..c4f97943782f 100644
--- a/perl-package/AI-MXNet/Makefile.PL
+++ b/perl-package/AI-MXNet/Makefile.PL
@@ -19,15 +19,16 @@ my %WriteMakefileArgs = (
   "LICENSE" => "apache_2_0",
   "NAME" => "AI::MXNet",
   "PREREQ_PM" => {
-    "AI::MXNetCAPI" => "1.0102",
-    "AI::NNVMCAPI" => "1.01",
+    "AI::MXNetCAPI" => "1.1",
+    "AI::NNVMCAPI" => "1.1",
     "Function::Parameters" => "1.0705",
+    "Hash::Ordered" => "0.012",
     "Mouse" => "v2.1.0",
     "PDL" => "2.007",
     "GraphViz" => "2.14"
   },
   "TEST_REQUIRES" => {},
-  "VERSION" => "1.0101",
+  "VERSION" => "1.1",
   "test" => {
     "TESTS" => "t/*.t"
   }
@@ -35,9 +36,10 @@ my %WriteMakefileArgs = (
 
 
 my %FallbackPrereqs = (
-  "AI::MXNetCAPI" => "1.0102",
-  "AI::NNVMCAPI" => "1.01",
+  "AI::MXNetCAPI" => "1.1",
+  "AI::NNVMCAPI" => "1.1",
   "Function::Parameters" => "1.0705",
+  "Hash::Ordered" => "0.012",
   "Mouse" => "v2.1.0",
   "PDL" => "2.007",
   "GraphViz" => "2.14"
diff --git a/perl-package/AI-MXNet/README b/perl-package/AI-MXNet/README
index 86b6cf18dbac..5b211d68ab75 100644
--- a/perl-package/AI-MXNet/README
+++ b/perl-package/AI-MXNet/README
@@ -1,5 +1,5 @@
 This archive contains the distribution AI-MXNet,
-version 1.0102:
+version 1.1:
 
   Perl interface to MXNet machine learning library
 
diff --git a/perl-package/AI-MXNet/examples/gluon/dcgan.pl b/perl-package/AI-MXNet/examples/gluon/dcgan.pl
new file mode 100755
index 000000000000..2bdc56149d7e
--- /dev/null
+++ b/perl-package/AI-MXNet/examples/gluon/dcgan.pl
@@ -0,0 +1,206 @@
+#!/usr/bin/perl
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use AI::MXNet qw(mx);
+use AI::MXNet::Gluon qw(gluon);
+use AI::MXNet::AutoGrad qw(autograd);
+use AI::MXNet::Gluon::NN qw(nn);
+use AI::MXNet::Base;
+use Getopt::Long qw(HelpMessage);
+use Time::HiRes qw(time);
+use PDL::IO::Pic;
+
+my $batch_size = 64;
+my $nz  = 100;
+my $ngf = 64;
+my $ndf = 64;
+my $nepoch = 25;
+my $lr =0.0002;
+my $beta1 = 0.5;
+my $nc = 3;
+## change to my $ctx = mx->cpu(); if needed
+my $ctx = mx->gpu();
+
+my $train_data = gluon->data->DataLoader(
+    gluon->data->vision->MNIST('./data', train=>1, transform => \&transformer),
+    batch_size=>$batch_size, shuffle=>1, last_batch=>'discard'
+);
+
+my $val_data = gluon->data->DataLoader(
+    gluon->data->vision->MNIST('./data', train=>0, transform=> \&transformer),
+    batch_size=>$batch_size, shuffle=>0
+);
+
+sub transformer
+{
+    my ($data, $label) = @_;
+    # resize to 64x64
+    $data = mx->image->imresize($data, 64, 64);
+    $data = $data->reshape([1, 64, 64]);
+    # normalize to [-1, 1]
+    $data = $data->astype('float32')/128 - 1;
+    # if image is greyscale, repeat 3 times to get RGB image.
+    if($data->shape->[0] == 1)
+    {
+        $data = mx->nd->tile($data, [3, 1, 1]);
+    }
+    return ($data, $label);
+}
+
+sub visualize
+{
+    my ($data, $fake, $iter) = @_;
+    mkdir "data_images";
+    mkdir "data_images/$iter";
+    mkdir "fake_images";
+    mkdir "fake_images/$iter";
+    for my $i (0..$batch_size-1)
+    {
+        my $d = ((pdl_shuffle($data->at($i)->at(0)->aspdl, [reverse(0..63)]) + 1)*128)->byte;
+        my $f = ((pdl_shuffle($fake->at($i)->at(0)->aspdl, [reverse(0..63)]) + 1)*128)->byte;
+        $d->wpic("data_images/$iter/$i.jpg");
+        $f->wpic("fake_images/$iter/$i.jpg");
+    }
+}
+
+# build the generator
+my $netG = nn->Sequential();
+$netG->name_scope(sub {
+    # input is Z, going into a convolution
+    $netG->add(nn->Conv2DTranspose($ngf * 8, 4, 1, 0, use_bias=>0));
+    $netG->add(nn->BatchNorm());
+    $netG->add(nn->Activation('relu'));
+    # state size-> ($ngf*8) x 4 x 4
+    $netG->add(nn->Conv2DTranspose($ngf * 4, 4, 2, 1, use_bias=>0));
+    $netG->add(nn->BatchNorm());
+    $netG->add(nn->Activation('relu'));
+    # state size-> ($ngf*8) x 8 x 8
+    $netG->add(nn->Conv2DTranspose($ngf * 2, 4, 2, 1, use_bias=>0));
+    $netG->add(nn->BatchNorm());
+    $netG->add(nn->Activation('relu'));
+    # state size-> ($ngf*8) x 16 x 16
+    $netG->add(nn->Conv2DTranspose($ngf, 4, 2, 1, use_bias=>0));
+    $netG->add(nn->BatchNorm());
+    $netG->add(nn->Activation('relu'));
+    # state size-> ($ngf*8) x 32 x 32
+    $netG->add(nn->Conv2DTranspose($nc, 4, 2, 1, use_bias=>0));
+    $netG->add(nn->Activation('tanh'));
+    # state size-> (nc) x 64 x 64
+});
+
+# build the discriminator
+my $netD = nn->Sequential();
+$netD->name_scope(sub {
+    # input is (nc) x 64 x 64
+    $netD->add(nn->Conv2D($ndf, 4, 2, 1, use_bias=>0));
+    $netD->add(nn->LeakyReLU(0.2));
+    # state size-> ($ndf) x 32 x 32
+    $netD->add(nn->Conv2D($ndf * 2, 4, 2, 1, use_bias=>0));
+    $netD->add(nn->BatchNorm());
+    $netD->add(nn->LeakyReLU(0.2));
+    # state size-> ($ndf) x 16 x 16
+    $netD->add(nn->Conv2D($ndf * 4, 4, 2, 1, use_bias=>0));
+    $netD->add(nn->BatchNorm());
+    $netD->add(nn->LeakyReLU(0.2));
+    # state size-> ($ndf) x 8 x 8
+    $netD->add(nn->Conv2D($ndf * 8, 4, 2, 1, use_bias=>0));
+    $netD->add(nn->BatchNorm());
+    $netD->add(nn->LeakyReLU(0.2));
+    # state size-> ($ndf) x 4 x 4
+    $netD->add(nn->Conv2D(2, 4, 1, 0, use_bias=>0));
+});
+
+# loss
+my $loss = gluon->loss->SoftmaxCrossEntropyLoss();
+
+# initialize the generator and the discriminator
+$netG->initialize(mx->init->Normal(0.02), ctx=>$ctx);
+$netD->initialize(mx->init->Normal(0.02), ctx=>$ctx);
+
+# trainer for the generator and the discriminator
+my $trainerG = gluon->Trainer($netG->collect_params(), 'adam', {learning_rate => $lr, beta1 => $beta1});
+my $trainerD = gluon->Trainer($netD->collect_params(), 'adam', {learning_rate => $lr, beta1 => $beta1});
+# ============printing==============
+my $real_label = mx->nd->ones([$batch_size], ctx=>$ctx);
+my $fake_label = mx->nd->zeros([$batch_size], ctx=>$ctx);
+
+my $metric = mx->metric->Accuracy();
+print "Training...\n";
+
+my $iter = 0;
+for my $epoch (0..$nepoch-1)
+{
+    my $tic = time;
+    my $btic = time;
+    my $fake; my $data;
+    while(defined(my $d = <$train_data>))
+    {
+        $data = $d->[0];
+        ############################
+        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
+        ###########################
+        # train with real_t
+        $data = $data->as_in_context($ctx);
+        my $noise = mx->nd->random->normal(0, 1, shape=>[$batch_size, $nz, 1, 1], ctx=>$ctx);
+
+        my ($output, $errD, $errG);
+        autograd->record(sub {
+            $output = $netD->($data);
+            $output = $output->reshape([$batch_size, 2]);
+            my $errD_real = $loss->($output, $real_label);
+            $metric->update([$real_label], [$output]);
+
+            $fake = $netG->($noise);
+            $output = $netD->($fake->detach());
+            $output = $output->reshape([$batch_size, 2]);
+            my $errD_fake = $loss->($output, $fake_label);
+            $errD = $errD_real + $errD_fake;
+            $errD->backward();
+            $metric->update([$fake_label], [$output]);
+        });
+        $trainerD->step($batch_size);
+
+        ############################
+        # (2) Update G network: maximize log(D(G(z)))
+        ###########################
+        autograd->record(sub {
+            $output = $netD->($fake);
+            $output = $output->reshape([-1, 2]);
+            $errG = $loss->($output, $real_label);
+            $errG->backward();
+        });
+
+        $trainerG->step($batch_size);
+        my ($name, $acc) = $metric->get();
+        if(not $iter%100)
+        {
+            AI::MXNet::Logging->info("speed: %.2f samples/s", $batch_size / (time-$btic));
+            AI::MXNet::Logging->info("discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d",
+                mx->nd->mean($errD)->asscalar(), mx->nd->mean($errG)->asscalar(), $acc, $iter, $epoch);
+        }
+        $iter++;
+        $btic = time;
+    }
+    my ($name, $acc) = $metric->get();
+    $metric->reset();
+    visualize($data, $fake, $epoch);
+    AI::MXNet::Logging->info("\nbinary training acc at epoch %d: %s=%f", $epoch, $name, $acc);
+    AI::MXNet::Logging->info("time: %f", time - $tic);
+}
diff --git a/perl-package/AI-MXNet/examples/gluon/mnist.pl b/perl-package/AI-MXNet/examples/gluon/mnist.pl
new file mode 100755
index 000000000000..2d4eff01d8ce
--- /dev/null
+++ b/perl-package/AI-MXNet/examples/gluon/mnist.pl
@@ -0,0 +1,136 @@
+#!/usr/bin/perl
+
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use AI::MXNet qw(mx);
+use AI::MXNet::Gluon qw(gluon);
+use AI::MXNet::AutoGrad qw(autograd);
+use AI::MXNet::Gluon::NN qw(nn);
+use AI::MXNet::Base;
+use Getopt::Long qw(HelpMessage);
+
+GetOptions(
+    'lr=f'           => \(my $lr           = 0.1),
+    'log-interval=i' => \(my $log_interval = 100),
+    'momentum=f'     => \(my $momentum     = 0.9),
+    'hybridize=i'    => \(my $hybridize    = 0  ),
+    'cuda=i'         => \(my $cuda         = 0  ),
+    'load_params=i'  => \(my $load_params  = 0  ),
+    'batch-size=i'   => \(my $batch_size   = 100),
+    'epochs=i'       => \(my $epochs       = 1 ),
+    'help'           => sub { HelpMessage(0) },
+) or HelpMessage(1);
+
+
+# define network
+
+my $net = nn->Sequential();
+$net->name_scope(sub {
+    $net->add(nn->Dense(128, activation=>'relu'));
+    $net->add(nn->Dense(64, activation=>'relu'));
+    $net->add(nn->Dense(10));
+});
+$net->hybridize() if $hybridize;
+$net->load_params('mnist.params') if $load_params;
+# data
+
+sub transformer
+{
+    my ($data, $label) = @_;
+    $data = $data->reshape([-1])->astype('float32')/255;
+    return ($data, $label);
+}
+
+my $train_data = gluon->data->DataLoader(
+    gluon->data->vision->MNIST('./data', train=>1, transform => \&transformer),
+    batch_size=>$batch_size, shuffle=>1, last_batch=>'discard'
+);
+
+my $val_data = gluon->data->DataLoader(
+    gluon->data->vision->MNIST('./data', train=>0, transform=> \&transformer),
+    batch_size=>$batch_size, shuffle=>0
+);
+
+# train
+
+sub test
+{
+    my $ctx = shift;
+    my $metric = mx->metric->Accuracy();
+    while(defined(my $d = <$val_data>))
+    {
+        my ($data, $label) = @$d;
+        $data = $data->as_in_context($ctx);
+        $label = $label->as_in_context($ctx);
+        my $output = $net->($data);
+        $metric->update([$label], [$output]);
+    }
+    return $metric->get;
+}
+
+sub train
+{
+    my ($epochs, $ctx) = @_;
+    # Collect all parameters from net and its children, then initialize them.
+    $net->initialize(mx->init->Xavier(magnitude=>2.24), ctx=>$ctx);
+    # Trainer is for updating parameters with gradient.
+    my $trainer = gluon->Trainer($net->collect_params(), 'sgd', { learning_rate => $lr, momentum => $momentum });
+    my $metric = mx->metric->Accuracy();
+    my $loss = gluon->loss->SoftmaxCrossEntropyLoss();
+
+    for my $epoch (0..$epochs-1)
+    {
+        # reset data iterator and metric at begining of epoch.
+        $metric->reset();
+        enumerate(sub {
+            my ($i, $d) = @_;
+            my ($data, $label) = @$d;
+            $data = $data->as_in_context($ctx);
+            $label = $label->as_in_context($ctx);
+            # Start recording computation graph with record() section.
+            # Recorded graphs can then be differentiated with backward.
+            my $output;
+            autograd->record(sub {
+                $output = $net->($data);
+                my $L = $loss->($output, $label);
+                $L->backward;
+            });
+            # take a gradient step with batch_size equal to data.shape[0]
+            $trainer->step($data->shape->[0]);
+            # update metric at last.
+            $metric->update([$label], [$output]);
+
+            if($i % $log_interval == 0 and $i > 0)
+            {
+                my ($name, $acc) = $metric->get();
+                print "[Epoch $epoch Batch $i] Training: $name=$acc\n";
+            }
+        }, \@{ $train_data });
+
+        my ($name, $acc) = $metric->get();
+        print "[Epoch $epoch] Training: $name=$acc\n";
+
+        my ($val_name, $val_acc) = test($ctx);
+        print "[Epoch $epoch] Validation: $val_name=$val_acc\n"
+    }
+    $net->save_params('mnist.params');
+}
+
+train($epochs, $cuda ? mx->gpu(0) : mx->cpu);
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet.pm b/perl-package/AI-MXNet/lib/AI/MXNet.pm
index 40e84a6078e6..bd7d5b4fc37d 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet.pm
@@ -25,7 +25,7 @@ use AI::MXNet::NDArray;
 use AI::MXNet::Symbol;
 use AI::MXNet::Executor;
 use AI::MXNet::Executor::Group;
-use AI::MXNet::Rtc;
+use AI::MXNet::CudaModule;
 use AI::MXNet::Random;
 use AI::MXNet::Initializer;
 use AI::MXNet::Optimizer;
@@ -44,9 +44,10 @@ use AI::MXNet::Visualization;
 use AI::MXNet::RecordIO;
 use AI::MXNet::Image;
 use AI::MXNet::Contrib;
-use AI::MXNet::Contrib::AutoGrad;
 use AI::MXNet::CachedOp;
-our $VERSION = '1.0102';
+use AI::MXNet::AutoGrad;
+use AI::MXNet::Gluon;
+our $VERSION = '1.1';
 
 sub import
 {
@@ -69,6 +70,7 @@ sub import
             sub rnd { 'AI::MXNet::Random' }
             sub random { 'AI::MXNet::Random' }
             sub Context { shift; AI::MXNet::Context->new(\@_) }
+            sub context { 'AI::MXNet::Context' }
             sub cpu { AI::MXNet::Context->cpu(\$_[1]//0) }
             sub gpu { AI::MXNet::Context->gpu(\$_[1]//0) }
             sub kv { 'AI::MXNet::KVStore' }
@@ -81,15 +83,22 @@ sub import
             sub rnn { 'AI::MXNet::RNN' }
             sub callback { 'AI::MXNet::Callback' }
             sub img { 'AI::MXNet::Image' }
+            sub image { 'AI::MXNet::Image' }
             sub contrib { 'AI::MXNet::Contrib' }
+            sub autograd { 'AI::MXNet::AutoGrad' }
             sub name { '$short_name' }
+            sub rtc { '$short_name' }
+            sub CudaModule { shift; AI::MXNet::CudaModule->new(\@_) }
             sub AttrScope { shift; AI::MXNet::Symbol::AttrScope->new(\@_) }
             *AI::MXNet::Symbol::AttrScope::current = sub { \$${short_name}::AttrScope; };
             \$${short_name}::AttrScope = AI::MXNet::Symbol::AttrScope->new;
             sub Prefix { AI::MXNet::Symbol::Prefix->new(prefix => \$_[1]) }
             *AI::MXNet::Symbol::NameManager::current = sub { \$${short_name}::NameManager; };
+            *AI::MXNet::Symbol::NameManager::set_current = sub { \$${short_name}::NameManager = \$_[1]; };
             \$${short_name}::NameManager = AI::MXNet::Symbol::NameManager->new;
             *AI::MXNet::Context::current_ctx = sub { \$${short_name}::Context; };
+            *AI::MXNet::Context::current_context = sub { \$${short_name}::Context; };
+            *AI::MXNet::Context::set_current = sub { \$${short_name}::Context = \$_[1]; };
             \$${short_name}::Context = AI::MXNet::Context->new(device_type => 'cpu', device_id => 0);
             1;
 EOP
@@ -172,7 +181,7 @@ AI::MXNet - Perl interface to MXNet machine learning library
 
 =head1 BUGS AND INCOMPATIBILITIES
 
-    Parity with Python inteface is mostly achieved, few deprecated
+    Parity with Python interface is mostly achieved, few deprecated
     and not often used features left unported for now.
 
 =head1 SEE ALSO
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/AutoGrad.pm b/perl-package/AI-MXNet/lib/AI/MXNet/AutoGrad.pm
new file mode 100644
index 000000000000..b49c0b69c52f
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/AutoGrad.pm
@@ -0,0 +1,477 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::AutoGrad;
+use strict;
+use warnings;
+use AI::MXNet::Base;
+use AI::MXNet::Function::Parameters;
+use Scalar::Util qw(blessed);
+use Carp qw(confess);
+
+sub import
+{
+    my ($class, $short_name) = @_;
+    if($short_name)
+    {
+        $short_name =~ s/[^\w:]//g;
+        if(length $short_name)
+        {
+            my $short_name_package =<<"EOP";
+            package $short_name;
+            use parent 'AI::MXNet::AutoGrad';
+            1;
+EOP
+            eval $short_name_package;
+        }
+    }
+}
+
+=head1 NAME
+
+    AI::MXNet::AutoGrad - Autograd for NDArray.
+=cut
+
+=head2 set_is_training
+
+    Set status to training/not training. When training, graph will be constructed
+    for gradient computation. Operators will also run with ctx.is_train=True. For example,
+    Dropout will drop inputs randomly when is_train=True while simply passing through
+    if is_train=False.
+
+    Parameters
+    ----------
+    is_train: bool
+
+    Returns
+    -------
+    previous state before this set.
+=cut
+
+
+method set_is_training(Bool $is_train)
+{
+    return scalar(check_call(AI::MXNetCAPI::AutogradSetIsTraining($is_train)));
+}
+
+=head2 set_is_recording
+
+    Set status to recording/not recording. When recording, graph will be constructed
+    for gradient computation.
+
+    Parameters
+    ----------
+    is_recoding: bool
+
+    Returns
+    -------
+    previous state before this set.
+=cut
+
+method set_is_recording(Bool $is_recording)
+{
+    return scalar(check_call(AI::MXNetCAPI::AutogradSetIsRecording($is_recording)));
+}
+
+=head2 is_recording
+
+    Get status on recording/not recording.
+
+    Returns
+    -------
+    Current state of recording.
+=cut
+
+method is_recording()
+{
+    return scalar(check_call(AI::MXNetCAPI::AutogradIsRecording()));
+}
+
+=head2 is_training
+
+    Get status on training/predicting.
+
+    Returns
+    -------
+    Current state of training/predicting.
+=cut
+
+method is_training()
+{
+    return scalar(check_call(AI::MXNetCAPI::AutogradIsTraining()));
+}
+
+=head2 mark_variables
+
+    Mark AI::MXNet::NDArrays as variables to compute gradient for autograd.
+
+    Parameters
+    ----------
+    ArrayRef[AI::MXNet::NDArray] $variables
+    ArrayRef[AI::MXNet::NDArray] $gradients
+    GradReq|ArrayRef[GradReq]   :$grad_reqs='write'
+=cut
+
+method mark_variables(
+    ArrayRef[AI::MXNet::NDArray]  $variables,
+    ArrayRef[AI::MXNet::NDArray]  $gradients,
+    GradReq|ArrayRef[GradReq]    :$grad_reqs='write'
+)
+{
+    my @variable_handles = map { $_->handle } @{ $variables };
+    my @gradient_handles = map { $_->handle } @{ $gradients };
+    my @grad_reqs;
+    if(not ref $grad_reqs)
+    {
+        @grad_reqs = (GRAD_REQ_MAP->{ $grad_reqs }) x scalar(@variable_handles);
+    }
+    else
+    {
+        @grad_reqs = map { GRAD_REQ_MAP->{ $_ } } @{ $grad_reqs };
+    }
+    check_call(
+        AI::MXNetCAPI::AutogradMarkVariables(
+            scalar(@variable_handles),
+            \@variable_handles,
+            \@grad_reqs,
+            \@gradient_handles
+        )
+    );
+}
+
+=head2 backward
+
+    Compute the gradients of heads w.r.t previously marked variables.
+
+    Parameters
+    ----------
+    $heads: ArrayRef[AI::MXNet::NDArray]
+        Output NDArray(s)
+    :$head_grads=: Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray|Undef]]
+        Gradients with respect to heads.
+    :$retain_graph=0: bool, optional
+        Whether to retain graph.
+    :$train_mode=1: bool, optional
+        Whether to do backward for training or predicting.
+=cut
+method backward(
+    AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray] $heads,
+    Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray|Undef]] :$head_grads=,
+    Bool :$retain_graph=0,
+    Bool :$train_mode=1
+)
+{
+    my ($head_handles, $hgrad_handles) = _parse_head($heads, $head_grads);
+    check_call(
+        AI::MXNetCAPI::AutogradBackwardEx(
+            scalar(@{ $head_handles }),
+            $head_handles,
+            $hgrad_handles,
+            0,
+            [],
+            $retain_graph,
+            0,
+            $train_mode
+        )
+    );
+}
+
+=head2 compute_gradient
+
+    Compute the gradients of outputs w.r.t variables.
+
+    Parameters
+    ----------
+    outputs: array ref of NDArray
+
+    Returns
+    -------
+    gradients: array ref of NDArray
+=cut
+
+
+method compute_gradient(ArrayRef[AI::MXNet::NDArray] $outputs)
+{
+    __PACKAGE__->backward($outputs);
+}
+
+=head2 grad_and_loss
+
+    Return function that computes both gradient of arguments and loss value.
+
+    Parameters
+    ----------
+    func: a perl sub
+        The forward (loss) function.
+    argnum: an int or a array ref of int
+        The index of argument to calculate gradient for.
+
+    Returns
+    -------
+    grad_and_loss_func: a perl sub
+        A function that would compute both the gradient of arguments and loss value.
+=cut
+
+method grad_and_loss(CodeRef $func, Maybe[Int|ArrayRef[Int]] $argnum=)
+{
+    return sub {
+        my @args = @_;
+        my @variables = @_;
+        if(defined $argnum)
+        {
+            my @argnum = ref $argnum ? @$argnum : ($argnum);
+            @variables = map { $args[$_] } @argnum;
+        }
+        map {
+            assert(
+                (blessed($_) and $_->isa('AI::MXNet::NDArray')),
+                "type of autograd input should NDArray")
+        } @variables;
+        my @grads = map { $_->zeros_like } @variables;
+        __PACKAGE__->mark_variables(\@variables, \@grads);
+        my $outputs;
+        __PACKAGE__->record(sub { $outputs = $func->(@args) });
+        __PACKAGE__->backward(ref $outputs eq 'ARRAY' ? $outputs : [$outputs]);
+        return (\@grads, $outputs);
+    };
+}
+
+=head2 grad
+
+    Compute the gradients of heads w.r.t variables. Gradients will be
+    returned as new NDArrays instead of stored into `variable.grad`.
+    Supports recording gradient graph for computing higher order gradients.
+
+    .. Note: Currently only a very limited set of operators support higher order
+    gradients.
+
+    Parameters
+    ----------
+    $heads: NDArray or array ref of NDArray
+        Output NDArray(s)
+    $variables: NDArray or list of NDArray
+        Input variables to compute gradients for.
+    :$head_grads=: NDArray or list of NDArray or undef
+        Gradients with respect to heads.
+    :$retain_graph=: bool
+        Whether to keep computation graph to differentiate again, instead
+        of clearing history and release memory. Defaults to the same value
+        as create_graph.
+    :$create_graph=0: bool
+        Whether to record gradient graph for computing higher order
+    $train_mode=1: bool, optional
+        Whether to do backward for training or prediction.
+
+    Returns
+    -------
+    NDArray or list of NDArray:
+        Gradients with respect to variables.
+
+    Examples
+    --------
+    >>> $x = mx->nd->ones([1]);
+    >>> $x->attach_grad();
+    >>> mx->autograd->record(sub {
+            $z = mx->nd->elemwise_add(mx->nd->exp($x), $x);
+        });
+    >>> $dx = mx->autograd->grad($z, [$x], create_graph=>1)
+    >>> $dx->backward();
+    >>> print($dx->grad->aspdl)
+    [3.71828175]
+=cut
+
+method grad(
+    AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray] $heads,
+    AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray] $variables,
+    Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray|Undef]] :$head_grads=,
+    Bool :$retain_graph=,
+    Bool :$create_graph=0,
+    Bool :$train_mode=1
+)
+{
+    my ($head_handles, $hgrad_handles) = _parse_head($heads, $head_grads);
+    my @var_handles;
+    if(blessed $variables)
+    {
+        @var_handles = ($variables->handle);
+    }
+    else
+    {
+        assert(scalar(@{ $variables }), "variables cannot be an empty array.");
+        @var_handles = map { $_->handle } @{ $variables };
+    }
+
+    $retain_graph //= $create_graph;
+
+    my ($grad_vars, $grad_stypes)
+        =
+    check_call(
+        AI::MXNetCAPI::AutogradBackwardEx(
+            scalar(@{ $head_handles }),
+            $head_handles,
+            $hgrad_handles,
+            scalar(@var_handles),
+            \@var_handles,
+            $retain_graph,
+            $create_graph,
+            $train_mode
+        )
+    );
+
+    my @ret;
+    zip(sub {
+        my ($handle, $stype) = @_;
+        push @ret, AI::MXNet::NDArray->new(handle => $handle, stype => $stype);
+    }, $grad_vars, $grad_stypes);
+    if(blessed $variables)
+    {
+        return $ret[0];
+    }
+    return \@ret;
+}
+
+=head2 train_mode
+
+    Executes $sub within an autograd training scope context.
+    Parameters
+    ----------
+    CodeRef $sub: a perl sub
+=cut
+
+method train_mode(CodeRef $sub)
+{
+    my $prev = __PACKAGE__->set_is_training(1);
+    eval { $sub->(); };
+    __PACKAGE__->set_is_training(0) unless $prev;
+    confess($@) if $@;
+}
+
+=head2 predict_mode
+
+    Executes $sub within an autograd predicting scope context.
+    Parameters
+    ----------
+    CodeRef $sub: a perl sub
+=cut
+
+method predict_mode(CodeRef $sub)
+{
+    my $prev = __PACKAGE__->set_is_training(0);
+    eval { $sub->(); };
+    __PACKAGE__->set_is_training(1) if $prev;
+    confess($@) if $@;
+}
+
+=head2 record
+
+    Executes $sub within an autograd recording scope context
+    and captures code that needs gradients to be calculated.
+    Parameters
+    ----------
+    CodeRef $sub: a perl sub
+    Maybe[Bool] :$train_mode=1
+=cut
+
+method record(CodeRef $sub, Maybe[Bool] :$train_mode=1)
+{
+    my $prev_train;
+    if(defined $train_mode)
+    {
+        $prev_train = __PACKAGE__->set_is_training($train_mode);
+    }
+    my $prev_recording = __PACKAGE__->set_is_recording(1);
+    eval { $sub->(); };
+    if(defined $train_mode)
+    {
+        $prev_train = __PACKAGE__->set_is_training($prev_train) if not $prev_train == $train_mode;
+    }
+    __PACKAGE__->set_is_recording(0) unless $prev_recording;
+    confess($@) if $@;
+}
+
+=head2 pause
+
+    Executes $sub within an autograd recording scope context
+    and captures code that needs gradients to be calculated.
+    Parameters
+    ----------
+    CodeRef $sub: a perl sub
+    Maybe[Bool] :$train_mode=0
+=cut
+
+method pause(CodeRef $sub, Maybe[Bool] :$train_mode=0)
+{
+    my $prev_train;
+    if(defined $train_mode)
+    {
+        $prev_train = __PACKAGE__->set_is_training($train_mode);
+    }
+    my $prev_recording = __PACKAGE__->set_is_recording(0);
+    eval { $sub->(); };
+    if(defined $train_mode)
+    {
+        $prev_train = __PACKAGE__->set_is_training($prev_train) if not $prev_train == $train_mode;
+    }
+    __PACKAGE__->set_is_recording(1) if $prev_recording;
+    confess($@) if $@;
+}
+
+=head2 get_symbol
+
+    Retrieve recorded computation history as `Symbol`.
+
+    Parameters
+    ----------
+    x : NDArray
+        Array representing the head of computation graph.
+    Returns
+    -------
+    Symbol
+        The retrieved Symbol.
+=cut
+
+method get_symbol(AI::MXNet::NDArray $x)
+{
+    my $handle = scalar(check_call(AI::MXNetCAPI::AutogradGetSymbol($x->handle)));
+    return AI::MXNet::Symbol->new(handle => $handle);
+}
+
+# parse head gradient for backward and grad.
+func _parse_head($heads, $head_grads)
+{
+    if(blessed $heads)
+    {
+        $heads = [$heads];
+    }
+    if(blessed $head_grads)
+    {
+        $head_grads = [$head_grads];
+    }
+    my @head_handles = map { $_->handle } @{ $heads };
+    my @hgrad_handles;
+    if(defined $head_grads)
+    {
+        assert(
+            (@{ $heads } == @{ $head_grads }),
+            "heads and head_grads must be lists of the same length"
+        );
+        @hgrad_handles = map { defined($_) ? $_->handle : undef } @{ $head_grads };
+    }
+    return (\@head_handles, \@hgrad_handles);
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Base.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Base.pm
index 0c42fa9306cb..a8da8470f574 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Base.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Base.pm
@@ -20,8 +20,8 @@ use strict;
 use warnings;
 use PDL;
 use PDL::Types qw();
-use AI::MXNetCAPI 1.0102;
-use AI::NNVMCAPI 1.01;
+use AI::MXNetCAPI 1.1;
+use AI::NNVMCAPI 1.1;
 use AI::MXNet::Types;
 use Time::HiRes;
 use Carp;
@@ -30,23 +30,28 @@ use base qw(Exporter);
 use List::Util qw(shuffle);
 
 @AI::MXNet::Base::EXPORT = qw(product enumerate assert zip check_call build_param_doc
-                              pdl cat dog svd bisect_left pdl_shuffle
+                              pdl cat dog svd bisect_left pdl_shuffle as_array
                               DTYPE_STR_TO_MX DTYPE_MX_TO_STR DTYPE_MX_TO_PDL
                               DTYPE_PDL_TO_MX DTYPE_MX_TO_PERL GRAD_REQ_MAP);
 @AI::MXNet::Base::EXPORT_OK = qw(pzeros pceil);
+
 use constant DTYPE_STR_TO_MX => {
     float32 => 0,
     float64 => 1,
     float16 => 2,
     uint8   => 3,
-    int32   => 4
+    int32   => 4,
+    int8    => 5,
+    int64   => 6
 };
 use constant DTYPE_MX_TO_STR => {
     0 => 'float32',
     1 => 'float64',
     2 => 'float16',
     3 => 'uint8',
-    4 => 'int32'
+    4 => 'int32',
+    5 => 'int8',
+    6 => 'int64'
 };
 use constant DTYPE_MX_TO_PDL => {
     0 => 6,
@@ -54,17 +59,22 @@ use constant DTYPE_MX_TO_PDL => {
     2 => 6,
     3 => 0,
     4 => 3,
+    5 => 0,
+    6 => 5,
     float32 => 6,
     float64 => 7,
     float16 => 6,
     uint8   => 0,
-    int32   => 3
+    int32   => 3,
+    int8    => 0,
+    int64   => 5
 };
 use constant DTYPE_PDL_TO_MX => {
     6 => 0,
     7 => 1,
     0 => 3,
     3 => 4,
+    5 => 6
 };
 use constant DTYPE_MX_TO_PERL => {
     0 => 'f',
@@ -72,11 +82,15 @@ use constant DTYPE_MX_TO_PERL => {
     2 => 'S',
     3 => 'C',
     4 => 'l',
+    5 => 'c',
+    6 => 'q',
     float32 => 'f',
     float64 => 'd',
     float16 => 'S',
     uint8   => 'C',
-    int32   => 'l'
+    int32   => 'l',
+    int8    => 'c',
+    int64   => 'q'
 };
 use constant GRAD_REQ_MAP => {
     null  => 0,
@@ -279,6 +293,52 @@ sub _notify_shutdown
     check_call(AI::MXNetCAPI::NotifyShutdown());
 }
 
+sub _indent
+{
+    my ($s_, $numSpaces) = @_;
+    my @s = split(/\n/, $s_);
+    if (@s == 1)
+    {
+        return $s_;
+    }
+    my $first = shift(@s);
+    @s = ($first, map { (' 'x$numSpaces) . $_ } @s);
+    return join("\n", @s);
+}
+
+sub as_array
+{
+    return ref $_[0] eq 'ARRAY' ? $_[0] : [$_[0]];
+}
+
+my %internal_arguments = (prefix => 1, params => 1, shared => 1);
+my %attributes_per_class;
+sub process_arguments
+{
+    my $orig  = shift;
+    my $class = shift;
+    if($class->can('python_constructor_arguments'))
+    {
+        if(not exists $attributes_per_class{$class})
+        {
+            %{ $attributes_per_class{$class} } = map { $_->name => 1 } $class->meta->get_all_attributes;
+        }
+        my %kwargs;
+        while(@_ >= 2 and not ref $_[-2] and (exists $attributes_per_class{$class}{ $_[-2] } or exists $internal_arguments{ $_[-2] }))
+        {
+            my $v = pop(@_);
+            my $k = pop(@_);
+            $kwargs{ $k } = $v;
+        }
+        if(@_)
+        {
+            @kwargs{ @{ $class->python_constructor_arguments }[0..@_-1] } = @_;
+        }
+        return $class->$orig(%kwargs);
+    }
+    return $class->$orig(@_);
+}
+
 END {
     _notify_shutdown();
     Time::HiRes::sleep(0.01);
@@ -288,5 +348,18 @@ END {
 *pceil  = \&ceil;
 ## making sure that we can stringify arbitrarily large piddles
 $PDL::toolongtoprint = 1000_000_000;
+## convenience subs
+{
+    my $orig_at = PDL->can('at');
+    no warnings 'redefine';
+    *PDL::at = sub {
+        my ($self, @args) = @_;
+        return $orig_at->($self, @args) if @args != 1;
+        return $orig_at->($self, @args) if $self->ndims == 1;
+        return $self->slice(('X')x($self->ndims-1), $args[0])->squeeze;
+    };
+    *PDL::len    = sub { shift->dim(-1) };
+    *PDL::dtype  = sub { DTYPE_MX_TO_STR->{ DTYPE_PDL_TO_MX->{ shift->type->numval } } };
+}
 
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Context.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Context.pm
index 2eca42436dc7..d21b690aee26 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Context.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Context.pm
@@ -65,7 +65,8 @@ use overload
     '""' => sub {
         my ($self) = @_;
         return sprintf("%s(%s)", $self->device_type, $self->device_id);
-    };
+    },
+    fallback => 1;
 =head1 NAME
 
     AI::MXNet::Context - A device context.
@@ -143,6 +144,13 @@ method current_ctx()
     return $AI::MXNet::current_ctx;
 }
 
+method set_current(AI::MXNet::Context $current)
+{
+    $AI::MXNet::current_ctx = $current;
+}
+
+*current_context = \&current_ctx;
+
 method deepcopy()
 {
     return __PACKAGE__->new(
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Contrib.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Contrib.pm
index a81030bdc6e0..2a6e18e14054 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Contrib.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Contrib.pm
@@ -24,6 +24,5 @@ use AI::MXNet::Contrib::NDArray;
 sub sym    { 'AI::MXNet::Contrib::Symbol'  }
 sub symbol { 'AI::MXNet::Contrib::Symbol'  }
 sub nd     { 'AI::MXNet::Contrib::NDArray' }
-sub autograd { 'AI::MXNet::Contrib::AutoGrad' }
 
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Contrib/AutoGrad.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Contrib/AutoGrad.pm
deleted file mode 100644
index ff659982b813..000000000000
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Contrib/AutoGrad.pm
+++ /dev/null
@@ -1,244 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-package AI::MXNet::Contrib::AutoGrad;
-use strict;
-use warnings;
-use AI::MXNet::Base;
-use AI::MXNet::Function::Parameters;
-use Scalar::Util qw(blessed);
-
-=head1 NAME
-
-    AI::MXNet::AutoGrad - Autograd for NDArray.
-=cut
-
-=head2 set_is_training
-
-    Set status to training/not training. When training, graph will be constructed
-    for gradient computation. Operators will also run with ctx.is_train=True. For example,
-    Dropout will drop inputs randomly when is_train=True while simply passing through
-    if is_train=False.
-
-    Parameters
-    ----------
-    is_train: bool
-
-    Returns
-    -------
-    previous state before this set.
-=cut
-
-
-method set_is_training(Bool $is_train)
-{
-    my $prev = scalar(check_call(AI::MXNetCAPI::AutogradSetIsTraining($is_train ? 1 : 0)));
-    return $prev ? 1 : 0
-}
-
-=head2 mark_variables
-
-    Mark AI::MXNet::NDArrays as variables to compute gradient for autograd.
-
-    Parameters
-    ----------
-    variables: array ref of AI::MXNet::NDArrays
-    gradients: array ref of AI::MXNet::NDArrays
-    grad_reqs: array ref of strings
-=cut
-
-method mark_variables(
-    ArrayRef[AI::MXNet::NDArray]  $variables,
-    ArrayRef[AI::MXNet::NDArray]  $gradients,
-    GradReq|ArrayRef[GradReq]     $grad_reqs='write'
-)
-{
-    my @variable_handles = map { $_->handle } @{ $variables };
-    my @gradient_handles = map { $_->handle } @{ $gradients };
-    my @grad_reqs;
-    if(not ref $grad_reqs)
-    {
-        @grad_reqs = (GRAD_REQ_MAP->{ $grad_reqs }) x scalar(@variable_handles);
-    }
-    else
-    {
-        @grad_reqs = map { GRAD_REQ_MAP->{ $_ } } @{ $grad_reqs };
-    }
-    check_call(
-        AI::MXNetCAPI::AutogradMarkVariables(
-            scalar(@variable_handles),
-            \@variable_handles,
-            \@grad_reqs,
-            \@gradient_handles
-        )
-    );
-}
-
-=head2 backward
-
-     Compute the gradients of outputs w.r.t variables.
-
-     Parameters
-     ----------
-     outputs: array ref of NDArray
-     out_grads: array ref of NDArray or undef
-     retain_graph: bool, defaults to false
-=cut
-
-
-method backward(
-    ArrayRef[AI::MXNet::NDArray] $outputs,
-    Maybe[ArrayRef[AI::MXNet::NDArray|Undef]] $out_grads=,
-    Bool $retain_graph=0
-)
-{
-    my @output_handles = map { $_->handle } @{ $outputs };
-    if(not defined $out_grads)
-    {
-        check_call(
-            AI::MXNetCAPI::AutogradBackward(
-                scalar(@output_handles),
-                \@output_handles,
-                [],
-                $retain_graph
-            )
-        );
-        return;
-    }
-
-    my @ograd_handles;
-    for my $arr (@$out_grads)
-    {
-        push @ograd_handles, (defined $arr ? $arr->handle : undef);
-    }
-    assert(
-        (@ograd_handles == @output_handles),
-        "outputs and out_grads must have the same length"
-    );
-
-    check_call(
-        AI::MXNetCAPI::AutogradBackward(
-            scalar(@output_handles),
-            \@output_handles,
-            \@ograd_handles,
-            $retain_graph
-        )
-    );
-}
-
-=head2 compute_gradient
-
-    Compute the gradients of outputs w.r.t variables.
-
-    Parameters
-    ----------
-    outputs: array ref of NDArray
-
-    Returns
-    -------
-    gradients: array ref of NDArray
-=cut
-
-
-method compute_gradient(ArrayRef[AI::MXNet::NDArray] $outputs)
-{
-    __PACKAGE__->backward($outputs);
-}
-
-=head2 grad_and_loss
-
-    Return function that computes both gradient of arguments and loss value.
-
-    Parameters
-    ----------
-    func: a perl sub
-        The forward (loss) function.
-    argnum: an int or a array ref of int
-        The index of argument to calculate gradient for.
-
-    Returns
-    -------
-    grad_and_loss_func: a perl sub
-        A function that would compute both the gradient of arguments and loss value.
-=cut
-
-method grad_and_loss(CodeRef $func, Maybe[Int|ArrayRef[Int]] $argnum=)
-{
-    return sub {
-        my @args = @_;
-        my @variables = @_;
-        if(defined $argnum)
-        {
-            my @argnum = ref $argnum ? @$argnum : ($argnum);
-            @variables = map { $_[$_] } @argnum;
-        }
-        map {
-            assert(
-                (blessed($_) and $_->isa('AI::MXNet::NDArray')),
-                "type of autograd input should NDArray")
-        } @variables;
-        my @grads = map { $_->zeros_like } @variables;
-        __PACKAGE__->mark_variables(\@variables, \@grads);
-        my $prev = __PACKAGE__->set_is_training(1);
-        my $outputs = $func->(@args);
-        __PACKAGE__->set_is_training(0) unless $prev;
-        __PACKAGE__->compute_gradient(ref $outputs eq 'ARRAY' ? $outputs : [$outputs]);
-        return (\@grads, $outputs);
-    };
-}
-
-=head2 grad
-
-    Return function that computes gradient of arguments.
-
-    Parameters
-    ----------
-    func: a perl sub
-        The forward (loss) function.
-    argnum: an int or arry ref of int
-        The index of argument to calculate gradient for.
-
-    Returns
-    -------
-    grad_func: a perl function
-        A function that would compute the gradient of arguments.
-=cut
-
-
-method grad(CodeRef $func, Maybe[Int|ArrayRef[Int]] $argnum=)
-{
-    my $grad_with_loss_func = __PACKAGE__->grad_and_loss($func, $argnum);
-    return sub {
-        return ($grad_with_loss_func->(@_))[0];
-    };
-}
-
-method train_section(CodeRef $sub)
-{
-    my $prev = __PACKAGE__->set_is_training(1);
-    $sub->();
-    __PACKAGE__->set_is_training(0) unless $prev;
-}
-
-method test_section(CodeRef $sub)
-{
-    my $prev = __PACKAGE__->set_is_training(0);
-    $sub->();
-    __PACKAGE__->set_is_training(1) if $prev;
-}
-
-1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/CudaModule.pm b/perl-package/AI-MXNet/lib/AI/MXNet/CudaModule.pm
new file mode 100644
index 000000000000..5fa66b26472e
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/CudaModule.pm
@@ -0,0 +1,294 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::CudaModule;
+use strict;
+use warnings;
+use AI::MXNet::Base;
+use Mouse;
+use AI::MXNet::Function::Parameters;
+
+our %DTYPE_CPP_TO_STR = qw(
+    float    float32
+    double   float64
+    __half   float16
+    uint8_t  uint8
+    int      int32
+    int32_t  int32
+    int8_t   int8
+    char     int8
+    int64_t  int64
+);
+
+=head1 DESCRIPTION
+
+    Interface to runtime cuda kernel compile module.
+    Compile and run CUDA code from Perl.
+
+    In CUDA 7.5, you need to prepend your kernel definitions
+    with 'extern "C"' to avoid name mangling::
+
+        $source = '
+        extern "C" __global__ void axpy(const float *x, float *y, float alpha) {
+            int i = threadIdx.x + blockIdx.x * blockDim.x;
+            y[i] += alpha * x[i];
+        }
+        ';
+        $module = mx->rtc->CudaModule(source);
+        $func = $module->get_kernel("axpy", "const float *x, float *y, float alpha");
+        $x = mx->nd->ones([10]), ctx=>mx->gpu(0));
+        $y = mx->nd->zeros([10]), ctx=>mx->gpu(0));
+        $func->launch([$x, $y, 3.0], mx->gpu(0), [1, 1, 1], [10, 1, 1]);
+        print $y->aspdl;
+
+    Starting from CUDA 8.0, you can instead export functions by name.
+    This also allows you to use templates::
+
+        my $source = '
+        template<typename DType>
+        __global__ void axpy(const DType *x, DType *y, DType alpha) {
+            int i = threadIdx.x + blockIdx.x * blockDim.x;
+            y[i] += alpha * x[i];
+        }
+        ';
+        $module = mx->rtc->CudaModule($source, exports=>['axpy<float>', 'axpy<double>']);
+        $func32 = $module->get_kernel("axpy<float>", "const float *x, float *y, float alpha");
+        $x = mx->nd->ones([10], dtype=>'float32', ctx=>mx->gpu(0));
+        $y = mx->nd->zeros([10], dtype=>'float32', ctx=>mx->gpu(0));
+        $func32->launch([$x, $y, 3.0], mx->gpu(0), [1, 1, 1], [10, 1, 1]);
+        print $y->aspdl;
+
+        $func64 = $module->get_kernel("axpy<double>", "const double *x, double *y, double alpha");
+        $x = mx->nd->ones([10], dtype=>'float64', ctx=>mx->gpu(0));
+        $y = mx->nd->zeros([10], dtype=>'float64', ctx=>mx->gpu(0));
+        $func32->launch([$x, $y, 3.0], mx->gpu(0), [1, 1, 1], [10, 1, 1]);
+        print $y->aspdl;
+
+
+    Parameters
+    ----------
+    source : str
+        Complete source code.
+    options : array ref of str
+        Compiler flags. For example, use "-I/usr/local/cuda/include" to
+        add cuda headers to include path.
+    exports : array ref of str
+        Export kernel names.
+=cut
+
+has 'source' => (is => 'rw', isa => 'Str', required => 1);
+has [qw/options exports/] => (is => 'rw', isa => 'Str|ArrayRef[Str]', default => sub { [] });
+has 'handle' => (is => 'rw', isa => 'CudaModuleHandle');
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { ['source', 'options', 'exports'] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->options([$self->options]) unless ref $self->options;
+    $self->options([$self->exports]) unless ref $self->exports;
+    my $handle = check_call(
+                    AI::MXNetCAPI::RtcCudaModuleCreate(
+                        $self->source,
+                        scalar(@{ $self->options }),
+                        $self->options,
+                        scalar(@{ $self->exports }),
+                        $self->exports
+                    )
+    );
+    $self->handle($handle);
+}
+
+sub DEMOLISH
+{
+    check_call(AI::MXNetCAPI::RtcCudaModuleFree(shift->handle));
+}
+
+=head2 get_kernel
+
+        Get CUDA kernel from compiled module.
+
+        Parameters
+        ----------
+        name : str
+            String name of the kernel.
+        signature : str
+            Function signature for the kernel. For example, if a kernel is
+            declared as::
+
+                extern "C" __global__ void axpy(const float *x, double *y, int alpha)
+
+            Then its signature should be::
+
+                const float *x, double *y, int alpha
+
+            or::
+
+                const float *, double *, int
+
+            Note that `*` in signature marks an argument as array and
+            `const` marks an argument as constant (input) array.
+
+        Returns
+        -------
+        AI::MXNet::CudaKernel
+            CUDA kernels that can be launched on GPUs.
+=cut
+
+method get_kernel(Str $name, Str $signature)
+{
+    my @is_ndarray;
+    my @is_const;
+    my @dtypes;
+    my $pattern = qr/^\s*(const)?\s*([\w_]+)\s*(\*)?\s*([\w_]+)?\s*$/;
+    $signature =~ s/\s+/ /g;
+    my @args = split(/,/, $signature);
+    for my $arg (@args)
+    {
+        if(not $arg =~ $pattern or $2 eq 'const')
+        {
+            confess(
+                "Invalid function prototype \"$arg\". Must be in the ".
+                'form of "(const) type (*) (name)'
+            );
+        }
+        push @is_const, $1 ? 1 : 0;
+        my $dtype = $2;
+        push @is_ndarray, $3 ? 1 : 0;
+        if(not exists $DTYPE_CPP_TO_STR{$dtype})
+        {
+            my $types = join(',', sort keys %DTYPE_CPP_TO_STR);
+            confess("Unsupported kernel argument type $arg. Supported types are: $types.");
+        }
+        push @dtypes, DTYPE_STR_TO_MX->{$DTYPE_CPP_TO_STR{$dtype}};
+    }
+
+    my $handle = check_call(
+        AI::MXNetCAPI::RtcCudaKernelCreate(
+            $self->handle,
+            $name,
+            scalar(@dtypes),
+            \@is_ndarray,
+            \@is_const,
+            \@dtypes
+        )
+    );
+    return AI::MXNet::CudaKernel->new($handle, $name, \@is_ndarray, \@dtypes);
+}
+
+package AI::MXNet::CudaKernel;
+use Mouse;
+use AI::MXNet::Base;
+
+=head1 NAME
+
+    AI::MXNet::CudaKernel
+=cut
+
+=head1 DESCRIPTION
+
+    Constructs CUDA kernel.
+    Intended to be created by calling AI::MXNet::CudaModule->get_kernel only.
+=cut
+
+has [qw/handle name is_ndarray dtypes/] => (is => 'rw');
+around BUILDARGS => sub {
+    my ($orig, $class, $handle, $name, $is_ndarray, $dtypes) = @_;
+    return $class->$orig(handle => $handle, name => $name, is_ndarray => $is_ndarray, dtypes => $dtypes);
+};
+
+sub BUILD
+{
+    my $self = shift;
+    $self->dtypes([map { DTYPE_MX_TO_STR->{$_} } @{ $self->dtypes }]);
+}
+
+sub DEMOLISH
+{
+    check_call(AI::MXNetCAPI::RtcCudaKernelFree(shift->handle));
+}
+
+=head2 launch
+
+        Launch cuda kernel.
+
+        Parameters
+        ----------
+        $args : array ref of NDArray or numbers
+            List of arguments for kernel. NDArrays are expected for pointer
+            types (e.g. `float*`, `double*`) while numbers are expected for
+            non-pointer types (e.g. `int`, `float`).
+        $ctx : AI::MXNet::Context
+            The context to launch kernel on. Must be GPU context.
+        $grid_dims : array ref of 3 integers
+            Grid dimensions for CUDA kernel.
+        $block_dims : array ref of 3 integers
+            Block dimensions for CUDA kernel.
+        $shared_mem=0 : integer, optional
+            Size of dynamically allocated shared memory. Defaults to 0.
+=cut
+
+method launch(
+    ArrayRef[AI::MXNet::NDArray|Num] $args,
+    AI::MXNet::Context $ctx,
+    CudaKernelShape $grid_dims,
+    CudaKernelShape $block_dims,
+    Int $shared_mem=0
+)
+{
+    assert(($ctx->device_type eq 'gpu'), "Cuda kernel can only be launched on GPU");
+    confess("CudaKernel(${\ $self->name }) expects ".scalar(@{$self->dtypes}). "arguments but got ".scalar(@$args).".")
+        unless (@{ $args } == @{ $self->dtypes });
+    my @void_args;
+    enumerate(sub {
+        my ($i, $arg, $is_nd, $dtype) = @_;
+        if($is_nd)
+        {
+            confess("The $i-th argument is expected to be a NDArray but got [$arg]")
+                unless blessed $arg;
+            push @void_args, $arg->handle;
+        }
+        else
+        {
+            my $perl_pack_type = DTYPE_MX_TO_PERL->{$dtype};
+            my $packed_arg;
+            ## special handling for float16
+            if($perl_pack_type eq 'S')
+            {
+                $packed_arg = pack("S", AI::MXNetCAPI::_float_to_half($arg));
+            }
+            else
+            {
+                $packed_arg = pack($perl_pack_type, $arg);
+
+            }
+            push @void_args, $packed_arg;
+        }
+    }, $args, $self->is_ndarray, $self->dtypes);
+    check_call(
+        AI::MXNetCAPI::RtcCudaKernelCall(
+            $self->handle,
+            $ctx->device_id,
+            \@void_args,
+            @{ $grid_dims },
+            @{ $block_dims },
+            $shared_mem
+        )
+    );
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Executor.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Executor.pm
index 20a6f580a3db..3a1a343c62a7 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Executor.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Executor.pm
@@ -38,7 +38,7 @@ has [qw/_arg_dict
         _aux_dict
         _output_dict
         outputs
-        _output_dirty/] => (is => 'rw', init_arg => undef);
+    /]                  => (is => 'rw', init_arg => undef);
 =head1 NAME
 
     AI::MXNet::Executor - The actual executing object of MXNet.
@@ -191,14 +191,6 @@ method forward(Int $is_train=0, %kwargs)
             $is_train
         )
     );
-    if($self->_output_dirty)
-    {
-        AI::MXNet::Logging->warning(
-            "Calling forward the second time after forward(is_train=1) "
-            ."without calling backward first. Is this intended?"
-        );
-    }
-    $self->_output_dirty($is_train);
     return $self->outputs;
 }
 
@@ -212,9 +204,17 @@ method forward(Int $is_train=0, %kwargs)
         The gradient on the outputs to be propagated back.
         This parameter is only needed when bind is called
         on outputs that are not a loss function.
+
+    is_train : bool, default 1
+        Whether this backward is for training or inference. Note that in rare
+        cases you want to call backward with is_train=0 to get gradient
+        during inference.
 =cut
 
-method backward(Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray]|HashRef[AI::MXNet::NDArray]] $out_grads=)
+method backward(
+    Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray]|HashRef[AI::MXNet::NDArray]] $out_grads=,
+    Bool $is_train=1
+)
 {
     $out_grads //= [];
     if(blessed $out_grads)
@@ -226,20 +226,13 @@ method backward(Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray]|HashRef[AI
         $out_grads = [ @{ $out_grads }{ @{ $self->symbol->list_outputs() } } ];
     }
     check_call(
-        AI::MXNetCAPI::ExecutorBackward(
+        AI::MXNetCAPI::ExecutorBackwardEx(
             $self->handle,
             scalar(@{ $out_grads }),
-            [map { $_->handle } @{ $out_grads }]
+            [map { $_->handle } @{ $out_grads }],
+            $is_train
         )
     );
-    if(not $self->_output_dirty)
-    {
-        AI::MXNet::Logging->warning(
-            "Calling backward without calling forward(is_train=True) "
-            ."first. Behavior is undefined."
-        );
-    }
-    $self->_output_dirty(0);
 }
 
 =head2 set_monitor_callback
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Executor/Group.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Executor/Group.pm
index 611c93148f25..7ac054333c13 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Executor/Group.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Executor/Group.pm
@@ -841,11 +841,12 @@ method _bind_ith_exec(
     my $context = $self->contexts->[$i];
     my $shared_data_arrays = $self->_p->shared_data_arrays->[$i];
     my %input_shapes = map { $_->name => $_->shape } @{ $data_shapes };
+    my %input_types  = map { $_->name => $_->dtype } @{ $data_shapes };
     if(defined $label_shapes)
     {
         %input_shapes = (%input_shapes, map { $_->name => $_->shape } @{ $label_shapes });
+        %input_types  = (%input_types,  map { $_->name => $_->dtype } @{ $label_shapes });
     }
-    my %input_types = map { $_->name => $_->dtype } @{ $data_shapes };
     my $executor = $self->symbol->simple_bind(
         ctx              => $context,
         grad_req         => $self->grad_req,
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon.pm
new file mode 100644
index 000000000000..687cd8c3a3e4
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon.pm
@@ -0,0 +1,54 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon;
+use strict;
+use warnings;
+use AI::MXNet::Gluon::Loss;
+use AI::MXNet::Gluon::Trainer;
+use AI::MXNet::Gluon::Utils;
+use AI::MXNet::Gluon::Data;
+use AI::MXNet::Gluon::NN;
+use AI::MXNet::Gluon::RNN;
+
+sub import
+{
+    my ($class, $short_name) = @_;
+    if($short_name)
+    {
+        $short_name =~ s/[^\w:]//g;
+        if(length $short_name)
+        {
+            my $short_name_package =<<"EOP";
+            package $short_name;
+            sub data { 'AI::MXNet::Gluon::Data' }
+            sub nn { 'AI::MXNet::Gluon::NN_' }
+            sub rnn { 'AI::MXNet::Gluon::RNN_' }
+            sub loss { 'AI::MXNet::Gluon::Loss_' }
+            sub utils { 'AI::MXNet::Gluon::Utils' }
+            sub Trainer { shift; AI::MXNet::Gluon::Trainer->new(\@_); }
+            sub Parameter { shift; AI::MXNet::Gluon::Parameter->new(\@_); }
+            sub ParameterDict { shift; AI::MXNet::Gluon::ParameterDict->new(\@_); }
+            \@${short_name}::ISA = ('AI::MXNet::Gluon_');
+            1;
+EOP
+            eval $short_name_package;
+        }
+    }
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Block.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Block.pm
new file mode 100644
index 000000000000..982822be5dc8
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Block.pm
@@ -0,0 +1,904 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+
+# Scope for collecting child 'Block's
+use strict;
+use warnings;
+use AI::MXNet::Gluon::Parameter;
+package AI::MXNet::Gluon::BlockScope;
+use AI::MXNet::Function::Parameters;
+my $_current;
+use Mouse;
+has '_block'      => (is => 'ro', init_arg => 'block');
+has [qw/_counter _old_scope
+    _name_scope/] => (is => 'rw', init_arg => undef);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_counter({});
+}
+
+# Creates prefix and params for new Block.
+method create($prefix, $params, $hint)
+{
+    my $current = $_current;
+    if(not defined $current)
+    {
+        if(not defined $prefix)
+        {
+            $prefix = AI::MXNet::Symbol::NameManager->current->get(undef, $hint) . '_';
+        }
+        if(not defined $params)
+        {
+            $params = AI::MXNet::Gluon::ParameterDict->new(prefix => $prefix);
+        }
+        else
+        {
+            $params = AI::MXNet::Gluon::ParameterDict->new(prefix => $params->prefix, shared => $params);
+        }
+        return ($prefix, $params);
+    }
+
+    if(not defined $prefix)
+    {
+        my $count = $current->_counter->{ $hint } // 0;
+        $prefix = sprintf('%s%d_', $hint, $count);
+        $current->_counter->{$hint} = $count + 1;
+    }
+    if(not defined $params)
+    {
+        my $parent = $current->_block->params;
+        $params = AI::MXNet::Gluon::ParameterDict->new(prefix => $parent->prefix.$prefix, shared => $parent->_shared);
+    }
+    else
+    {
+        $params = AI::MXNet::Gluon::ParameterDict->new(prefix => $params->prefix, $params);
+    }
+    return ($current->_block->prefix.$prefix, $params);
+}
+
+method __enter__()
+{
+    $self->_old_scope($_current);
+    $_current = $self;
+    $self->_name_scope(AI::MXNet::Symbol::NameManager->current);
+    AI::MXNet::Symbol::NameManager->set_current(AI::MXNet::Symbol::Prefix->new(prefix => $self->_block->prefix));
+    return $self;
+}
+
+method __exit__()
+{
+    AI::MXNet::Symbol::NameManager->set_current($self->_name_scope);
+    $self->_name_scope(undef);
+    $_current = $self->_old_scope;
+}
+
+package AI::MXNet::Gluon::Block;
+use AI::MXNet::Gluon::Mouse;
+
+=head2 NAME
+
+    AI::MXNet::Gluon::Block - Base class for all neural network layers and models.
+
+=head2 DESCRIPTION
+
+    Base class for all neural network layers and models. Your models should
+    subclass this class.
+
+    `Block` can be nested recursively in a tree structure. You can create and
+    assign child `Block` as regular attributes::
+
+        from mxnet.gluon import Block, nn
+        from mxnet import ndarray as F
+
+        class Model(Block):
+            def __init__(self, **kwargs):
+                super(Model, self).__init__(**kwargs)
+                # use name_scope to give child Blocks appropriate names.
+                # It also allows sharing Parameters between Blocks recursively.
+                with self.name_scope():
+                    self.dense0 = nn.Dense(20)
+                    self.dense1 = nn.Dense(20)
+
+                x = F.relu(self.dense0(x))
+                return F.relu(self.dense1(x))
+
+        model = Model()
+        model.initialize(ctx=mx.cpu(0))
+        model(F.zeros((10, 10), ctx=mx.cpu(0)))
+
+
+    Child `Block` assigned this way will be registered and `collect_params`
+    will collect their Parameters recursively.
+
+    Parameters
+    ----------
+    prefix : str
+        Prefix acts like a name space. It will be prepended to the names of all
+        Parameters and child `Block`s in this `Block`'s `name_scope`. Prefix
+        should be unique within one model to prevent name collisions.
+    params : ParameterDict or None
+        `ParameterDict` for sharing weights with the new `Block`. For example,
+        if you want `dense1` to share `dense0`'s weights, you can do::
+
+            dense0 = nn.Dense(20)
+            dense1 = nn.Dense(20, params=dense0.collect_params())
+=cut
+
+method _flatten(
+    $args
+)
+{
+    if(blessed $args and $args->isa('AI::MXNet::NDArray'))
+    {
+        return ([$args], 0);
+    }
+    elsif(blessed $args and $args->isa('AI::MXNet::Symbol'))
+    {
+        my $length = @{ $args->list_outputs() };
+        $length = $length > 1 ? $length : 0;
+        return ([$args], $length)
+    }
+    my @flat;
+    my @fmts;
+    for my $i (@{ $args })
+    {
+        my ($arg, $fmt) = __PACKAGE__->_flatten($i);
+        push @flat, @{ $arg };
+        push @fmts, $fmt;
+    }
+    return (\@flat, \@fmts);
+}
+
+method _regroup(
+    $args, $fmt
+)
+{
+    my $in_symbol = (blessed $args and $args->isa('AI::MXNet::Symbol'));
+    my @ret;
+    if(not ref $fmt)
+    {
+        my $len = @{$args} - 1;
+        if($fmt == 0)
+        {
+            @ret = ([@{$args}[1..$len]]);
+            if($in_symbol)
+            {
+                $ret[0] = AI::MXNet::Symbol->Group($ret[0]);
+            }
+            return (@{$args}[0], $ret[0]);
+        }
+        @ret = ([@{$args}[0..$fmt-1]], [@{$args}[$fmt..$len]]);
+        if($in_symbol)
+        {
+            @ret = map { AI::MXNet::Symbol->Group($_) } @ret;
+        }
+        return @ret;
+    }
+    for my $i (@{ $fmt })
+    {
+        my $res;
+        ($res, $args) = __PACKAGE__->_regroup($args, $i);
+        push @ret, $res;
+    }
+    return (\@ret, $args);
+}
+
+has _prefix => (is => 'rw', init_arg => 'prefix', isa => 'Str');
+has _params => (is => 'rw', init_arg => 'params', isa => 'Maybe[AI::MXNet::Gluon::ParameterDict]');
+has [qw/_name _scope/] => (is => 'rw', init_arg => undef);
+has [qw/_children/]    => (is => 'rw', init_arg => undef, default => sub { [] });
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+
+sub AUTOLOAD {
+    my $name = $AI::MXNet::Gluon::Block::AUTOLOAD;
+    $name =~ s/.*:://;
+    my $self = shift;
+    AI::MXNet::Gluon::Mouse::has($name => (is => 'rw', 'init_arg' => undef, 'caller' => ref $self));
+    $self->$name(@_);
+}
+
+sub BUILD
+{
+    my $self = shift;
+    my ($prefix, $params) = AI::MXNet::Gluon::BlockScope->create($self->_prefix, $self->_params, $self->_alias);
+    $self->_prefix($prefix);
+    $self->_params($params);
+    my $name = $prefix;
+    $name =~ s/_$//;
+    $self->_name($name);
+    $self->_scope(AI::MXNet::Gluon::BlockScope->new(block => $self));
+}
+
+method _class_name()
+{
+    my $class = ref $self || $self;
+    $class =~ s/^.+:://;
+    $class;
+}
+
+method __setattr__($name, $current, $prev=)
+{
+    if(defined $prev)
+    {
+        if(
+            (
+                blessed $prev
+                    and
+                ($prev->isa('AI::MXNet::Gluon::Parameter') or $prev->isa('AI::MXNet::Gluon::Block'))
+            )
+            and not (blessed $current and (ref($prev) eq ref($current)))
+        )
+        {
+            confess(
+                sprintf(
+                    "Changing attribute type for %s from %s to %s is not allowed.",
+                    $self->name,
+                    ref($prev),
+                    ref($current)||'no ref'
+                )
+            );
+        }
+        if(blessed $current and $current->isa('AI::MXNet::Gluon::Block'))
+        {
+            for(my $i = 0; $i < @{ $self->_children }; $i++)
+            {
+                if(Scalar::Util::refaddr($self->_children->[$i]) eq Scalar::Util::refaddr($prev))
+                {
+                    $self->_children->[$i] = $current;
+                }
+            }
+        }
+    }
+    if(blessed $current and $current->isa('AI::MXNet::Gluon::Block'))
+    {
+        $self->register_child($current);
+    }
+}
+
+method _alias()
+{
+    lc $self->_class_name;
+}
+
+method attributes_hash()
+{
+    +{ map { $_ => $self->$_ } $self->meta->get_attribute_list };
+}
+
+use overload
+    '""' => sub
+    {
+        my $self = shift;
+        my $s = "%s(\n{%s}\n)";
+        my @blocks;
+        my %attributes_hash = %{ $self->attributes_hash };
+        while(my ($k, $v) = each %attributes_hash)
+        {
+            if(blessed $v and $v->isa(__PACKAGE__))
+            {
+                push @blocks, "  ($k): ".AI::MXNet::Base::_indent("$v", 2);
+            }
+        }
+        sprintf("%s(\n{%s}\n)", $self->_class_name, join("\n", @blocks));
+    },
+    '&{}' => sub { my $self = shift; sub { $self->call(@_) } };
+
+method prefix()
+{
+    $self->_prefix;
+}
+
+method name()
+{
+    $self->_name;
+}
+
+method class()
+{
+    __PACKAGE__;
+}
+
+method name_scope(CodeRef $sub)
+{
+    $self->_scope->__enter__;
+    $sub->();
+    $self->_scope->__exit__;
+}
+
+=head2 params
+
+        Returns this `Block`'s parameter dictionary (does not include its
+        children's parameters).
+=cut
+
+method params()
+{
+    return $self->_params;
+}
+
+=head2 collect_params
+
+        Returns a `ParameterDict` containing this `Block` and all of its
+        children's Parameters.
+=cut
+
+method collect_params()
+{
+    my $ret = AI::MXNet::Gluon::ParameterDict->new(prefix => $self->_params->prefix);
+    $ret->update($self->params);
+    for my $cld (@{ $self->_children })
+    {
+        $ret->update($cld->collect_params());
+    }
+    return $ret;
+}
+
+=head2 save
+
+        Save parameters to file.
+
+        filename : str
+            Path to file.
+=cut
+
+method save_params($filename)
+{
+    $self->collect_params->save($filename, $self->prefix);
+}
+
+=head2 load
+
+        Load parameters from file.
+
+        $filename : str
+            Path to parameter file.
+        :$ctx= : Context or list of Context
+            Context(s) initialize loaded parameters on.
+        :$allow_missing : bool, default False
+            Whether to silently skip loading parameters not represents in the file.
+        :$ignore_extra : bool, default False
+            Whether to silently ignore parameters from the file that are not
+            present in this Block.
+=cut
+
+method load_params(
+    Str   $filename,
+    Maybe [AI::MXNet::Context|ArrayRef[AI::MXNet::Context]] :$ctx=,
+    Bool  :$allow_missing=0,
+    Bool  :$ignore_extra=0
+)
+{
+    $self->collect_params->load(
+        $filename,
+        ($ctx ? (ctx   => $ctx) : ()),
+        allow_missing  => $allow_missing,
+        ignore_extra   => $ignore_extra,
+        restore_prefix => $self->prefix
+    );
+}
+
+=head2 register_child
+
+        Registers block as a child of self. `Block`s assigned to self as
+        attributes will be registered automatically.
+=cut
+
+method register_child(AI::MXNet::Gluon::Block $block)
+{
+    push @{ $self->_children }, $block;
+}
+
+=head2 initialize
+
+        Initializes `Parameter`s of this `Block` and its children.
+
+        Equivalent to `block.collect_params().initialize(...)`
+=cut
+
+method initialize(
+    Initializer $init=AI::MXNet::Initializer->Uniform(),
+    AI::MXNet::Context|ArrayRef[AI::MXNet::Context] :$ctx=AI::MXNet::Context->current_ctx,
+    Bool :$verbose=0
+)
+{
+    $self->collect_params->initialize(init => $init, ctx => $ctx, verbose => $verbose);
+}
+
+
+=head2 hybridize
+
+        Activates or deactivates `HybridBlock`s recursively. Has no effect on
+        non-hybrid children.
+
+        Parameters
+        ----------
+        active : bool, default True
+            Whether to turn hybrid on or off.
+=cut
+
+method hybridize(Bool $active=1)
+{
+    $_->hybridize($active) for @{ $self->_children };
+}
+
+method call(@args)
+{
+    return $self->forward(@args);
+}
+
+=head2 forward
+
+        Overrides to implement forward computation using `NDArray`. Only
+        accepts positional arguments.
+
+        Parameters
+        ----------
+        @args : array of NDArray
+            Input tensors.
+=cut
+
+method forward(@args)
+{
+    confess("Not Implemented");
+}
+
+method register(Str $container)
+{
+    my $sub_name = $self->_class_name;
+    no strict 'refs';
+    *{$container.'_::'.$sub_name} = sub { shift; $self->new(@_) };
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon');
+
+package AI::MXNet::Gluon::HybridBlock;
+
+=head2 NAME
+
+    AI::MXNet::Gluon::HybridBlock
+
+=head2 DESCRIPTION
+
+    `HybridBlock` supports forwarding with both Symbol and NDArray.
+
+    Forward computation in `HybridBlock` must be static to work with `Symbol`s,
+    i.e. you cannot call `.asnumpy()`, `.shape`, `.dtype`, etc on tensors.
+    Also, you cannot use branching or loop logic that bases on non-constant
+    expressions like random numbers or intermediate results, since they change
+    the graph structure for each iteration.
+
+    Before activating with `hybridize()`, `HybridBlock` works just like normal
+    `Block`. After activation, `HybridBlock` will create a symbolic graph
+    representing the forward computation and cache it. On subsequent forwards,
+    the cached graph will be used instead of `hybrid_forward`.
+
+    Refer `Hybrid tutorial <http://mxnet.io/tutorials/gluon/hybrid.html>`_ to see
+    the end-to-end usage.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::Block';
+has [qw/
+        _reg_params _cached_graph
+        _cached_op _cached_params
+        _out_format _in_format
+        _active _in_idx
+/] => (is => 'rw', init_arg => undef);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_reg_params({});
+    $self->_cached_graph([]);
+    $self->_active(0);
+}
+
+method __setattr__($name, $current, $prev=)
+{
+    $self->SUPER::__setattr__($name, $current, $prev);
+    if(blessed $current and $current->isa('AI::MXNet::Gluon::Parameter'))
+    {
+        $self->_reg_params->{ $name } = $current;
+    }
+}
+
+method register_child(AI::MXNet::Gluon::HybridBlock $block)
+{
+    push @{ $self->_children }, $block;
+}
+
+method hybridize(Bool $active=1)
+{
+    $self->_active($active);
+    $self->SUPER::hybridize($active);
+}
+
+method _get_graph(@args)
+{
+    if(not @{ $self->_cached_graph })
+    {
+        my $args = [@args];
+        my ($in_format, $out_format);
+        ($args, $in_format) = __PACKAGE__->_flatten($args);
+        $self->_in_format($in_format);
+        my @inputs = map { AI::MXNet::Symbol->var("input_$_") } 0 .. @$args-1;
+        my ($grouped_inputs) = __PACKAGE__->_regroup(\@inputs, $self->_in_format);
+        my %params = map { $_ => $self->_reg_params->{$_}->var } keys %{ $self->_reg_params };
+        my @out;
+        $self->name_scope(sub {
+            @out = $self->hybrid_forward('AI::MXNet::Symbol', @{ $grouped_inputs }, %params);
+        });
+        my $out = @out > 1 ? [@out] : $out[0];
+        ($out, $out_format) = __PACKAGE__->_flatten($out);
+        $self->_out_format($out_format);
+        @{ $self->_cached_graph } = (\@inputs, AI::MXNet::Symbol->Group($out));
+    }
+    return @{ $self->_cached_graph };
+}
+
+=head2 infer_shape
+
+        Infers shape of Parameters from inputs.
+=cut
+
+method infer_shape(@args)
+{
+    my ($inputs, $out) = $self->_get_graph(@args);
+    my $args = \@args;
+    ($args) = __PACKAGE__->_flatten($args);
+    my %in;
+    zip(sub {
+        my ($i, $j) = @_;
+        $in{ $i->name } = $j->shape;
+    }, $inputs, $args);
+    my ($arg_shapes, undef, $aux_shapes) = $out->infer_shape(%in);
+    my %sdict;
+    zip(sub {
+        my ($i, $j) = @_;
+        $sdict{ $i } = $j;
+    }, $out->list_arguments(), $arg_shapes);
+    my %aux;
+    zip(sub {
+        my ($i, $j) = @_;
+        $aux{ $i } = $j;
+    }, $out->list_auxiliary_states(), $aux_shapes);
+    %sdict = (%sdict, %aux);
+    for my $i ($self->collect_params->values)
+    {
+        $i->shape($sdict{ $i->name })
+    }
+}
+
+method _build_cache(@args)
+{
+    my ($inputs, $out) = $self->_get_graph(@args);
+    $self->_cached_op(AI::MXNet::NDArray->CachedOp($out));
+    my %params = %{ $self->collect_params };
+    $self->_cached_params([map { $params{ $_ } } @{ $out->list_inputs }]);
+    assert(
+        (
+            ((keys %params) + (@{ $self->_cached_graph->[0] }))
+                ==
+            @{ $out->list_inputs }
+        ),
+        "Wrong number of inputs."
+    );
+    my %name2pos;
+    enumerate(sub {
+        my ($i, $var) = @_;
+        $name2pos{ $var->name } = $i;
+    }, $inputs);
+    my @in_idx;
+    enumerate(sub {
+        my ($i, $name) = @_;
+        if(not exists $params{ $name })
+        {
+            push @in_idx, [$i, $name2pos{ $name }];
+        }
+    }, $out->list_inputs);
+    $self->_in_idx(\@in_idx);
+}
+
+use Data::Dumper;
+method _call_cached_op(@args)
+{
+    if(not defined $self->_cached_op)
+    {
+        $self->_build_cache(@args);
+    }
+
+    my @cargs;
+    eval {
+        @cargs = map { defined($_) ? $_->data() : undef } @{ $self->_cached_params };
+    };
+    if($@)
+    {
+        if($@ =~ /DeferredInitializationError/)
+        {
+            $self->infer_shape(@args);
+            map { $_->_finish_deferred_init if defined } @{ $self->_cached_params };
+            @cargs = map { defined($_) ? $_->data() : undef } @{ $self->_cached_params };
+        }
+        else
+        {
+            confess($@);
+        }
+    }
+    my $args = [@args];
+    my $fmt;
+    ($args, $fmt) = __PACKAGE__->_flatten($args);
+    assert((Dumper($fmt) eq Dumper($self->_in_format)), "Invalid input format");
+    for (@{ $self->_in_idx })
+    {
+        $cargs[$_->[0]] = $args->[$_->[1]];
+    }
+    my $out = $self->_cached_op->(@cargs);
+    if(blessed $out and $out->isa('AI::MXNet::NDArray'))
+    {
+        $out = [$out];
+    }
+    my $ret = (__PACKAGE__->_regroup($out, $self->_out_format))[0];
+    if(ref($ret) eq 'ARRAY' and wantarray)
+    {
+        return @$ret;
+    }
+    else
+    {
+        return $ret;
+    }
+}
+
+=head2 forward
+
+        Defines the forward computation. Arguments can be either
+        `NDArray` or `Symbol`.
+=cut
+
+method forward($x, @args)
+{
+    if(blessed $x and $x->isa('AI::MXNet::NDArray'))
+    {
+        my @out;
+        my $out;
+        my $ctx = $x->context;
+        my $current_ctx = AI::MXNet::Context->current_ctx;
+        AI::MXNet::Context->set_current($ctx);
+        if($self->_active)
+        {
+            if(wantarray)
+            {
+                my @out = $self->_call_cached_op($x, @args);
+                AI::MXNet::Context->set_current($current_ctx);
+                return @out;
+            }
+            else
+            {
+                my $out = $self->_call_cached_op($x, @args);
+                AI::MXNet::Context->set_current($current_ctx);
+                return $out;
+            }
+        }
+        my %params;
+        eval {
+            %params = map { $_ => $self->_reg_params->{ $_ }->data($ctx) } keys %{ $self->_reg_params };
+        };
+        if($@)
+        {
+            if($@ =~ /DeferredInitializationError/)
+            {
+                $self->infer_shape($x, @args);
+                $_->_finish_deferred_init for $self->collect_params->values;
+                %params = map { $_ => $self->_reg_params->{ $_ }->data($ctx) } keys %{ $self->_reg_params };
+            }
+            else
+            {
+                confess($@);
+            }
+        }
+        @out = $self->hybrid_forward('AI::MXNet::NDArray', $x, @args, %params);
+        AI::MXNet::Context->set_current($current_ctx);
+        return wantarray ? @out : $out[0];
+    }
+    assert(
+        (blessed $x and $x->isa('AI::MXNet::Symbol')),
+        "HybridBlock requires the first argument to forward be either ".
+        "Symbol or NDArray, but got [".ref($x)."]"
+    );
+    my %params = map { $_ => $self->_reg_params->{ $_ }->var } keys %{ $self->_reg_params };
+    my @ret;
+    $self->name_scope(sub {
+        @ret = $self->hybrid_forward('AI::MXNet::Symbol', $x, @args, %params);
+    });
+    return wantarray ? @ret : $ret[0];
+}
+
+=head2 hybrid_forward
+
+        Overrides to construct symbolic graph for this `Block`.
+
+        Parameters
+        ----------
+        x : Symbol or NDArray
+            The first input tensor.
+        *args : list of Symbol or list of NDArray
+            Additional input tensors.
+=cut
+
+method hybrid_forward($F, $x, @args)
+{
+    confess("NotImplementedError");
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon');
+
+package AI::MXNet::Gluon::SymbolBlock;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::SymbolBlock - Construct block from symbol.
+=cut
+
+=head1 DESCRIPTION
+
+    Construct block from symbol. This is useful for using pre-trained models
+    as feature extractors. For example, you may want to extract get the output
+    from fc2 layer in AlexNet.
+
+    Parameters
+    ----------
+    outputs : Symbol or list of Symbol
+        The desired output for SymbolBlock.
+    inputs : Symbol or list of Symbol
+        The Variables in output's argument that should be used as inputs.
+    params : ParameterDict
+        Parameter dictionary for arguments and auxililary states of outputs
+        that are not inputs.
+
+    Examples
+    --------
+    >>> # To extract the feature from fc1 and fc2 layers of AlexNet:
+    >>> alexnet = gluon.model_zoo.vision.alexnet(pretrained=True, ctx=mx.cpu(),
+                                                 prefix='model_')
+    >>> inputs = mx.sym.var('data')
+    >>> out = alexnet(inputs)
+    >>> internals = out.get_internals()
+    >>> print(internals.list_outputs())
+    ['data', ..., 'model_dense0_relu_fwd_output', ..., 'model_dense1_relu_fwd_output', ...]
+    >>> outputs = [internals['model_dense0_relu_fwd_output'],
+                   internals['model_dense1_relu_fwd_output']]
+    >>> # Create SymbolBlock that shares parameters with alexnet
+    >>> feat_model = gluon.SymbolBlock(outputs, inputs, params=alexnet.collect_params())
+    >>> x = mx.nd.random_normal(shape=(16, 3, 224, 224))
+    >>> print(feat_model(x))
+=cut
+
+has [qw/outputs inputs/] => (is => 'rw', isa => 'AI::MXNet::Symbol|ArrayRef[AI::MXNet::Symbol]');
+method python_constructor_arguments() { [qw/outputs inputs/] }
+
+sub BUILD
+{
+    my ($self, $orig_params) = @_;
+    $self->_prefix('');
+    $self->_params(AI::MXNet::Gluon::ParameterDict->new(prefix => '', shared => $orig_params->{params}));
+    if(blessed $self->inputs and @{ $self->inputs->list_outputs } == 1)
+    {
+        $self->inputs([$self->inputs]);
+    }
+    if(blessed $self->outputs and @{ $self->outputs->list_outputs } == 1)
+    {
+        $self->outputs([$self->outputs]);
+    }
+    my ($syms, $in_format) = __PACKAGE__->_flatten($self->inputs);
+    my ($out, $out_format) = __PACKAGE__->_flatten($self->outputs);
+    $self->_in_format($in_format);
+    $self->_out_format($out_format);
+    $out = AI::MXNet::Symbol->Group($out);
+
+    my %input_names;
+    for my $i (@{ $syms })
+    {
+        assert(
+            (@{ $i->get_internals->list_outputs() } == 1),
+            "Input symbols must be variable, but $i is an output of operators"
+        );
+        $input_names{ $i->name } = 1;
+    }
+
+    for my $i (@{ $out->list_arguments })
+    {
+        if(not exists $input_names{$i})
+        {
+            $self->params->get($i, allow_deferred_init => 1);
+        }
+    }
+
+    for my $i (@{ $out->list_auxiliary_states })
+    {
+        if(not exists $input_names{$i})
+        {
+            $self->params->get($i, grad_req => 'null', allow_deferred_init => 1);
+        }
+    }
+
+    $self->_cached_graph([$syms, $out]);
+    $self->_build_cache;
+}
+
+method forward($x, @args)
+{
+    if(blessed $x and $x->isa('AI::MXNet::NDArray'))
+    {
+        my @out;
+        my $out;
+        my $ctx = $x->context;
+        my $current_ctx = AI::MXNet::Context->current_ctx;
+        AI::MXNet::Context->set_current($ctx);
+        if(wantarray)
+        {
+            my @out = $self->_call_cached_op($x, @args);
+            AI::MXNet::Context->set_current($current_ctx);
+            return @out;
+        }
+        else
+        {
+            my $out = $self->_call_cached_op($x, @args);
+            AI::MXNet::Context->set_current($current_ctx);
+            return $out;
+        }
+    }
+    assert(
+        (blessed $x and $x->isa('AI::MXNet::Symbol')),
+        "HybridBlock requires the first argument to forward be either ".
+        "Symbol or NDArray, but got [".ref($x)."]"
+    );
+    my $args = \@args;
+    my $in_fmt;
+    ($args, $in_fmt) = __PACKAGE__->_flatten([$x, @$args]);
+    assert((Data::Dumper::Dumper($in_fmt) eq Data::Dumper::Dumper($self->_in_format)), "Invalid input format");
+    my $ret = $self->_cached_graph->[1]->deepcopy;
+    my %in;
+    zip(sub {
+        my ($k, $v) = @_;
+        $in{$k->name} = $v;
+    }, $self->_cached_graph->[0], $args);
+    $ret->_compose(%in);
+    $ret = (__PACKAGE__->_regroup($ret, $self->_out_format))[0];
+    if(ref($ret) eq 'ARRAY' and wantarray)
+    {
+        return @$ret;
+    }
+    else
+    {
+        return $ret;
+    }
+}
+
+method hybrid_forward(@args)
+{
+    confess('NotImplementedError');
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon');
+
+1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data.pm
new file mode 100644
index 000000000000..e2287c23dafe
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data.pm
@@ -0,0 +1,29 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::Data;
+use strict;
+use warnings;
+use AI::MXNet::Gluon::Data::Set;
+use AI::MXNet::Gluon::Data::Sampler;
+use AI::MXNet::Gluon::Data::Loader;
+use AI::MXNet::Gluon::Data::Vision;
+sub vision { 'AI::MXNet::Gluon::Data::Vision' }
+
+1;
+
+
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Loader.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Loader.pm
new file mode 100644
index 000000000000..e6a0e7461a80
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Loader.pm
@@ -0,0 +1,186 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Loader::DataLoader - Dataset generator.
+=cut
+
+=head1 DESCRIPTION
+
+    Loads data from a dataset and returns mini-batches of data.
+
+    Parameters
+    ----------
+    dataset : Dataset
+        Source dataset. Note that numpy and mxnet arrays can be directly used
+        as a Dataset.
+    batch_size : int
+        Size of mini-batch.
+    shuffle : bool
+        Whether to shuffle the samples.
+    sampler : Sampler
+        The sampler to use. Either specify sampler or shuffle, not both.
+    last_batch : {'keep', 'discard', 'rollover'}
+        How to handle the last batch if batch_size does not evenly divide
+        `len(dataset)`.
+
+        keep - A batch with less samples than previous batches is returned.
+        discard - The last batch is discarded if its incomplete.
+        rollover - The remaining samples are rolled over to the next epoch.
+    batch_sampler : Sampler
+        A sampler that returns mini-batches. Do not specify batch_size,
+        shuffle, sampler, and last_batch if batch_sampler is specified.
+=cut
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::Data::Loader::DataLoader;
+use AI::MXNet::Function::Parameters;
+use Mouse;
+
+method _class_name()
+{
+    my $class = ref $self || $self;
+    $class =~ s/^.+:://;
+    $class;
+}
+
+method register(Str $container)
+{
+    my $sub_name = $self->_class_name;
+    no strict 'refs';
+    *{$container.'::'.$sub_name} = sub { shift; $self->new(@_) };
+}
+
+# Collate data into batch.
+func _batchify($data, $dtype)
+{
+    if(blessed $data->[0] and $data->[0]->isa('AI::MXNet::NDArray'))
+    {
+        return AI::MXNet::NDArray->stack(@{ $data });
+    }
+    elsif(ref $data->[0] eq 'ARRAY')
+    {
+        my (@data, @label);
+        for my $i (@$data)
+        {
+            my ($d, $l) = @$i;
+            push @data, $d;
+            push @label, $l;
+        }
+        return [_batchify(\@data, $dtype), _batchify(\@label, $dtype)];
+    }
+    else
+    {
+        return AI::MXNet::NDArray->array($data, dtype => $dtype);
+    }
+}
+
+has 'dataset'       => (is => 'rw', isa => 'AI::MXNet::Gluon::Data::Set|AI::MXNet::NDArray|PDL', required => 1);
+has 'batch_size'    => (is => 'ro', isa => 'Int');
+has 'shuffle'       => (is => 'ro', isa => 'Bool', default => 0);
+has 'sampler'       => (is => 'rw', isa => 'AI::MXNet::Gluon::Data::Sampler');
+has 'batch_sampler' => (is => 'rw', isa => 'AI::MXNet::Gluon::Data::Sampler');
+has 'last_batch'    => (is => 'rw', isa => 'Str', default => 'keep');
+
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { ['dataset', 'batch_size'] }
+
+sub BUILD
+{
+    my $self = shift;
+    if($self->dataset->isa('PDL'))
+    {
+        $self->dataset(AI::MXNet::NDArray->array($self->dataset));
+    }
+    if(not defined $self->batch_sampler)
+    {
+        if(not defined $self->batch_size)
+        {
+            confess("batch_size must be specified unless batch_sampler is specified");
+        }
+        if(not defined $self->sampler)
+        {
+            if($self->shuffle)
+            {
+                $self->sampler(
+                    AI::MXNet::Gluon::Data::Sampler::RandomSampler->new(
+                        length => $self->dataset->len
+                    )
+                );
+            }
+            else
+            {
+                $self->sampler(
+                    AI::MXNet::Gluon::Data::Sampler::SequentialSampler->new(
+                        length => $self->dataset->len,
+                    )
+                );
+            }
+        }
+        elsif($self->shuffle)
+        {
+            confess("shuffle must not be specified if sampler is specified");
+        }
+        $self->batch_sampler(
+            AI::MXNet::Gluon::Data::Sampler::BatchSampler->new(
+                sampler => $self->sampler,
+                batch_size => $self->batch_size,
+                last_batch => $self->last_batch
+            )
+        );
+    }
+    elsif(defined $self->batch_size or $self->shuffle or defined $self->sampler or defined $self->last_batch)
+    {
+        confess("batch_size, shuffle, sampler and last_batch must ".
+                "not be specified if batch_sampler is specified.");
+    }
+}
+
+use overload
+    '<>' => sub {
+        my $self = shift;
+        my $sampler = $self->batch_sampler;
+        my $batch = <$sampler>;
+        if(not defined $batch)
+        {
+            return undef;
+        };
+        return _batchify([map { $self->dataset->at($_) } @{ $batch }], eval { $self->dataset->label->dtype }//'int32');
+    };
+
+method len()
+{
+    $self->batch_sampler->len;
+}
+
+use overload '@{}' => sub { shift->list };
+
+method list()
+{
+    my @ret;
+    while(defined(my $data = <$self>))
+    {
+        push @ret, $data;
+    }
+    return \@ret;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Sampler.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Sampler.pm
new file mode 100644
index 000000000000..e19f9d8c5f32
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Sampler.pm
@@ -0,0 +1,285 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+
+package AI::MXNet::Gluon::Data::Sampler;
+use AI::MXNet::Function::Parameters;
+use Mouse;
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+
+method _class_name()
+{
+    my $class = ref $self || $self;
+    $class =~ s/^.+:://;
+    $class;
+}
+
+method register(Str $container)
+{
+    my $sub_name = $self->_class_name;
+    no strict 'refs';
+    *{$container.'::'.$sub_name} = sub { shift; $self->new(@_) };
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Sampler
+=cut
+
+=head1 DESCRIPTION
+
+    Base class for samplers.
+
+    All samplers should subclass AI::MXNet::Gluon::Data::Sampler 
+    and define method 'len' and 'next'
+    methods.
+=cut
+
+use overload '<>' =>  sub { shift->next },
+             '@{}' => sub { shift->list };
+
+method list()
+{
+    my @ret;
+    while(defined(my $data = <$self>))
+    {
+        push @ret, $data;
+    }
+    return \@ret;
+}
+
+method len() { confess('Not Implemented') }
+method next() { confess('Not Implemented') }
+
+package AI::MXNet::Gluon::Data::Sampler::SequentialSampler;
+use Mouse;
+extends 'AI::MXNet::Gluon::Data::Sampler';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Sampler::SequentialSampler
+=cut
+
+=head1 DESCRIPTION
+
+    Samples elements from [0, length) sequentially.
+
+    Parameters
+    ----------
+    length : int
+        Length of the sequence.
+=cut
+has 'length'   => (is => 'ro', isa => 'Int', required => 1);
+has '_current' => (is => 'rw', init_arg => undef, default => 0);
+method python_constructor_arguments() { ['length'] }
+
+method next()
+{
+    my $current = $self->_current;
+    if($self->_current == $self->length)
+    {
+        $self->reset;
+        return undef;
+    }
+    else
+    {
+        $self->_current($self->_current + 1);
+        return $current;
+    }
+};
+
+method reset() { $self->_current(0) }
+method len() { $self->length }
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+package AI::MXNet::Gluon::Data::Sampler::RandomSampler;
+use Mouse;
+use List::Util qw(shuffle);
+extends 'AI::MXNet::Gluon::Data::Sampler';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Sampler::RandomSampler
+=cut
+
+=head1 DESCRIPTION
+
+    Samples elements from [0, length) randomly without replacement.
+
+    Parameters
+    ----------
+    length : int
+        Length of the sequence.
+=cut
+has 'length'   => (is => 'ro', isa => 'Int', required => 1);
+has '_current' => (is => 'rw', init_arg => undef, default => 0);
+has '_indices' => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { ['length'] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_indices([shuffle(0..$self->length-1)]);
+}
+
+method next()
+{
+    my $current = $self->_current;
+    if($self->_current == $self->length)
+    {
+        $self->reset;
+        return undef;
+    }
+    else
+    {
+        $self->_current($self->_current + 1);
+        return $self->_indices->[$current];
+    }
+};
+
+method reset() { @{ $self->_indices } = shuffle(@{ $self->_indices }); $self->_current(0) }
+method len() { $self->length }
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+package AI::MXNet::Gluon::Data::Sampler::BatchSampler;
+use Mouse;
+use List::Util qw(shuffle);
+extends 'AI::MXNet::Gluon::Data::Sampler';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Sampler::BatchSampler
+=cut
+
+=head1 DESCRIPTION
+
+    Wraps over another AI::MXNet::Gluon::Data::Sampler and return mini-batches of samples.
+
+    Parameters
+    ----------
+    sampler : AI::MXNet::Gluon::Data::Sampler
+        The source Sampler.
+    batch_size : int
+        Size of mini-batch.
+    last_batch : {'keep', 'discard', 'rollover'}
+        Specifies how the last batch is handled if batch_size does not evenly
+        divide sequence length.
+
+        If 'keep', the last batch will be returned directly, but will contain
+        less element than `batch_size` requires.
+
+        If 'discard', the last batch will be discarded.
+
+        If 'rollover', the remaining elements will be rolled over to the next
+        iteration.
+
+    Examples
+    --------
+    >>> $sampler = gluon->data->SequentialSampler(10)
+    >>> $batch_sampler = gluon->data->BatchSampler($sampler, batch_size => 3, last_batch => 'keep');
+    >>> @{ $batch_sampler }
+    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
+=cut
+has 'batch_size' => (is => 'ro', isa => 'Int', required => 1);
+has 'sampler'    => (is => 'ro', isa => 'AI::MXNet::Gluon::Data::Sampler', required => 1);
+has 'last_batch' => (is => 'ro', isa => 'Str', default => 'keep');
+has '_prev'      => (is => 'rw', init_arg => undef);
+has '_kept'      => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { ['sampler', 'batch_size', 'last_batch'] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_prev([]);
+}
+
+method next()
+{
+    if($self->_kept)
+    {
+        $self->_kept(0);
+        return undef;
+    }
+    $self->_kept(0);
+    my $batch = $self->_prev;
+    $self->_prev([]);
+    my $sampler = $self->sampler;
+    while(defined(my $i = <$sampler>))
+    {
+        push @{ $batch }, $i;
+        if(@{ $batch } == $self->batch_size)
+        {
+            return $batch;
+        }
+    }
+    if(@{ $batch })
+    {
+        if($self->last_batch eq 'keep')
+        {
+            $self->_kept(1);
+            return $batch;
+        }
+        elsif($self->last_batch eq 'discard')
+        {
+            return undef;
+        }
+        elsif($self->last_batch eq 'rollover')
+        {
+            $self->_prev($batch);
+            return undef;
+        }
+        else
+        {
+            confess(
+                "last_batch must be one of 'keep', 'discard', or 'rollover', ".
+                "but got ${\ $self->last_batch }"
+            );
+        }
+    }
+    return undef;
+}
+
+method len()
+{
+    if($self->last_batch eq 'keep')
+    {
+        return int(($self->sampler->len + $self->batch_size - 1) / $self->batch_size);
+    }
+    elsif($self->last_batch eq 'discard')
+    {
+        return int($self->sampler->len/$self->batch_size);
+    }
+    elsif($self->last_batch eq 'rollover')
+    {
+        return int((@{ $self->_prev } + $self->sampler->len) / $self->batch_size);
+    }
+    else
+    {
+        confess(
+            "last_batch must be one of 'keep', 'discard', or 'rollover', ".
+            "but got ${\ $self->last_batch }"
+        );
+    }
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Set.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Set.pm
new file mode 100644
index 000000000000..753659055e34
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Set.pm
@@ -0,0 +1,155 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::Data::Set;
+use AI::MXNet::Function::Parameters;
+use Mouse;
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method _class_name()
+{
+    my $class = ref $self || $self;
+    $class =~ s/^.+:://;
+    $class;
+}
+
+method register(Str $container)
+{
+    my $sub_name = $self->_class_name;
+    no strict 'refs';
+    *{$container.'::'.$sub_name} = sub { shift; $self->new(@_) };
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Set
+=cut
+
+=head1 DESCRIPTION
+
+    Abstract dataset class. All datasets should have this interface.
+
+    Subclasses need to override method at($i), which returns the i-th
+    element, method len() which returns the total number elements.
+
+    AI::MXNet::NDArray can be directly used as a dataset.
+=cut
+
+method at(Index $idx) { confess("Not Implemented") }
+
+method len() { confess("Not Implemented") }
+
+package AI::MXNet::Gluon::Data::ArrayDataset;
+use AI::MXNet::Base;
+use Mouse;
+extends 'AI::MXNet::Gluon::Data::Set';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::ArrayDataset
+=cut
+
+=head1 DESCRIPTION
+
+    A dataset with a data array and a label array.
+
+    The i-th sample is `(data[i], label[i])`.
+
+    Parameters
+    ----------
+    data : AI::MXNet::NDArray or PDL
+        The data array.
+    label : AI::MXNet::NDArray or PDL
+        The label array.
+=cut
+has [qw/data label/] => (is => 'rw', isa => 'PDL|AI::MXNet::NDArray', required => 1);
+method python_constructor_arguments() { ['data', 'label'] }
+
+sub BUILD
+{
+    my $self = shift;
+    assert(($self->data->len == $self->label->len), "data and label lengths must be the same");
+    if($self->label->isa('AI::MXNet::NDArray') and @{$self->label->shape} == 1)
+    {
+        $self->label($self->label->aspdl);
+    }
+    if($self->data->isa('PDL'))
+    {
+        $self->data(AI::MXNet::NDArray->array($self->data));
+    }
+}
+
+method at(Index $idx)
+{
+    return [
+        $self->data->at($idx),
+        $self->label->at($idx)
+    ];
+}
+
+method len()
+{
+    return $self->data->len
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+package AI::MXNet::Gluon::Data::RecordFileSet;
+use AI::MXNet::Base;
+use Mouse;
+extends 'AI::MXNet::Gluon::Data::Set';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::RecordFileSet
+=cut
+
+=head1 DESCRIPTION
+
+    A dataset wrapping over a RecordIO (.rec) file.
+
+    Each sample is a string representing the raw content of an record.
+
+    Parameters
+    ----------
+    filename : str
+        Path to rec file.
+=cut
+has 'filename' => (is => 'ro', isa =>'Str', required => 1);
+has '_record'  => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { ['filename'] }
+
+sub BUILD
+{
+    my $self = shift;
+    my $idx_file = $self->filename;
+    $idx_file =~ s/\.[^.]+$/.idx/;
+    $self->_record(
+        AI::MXNet::IndexedRecordIO->new(
+            idx_path => $idx_file, uri => $self->filename, flag => 'r'
+        )
+    );
+}
+
+method at(Index $idx) { return $self->_record->read_idx($idx); }
+
+method len() { return scalar(@{ $self->_record->keys }) }
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Vision.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Vision.pm
new file mode 100644
index 000000000000..5711af350e5f
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Data/Vision.pm
@@ -0,0 +1,433 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::Data::Vision::DownloadedDataSet;
+use strict;
+use warnings;
+use File::Path qw(make_path);
+use Archive::Tar;
+use IO::Zlib;
+use IO::File;
+use Mouse;
+use AI::MXNet::Function::Parameters;
+has 'root'           => (is => 'ro', isa => 'Str', required => 1);
+has 'train'          => (is => 'ro', isa => 'Bool', required => 1);
+has 'transform'      => (is => 'ro', isa => 'Maybe[CodeRef]');
+has [qw(data label)] => (is => 'rw', init_arg => undef);
+extends 'AI::MXNet::Gluon::Data::Set';
+method python_constructor_arguments() { ['root', 'train', 'transform'] }
+
+sub BUILD
+{
+    my $self = shift;
+    my $root = $self->root;
+    $root =~ s/~/$ENV{HOME}/;
+    if(not -d $root)
+    {
+        make_path($root);
+    }
+    $self->_get_data;
+}
+
+method at(Index $idx)
+{
+    if(defined $self->transform)
+    {
+        return [$self->transform->($self->data->at($idx), $self->label->at($idx))];
+    }
+    return [$self->data->at($idx), $self->label->at($idx)];
+}
+
+method len() { $self->label->len }
+method _get_data() { confess("Not Implemented") }
+
+package AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::MNIST;
+use Mouse;
+use AI::MXNet::Gluon::Utils qw(download);
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::Data::Vision::DownloadedDataSet';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::MNIST
+=cut
+
+=head1 DESCRIPTION
+
+    MNIST handwritten digits dataset from `http://yann.lecun.com/exdb/mnist`_.
+
+    Each sample is an image (in 3D NDArray) with shape (28, 28, 1).
+
+    Parameters
+    ----------
+    root : str
+        Path to temp folder for storing data.
+        Defaults to ~/.mxnet/datasets/mnist
+    train : bool
+        Whether to load the training or testing set.
+        Defaults to True
+    transform : function
+        A user defined callback that transforms each instance. For example
+
+    transform => sub { my ($data, $label) = @_; return ($data->astype('float32')/255, $label) }
+=cut
+
+has [qw/_base_url _train_data _train_label _test_data _test_label/] => (is => 'rw');
+has '+root'  => (default => '~/.mxnet/datasets/mnist');
+has '+train' => (default => 1);
+has '_base_url'    => (is => 'ro', default => 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/');
+has '_train_data'  => (is => 'ro', default => sub { ['train-images-idx3-ubyte.gz',
+                                                     '6c95f4b05d2bf285e1bfb0e7960c31bd3b3f8a7d'] });
+has '_train_label' => (is => 'ro', default => sub { ['train-labels-idx1-ubyte.gz',
+                                                     '2a80914081dc54586dbdf242f9805a6b8d2a15fc'] });
+has '_test_data'   => (is => 'ro', default => sub { ['t10k-images-idx3-ubyte.gz',
+                                                     'c3a25af1f52dad7f726cce8cacb138654b760d48'] });
+has '_test_label'  => (is => 'ro', default => sub { ['t10k-labels-idx1-ubyte.gz',
+                                                     '763e7fa3757d93b0cdec073cef058b2004252c17'] });
+
+method _get_data()
+{
+    my ($data, $label);
+    if($self->train)
+    {
+        ($data, $label) = ($self->_train_data, $self->_train_label);
+    }
+    else
+    {
+        ($data, $label) = ($self->_test_data, $self->_test_label);
+    }
+    my $data_file = download($self->_base_url . $data->[0], path => $self->root,
+                             sha1_hash => $data->[1]);
+    my $label_file = download($self->_base_url . $label->[0], path => $self->root,
+                             sha1_hash => $label->[1]);
+    my $fh = new IO::Zlib;
+    my ($l, $d);
+    if ($fh->open($label_file, "rb"))
+    {
+        $fh->read($l, 100_000_000);
+        $l = substr($l, 8);
+        my $p = PDL->new_from_specification(PDL::Type->new(0), length($l));
+        ${$p->get_dataref} = $l;
+        $p->upd_data;
+        $l = $p;
+        $fh->close;
+        $l = AI::MXNet::NDArray->array($l, dtype => 'int32')->aspdl;
+    }
+    if ($fh->open($data_file, "rb"))
+    {
+        $fh->read($d, 100_000_000);
+        $d = substr($d, 16);
+        my $p = PDL->new_from_specification(PDL::Type->new(0), length($d));
+        ${$p->get_dataref} = $d;
+        $p->upd_data;
+        $d = $p;
+        $fh->close;
+        $d->reshape(1, 28, 28, $l->dim(-1));
+    }
+    $self->data(AI::MXNet::NDArray->array($d, dtype => 'uint8'));
+    $self->label($l);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data::Vision');
+
+package AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::FashionMNIST;
+use Mouse;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::FashionMNIST
+=cut
+
+=head1 DESCRIPTION
+
+    A dataset of Zalando's article images consisting of fashion products,
+    a drop-in replacement of the original MNIST dataset from
+    `https://github.com/zalandoresearch/fashion-mnist`_.
+
+    Each sample is an image (in 3D NDArray) with shape (28, 28, 1).
+
+    Parameters
+    ----------
+    root : str
+        Path to temp folder for storing data.
+        Defaults to ~/.mxnet/datasets/mnist
+    train : bool
+        Whether to load the training or testing set.
+        Defaults to True
+    transform : function
+        A user defined callback that transforms each instance. For example
+
+    transform => sub { my ($data, $label) = @_; return ($data->astype('float32')/255, $label) }
+=cut
+
+extends 'AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::MNIST';
+has '+root'         => (default => '~/.mxnet/datasets/fashion-mnist');
+has '+_base_url'    => (default => 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/');
+has '+_train_data'  => (default => sub { ['train-images-idx3-ubyte.gz',
+                                          '0cf37b0d40ed5169c6b3aba31069a9770ac9043d'] });
+has '+_train_label' => (default => sub { ['train-labels-idx1-ubyte.gz',
+                                          '236021d52f1e40852b06a4c3008d8de8aef1e40b'] });
+has '+_test_data'   => (default => sub { ['t10k-images-idx3-ubyte.gz',
+                                          '626ed6a7c06dd17c0eec72fa3be1740f146a2863'] });
+has '+_test_label'  => (default => sub { ['t10k-labels-idx1-ubyte.gz',
+                                          '17f9ab60e7257a1620f4ad76bbbaf857c3920701'] });
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data::Vision');
+
+package AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::CIFAR10;
+use Mouse;
+use AI::MXNet::Gluon::Utils qw(download);
+use AI::MXNet::Base;
+use Cwd;
+extends 'AI::MXNet::Gluon::Data::Vision::DownloadedDataSet';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Vision::DownloadedDataSet::CIFAR10
+=cut
+
+=head1 DESCRIPTION
+
+    CIFAR10 image classification dataset from `https://www.cs.toronto.edu/~kriz/cifar.html`_.
+
+    Each sample is an image (in 3D NDArray) with shape (32, 32, 1).
+
+    Parameters
+    ----------
+    root : str
+        Path to temp folder for storing data.
+    train : bool
+        Whether to load the training or testing set.
+    transform : function
+        A user defined callback that transforms each instance. For example:
+
+    transform => sub { my ($data, $label) = @_; return ($data->astype('float32')/255, $label) }
+=cut
+has '+root'  => (default => '~/.mxnet/datasets/cifar10');
+has '+train' => (default => 1);
+has '_file_hashes' => (is => 'ro', default => sub { +{
+    qw/data_batch_1.bin aadd24acce27caa71bf4b10992e9e7b2d74c2540
+       data_batch_2.bin c0ba65cce70568cd57b4e03e9ac8d2a5367c1795
+       data_batch_3.bin 1dd00a74ab1d17a6e7d73e185b69dbf31242f295
+       data_batch_4.bin aab85764eb3584312d3c7f65fd2fd016e36a258e
+       data_batch_5.bin 26e2849e66a845b7f1e4614ae70f4889ae604628
+       test_batch.bin   67eb016db431130d61cd03c7ad570b013799c88c/
+    } });
+
+method _read_batch(Str $filename)
+{
+    my $data = join('', IO::File->new($filename)->getlines);
+    $data = PDL->new_from_specification(PDL::Type->new(0), length($data))->reshape(3073, length($data)/3073);
+    $data = AI::MXNet::NDArray->array($data, dtype => 'uint8');
+    return (
+        $data->slice('X', [1, -1])->sever->reshape([-1, 3, 32, 32])->transpose([0, 2, 3, 1]),
+        $data->slice('X', 0)->astype('int32')
+    );
+}
+
+method _get_data()
+{
+    my @file_paths = map { [$_, join('/', $self->root, 'cifar-10-batches-bin/', $_)] } keys %{ $self->_file_hashes };
+    if(grep { not -f $_->[1] or not check_sha1($_->[1], $self->_file_hashes->{ $_->[0] }) } @file_paths)
+    {
+        my $filename = download(
+            'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/cifar10/cifar-10-binary.tar.gz',
+            path => $self->root,
+            sha1_hash => 'fab780a1e191a7eda0f345501ccd62d20f7ed891'
+        );
+        my $tar = Archive::Tar->new($filename);
+        my $cwd = cwd();
+        chdir($self->root);
+        $tar->extract;
+        chdir($cwd);
+    }
+    my ($data, $label);
+    if($self->train)
+    {
+        my (@data, @label);
+        for my $i (1..5)
+        {
+            my $filename = join('/', $self->root, "data_batch_$i.bin");
+            my ($data, $label) = $self->_read_batch($filename);
+            push @data, $data;
+            push @label, $label;
+        }
+        $data = AI::MXNet::NDArray->concatenate(\@data);
+        $label = AI::MXNet::NDArray->concatenate(\@label);
+    }
+    else
+    {
+        my $filename = join('/', $self->root, "test_batch.bin");
+        ($data, $label) = $self->_read_batch($filename);
+    }
+    $self->data(\@{$data});
+    $self->label($label->aspdl);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data::Vision');
+
+package AI::MXNet::Gluon::Data::Vision::RecordFileSet::ImageRecordDataset;
+use Mouse;
+extends 'AI::MXNet::Gluon::Data::RecordFileSet';
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Vision::RecordFileSet::ImageRecordDataset
+=cut
+
+=head1 DESCRIPTION
+
+    A dataset wrapping over a RecordIO file containing images.
+
+    Each sample is an image and its corresponding label.
+
+    Parameters
+    ----------
+    filename : str
+        Path to rec file.
+    flag : {0, 1}, default 1
+        If 0, always convert images to greyscale.
+
+        If 1, always convert images to colored (RGB).
+    transform : function
+        A user defined callback that transforms each instance. For example::
+=cut
+has 'flag'      => (is => 'rw', isa => 'Bool', default => 1);
+has 'transform' => (is => 'rw', isa => 'Maybe[CodeRef]');
+
+method at(Int $idx)
+{
+    my $record = $self->SUPER::at($idx);
+    my ($header, $img) = AI::MXNet::RecordIO->unpack($record);
+    if(defined $self->transform)
+    {
+        my $data = [AI::MXNet::Image->imdecode($img)];
+        return [$self->transform->(
+            AI::MXNet::Image->imdecode($img, flag => $self->flag), $header->label
+        )];
+    }
+    return [AI::MXNet::Image->imdecode($img, flag => $self->flag), $header->label];
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data::Vision');
+
+package AI::MXNet::Gluon::Data::Vision::Set::ImageFolderDataset;
+use Mouse;
+extends 'AI::MXNet::Gluon::Data::Set';
+=head1 NAME
+
+    AI::MXNet::Gluon::Data::Vision::ImageFolderDataset
+=cut
+
+=head1 DESCRIPTION
+
+    A dataset for loading image files stored in a folder structure like::
+
+        root/car/0001.jpg
+        root/car/xxxa.jpg
+        root/car/yyyb.jpg
+        root/bus/123.jpg
+        root/bus/023.jpg
+        root/bus/wwww.jpg
+
+    Parameters
+    ----------
+    root : str
+        Path to root directory.
+    flag : {0, 1}, default 1
+        If 0, always convert loaded images to greyscale (1 channel).
+        If 1, always convert loaded images to colored (3 channels).
+    transform : callable
+        A function that takes data and label and transforms them::
+
+            transform = lambda data, label: (data.astype(np.float32)/255, label)
+
+    Attributes
+    ----------
+    synsets : list
+        List of class names. `synsets[i]` is the name for the integer label `i`
+    items : list of tuples
+        List of all images in (filename, label) pairs.
+=cut
+has 'root'      => (is => 'rw', isa => 'Str');
+has 'flag'      => (is => 'rw', isa => 'Bool', default => 1);
+has 'transform' => (is => 'rw', isa => 'Maybe[CodeRef]');
+has [qw/exts
+    synsets
+    items/]     => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { ['root', 'flag', 'transform'] }
+
+sub BUILD
+{
+    my $self = shift;
+    my $root = $self->root;
+    $root =~ s/~/$ENV{HOME}/;
+    $self->root($root);
+    $self->exts({'.jpg', 1, '.jpeg', 1, '.png', 1});
+    $self->list_images($self->root);
+}
+
+method list_images(Str $root)
+{
+    $self->synsets([]);
+    $self->items([]);
+
+    for my $path (sort(glob("$root/*")))
+    {
+        my $folder = $path;
+        $folder =~ s,^.+/,,;
+        if(not -d $path)
+        {
+            AI::MXNet::Logging->warning("Ignoring %s, which is not a directory.", $folder);
+            next;
+        }
+        my $label = @{ $self->synsets };
+        push @{ $self->synsets }, $folder;
+        for my $filename (sort(glob("$path/*")))
+        {
+            my ($ext) = $filename =~ /(\.[^\.]+)$/;
+            if(not $ext or not exists $self->exts->{lc $ext})
+            {
+                AI::MXNet::Logging->warning(
+                    'Ignoring %s of type %s. Only support .jpg, .jpeg, .png',
+                    $filename, $ext//'undef'
+                );
+                next;
+            }
+            push @{ $self->items }, [$filename, AI::MXNet::NDArray->array([$label], dtype => 'int32')->aspdl];
+        }
+    }
+}
+
+method at(Int $idx)
+{
+    my $img = AI::MXNet::Image->imread($self->items->[$idx][0], flag => $self->flag);
+    my $label = $self->items->[$idx][1];
+    if(defined $self->transform)
+    {
+        return [$self->transform($img, $label)];
+    }
+    return [$img, $label];
+}
+
+method len()
+{
+    return scalar(@{ $self->items });
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Data::Vision');
+
+1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Loss.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Loss.pm
new file mode 100644
index 000000000000..74590c649b75
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Loss.pm
@@ -0,0 +1,517 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::Loss;
+use AI::MXNet::Gluon::Block;
+use AI::MXNet::Function::Parameters;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Loss - Base class for loss.
+=cut
+
+=head2 DESCRIPTION
+
+    Base class for loss.
+
+    Parameters
+    ----------
+    weight : float or None
+        Global scalar weight for loss.
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+
+=head2 _apply_weighting
+
+    Apply weighting to loss.
+
+    Parameters
+    ----------
+    loss : Symbol
+        The loss to be weighted.
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch separately, `sample_weight` should have
+        shape (64, 1).
+
+    Returns
+    -------
+    loss : Symbol
+        Weighted loss
+=cut
+
+
+method _apply_weighting(Str $F, GluonInput $loss, Maybe[Num] $weight=, Maybe[GluonInput] $sample_weight=)
+{
+    if(defined $sample_weight)
+    {
+        $loss = $F->broadcast_mul($loss, $sample_weight);
+    }
+    if(defined $weight)
+    {
+        $loss = $loss * $weight;
+    }
+    return $loss;
+}
+
+# for symbolic output.shape is not available so we reshape
+# to empty shape and let it be inferred from output's shape
+# via the '-' operator later.
+
+method _reshape_label_as_output(GluonClass $F, GluonInput $output, GluonInput $label)
+{
+    if($F eq 'AI::MXNet::NDArray')
+    {
+        return $label->reshape($output->shape);
+    }
+    else
+    {
+        return $label->reshape(shape => []);
+    }
+}
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+has 'weight'     => (is => 'rw', isa => 'Num');
+has 'batch_axis' => (is => 'rw', isa => 'Int', default => 0);
+
+use overload '""' => sub {
+        my $self = shift;
+        sprintf(
+            "%s(batch_axis=%s, w=%s)",
+            $self->_class_name,
+            $self->batch_axis,
+            $self->weight
+        );
+    };
+
+method hybrid_forward($F, $x, @args)
+{
+    confess('NotImplementedError');
+}
+
+package AI::MXNet::Gluon::L2Loss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::L2Loss
+=cut
+
+=head1 DESCRIPTION
+
+    Calculates the mean squared error between output and label:
+
+    Output and label can have arbitrary shape as long as they have the same
+    number of elements.
+
+    Parameters
+    ----------
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+has '+weight'     => (default => 1);
+has '+batch_axis' => (default => 0);
+
+method hybrid_forward(GluonClass $F, GluonInput $output, GluonInput $label, Maybe[GluonInput] $sample_weight=)
+{
+    $label = __PACKAGE__->_reshape_label_as_output($F, $output, $label);
+    my $loss = $F->square($output - $label);
+    $loss = __PACKAGE__->_apply_weighting($F, $loss, $self->weight/2, $sample_weight);
+    return $F->mean($loss, axis => $self->batch_axis, exclude => 1);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::L1Loss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+has '+weight'     => (default => 1);
+has '+batch_axis' => (default => 0);
+
+=head1 NAME
+
+    AI::MXNet::Gluon::L1Loss
+=cut
+
+=head1 DESCRIPTION
+
+    Calculates the mean absolute error between output and label:
+
+    .. math::
+        L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert.
+
+    Output and label must have the same shape.
+
+    Parameters
+    ----------
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+
+method hybrid_forward(GluonClass $F, GluonInput $output, GluonInput $label, Maybe[GluonInput] $sample_weight=)
+{
+    $label = __PACKAGE__->_reshape_label_as_output($F, $output, $label);
+    my $loss = $F->abs($output - $label);
+    $loss = __PACKAGE__->_apply_weighting($F, $loss, $self->weight, $sample_weight);
+    return $F->mean($loss, axis => $self->batch_axis, exclude => 1);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::SigmoidBinaryCrossEntropyLoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+has 'from_sigmoid' => (is => 'ro', isa => 'Bool', default => 0);
+has '+batch_axis'  => (default => 0);
+
+=head1 NAME
+
+    AI::MXNet::Gluon::SigmoidBinaryCrossEntropyLoss
+=cut
+
+=head1 DESCRIPTION
+
+    The cross-entropy loss for binary classification. (alias: SigmoidBCELoss)
+
+    BCE loss is useful when training logistic regression.
+
+    .. math::
+        loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
+
+
+    Parameters
+    ----------
+    from_sigmoid : bool, default is `False`
+        Whether the input is from the output of sigmoid. Set this to false will make
+        the loss calculate sigmoid and then BCE, which is more numerically stable through
+        log-sum-exp trick.
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+
+method hybrid_forward(GluonClass $F, GluonInput $output, GluonInput $label, Maybe[GluonInput] $sample_weight=)
+{
+    $label = __PACKAGE__->_reshape_label_as_output($F, $output, $label);
+    my $loss;
+    if(not $self->from_sigmoid)
+    {
+        my $max_val = (-$output)->maximum(0);
+        $loss = $output - $output*$label + $max_val + $F->log($F->exp(-$max_val)+$F->exp(-$output-$max_val));
+    }
+    else
+    {
+        $loss = -($F->log($output+1e-12)*$label + $F->log(1-$output+1e-8)*(1-$label));
+    }
+    $loss = __PACKAGE__->_apply_weighting($F, $loss, $self->weight, $sample_weight);
+    return $F->mean($loss, axis => $self->batch_axis, exclude => 1);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::SigmoidBCELoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::SigmoidBinaryCrossEntropyLoss';
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::SoftmaxCrossEntropyLoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+
+
+=head1 NAME
+
+    AI::MXNet::Gluon::SoftmaxCrossEntropyLoss
+=cut
+
+=head1 DESCRIPTION
+
+    Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)
+
+    If `sparse_label` is `True`, label should contain integer category indicators:
+
+    .. math::
+        p = {softmax}({output})
+
+        L = -\\sum_i {log}(p_{i,{label}_i})
+
+    Label's shape should be output's shape without the `axis` dimension. i.e. for
+    `output.shape` = (1,2,3,4) and axis = 2, `label.shape` should be (1,2,4).
+
+    If `sparse_label` is `False`, label should contain probability distribution
+    with the same shape as output:
+
+    .. math::
+        p = {softmax}({output})
+
+        L = -\\sum_i \\sum_j {label}_j {log}(p_{ij})
+
+    Parameters
+    ----------
+    axis : int, default -1
+        The axis to sum over when computing softmax and entropy.
+    sparse_label : bool, default True
+        Whether label is an integer array instead of probability distribution.
+    from_logits : bool, default False
+        Whether input is a log probability (usually from log_softmax) instead
+        of unnormalized numbers.
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+
+has 'axis'         => (is => 'ro', isa => 'Int', default => -1);
+has '+batch_axis'  => (default => 0);
+has 'sparse_label' => (is => 'ro', isa => 'Bool', default => 1);
+has 'from_logits'  => (is => 'ro', isa => 'Bool', default => 0);
+
+method hybrid_forward(GluonClass $F, GluonInput $output, GluonInput $label, Maybe[GluonInput] $sample_weight=)
+{
+    if(not $self->from_logits)
+    {
+        $output = $F->log_softmax($output);
+    }
+    my $loss;
+    if($self->sparse_label)
+    {
+        $loss = -$F->pick($output, $label, axis=>$self->axis, keepdims => 1);
+    }
+    else
+    {
+        $loss = -$F->sum($output*$label, axis => $self->axis, keepdims => 1);
+    }
+    $loss = __PACKAGE__->_apply_weighting($F, $loss, $self->weight, $sample_weight);
+    return $F->mean($loss, axis => $self->batch_axis, exclude => 1);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::SoftmaxCELoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::SoftmaxCrossEntropyLoss';
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+
+package AI::MXNet::Gluon::KLDivLoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+has '+batch_axis'  => (default => 0);
+has 'from_logits'  => (is => 'ro', isa => 'Bool', default => 1);
+
+=head1 NAME
+
+    AI::MXNet::Gluon::KLDivLoss
+=cut
+
+=head1 DESCRIPTION
+
+    The Kullback-Leibler divergence loss.
+
+    KL divergence is a useful distance measure for continuous distributions
+    and is often useful when performing direct regression over the space of
+    (discretely sampled) continuous output distributions.
+
+    .. _Kullback-Leibler divergence:
+        https://en.wikipedia.org/wiki/Kullback-Leibler_divergence
+    .. math::
+        L = 1/n \\sum_i (label_i * (log(label_i) - output_i))
+
+    Label's shape should be the same as output's.
+
+    Parameters
+    ----------
+    from_logits : bool, default is `True`
+        Whether the input is log probability (usually from log_softmax) instead
+        of unnormalized numbers.
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+    batch_axis : int, default 0
+        The axis that represents mini-batch.
+=cut
+
+method hybrid_forward(GluonClass $F, GluonInput $output, GluonInput $label, Maybe[GluonInput] $sample_weight=)
+{
+    if(not $self->from_logits)
+    {
+        $output = $F->log_softmax($output);
+    }
+    my $loss = $label * ($F->log($label+1e-12) - $output);
+    $loss = __PACKAGE__->_apply_weighting($F, $loss, $self->weight, $sample_weight);
+    return $F->mean($loss, axis => $self->batch_axis, exclude => 1);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+package AI::MXNet::Gluon::CTCLoss;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Loss';
+has 'layout'        => (is => 'rw', isa => 'Str', default => 'NTC');
+has 'label_layout'  => (is => 'rw', isa => 'Str', default => 'NT');
+
+=head1 NAME
+
+    AI::MXNet::Gluon::CTCLoss
+=cut
+
+=head1 DESCRIPTION
+
+    Connectionist Temporal Classification Loss.
+
+    See `"Connectionist Temporal Classification: Labelling Unsegmented
+    Sequence Data with Recurrent Neural Networks"
+    <http://www.cs.toronto.edu/~graves/icml_2006.pdf>`_ paper for more information.
+
+    Parameters
+    ----------
+    layout : str, default 'NTC'
+        Layout of the output sequence activation vector.
+    label_layout : str, default 'NT'
+        Layout of the labels.
+    weight : float or None
+        Global scalar weight for loss.
+    sample_weight : Symbol or None
+        Per sample weighting. Must be broadcastable to
+        the same shape as loss. For example, if loss has
+        shape (64, 10) and you want to weight each sample
+        in the batch, `sample_weight` should have shape (64, 1).
+        This should be used as the fifth argument when calling this loss.
+
+    Input shapes:
+        `data` is an activation tensor (i.e. before softmax).
+        Its shape depends on `layout`. For `layout='TNC'`, this
+        input has shape `(sequence_length, batch_size, alphabet_size)`
+        Note that the last dimension with index `alphabet_size-1` is reserved for special
+        blank character.
+
+        `label` is the label index matrix with zero-indexed labels.
+        Its shape depends on `label_layout`. For `label_layout='TN'`, this
+        input has shape `(label_sequence_length, batch_size)`. Padding mask of value ``-1``
+        is available for dealing with unaligned label lengths.
+        When `label_lengths` is specified, label lengths are directly used and padding mask
+        is not allowed in the label.
+        When `label_lengths` is not specified, the first occurrence of ``-1``
+        in each sample marks the end of the label sequence of that sample.
+
+        For example, suppose the vocabulary is `[a, b, c]`, and in one batch we have three
+        sequences 'ba', 'cbb', and 'abac'. We can index the labels as `{'a': 0, 'b': 1, 'c': 2}`.
+        The alphabet size should be 4, and we reserve the channel index 3 for blank label
+        in data tensor. The padding mask value for extra length is -1, so the resulting `label`
+        tensor should be padded to be::
+
+          [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]
+
+        `data_lengths` is optional and defaults to None.
+        When specified, it represents the actual lengths of data.
+        The shape should be (batch_size,).
+        If None, the data lengths are treated as being equal to the max sequence length.
+        This should be used as the third argument when calling this loss.
+
+        `label_lengths` is optional and defaults to None.
+        When specified, it represents the actual lengths of labels.
+        The shape should be (batch_size,).
+        If None, the label lengths are derived from the first occurrence of
+        the value specified by `padding_mask`.
+        This should be used as the fourth argument when calling this loss.
+
+    Output shape:
+        The CTC loss output has the shape (batch_size,).
+=cut
+use AI::MXNet::Base;
+
+sub BUILD
+{
+    my $self = shift;
+    assert(
+        (grep { $_ eq $self->layout } ('NTC', 'TNC')),\
+        "Only 'NTC' and 'TNC' layouts for output are supported. Got: ${\ $self->layout }"
+    );
+    assert(
+        (grep { $_ eq $self->label_layout } ('NT', 'TN')),\
+        "Only 'NT' and 'TN' layouts for label are supported. Got: ${\ $self->label_layout }"
+    );
+    $self->batch_axis(index($self->label_layout, 'N'));
+}
+
+method hybrid_forward(
+    GluonClass $F, GluonInput $data, GluonInput $label,
+    Maybe[GluonInput] $data_lengths=, Maybe[GluonInput] $label_lengths=, Maybe[GluonInput] $sample_weight=
+)
+{
+    if($self->layout eq 'NTC')
+    {
+        $data = $F->swapaxes($data, dim1 => 0, dim2 => 1);
+    }
+    if($self->batch_axis == 1)
+    {
+        $label = $F->swapaxes($label, dim1 => 0, dim2 => 1);
+    }
+    my $loss = $F->contrib->CTCLoss(
+        $data, $label,
+        (defined $data_lengths ? $data_lengths : ()),
+        (defined $label_lengths ? $label_lengths : ()),
+        use_data_lengths  => defined $data_lengths ? 1 : 0,
+        use_label_lengths => defined $label_lengths ? 1 : 0,
+        blank_label=>'last'
+    );
+    return $self->_apply_weighting($F, $loss, $self->weight, $sample_weight);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::Loss');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Mouse.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Mouse.pm
new file mode 100644
index 000000000000..2d1e9cf6f09a
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Mouse.pm
@@ -0,0 +1,63 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::Mouse;
+use strict;
+use warnings;
+use Mouse;
+use Mouse::Exporter;
+no Mouse;
+
+Mouse::Exporter->setup_import_methods(
+    as_is   => [
+        'has',
+        \&Mouse::extends,
+        \&Mouse::with,
+        \&Mouse::before,
+        \&Mouse::after,
+        \&Mouse::around,
+        \&Mouse::override,
+        \&Mouse::super,
+        \&Mouse::augment,
+        \&Mouse::inner,
+        \&Scalar::Util::blessed,
+        \&Carp::confess
+    ]
+);
+
+sub init_meta { return Mouse::init_meta(@_) }
+sub has
+{
+    my $name = shift;
+    my %args = @_;
+    my $caller = delete $args{caller} // caller;
+    my $meta = $caller->meta;
+
+    $meta->throw_error(q{Usage: has 'name' => ( key => value, ... )})
+        if @_ % 2; # odd number of arguments
+
+    for my $n (ref($name) ? @{$name} : $name){
+        $meta->add_attribute(
+            $n,
+            trigger => sub { my $self = shift; $self->__setattr__($n, @_); },
+            %args
+        );
+    }
+    return;
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN.pm
new file mode 100644
index 000000000000..16b0415aa0b0
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN.pm
@@ -0,0 +1,43 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::NN;
+use strict;
+use warnings;
+use AI::MXNet::Gluon::Block;
+use AI::MXNet::Gluon::NN::BasicLayers;
+use AI::MXNet::Gluon::NN::ConvLayers;
+
+sub import
+{
+    my ($class, $short_name) = @_;
+    if($short_name)
+    {
+        $short_name =~ s/[^\w:]//g;
+        if(length $short_name)
+        {
+            my $short_name_package =<<"EOP";
+            package $short_name;
+            \@${short_name}::ISA = ('AI::MXNet::Gluon::NN_');
+            1;
+EOP
+            eval $short_name_package;
+        }
+    }
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/BasicLayers.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/BasicLayers.pm
new file mode 100644
index 000000000000..6d85c9abf713
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/BasicLayers.pm
@@ -0,0 +1,668 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::NN::Sequential;
+use AI::MXNet::Function::Parameters;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Sequential
+=cut
+
+=head2 DESCRIPTION
+
+    Stacks `Block`s sequentially.
+
+    Example::
+
+        my $net = nn->Sequential()
+        # use net's name_scope to give child Blocks appropriate names.
+        net->name_scope(sub {
+            $net->add($nn->Dense(10, activation=>'relu'));
+            $net->add($nn->Dense(20));
+        });
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::Block';
+
+=head
+
+    Adds block on top of the stack.
+=cut
+
+method add(AI::MXNet::Gluon::Block $block)
+{
+    $self->register_child($block);
+}
+
+
+method forward($x)
+{
+    for my $block (@{ $self->_children })
+    {
+        $x = $block->($x);
+    }
+    return $x;
+}
+
+use overload
+    '""' => sub
+    {
+        my $self = shift;
+        my $s = "%s(\n{%s}\n)";
+        my @blocks;
+        my $k = 0;
+        for my $v (@{ $self->{_children} })
+        {
+            push @blocks, "  ($k): ".AI::MXNet::Base::_indent("$v", 2);
+            $k++;
+        }
+        sprintf("%s(\n{%s}\n)", $self->_class_name, join("\n", @blocks));
+    },
+    '@{}' => sub { shift->_children };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::HybridSequential;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::HybridSequential
+=cut
+
+=head2 DESCRIPTION
+
+    Stacks `Block`s sequentially.
+
+    Example::
+
+        my $net = nn->Sequential()
+        # use net's name_scope to give child Blocks appropriate names.
+        net->name_scope(sub {
+            $net->add($nn->Dense(10, activation=>'relu'));
+            $net->add($nn->Dense(20));
+        });
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head
+
+    Adds block on top of the stack.
+=cut
+
+method add(AI::MXNet::Gluon::HybridBlock $block)
+{
+    $self->register_child($block);
+}
+
+
+method forward($x)
+{
+    for my $block (@{ $self->_children })
+    {
+        $x = $block->($x);
+    }
+    return $x;
+}
+
+use overload
+    '""' => sub
+    {
+        my $self = shift;
+        my $s = "%s(\n{%s}\n)";
+        my @blocks;
+        my $k = 0;
+        for my $v (@{ $self->{_children} })
+        {
+            push @blocks, "  ($k): ".AI::MXNet::Base::_indent("$v", 2);
+            $k++;
+        }
+        sprintf("%s(\n{%s}\n)", $self->_class_name, join("\n", @blocks));
+    },
+    '@{}' => sub { shift->_children };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Dense;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+method python_constructor_arguments()
+{
+    ['units'];
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Dense
+
+=head1 DESCRIPTION
+
+    Just your regular densely-connected NN layer.
+
+    `Dense` implements the operation:
+    `output = activation(dot(input, weight) + bias)`
+    where `activation` is the element-wise activation function
+    passed as the `activation` argument, `weight` is a weights matrix
+    created by the layer, and `bias` is a bias vector created by the layer
+    (only applicable if `use_bias` is `True`).
+
+    Note: the input must be a tensor with rank 2. Use `flatten` to convert it
+    to rank 2 manually if necessary.
+
+    Parameters
+    ----------
+    units : int
+        Dimensionality of the output space.
+    activation : str
+        Activation function to use. See help on `Activation` layer.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    flatten : bool, default true
+        Whether the input tensor should be flattened.
+        If true, all but the first axis of input data are collapsed together.
+        If false, all but the last axis of input data are kept the same, and the transformation
+        applies on the last axis.
+    weight_initializer : str or `Initializer`
+        Initializer for the `kernel` weights matrix.
+    bias_initializer: str or `Initializer`
+        Initializer for the bias vector.
+    in_units : int, optional
+        Size of the input data. If not specified, initialization will be
+        deferred to the first time `forward` is called and `in_units`
+        will be inferred from the shape of input data.
+    prefix : str or None
+        See document of `Block`.
+    params : ParameterDict or None
+    weight_initializer : str or `Initializer`
+        Initializer for the `kernel` weights matrix.
+    bias_initializer: str or `Initializer`
+        Initializer for the bias vector.
+    in_units : int, optional
+        Size of the input data. If not specified, initialization will be
+        deferred to the first time `forward` is called and `in_units`
+        will be inferred from the shape of input data.
+    prefix : str or None
+        See document of `Block`.
+    params : ParameterDict or None
+        See document of `Block`.
+
+    If flatten is set to be True, then the shapes are:
+    Input shape:
+        An N-D input with shape
+        `(batch_size, x1, x2, ..., xn) with x1 * x2 * ... * xn equal to in_units`.
+
+    Output shape:
+        The output would have shape `(batch_size, units)`.
+
+    If ``flatten`` is set to be false, then the shapes are:
+    Input shape:
+        An N-D input with shape
+        `(x1, x2, ..., xn, in_units)`.
+
+    Output shape:
+        The output would have shape `(x1, x2, ..., xn, units)`.
+=cut
+
+has 'units'               => (is => 'rw', isa => 'Int', required => 1);
+has 'activation'          => (is => 'rw', isa => 'Str');
+has 'use_bias'            => (is => 'rw', isa => 'Bool', default => 1);
+has 'flatten'             => (is => 'rw', isa => 'Bool', default => 1);
+has 'weight_initializer'  => (is => 'rw', isa => 'Initializer');
+has 'bias_initializer'    => (is => 'rw', isa => 'Initializer', default => 'zeros');
+has 'in_units'            => (is => 'rw', isa => 'Int', default => 0);
+has [qw/weight bias act/] => (is => 'rw', init_arg => undef);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->name_scope(sub {
+        $self->weight(
+            $self->params->get(
+                'weight', shape => [$self->units, $self->in_units],
+                init => $self->weight_initializer,
+                allow_deferred_init => 1
+            )
+        );
+        if($self->use_bias)
+        {
+            $self->bias(
+                $self->params->get(
+                    'bias', shape => [$self->units],
+                    init => $self->bias_initializer,
+                    allow_deferred_init => 1
+                )
+            );
+        }
+        if(defined $self->activation)
+        {
+            $self->act(
+                AI::MXNet::Gluon::NN::Activation->new(
+                    activation => $self->activation,
+                    prefix => $self->activation.'_'
+                )
+            );
+        }
+    });
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $x, GluonInput :$weight, Maybe[GluonInput] :$bias=)
+{
+    my $act;
+    if(not defined $bias)
+    {
+        $act = $F->FullyConnected($x, $weight, no_bias => 1, num_hidden => $self->units, name => 'fwd');
+    }
+    else
+    {
+        $act = $F->FullyConnected($x, $weight, $bias, num_hidden => $self->units, flatten => $self->flatten, name => 'fwd')
+    }
+    if(defined $self->act)
+    {
+        $act = $self->act->($act);
+    }
+    return $act;
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    "${\ $self->_class_name }(${\ $self->units } -> ${\ $self->in_units },"
+    ." @{[ $self->act ? $self->act : 'linear' ]})"
+};
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Activation;
+
+=head1 
+
+    AI::MXNet::Gluon::NN::Activation
+=cut
+
+=head1 DESCRIPTION
+
+    Applies an activation function to input.
+
+    Parameters
+    ----------
+    activation : str
+        Name of activation function to use.
+        See mxnet.ndarray.Activation for available choices.
+
+    Input shape:
+        Arbitrary.
+
+    Output shape:
+        Same shape as input.
+=cut
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+has 'activation' => (is => 'ro', isa => 'Str', required => 1);
+
+method python_constructor_arguments()
+{
+    ['activation'];
+}
+
+method _alias()
+{
+    return $self->activation;
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $x)
+{
+    return $F->Activation($x, act_type => $self->activation, name=>'fwd');
+}
+
+use overload '""' => sub { my $self = shift; "${\ $self->_class_name }(${\ $self->activation })"; };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Dropout;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Dropout
+=cut
+
+=head1 DESCRIPTION
+
+    Applies Dropout to the input.
+
+    Dropout consists in randomly setting a fraction `rate` of input units
+    to 0 at each update during training time, which helps prevent overfitting.
+
+    Parameters
+    ----------
+    rate : float
+        Fraction of the input units to drop. Must be a number between 0 and 1.
+
+
+    Input shape:
+        Arbitrary.
+
+    Output shape:
+        Same shape as input.
+
+    References
+    ----------
+        `Dropout: A Simple Way to Prevent Neural Networks from Overfitting
+        <http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf>`_
+=cut
+has 'rate' => (is => 'ro', isa => 'Dropout', required => 1);
+method python_constructor_arguments() { ['rate'] }
+
+method hybrid_forward(GluonClass $F, GluonInput $x)
+{
+    return $F->Dropout($x, p => $self->rate, name => 'fwd');
+}
+
+use overload '""' => sub { my $self = shift; "${\ $self->_class_name }(p = ${\ $self->rate })"; };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::BatchNorm;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::BatchNorm
+=cut
+
+=head1 DESCRIPTION
+
+    Batch normalization layer (Ioffe and Szegedy, 2014).
+    Normalizes the input at each batch, i.e. applies a transformation
+    that maintains the mean activation close to 0 and the activation
+    standard deviation close to 1.
+
+    Parameters
+    ----------
+    axis : int, default 1
+        The axis that should be normalized. This is typically the channels
+        (C) axis. For instance, after a `Conv2D` layer with `layout='NCHW'`,
+        set `axis=1` in `BatchNorm`. If `layout='NHWC'`, then set `axis=3`.
+    momentum: float, default 0.9
+        Momentum for the moving average.
+    epsilon: float, default 1e-5
+        Small float added to variance to avoid dividing by zero.
+    center: bool, default True
+        If True, add offset of `beta` to normalized tensor.
+        If False, `beta` is ignored.
+    scale: bool, default True
+        If True, multiply by `gamma`. If False, `gamma` is not used.
+        When the next layer is linear (also e.g. `nn.relu`),
+        this can be disabled since the scaling
+        will be done by the next layer.
+    beta_initializer: str or `Initializer`, default 'zeros'
+        Initializer for the beta weight.
+    gamma_initializer: str or `Initializer`, default 'ones'
+        Initializer for the gamma weight.
+    moving_mean_initializer: str or `Initializer`, default 'zeros'
+        Initializer for the moving mean.
+    moving_variance_initializer: str or `Initializer`, default 'ones'
+        Initializer for the moving variance.
+    in_channels : int, default 0
+        Number of channels (feature maps) in input data. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+
+
+    Input shape:
+        Arbitrary.
+
+    Output shape:
+        Same shape as input.
+=cut
+
+has 'axis'             => (is => 'ro', isa => 'DimSize',     default => 1);
+has 'momentum'         => (is => 'ro', isa => 'Num',         default => 0.9);
+has 'epsilon'          => (is => 'ro', isa => 'Num',         default => 1e-5);
+has 'center'           => (is => 'ro', isa => 'Bool',        default => 1);
+has 'scale'            => (is => 'ro', isa => 'Bool',        default => 1);
+has 'beta_initializer' => (is => 'ro', isa => 'Initializer', default => 'zeros');
+has [qw/gamma_initializer
+        running_mean_initializer
+        running_variance_initializer
+    /]                 => (is => 'ro', isa => 'Initializer', default => 'ones');
+has 'in_channels'      => (is => 'ro', isa => 'DimSize',     default => 0);
+has [qw/_kwargs
+        gamma
+        beta
+        running_mean
+        running_var/]  => (is => 'rw', init_arg => undef);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_kwargs({
+        axis => $self->axis,
+        eps => $self->epsilon,
+        momentum => $self->momentum,
+        fix_gamma => $self->scale ? 0 : 1
+    });
+
+    $self->gamma(
+        $self->params->get(
+            'gamma', grad_req => $self->scale ? 'write' : 'null',
+            shape => [$self->in_channels], init => $self->gamma_initializer,
+            allow_deferred_init => 1, differentiable => $self->scale
+        )
+    );
+    $self->beta(
+        $self->params->get(
+            'beta', grad_req => $self->center ? 'write' : 'null',
+            shape => [$self->in_channels], init => $self->beta_initializer,
+            allow_deferred_init => 1, differentiable => $self->center
+        )
+    );
+    $self->running_mean(
+        $self->params->get(
+            'running_mean', grad_req => 'null',
+            shape => [$self->in_channels], init => $self->running_mean_initializer,
+            allow_deferred_init => 1, differentiable => 0
+        )
+    );
+    $self->running_var(
+        $self->params->get(
+            'running_var', grad_req => $self->center ? 'write' : 'null',
+            shape => [$self->in_channels], init => $self->running_variance_initializer,
+            allow_deferred_init => 1, differentiable => 0
+        )
+    );
+}
+
+method hybrid_forward(
+    GluonClass $F, GluonInput $x,
+    GluonInput :$gamma, GluonInput :$beta,
+    GluonInput :$running_mean, GluonInput :$running_var
+)
+{
+    return $F->BatchNorm(
+        $x, $gamma, $beta, $running_mean, $running_var,
+        name =>'fwd', %{ $self->_kwargs }
+    );
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    my $f = "%s(%s".($self->in_channels ? ", in_channels=".$self->in_channels : '').')';
+    my $content = join(", ", map { join('=', $_, $self->_kwargs->{$_}) } keys %{ $self->_kwargs });
+    return sprintf($f, $self->_class_name, $content);
+};
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::LeakyReLU;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::LeakyReLU
+=cut
+
+=head1 DESCRIPTION
+
+    Leaky version of a Rectified Linear Unit.
+
+    It allows a small gradient when the unit is not active
+
+        `f(x) = alpha * x for x < 0`,
+        `f(x) = x for x >= 0`.
+
+    Parameters
+    ----------
+    alpha : float
+        slope coefficient for the negative half axis. Must be >= 0.
+
+
+    Input shape:
+        Arbitrary.
+
+    Output shape:
+        Same shape as input.
+=cut
+has 'alpha' => (is => 'ro', isa => 'Num', required => 1);
+method python_constructor_arguments()
+{
+    ['alpha'];
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $x)
+{
+    return $F->LeakyReLU($x, act_type => 'leaky', slope => $self->alpha, name => 'fwd');
+}
+
+use overload '""' => sub { my $self = shift; "${\ $self->_class_name }(${\ $self->alpha })"; };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Embedding;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Embedding
+=cut
+
+=head1 DESCRIPTION
+
+    Turns non-negative integers (indexes/tokens) into dense vectors
+    of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]
+
+
+    Parameters
+    ----------
+    input_dim : int
+        Size of the vocabulary, i.e. maximum integer index + 1.
+    output_dim : int
+        Dimension of the dense embedding.
+    dtype : str or np.dtype, default 'float32'
+        Data type of output embeddings.
+    weight_initializer : Initializer
+        Initializer for the `embeddings` matrix.
+
+
+    Input shape:
+        2D tensor with shape: `(N, M)`.
+
+    Output shape:
+        3D tensor with shape: `(N, M, output_dim)`.
+=cut
+
+has [qw/input_dim
+    output_dim/]         => (is => 'ro', isa => 'DimSize', required => 1);
+has 'dtype'              => (is => 'ro', isa => 'Dtype', default => 'float32');
+has 'weight_initalizer'  => (is => 'ro', isa => 'Maybe[Initializer]');
+has [qw/_kwargs weight/] => (is => 'rw', init_arg => undef);
+method python_constructor_arguments()
+{
+    ['input_dim', 'output_dim'];
+}
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_kwargs({
+        input_dim => $self->input_dim,
+        output_dim =>  $self->output_dim,
+        dtype => $self->dtype
+    });
+    $self->weight(
+        $self->params->get(
+            'weight',
+            shape => [$self->input_dim, $self->output_dim],
+            init => $self->weight_initializer,
+            allow_deferred_init => 1
+        )
+    );
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $x, GluonInput :$weight)
+{
+    return $F->Embedding($x, $weight, name => 'fwd', %{ $self->_kwargs });
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    "${\ $self->_class_name }(${\ $self->input_dim } -> ${\ $self->output_dim }, ${\ $self->dtype })";
+};
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Flatten;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Flatten
+=cut
+
+=head1 DESCRIPTION
+
+    Flattens the input to two dimensional.
+
+    Input shape:
+        Arbitrary shape `(N, a, b, c, ...)`
+
+    Output shape:
+        2D tensor with shape: `(N, a*b*c...)`
+=cut
+
+method hybrid_forward(GluonClass $F, GluonInput $x)
+{
+    return $x->reshape([0, -1]);
+}
+
+use overload '""' => sub { shift->_class_name };
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/ConvLayers.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/ConvLayers.pm
new file mode 100644
index 000000000000..f56f8f5d333a
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/NN/ConvLayers.pm
@@ -0,0 +1,1363 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::NN::Conv;
+use AI::MXNet::Function::Parameters;
+use AI::MXNet::Symbol;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+func _infer_weight_shape($op_name, $data_shape, $kwargs)
+{
+    my $sym = AI::MXNet::Symbol->$op_name(
+        AI::MXNet::Symbol->var('data', shape => $data_shape), %{ $kwargs }
+    );
+    return ($sym->infer_shape_partial)[0];
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv
+=cut
+
+=head1 DESCRIPTION
+
+    Abstract nD convolution layer (private, used as implementation base).
+
+    This layer creates a convolution kernel that is convolved
+    with the layer input to produce a tensor of outputs.
+    If `use_bias` is `True`, a bias vector is created and added to the outputs.
+    Finally, if `activation` is not `None`,
+    it is applied to the outputs as well.
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space
+        i.e. the number of output channels in the convolution.
+    kernel_size : int or tuple/list of n ints
+        Specifies the dimensions of the convolution window.
+    strides: int or tuple/list of n ints,
+        Specifies the strides of the convolution.
+    padding : int or tuple/list of n ints,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation: int or tuple/list of n ints,
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two convolution
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str,
+        Dimension ordering of data and weight. Can be 'NCW', 'NWC', 'NCHW',
+        'NHWC', 'NCDHW', 'NDHWC', etc. 'N', 'C', 'H', 'W', 'D' stands for
+        batch, channel, height, width and depth dimensions respectively.
+        Convolution is performed over 'D', 'H', and 'W' dimensions.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias: bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer: str or `Initializer`
+        Initializer for the bias vector.
+=cut
+
+has 'channels'           => (is => 'rw', isa => 'Int', required => 1);
+has 'in_channels'        => (is => 'rw', isa => 'Int', default => 0);
+has 'kernel_size'        => (is => 'rw', isa => 'DimSize|Shape', required => 1);
+has [qw/strides
+        padding
+        dilation/]       => (is => 'rw', isa => 'DimSize|Shape');
+has 'groups'             => (is => 'rw', isa => 'Int');
+has [qw/layout
+    activation/]         => (is => 'rw', isa => 'Str');
+has 'op_name'            => (is => 'rw', isa => 'Str', default => 'Convolution');
+has 'use_bias'           => (is => 'rw', isa => 'Bool', default => 1);
+has 'weight_initializer' => (is => 'rw', isa => 'Maybe[Initializer]');
+has 'bias_initializer'   => (is => 'rw', isa => 'Maybe[Initializer]', default => 'zeros');
+has 'adj'                => (is => 'rw');
+has [qw/weight bias
+        kwargs act/]     => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { [qw/channels kernel_size strides padding dilation/] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_update_kernel_size;
+    $self->name_scope(sub {
+        if(not ref $self->strides)
+        {
+            $self->strides([($self->strides) x @{ $self->kernel_size }]);
+        }
+        if(not ref $self->padding)
+        {
+            $self->padding([($self->padding) x @{ $self->kernel_size }]);
+        }
+        if(not ref $self->dilation)
+        {
+            $self->dilation([($self->dilation) x @{ $self->kernel_size }]);
+        }
+        $self->kwargs({
+            kernel => $self->kernel_size, stride => $self->strides, dilate => $self->dilation,
+            pad => $self->padding, num_filter => $self->channels, num_group => $self->groups,
+            no_bias => $self->use_bias ? 0 : 1, layout => $self->layout
+        });
+        if(defined $self->adj)
+        {
+            $self->kwargs->{adj} = $self->adj;
+        }
+
+        my @dshape = (0)x(@{ $self->kernel_size } + 2);
+        $dshape[index($self->layout, 'N')] = 1;
+        $dshape[index($self->layout, 'C')] = $self->in_channels;
+        my $wshapes = _infer_weight_shape($self->op_name, \@dshape, $self->kwargs);
+        $self->weight(
+            $self->params->get(
+                'weight', shape => $wshapes->[1],
+                init => $self->weight_initializer,
+                allow_deferred_init => 1
+            )
+        );
+        if($self->use_bias)
+        {
+            $self->bias(
+                $self->params->get(
+                    'bias', shape => $wshapes->[2],
+                    init => $self->bias_initializer,
+                    allow_deferred_init => 1
+                )
+            );
+        }
+        if(defined $self->activation)
+        {
+            $self->act(
+                AI::MXNet::Gluon::NN::Activation->new(
+                    activation => $self->activation,
+                    prefix     => $self->activation.'_'
+                )
+            );
+        }
+    });
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $x, GluonInput :$weight, Maybe[GluonInput] :$bias=)
+{
+    my $op_name = $self->op_name;
+    my $act = $F->$op_name($x, $weight, defined $bias ? $bias : (), name => 'fwd', %{ $self->kwargs });
+    if(defined $self->act)
+    {
+        $act = $self->act->($act);
+    }
+    return $act;
+}
+
+method _alias() { 'conv' }
+
+use Data::Dumper;
+use overload '""' => sub {
+    my $self = shift;
+    my $s = '%s(%s, kernel_size=(%s), stride=(%s)';
+    my $len_kernel_size = @{ $self->kwargs->{kernel} };
+    if(Dumper($self->kwargs->{pad}) ne Dumper([(0)x$len_kernel_size]))
+    {
+        $s .= ', padding=(' . join(',', @{ $self->kwargs->{pad} }) . ')';
+    }
+    if(Dumper($self->kwargs->{dilate}) ne Dumper([(1)x$len_kernel_size]))
+    {
+        $s .= ', dilation=(' . join(',', @{ $self->kwargs->{dilate} }) . ')';
+    }
+    if($self->can('out_pad') and Dumper($self->out_pad) ne Dumper([(0)x$len_kernel_size]))
+    {
+        $s .= ', output_padding=(' . join(',', @{ $self->kwargs->{dilate} }) . ')';
+    }
+    if($self->kwargs->{num_group} != 1)
+    {
+        $s .= ', groups=' . $self->kwargs->{num_group};
+    }
+    if(not defined $self->bias)
+    {
+        $s .= ', bias=False';
+    }
+    $s .= ')';
+    return sprintf(
+        $s,
+        $self->_class_name,
+        $self->in_channels
+            ? sprintf("%d -> %d", $self->in_channels, $self->channels)
+            : sprintf("%d", $self->channels),
+        join(',', @{ $self->kwargs->{kernel} }),
+        join(',', @{ $self->kwargs->{stride} })
+    );
+};
+
+package AI::MXNet::Gluon::NN::Conv1D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv1D
+=cut
+
+=head1 DESCRIPTION
+
+    1D convolution layer (e.g. temporal convolution).
+
+    This layer creates a convolution kernel that is convolved
+    with the layer input over a single spatial (or temporal) dimension
+    to produce a tensor of outputs.
+    If `use_bias` is True, a bias vector is created and added to the outputs.
+    Finally, if `activation` is not `None`,
+    it is applied to the outputs as well.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 1 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 1 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 1 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 1 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout: str, default 'NCW'
+        Dimension ordering of data and weight. Can be 'NCW', 'NWC', etc.
+        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions
+        respectively. Convolution is applied on the 'W' dimension.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 3D array of shape
+        (batch_size, in_channels, width) if `layout` is `NCW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 3D array of shape
+        (batch_size, channels, out_width) if `layout` is `NCW`.
+        out_width is calculated as::
+
+            out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1
+=cut
+
+has '+strides'    => (default => 1);
+has '+padding'    => (default => 0);
+has '+dilation'   => (default => 1);
+has '+groups'     => (default => 1);
+has '+layout'     => (default => 'NCW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([$self->kernel_size]);
+    }
+    confess("kernel_size must be a number or an array ref of 1 ints")
+        unless @{ $self->kernel_size } == 1;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Conv2D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv2D
+=cut
+
+=head1 DESCRIPTION
+
+    2D convolution layer (e.g. spatial convolution over images).
+
+    This layer creates a convolution kernel that is convolved
+    with the layer input to produce a tensor of
+    outputs. If `use_bias` is True,
+    a bias vector is created and added to the outputs. Finally, if
+    `activation` is not `None`, it is applied to the outputs as well.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 2 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 2 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 2 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 2 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str, default 'NCHW'
+        Dimension ordering of data and weight. Can be 'NCHW', 'NHWC', etc.
+        'N', 'C', 'H', 'W' stands for batch, channel, height, and width
+        dimensions respectively. Convolution is applied on the 'H' and
+        'W' dimensions.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 4D array of shape
+        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 4D array of shape
+        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
+
+        out_height and out_width are calculated as::
+
+            out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
+            out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1
+=cut
+
+has '+strides'    => (default => sub { [1, 1] });
+has '+padding'    => (default => sub { [0, 0] });
+has '+dilation'   => (default => sub { [1, 1] });
+has '+groups'     => (default => 1);
+has '+layout'     => (default => 'NCHW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([($self->kernel_size)x2]);
+    }
+    confess("kernel_size must be a number or an array ref of 2 ints")
+        unless @{ $self->kernel_size } == 2;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Conv3D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv3D
+=cut
+
+=head1 DESCRIPTION
+
+    3D convolution layer (e.g. spatial convolution over volumes).
+
+    This layer creates a convolution kernel that is convolved
+    with the layer input to produce a tensor of
+    outputs. If `use_bias` is `True`,
+    a bias vector is created and added to the outputs. Finally, if
+    `activation` is not `None`, it is applied to the outputs as well.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 3 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 3 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 3 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 3 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str, default 'NCDHW'
+        Dimension ordering of data and weight. Can be 'NCDHW', 'NDHWC', etc.
+        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and
+        depth dimensions respectively. Convolution is applied on the 'D',
+        'H' and 'W' dimensions.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 5D array of shape
+        (batch_size, in_channels, depth, height, width) if `layout` is `NCDHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 5D array of shape
+        (batch_size, channels, out_depth, out_height, out_width) if `layout` is
+        `NCDHW`.
+
+        out_depth, out_height and out_width are calculated as::
+
+            out_depth = floor((depth+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
+            out_height = floor((height+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1
+            out_width = floor((width+2*padding[2]-dilation[2]*(kernel_size[2]-1)-1)/stride[2])+1
+=cut
+
+has '+strides'    => (default => sub { [1, 1, 1] });
+has '+padding'    => (default => sub { [0, 0, 0] });
+has '+dilation'   => (default => sub { [1, 1, 1] });
+has '+groups'     => (default => 1);
+has '+layout'     => (default => 'NCDHW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([($self->kernel_size)x3]);
+    }
+    confess("kernel_size must be a number or an array ref of 3 ints")
+        unless @{ $self->kernel_size } == 3;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Conv1DTranspose;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv1DTranspose
+=cut
+
+=head1 DESCRIPTION
+
+    Transposed 1D convolution layer (sometimes called Deconvolution).
+
+    The need for transposed convolutions generally arises
+    from the desire to use a transformation going in the opposite direction
+    of a normal convolution, i.e., from something that has the shape of the
+    output of some convolution to something that has the shape of its input
+    while maintaining a connectivity pattern that is compatible with
+    said convolution.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 3 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 3 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 3 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 3 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str, default 'NCW'
+        Dimension ordering of data and weight. Can be 'NCW', 'NWC', etc.
+        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions
+        respectively. Convolution is applied on the 'W' dimension.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 3D array of shape
+        (batch_size, in_channels, width) if `layout` is `NCW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 3D array of shape
+        (batch_size, channels, out_width) if `layout` is `NCW`.
+
+        out_width is calculated as::
+
+            out_width = (width-1)*strides-2*padding+kernel_size+output_padding
+=cut
+
+has 'output_padding' => (is => 'rw', isa => 'DimSize|Shape', default => 0);
+has '+adj'           => (default => sub { shift->output_padding }, lazy => 1);
+has '+op_name'       => (default => 'Deconvolution');
+has '+strides'       => (default => 1);
+has '+padding'       => (default => 0 );
+has '+dilation'      => (default => 1);
+has '+groups'        => (default => 1);
+has '+layout'        => (default => 'NCW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([$self->kernel_size]);
+    }
+    if(not ref $self->output_padding)
+    {
+        $self->output_padding([$self->output_padding]);
+    }
+    confess("kernel_size must be a number or an array ref of 1 ints")
+        unless @{ $self->kernel_size } == 1;
+    confess("output_padding must be a number or an array ref of 1 ints")
+        unless @{ $self->output_padding } == 1;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Conv2DTranspose;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv2DTranspose
+=cut
+
+=head1 DESCRIPTION
+
+    Transposed 2D convolution layer (sometimes called Deconvolution).
+
+    The need for transposed convolutions generally arises
+    from the desire to use a transformation going in the opposite direction
+    of a normal convolution, i.e., from something that has the shape of the
+    output of some convolution to something that has the shape of its input
+    while maintaining a connectivity pattern that is compatible with
+    said convolution.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 3 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 3 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 3 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 3 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str, default 'NCHW'
+        Dimension ordering of data and weight. Can be 'NCHW', 'NHWC', etc.
+        'N', 'C', 'H', 'W' stands for batch, channel, height, and width
+        dimensions respectively. Convolution is applied on the 'H' and
+        'W' dimensions.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 4D array of shape
+        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 4D array of shape
+        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
+
+        out_height and out_width are calculated as::
+
+            out_height = (height-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]
+            out_width = (width-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]
+=cut
+
+has 'output_padding'      => (is => 'rw', isa => 'DimSize|Shape', default => 0);
+has '+adj'        => (default => sub { shift->output_padding }, lazy => 1);
+has '+op_name'    => (default => 'Deconvolution');
+has '+strides'    => (default => sub { [1, 1] });
+has '+padding'    => (default => sub { [0, 0] });
+has '+dilation'   => (default => sub { [1, 1] });
+has '+groups'     => (default => 1);
+has '+layout'     => (default => 'NCHW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([($self->kernel_size)x2]);
+    }
+    if(not ref $self->output_padding)
+    {
+        $self->output_padding([($self->output_padding)x2]);
+    }
+    confess("kernel_size must be a number or an array ref of 2 ints")
+        unless @{ $self->kernel_size } == 2;
+    confess("output_padding must be a number or an array ref of 2 ints")
+        unless @{ $self->output_padding } == 2;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::Conv3DTranspose;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Conv';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::Conv3DTranspose
+=cut
+
+=head1 DESCRIPTION
+
+    Transposed 3D convolution layer (sometimes called Deconvolution).
+
+    The need for transposed convolutions generally arises
+    from the desire to use a transformation going in the opposite direction
+    of a normal convolution, i.e., from something that has the shape of the
+    output of some convolution to something that has the shape of its input
+    while maintaining a connectivity pattern that is compatible with
+    said convolution.
+
+    If `in_channels` is not specified, `Parameter` initialization will be
+    deferred to the first time `forward` is called and `in_channels` will be
+    inferred from the shape of input data.
+
+
+    Parameters
+    ----------
+    channels : int
+        The dimensionality of the output space, i.e. the number of output
+        channels (filters) in the convolution.
+    kernel_size :int or tuple/list of 3 int
+        Specifies the dimensions of the convolution window.
+    strides : int or tuple/list of 3 int,
+        Specify the strides of the convolution.
+    padding : int or a tuple/list of 3 int,
+        If padding is non-zero, then the input is implicitly zero-padded
+        on both sides for padding number of points
+    dilation : int or tuple/list of 3 int
+        Specifies the dilation rate to use for dilated convolution.
+    groups : int
+        Controls the connections between inputs and outputs.
+        At groups=1, all inputs are convolved to all outputs.
+        At groups=2, the operation becomes equivalent to having two conv
+        layers side by side, each seeing half the input channels, and producing
+        half the output channels, and both subsequently concatenated.
+    layout : str, default 'NCDHW'
+        Dimension ordering of data and weight. Can be 'NCDHW', 'NDHWC', etc.
+        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and
+        depth dimensions respectively. Convolution is applied on the 'D',
+        'H', and 'W' dimensions.
+    in_channels : int, default 0
+        The number of input channels to this layer. If not specified,
+        initialization will be deferred to the first time `forward` is called
+        and `in_channels` will be inferred from the shape of input data.
+    activation : str
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
+        If you don't specify anything, no activation is applied
+        (ie. "linear" activation: `a(x) = x`).
+    use_bias : bool
+        Whether the layer uses a bias vector.
+    weight_initializer : str or `Initializer`
+        Initializer for the `weight` weights matrix.
+    bias_initializer : str or `Initializer`
+        Initializer for the bias vector.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 5D array of shape
+        (batch_size, in_channels, depth, height, width) if `layout` is `NCDHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 5D array of shape
+        (batch_size, channels, out_depth, out_height, out_width) if `layout` is `NCDHW`.
+        out_depth, out_height and out_width are calculated as::
+
+            out_depth = (depth-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]
+            out_height = (height-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]
+            out_width = (width-1)*strides[2]-2*padding[2]+kernel_size[2]+output_padding[2]
+=cut
+
+has 'output_padding'      => (is => 'rw', isa => 'DimSize|Shape', default => 0);
+has '+adj'        => (default => sub { shift->output_padding }, lazy => 1);
+has '+op_name'    => (default => 'Deconvolution');
+has '+strides'    => (default => sub { [1, 1, 1] });
+has '+padding'    => (default => sub { [0, 0, 0] });
+has '+dilation'   => (default => sub { [1, 1, 1] });
+has '+groups'     => (default => 1);
+has '+layout'     => (default => 'NCDHW');
+
+method _update_kernel_size()
+{
+    if(not ref $self->kernel_size)
+    {
+        $self->kernel_size([($self->kernel_size)x3]);
+    }
+    if(not ref $self->output_padding)
+    {
+        $self->output_padding([($self->output_padding)x3]);
+    }
+    confess("kernel_size must be a number or an array ref of 3 ints")
+        unless @{ $self->kernel_size } == 3;
+    confess("output_padding must be a number or an array ref of 3 ints")
+        unless @{ $self->output_padding } == 3;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+# Abstract class for different pooling layers.
+package AI::MXNet::Gluon::NN::Pooling;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+
+has 'pool_size'   => (is => 'rw', isa => 'DimSize|Shape', required => 1);
+has 'strides'     => (is => 'rw', isa => 'Maybe[DimSize|Shape]');
+has 'padding'     => (is => 'rw', isa => 'DimSize|Shape');
+has 'ceil_mode'   => (is => 'rw', isa => 'Bool', default => 0);
+has 'global_pool' => (is => 'rw', isa => 'Bool', default => 0);
+has 'kwargs'      => (is => 'rw', init_arg => undef);
+has 'pool_type'   => (is => 'rw', isa => 'PoolType');
+has 'layout'      => (is => 'rw');
+method python_constructor_arguments() { [qw/pool_size strides padding/] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_update_pool_size;
+    if(not defined $self->strides)
+    {
+        $self->strides($self->pool_size);
+    }
+    if(not ref $self->strides)
+    {
+        $self->strides([($self->strides)x@{ $self->pool_size }]);
+    }
+    if(not ref $self->padding)
+    {
+        $self->padding([($self->padding)x@{ $self->pool_size }]);
+    }
+    $self->kwargs({
+        kernel => $self->pool_size, stride => $self->strides, pad => $self->padding,
+        global_pool => $self->global_pool, pool_type => $self->pool_type,
+        pooling_convention => $self->ceil_mode ? 'full' : 'valid'
+    });
+}
+
+method _alias() { 'pool' }
+
+method hybrid_forward(GluonClass $F, GluonInput $x)
+{
+    return $F->Pooling($x, name=>'fwd', %{ $self->kwargs });
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    sprintf(
+        '%s(size=(%s), stride=(%s), padding=(%s), ceil_mode=%d)',
+        $self->_class_name,
+        join(',', @{ $self->kwargs->{kernel} }),
+        join(',', @{ $self->kwargs->{stride} }),
+        join(',', @{ $self->kwargs->{pad} }),
+        $self->kwargs->{pooling_convention} eq 'full' ? 1 : 0
+    )
+};
+
+package AI::MXNet::Gluon::NN::MaxPool1D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Pooling';
+method python_constructor_arguments() { [qw/pool_size strides padding layout ceil_mode/] }
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::MaxPool1D
+=cut
+
+=head1 DESCRIPTION
+
+    Max pooling operation for one dimensional data.
+
+
+    Parameters
+    ----------
+    pool_size: int
+        Size of the max pooling windows.
+    strides: int, or None
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCW'
+        Dimension ordering of data and weight. Can be 'NCW', 'NWC', etc.
+        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions
+        respectively. Pooling is applied on the W dimension.
+    ceil_mode : bool, default False
+        When `True`, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 3D array of shape
+        (batch_size, channels, width) if `layout` is `NCW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 3D array of shape
+        (batch_size, channels, out_width) if `layout` is `NCW`.
+
+        out_width is calculated as::
+
+            out_width = floor((width+2*padding-pool_size)/strides)+1
+
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
+        equation.
+=cut
+
+
+has '+pool_size' => (default => 2);
+has '+padding'   => (default => 0);
+has '+layout'    => (default => 'NCW');
+has '+pool_type' => (default => 'max');
+
+method _update_pool_size()
+{
+    confess("Only supports NCW layout for now")
+        unless $self->layout eq 'NCW';
+    if(not ref $self->pool_size)
+    {
+        $self->pool_size([$self->pool_size]);
+    }
+    confess("pool_size must be a number or an array ref of 1 ints")
+        unless @{ $self->pool_size } == 1;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::MaxPool2D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Pooling';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::MaxPool2D
+=cut
+
+=head1 DESCRIPTION
+
+    Max pooling operation for two dimensional (spatial) data.
+
+
+    Parameters
+    ----------
+    pool_size: int or list/tuple of 2 ints,
+        Size of the max pooling windows.
+    strides: int, list/tuple of 2 ints, or None.
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int or list/tuple of 2 ints,
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCHW'
+        Dimension ordering of data and weight. Can be 'NCHW', 'NHWC', etc.
+        'N', 'C', 'H', 'W' stands for batch, channel, height, and width
+        dimensions respectively. padding is applied on 'H' and 'W' dimension.
+    ceil_mode : bool, default False
+        When `True`, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 4D array of shape
+        (batch_size, channels, height, width) if `layout` is `NCHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 4D array of shape
+        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
+
+        out_height and out_width are calculated as::
+
+            out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
+            out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1
+
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
+        equation.
+=cut
+
+has '+pool_size' => (default => sub { [2, 2] });
+has '+padding'   => (default => 0);
+has '+layout'    => (default => 'NCHW');
+has '+pool_type' => (default => 'max');
+
+method _update_pool_size()
+{
+    confess("Only supports NCHW layout for now")
+        unless $self->layout eq 'NCHW';
+    if(not ref $self->pool_size)
+    {
+        $self->pool_size([($self->pool_size)x2]);
+    }
+    confess("pool_size must be a number or an array ref of 2 ints")
+        unless @{ $self->pool_size } == 2;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::MaxPool3D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::Pooling';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::MaxPool3D
+=cut
+
+=head1 DESCRIPTION
+
+    Max pooling operation for 3D data (spatial or spatio-temporal).
+
+
+    Parameters
+    ----------
+    pool_size: int or list/tuple of 3 ints,
+        Size of the max pooling windows.
+    strides: int, list/tuple of 3 ints, or None.
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int or list/tuple of 3 ints,
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCDHW'
+        Dimension ordering of data and weight. Can be 'NCDHW', 'NDHWC', etc.
+        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and
+        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'
+        dimension.
+    ceil_mode : bool, default False
+        When `True`, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 5D array of shape
+        (batch_size, channels, depth, height, width) if `layout` is `NCDHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 5D array of shape
+        (batch_size, channels, out_depth, out_height, out_width) if `layout`
+        is `NCDHW`.
+
+        out_depth, out_height and out_width are calculated as ::
+
+            out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
+            out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1
+            out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1
+
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
+        equation.
+=cut
+
+has '+pool_size' => (default => sub { [2, 2, 2] });
+has '+padding'   => (default => 0);
+has '+layout'    => (default => 'NCDHW');
+has '+pool_type' => (default => 'max');
+
+method _update_pool_size()
+{
+    confess("Only supports NCDHW layout for now")
+        unless $self->layout eq 'NCDHW';
+    if(not ref $self->pool_size)
+    {
+        $self->pool_size([($self->pool_size)x3]);
+    }
+    confess("pool_size must be a number or an array ref of 3 ints")
+        unless @{ $self->pool_size } == 3;
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::AvgPool1D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool1D';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::AvgPool1D
+=cut
+
+=head1 DESCRIPTION
+
+    Average pooling operation for temporal data.
+
+    Parameters
+    ----------
+    pool_size: int
+        Size of the max pooling windows.
+    strides: int, or None
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCW'
+        Dimension ordering of data and weight. Can be 'NCW', 'NWC', etc.
+        'N', 'C', 'W' stands for batch, channel, and width (time) dimensions
+        respectively. padding is applied on 'W' dimension.
+    ceil_mode : bool, default False
+        When `True`, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 3D array of shape
+        (batch_size, channels, width) if `layout` is `NCW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 3D array of shape
+        (batch_size, channels, out_width) if `layout` is `NCW`.
+
+        out_width is calculated as::
+
+            out_width = floor((width+2*padding-pool_size)/strides)+1
+
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
+        equation.
+=cut
+
+has '+pool_type' => (default => 'avg');
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::AvgPool2D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool2D';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::AvgPool2D
+=cut
+
+=head1 DESCRIPTION
+
+    Average pooling operation for spatial data.
+
+    Parameters
+    ----------
+    pool_size: int or list/tuple of 2 ints,
+        Size of the max pooling windows.
+    strides: int, list/tuple of 2 ints, or None.
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int or list/tuple of 2 ints,
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCHW'
+        Dimension ordering of data and weight. Can be 'NCHW', 'NHWC', etc.
+        'N', 'C', 'H', 'W' stands for batch, channel, height, and width
+        dimensions respectively. padding is applied on 'H' and 'W' dimension.
+    ceil_mode : bool, default False
+        When True, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 4D array of shape
+        (batch_size, channels, height, width) if `layout` is `NCHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 4D array of shape
+        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
+
+        out_height and out_width are calculated as::
+
+            out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
+            out_width = floor((width+2*padding[1]-pool_size[1])/strides[1])+1
+
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
+        equation.
+=cut
+
+has '+pool_type' => (default => 'avg');
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::AvgPool3D;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool3D';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::AvgPool3D
+=cut
+
+=head1 DESCRIPTION
+
+    Average pooling operation for 3D data (spatial or spatio-temporal).
+
+    Parameters
+    ----------
+    pool_size: int or list/tuple of 3 ints,
+        Size of the max pooling windows.
+    strides: int, list/tuple of 3 ints, or None.
+        Factor by which to downscale. E.g. 2 will halve the input size.
+        If `None`, it will default to `pool_size`.
+    padding: int or list/tuple of 3 ints,
+        If padding is non-zero, then the input is implicitly
+        zero-padded on both sides for padding number of points.
+    layout : str, default 'NCDHW'
+        Dimension ordering of data and weight. Can be 'NCDHW', 'NDHWC', etc.
+        'N', 'C', 'H', 'W', 'D' stands for batch, channel, height, width and
+        depth dimensions respectively. padding is applied on 'D', 'H' and 'W'
+        dimension.
+    ceil_mode : bool, default False
+        When True, will use ceil instead of floor to compute the output shape.
+
+
+    Input shape:
+        This depends on the `layout` parameter. Input is 5D array of shape
+        (batch_size, channels, depth, height, width) if `layout` is `NCDHW`.
+
+    Output shape:
+        This depends on the `layout` parameter. Output is 5D array of shape
+        (batch_size, channels, out_depth, out_height, out_width) if `layout`
+        is `NCDHW`.
+
+        out_depth, out_height and out_width are calculated as ::
+
+            out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
+            out_height = floor((height+2*padding[1]-pool_size[1])/strides[1])+1
+            out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1
+
+        When `ceil_mode` is `True,` ceil will be used instead of floor in this
+        equation.
+=cut
+
+has '+pool_type' => (default => 'avg');
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalMaxPool1D;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalMaxPool1D
+=cut
+
+=head1 DESCRIPTION
+
+    Global max pooling operation for temporal data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool1D';
+has '+pool_size'   => (default => sub { [1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalMaxPool2D;
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalMaxPool2D
+=cut
+
+=head1 DESCRIPTION
+
+    Global max pooling operation for spatial data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool2D';
+
+has '+pool_size'   => (default => sub { [1, 1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalMaxPool3D;
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalMaxPool3D
+=cut
+
+=head1 DESCRIPTION
+
+    Global max pooling operation for 3D data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::MaxPool3D';
+has '+pool_size'   => (default => sub { [1, 1, 1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalAvgPool1D;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalAvgPool1D
+=cut
+
+=head1 DESCRIPTION
+
+    Global average pooling operation for temporal data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::AvgPool1D';
+has '+pool_size'   => (default => sub { [1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalAvgPool2D;
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalAvgPool2D
+=cut
+
+=head1 DESCRIPTION
+
+    Global average pooling operation for spatial data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::AvgPool2D';
+
+has '+pool_size'   => (default => sub { [1, 1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+package AI::MXNet::Gluon::NN::GlobalAvgPool3D;
+=head1 NAME
+
+    AI::MXNet::Gluon::NN::GlobalAvgPool2D
+=cut
+
+=head1 DESCRIPTION
+
+    Global average pooling operation for 3D data.
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::NN::AvgPool3D';
+has '+pool_size'   => (default => sub { [1, 1, 1] });
+has '+global_pool' => (default => 1);
+has '+ceil_mode'   => (default => 1);
+
+__PACKAGE__->register('AI::MXNet::Gluon::NN');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Parameter.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Parameter.pm
new file mode 100644
index 000000000000..0341fd7e6636
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Parameter.pm
@@ -0,0 +1,926 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use Hash::Ordered;
+package AI::MXNet::Gluon::Parameter;
+use AI::MXNet::Function::Parameters;
+
+=head1 NAME 
+
+    AI::MXNet::Gluon::Parameter - A Container holding parameters (weights) of AI::MXNEt::Gluon::Block(s).
+=cut
+
+=head1 DESCRIPTION
+
+    AI::MXNet::Gluon::Parameter holds a copy of the parameter on each AI::MXNet::Context after
+    it is initialized with AI::MXNet::Gluon::Parameter->initialize(...)`. If grad_req is
+    not 'null', it will also hold a gradient array on each AI::MXNet::Context
+
+        $ctx = mx->gpu(0);
+        $x = mx->nd->zeros([16, 100], ctx=>$ctx);
+        $w = mx->gluon->Parameter('fc_weight', shape=>[64, 100], init=>mx->init->Xavier());
+        $b = mx->gluon->Parameter('fc_bias', shape=>[64], init=>mx->init->Zero());
+        $w->initialize(ctx=>$ctx);
+        $b->initialize(ctx=>ctx);
+        $out = mx->nd->FullyConnected($x, $w->data($ctx), $b->data($ctx), num_hidden=>64);
+
+    Parameters
+    ----------
+    name : str
+        Name of this parameter.
+    grad_req : {'write', 'add', 'null'}, default 'write'
+        Specifies how to update gradient to grad arrays.
+
+        - 'write' means everytime gradient is written to grad NDArray.
+        - 'add' means everytime gradient is added to the grad NDArray. You need
+          to manually call zero_grad() to clear the gradient buffer before each
+          iteration when using this option.
+        - 'null' means gradient is not requested for this parameter. gradient arrays
+          will not be allocated.
+    shape : array ref of int, default None
+        Shape of this parameter. By default shape is not specified. Parameter with
+        unknown shape can be used for `Symbol` API, but `init` will throw an error
+        when using `NDArray` API.
+    dtype : Dtype, default 'float32'
+        Data type of this parameter. For example, 'float64'.
+    lr_mult : float, default 1.0
+        Learning rate multiplier. Learning rate will be multiplied by lr_mult
+        when updating this parameter with optimizer.
+    wd_mult : float, default 1.0
+        Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.
+    init : Initializer, default None
+        Initializer of this parameter. Will use the global initializer by default.
+
+    Attributes
+    ----------
+    grad_req : {'write', 'add', 'null'}
+        This can be set before or after initialization. Setting grad_req to null
+        with $x->grad_req = 'null' saves memory and computation when you don't
+        need gradient w.r.t x.
+=cut
+
+use Mouse;
+use AI::MXNet::Base;
+use overload '""' => sub {
+        my $self = shift;
+        "Parameter " . $self->name.
+        " (shape=(" . join(',', @{ $self->shape//[] }) .")".
+        ", dtype=" . $self->dtype.")"
+    },
+    fallback => 1;
+
+around BUILDARGS => sub {
+    my $orig  = shift;
+    my $class = shift;
+    if(@_ % 2)
+    {
+        my $name = shift;
+        return $class->$orig(name => $name, @_);
+    }
+    else
+    {
+        return $class->$orig(@_);
+    }
+};
+
+sub BUILD
+{
+    my $self = shift;
+    $self->grad_req($self->_grad_req);
+    $self->_deferred_init([]);
+}
+
+has 'name'                => (is => 'ro', isa => 'Str', required => 1);
+has '_grad_req'           => (is => 'rw', isa => 'GradReq', init_arg => 'grad_req', default => 'write');
+has 'shape'               => (is => 'rw', isa => 'Shape');
+has 'dtype'               => (is => 'rw', isa => 'Dtype', default => 'float32');
+has [qw/lr_mult wd_mult/] => (is => 'rw', isa => 'Num', default => 1);
+has 'init'                => (is => 'rw', isa => 'Maybe[Initializer]');
+has 'allow_deferred_init' => (is => 'rw', isa => 'Bool', default => 0);
+has 'differentiable'      => (is => 'rw', isa => 'Bool', default => 1);
+has [qw/_var _data _grad
+    _deferred_init
+    _ctx_list _ctx_map/]  => (is => 'rw', init_arg => undef);
+
+method grad_req(Maybe[GradReq] $req=)
+{
+    return $self->_grad_req unless defined $req;
+    if(not $self->differentiable)
+    {
+        $req = 'null';
+    }
+    return if $self->_grad_req eq $req;
+    $self->_grad_req($req);
+    if($req eq 'null' and defined $self->_grad)
+    {
+        $self->_grad(undef);
+        $self->_data([map { $_->detach } @{ $self->_data }]);
+    }
+    elsif(defined $self->_data)
+    {
+        $self->_init_grad();
+    }
+}
+
+method _check_and_get($arr_list, $ctx)
+{
+    if(defined $arr_list)
+    {
+        if(ref $ctx eq 'ARRAY')
+        {
+            return $arr_list;
+        }
+        if(not defined $ctx)
+        {
+            if(@{ $arr_list } == 1)
+            {
+                return $arr_list->[0];
+            }
+            else
+            {
+                $ctx = AI::MXNet::Context->current_ctx;
+            }
+        }
+        my $idx;
+        if(ref $self->_ctx_map->[$ctx->device_type_id])
+        {
+            $idx = $self->_ctx_map->[$ctx->device_type_id][$ctx->device_id];
+        }
+        if(defined $idx)
+        {
+            return $arr_list->[$idx];
+        }
+        confess(
+            "Parameter ${\ $self->name } was not initialized on context $ctx. ".
+            "It was only initialized on @{ $self->_ctx_list }."
+        );
+    }
+    if(@{ $self->_deferred_init })
+    {
+        confess("DeferredInitializationError: ".
+            "Parameter ${\ $self->name } has not been initialized yet because initialization was ".
+            "deferred. Actual initialization happens during the first forward pass. ".
+            "Please pass one batch of data through the network before accessing Parameters. ".
+            "You can also avoid deferred initialization by specifying in_units, ".
+            "num_features, etc., for network layers.");
+    }
+    confess(
+        "Parameter ${\ $self->name } has not been initialized. Note that ".
+        "you should initialize parameters and create Trainer ".
+        "with Block.collect_params() instead of Block.params ".
+        "because the later does not include Parameters of ".
+        "nested child Blocks"
+    );
+}
+
+# (Re)initializes by loading from data.
+method _load_init($data, $ctx)
+{
+    if($self->shape)
+    {
+        zip(sub {
+            my ($i, $j) = @_;
+            assert(
+                ($i == 0 or $i == $j),
+                sprintf(
+                    "Failed loading Parameter %s from saved params: ".
+                    "shape incompatible expacted (%s) vs saved (%s)",
+                    $self->name, "@{$self->shape}", "@{$data->shape}"
+                )
+            );
+        }, $self->shape, $data->shape);
+    }
+    if($self->dtype)
+    {
+        assert(
+            ($self->dtype eq $data->dtype),
+            sprintf(
+                "Failed loading Parameter %s from saved params: ".
+                "dtype incompatible expacted %s vs saved %s",
+                $self->name, $self->dtype, $data->dtype
+            )
+        );
+    }
+    if(blessed ($ctx) and $ctx->isa('AI::MXNet::Context'))
+    {
+        $ctx = [$ctx];
+    }
+    if(not defined $self->_data)
+    {
+        if(@{ $self->_deferred_init })
+        {
+            assert(
+                ($ctx eq $self->_deferred_init->[1]),
+                sprintf(
+                    "Failed to load Parameter %s on %s because it was ".
+                    "previous initialized on %s.",
+                    $self->name, $ctx, $self->list_ctx
+                )
+            );
+        }
+        $self->_init_impl($data, $ctx);
+    }
+    else
+    {
+        assert(
+            (join('', @{ $ctx }) eq join('', @{ $self->list_ctx })),
+            sprintf(
+                "Failed to load Parameter %s on %s because it was ".
+                "previous initialized on %s.",
+                $self->name, "@$ctx", "@{$self->list_ctx}"
+            )
+        );
+        $self->set_data($data);
+    }
+    $self->_deferred_init([]);
+}
+
+# Finishes deferred initialization.
+method _finish_deferred_init()
+{
+    return unless @{ $self->_deferred_init };
+    my ($init, $ctx, $default_init) = @{ $self->_deferred_init };
+    $self->_deferred_init([]);
+    assert(
+        (defined($self->shape) and product(@{ $self->shape }) > 0),
+        sprintf(
+            "Cannot initialize Parameter %s because it has ".
+            "invalid shape: %s. Please specify in_units, ".
+            "in_channels, etc for `Block`s.",
+            $self->name, $self->shape
+        )
+    );
+    AI::MXNet::AutoGrad->pause(sub {
+        my $data = AI::MXNet::NDArray->zeros(
+            $self->shape, dtype => $self->dtype, ctx => AI::MXNet::Context->cpu
+        );
+        AI::MXNet::Initializer->new->(
+            AI::MXNet::InitDesc->new(
+                name => $self->name,
+                attrs => { __init__ => defined $init ? "$init" : "$default_init" }
+            ),
+            $data
+        );
+        $self->_init_impl($data, $ctx);
+    });
+}
+
+# Sets data and grad.
+method _init_impl($data, $ctx_list)
+{
+    $self->_ctx_list([@{ $ctx_list }]);
+    $self->_ctx_map([]);
+    enumerate(sub {
+        my ($i, $ctx) = @_;
+        while(@{ $self->_ctx_map } <= $ctx->device_type_id)
+        {
+            push @{ $self->_ctx_map }, [];
+        }
+        my $dev_list = $self->_ctx_map->[$ctx->device_type_id];
+        while(@{ $dev_list } <= $ctx->device_id)
+        {
+            push @{ $dev_list }, undef;
+        }
+        $dev_list->[$ctx->device_id] = $i;
+    }, $self->_ctx_list);
+    $self->_data([map { $data->copyto($_) } @{ $self->_ctx_list }]);
+    $self->_init_grad;
+}
+
+# Initialize grad buffers.
+method _init_grad()
+{
+    if($self->grad_req eq 'null')
+    {
+        $self->_grad(undef);
+        return;
+    }
+    $self->_grad([map { AI::MXNet::NDArray->zeros_like($_) } @{ $self->_data }]);
+    AI::MXNet::AutoGrad->mark_variables($self->list_data, $self->list_grad, grad_reqs => $self->grad_req);
+}
+
+# Reduce data from multiple context.
+
+method _reduce()
+{
+    my $block = $self->list_data;
+    my $data = AI::MXNet::NDArray->add_n(map { $_->copyto(AI::MXNet::Context->cpu) } @{ $block }) / @{ $block };
+    return $data;
+}
+
+
+=head2 initialize
+
+        Initializes parameter and gradient arrays. Only used for `NDArray` API.
+
+        Parameters
+        ----------
+        :$init : Initializer
+            The initializer to use. Overrides AI::MXNet::Gluon::Parameter->init and default_init.
+        :$ctx : AI::MXNet::Context or array ref of AI::MXNet::Context, defaults to AI::MXNet::Context->current_ctx().
+            Initialize Parameter on given context. If ctx is a list of Context, a
+            copy will be made for each context.
+            Copies are independent arrays. User is responsible for keeping
+            their values consistent when updating. Normally gluon->Trainer does this for you.
+        :$default_init : Initializer
+            Default initializer is used when both 'init' and AI::MXNet::Gluon::Parameter->init are undefined.
+        :$force_reinit : bool, default False
+            Whether to force re-initialization if parameter is already initialized.
+
+        Examples
+        --------
+        >>> $weight = mx->gluon->Parameter('weight', shape=>[2, 2]);
+        >>> $weight->initialize(ctx=>mx->cpu(0));
+        >>> print $weight->data
+        [[-0.01068833  0.01729892]
+         [ 0.02042518 -0.01618656]]
+        <NDArray 2x2 @cpu(0)>
+        >>> print $weight->grad()
+        [[ 0.  0.]
+         [ 0.  0.]]
+        <NDArray 2x2 @cpu(0)>
+        >>> $weight->initialize(ctx=>[mx->gpu(0), mx->gpu(1)]);
+        >>> print $weight->data(mx->gpu(0));
+        [[-0.00873779 -0.02834515]
+         [ 0.05484822 -0.06206018]]
+        <NDArray 2x2 @gpu(0)>
+        >>> print $weight->data(mx->gpu(1))
+        [[-0.00873779 -0.02834515]
+         [ 0.05484822 -0.06206018]]
+        <NDArray 2x2 @gpu(1)>
+=cut
+
+method initialize(
+    Maybe[Initializer]                                     :$init=,
+    Maybe[AI::MXNet::Context|ArrayRef[AI::MXNet::Context]] :$ctx=AI::MXNet::Context->current_ctx,
+    Initializer                                            :$default_init=AI::MXNet::Initializer->Uniform,
+    Bool                                                   :$force_reinit=0
+)
+{
+    $ctx //=AI::MXNet::Context->current_ctx;
+    if(defined $self->_data and not $force_reinit)
+    {
+        AI::MXNet::Logging->warning(
+            "Parameter %s is already initialized, ignoring. ".
+            "Set force_reinit=True to re-initialize.", $self->name
+        );
+        return;
+    }
+    $self->_data(undef);
+    $self->_grad(undef);
+    if(blessed($ctx) and $ctx->isa('AI::MXNet::Context'))
+    {
+        $ctx = [$ctx];
+    }
+    if(not defined $init)
+    {
+        if(defined $self->init)
+        {
+            $init = $self->init;
+        }
+        else
+        {
+            $init = $default_init;
+        }
+    }
+    if(not defined $self->shape or not @{ $self->shape } or product(@{ $self->shape }) <= 0)
+    {
+        if($self->allow_deferred_init)
+        {
+            $self->_deferred_init([$init, $ctx, $default_init]);
+            return;
+        }
+        confess("Cannot initialize Parameter ${\ $self->name } because it has ".
+                "invalid shape: @{$self->shape//[]}.");
+    }
+    $self->_deferred_init([$init, $ctx, $default_init]);
+    $self->_finish_deferred_init;
+}
+
+=head2 reset_ctx
+
+        Re-assign Parameter to other contexts.
+
+        :$ctx : AI::MXNet::Context or array ref of AI::MXNet::Context, default AI::MXNet::Context->current_ctx.
+        Assign Parameter to given context. If ctx is a list of Context, a
+        copy will be made for each context.
+=cut
+
+method reset_ctx(Maybe[AI::MXNet::Context|ArrayRef[AI::MXNet::Context]] :$ctx=AI::MXNet::Context->current_ctx)
+{
+    if(blessed($ctx) and $ctx->isa('AI::MXNet::Context'))
+    {
+        $ctx = [$ctx];
+    }
+    if(defined $self->_data)
+    {
+        my $data = $self->_reduce;
+        AI::MXNet::AutoGrad->pause(sub {
+            $self->_init_impl($data, $ctx);
+        });
+    }
+    elsif(@{ $self->_deferred_init })
+    {
+        my ($init, undef, $default_init) = @{ $self->_deferred_init };
+        $self->_deferred_init([$init, $ctx, $default_init]);
+    }
+    else
+    {
+        confess("Cannot reset context for Parameter ${ \ $self->name } because it ".
+                "has not been initialized.");
+    }
+}
+
+=head2 set_data
+
+    Sets this parameter's value on all contexts to data.
+=cut
+
+method set_data($data)
+{
+    assert(
+        (defined $self->_data),
+        "Parameter ${\ $self->name } has not been initialized"
+    );
+    for my $arr (@{ $self->list_data })
+    {
+        $arr .= $data;
+    }
+}
+
+=head2 data
+
+        Returns a copy of this parameter on one context. Must have been
+        initialized on this context before.
+
+        Parameters
+        ----------
+        ctx : Context
+            Desired context.
+
+        Returns
+        -------
+        NDArray on ctx
+=cut
+
+method data(Maybe[AI::MXNet::Context] $ctx=)
+{
+    return $self->_check_and_get($self->_data, $ctx);
+}
+
+=head2 list_data
+
+        Returns copies of this parameter on all contexts, in the same order
+        as creation.
+=cut
+
+method list_data()
+{
+    return $self->_check_and_get($self->_data, [])
+}
+
+=head2 grad
+
+        Returns a gradient buffer for this parameter on one context.
+
+        Parameters
+        ----------
+        ctx : Context
+            Desired context.
+=cut
+
+method grad(Maybe [AI::MXNet::Context] $ctx=)
+{
+    if(defined $self->_data and not defined $self->_grad)
+    {
+        confess(
+            "Cannot get gradient array for Parameter ${\ $self->name } ".
+            "because grad_req='null'"
+        );
+    }
+    return $self->_check_and_get($self->_grad, $ctx);
+}
+
+=head2 list_grad
+
+        Returns gradient buffers on all contexts, in the same order
+        as 'values'.
+=cut
+
+method list_grad()
+{
+    if(defined $self->_data and not defined $self->_grad)
+    {
+        confess(
+            "Cannot get gradient array for Parameter ${\ $self->name } ".
+            "because grad_req='null'"
+        );
+    }
+    return $self->_check_and_get($self->_grad, []);
+}
+
+=head2 list_ctx
+
+        Returns a list of contexts this parameter is initialized on.
+=cut
+
+method list_ctx()
+{
+    if(not defined $self->_data)
+    {
+        if(@{ $self->_deferred_init })
+        {
+            return $self->_deferred_init->[1];
+        }
+        confess("Parameter ${\ $self->name } has not been initialized");
+    }
+    return $self->_ctx_list;
+}
+
+=head2 zero_grad
+
+        Sets gradient buffer on all contexts to 0. No action is taken if
+        parameter is uninitialized or doesn't require gradient.
+=cut
+
+method zero_grad()
+{
+    return unless defined $self->_grad;
+    map { $_ .= 0 } @{ $self->_grad };
+}
+
+=head2 var
+
+        Returns a symbol representing this parameter.
+=cut
+
+method var()
+{
+    if(not defined $self->_var)
+    {
+        $self->_var(
+            AI::MXNet::Symbol->var(
+                $self->name, shape => $self->shape, dtype => $self->dtype,
+                lr_mult => $self->lr_mult, wd_mult => $self->wd_mult,
+                init => $self->init
+            )
+        );
+    }
+    return $self->_var;
+}
+
+
+package AI::MXNet::Gluon::ParameterDict;
+use AI::MXNet::Base;
+=head1 NAME
+
+    AI::MXNet::Gluon::ParameterDict - A dictionary managing a set of parameters.
+=cut
+
+=head1 DESCRIPTION
+
+    Parameters
+    ----------
+    prefix : str, default ''
+        The prefix to be prepended to all Parameters' names created by this dict.
+    shared : ParameterDict or undef
+        If not undef, when this dict's `get` method creates a new parameter, will
+        first try to retrieve it from `shared` dict. Usually used for sharing
+        parameters with another `Block`.
+=cut
+
+use Mouse;
+has _prefix => (is => 'ro', isa => 'Str', init_arg => 'prefix', default => '');
+has _shared => (is => 'rw', isa => 'Maybe[AI::MXNet::Gluon::ParameterDict]', init_arg => 'shared');
+has _params => (is => 'rw', init_arg => undef);
+
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { [qw/prefix shared/] }
+
+sub BUILD
+{
+    my $self = shift;
+    $self->_params(Hash::Ordered->new);
+}
+
+use overload
+    '""'   => sub {
+        my $self = shift;
+        my $name = $self->_prefix ? $self->_prefix." " : '';
+        my $content = join("\n", map { AI::MXNet::Base::_indent("   $_", 2) } $self->values);
+        return "$name(\n$content\n)";
+    },
+    '%{}'  => sub { my %tmp = shift->_params->as_list; \%tmp },
+    '@{}'  => sub { my @tmp = shift->_params->as_list; \@tmp },
+    fallback => 1;
+
+method items()
+{
+    return @{$self};
+}
+
+method keys()
+{
+    return $self->_params->keys;
+}
+
+method values()
+{
+    return $self->_params->values;
+}
+
+method prefix()
+{
+    $self->_prefix;
+}
+
+
+method _get_impl($name)
+{
+    if($self->_params->exists($name))
+    {
+        return $self->_params->get($name);
+    }
+    if(defined $self->_shared and $self->_shared->_params->exists($name))
+    {
+        $self->_params->set($name => $self->_shared->_params->get($name));
+        return $self->_params->get($name);
+    }
+    return undef;
+}
+
+=head get
+
+        Retrieves a 'AI::MXNet::Gluon::Parameter' with name '$self->prefix.$name'. If not found,
+        'get' will first try to retrieve it from 'shared' dict. If still not
+        found, 'get' will create a new 'AI::MXNet::Gluon::Parameter' with key-word arguments and
+        insert it to self.
+
+        Parameters
+        ----------
+        name : str
+            Name of the desired Parameter. It will be prepended with this dictionary's
+            prefix.
+        %kwargs : hash
+            The rest of key-word arguments for the created `Parameter`.
+
+        Returns
+        -------
+        Parameter
+            The created or retrieved `Parameter`.
+=cut
+
+use Data::Dumper;
+method get(Str $name, %kwargs)
+{
+    $name = $self->prefix . $name;
+    my $param = $self->_get_impl($name);
+    if(not defined $param)
+    {
+        $param = AI::MXNet::Gluon::Parameter->new($name, %kwargs);
+        $self->_params->set($name => $param);
+    }
+    else
+    {
+        while(my ($k, $v) = each %kwargs)
+        {
+            if($param->can($k))
+            {
+                assert(
+                    (not defined $v or Dumper($v) eq Dumper($param->$k)),
+                    "Cannot retrieve Parameter $name because desired attribute ".
+                    "does not match with stored for attribute $k: ".
+                    "desired ".Dumper($v)." vs stored ". Dumper($param->$k)
+                );
+            }
+            else
+            {
+                confess("unknown param $k, $v");
+            }
+        }
+    }
+    return $param;
+}
+
+=head2 update
+
+    Copies all Parameters in $other to self.
+=cut
+
+method update($other)
+{
+    my @keys = $other->keys;
+    for my $k (@keys)
+    {
+        if($self->_params->exists($k))
+        {
+            assert(
+                ($self->_params->get($k) eq $other->_params->get($k)),
+                "Cannot update self with other because they have different ".
+                "Parameters with the same name $k"
+            );
+        }
+        else
+        {
+            $self->_params->set($k => $other->_params->get($k));
+        }
+    }
+}
+
+=head2 initialize
+
+        Initializes all Parameters managed by this dictionary to be used for 'NDArray'
+        API. It has no effect when using 'Symbol' API.
+
+        Parameters
+        ----------
+        :$init : Initializer
+            Global default Initializer to be used when AI::MXNet::Gluon::Parameter->init is undef.
+            Otherwise, AI::MXNet::Gluon::Parameter->init takes precedence.
+        :$ctx : AI::MXNet::Context or array ref of AI::MXNet::Context objects
+            Keeps a copy of Parameters on one or many context(s).
+        :$force_reinit : bool, default False
+            Whether to force re-initialization if parameter is already initialized.
+=cut
+
+
+method initialize(
+    Initializer                                            :$init=AI::MXNet::Initializer->Uniform(),
+    Maybe[AI::MXNet::Context|ArrayRef[AI::MXNet::Context]] :$ctx=,
+    Bool                                                   :$verbose=0,
+    Bool                                                   :$force_reinit=0
+)
+{
+    if($verbose)
+    {
+        $init->set_verbosity(verbose=>$verbose);
+    }
+    $_->initialize(ctx => $ctx, default_init => $init, force_reinit => $force_reinit) for $self->values;
+}
+
+=head2 zero_grad
+
+    Sets all Parameters' gradient buffer to 0.
+=cut
+
+method zero_grad()
+{
+    $_->zero_grad for $self->values;
+}
+
+=head2 reset_ctx
+
+    Re-assign all Parameters to other contexts.
+
+    $ctx : AI::MXNet::Context or array ref of AI::MXNet::Context objects, defaults to AI::MXNet::Context->current_ctx().
+            Assign Parameter to given context. If $ctx is an array ref of AI::MXNet::Context objects, a
+            copy will be made for each context.
+=cut
+
+method reset_ctx(AI::MXNet::Context|ArrayRef[AI::MXNet::Conetxt] $ctx=AI::MXNet::Context->current_ctx)
+{
+    $_->reset_ctx($ctx) for $self->values;
+}
+
+=head2 setattr
+
+        Set an attribute to a new value for all Parameters.
+
+        For example, set grad_req to null if you don't need gradient w.r.t a
+        model's Parameters::
+
+            $model->collect_params()->setattr(grad_req => 'null');
+
+        or change the learning rate multiplier::
+
+            $model->collect_params()->setattr(lr_mult => 0.5);
+
+        Parameters
+        ----------
+        $name : str
+            Name of the attribute.
+        $value : valid type for attribute name
+            The new value for the attribute.
+=cut
+
+method setattr($name, $value)
+{
+    $_->$name($value) for $self->values;
+}
+
+
+=head2 save
+
+    Save parameters to file.
+
+    $filename : str
+        Path to parameter file.
+    $strip_prefix : str, default ''
+    Strip prefix from parameter names before saving.
+=cut
+
+method save(Str $filename, Str $strip_prefix='')
+{
+    my %arg_dict = ();
+    for my $param ($self->values())
+    {
+        my $weight = $param->_reduce();
+        if(not $param->name =~ /^$strip_prefix/)
+        {
+            confess(
+                "Prefix $strip_prefix is to be striped before saving, but Parameter ".
+                "${\ $param->name } does not start with $strip_prefix. If you are using Block.save_params, ".
+                "This may be due to your Block shares parameters from other ".
+                "Blocks or you forgot to use `with name_scope()`` during init. ".
+                "Consider switching to Block.collect_params.save and ".
+                "Block.collect_params.load instead."
+            );
+        }
+        $arg_dict{ substr($param->name, length $strip_prefix) } = $weight;
+    }
+    AI::MXNet::NDArray->save($filename, \%arg_dict);
+}
+
+=head2
+
+        Load parameters from file.
+
+        $filename : str
+            Path to parameter file.
+        :$ctx : AI::MXNet::Context or array ref of AI::MXNet::Context objects
+            Context(s) initialize loaded parameters on.
+        :$allow_missing : bool, default False
+            Whether to silently skip loading parameters not represents in the file.
+        :$ignore_extra : bool, default False
+            Whether to silently ignore parameters from the file that are not
+            present in this ParameterDict.
+        :$restore_prefix : str, default ''
+            prepend prefix to names of stored parameters before loading.
+=cut
+method load(
+    Str                                              $filename,
+    AI::MXNet::Context|ArrayRef[AI::MXNet::Context] :$ctx=AI::MXNet::Context->current_ctx,
+    Bool                                            :$allow_missing=0,
+    Bool                                            :$ignore_extra=0,
+    Str                                             :$restore_prefix=''
+)
+{
+    if($restore_prefix)
+    {
+        for my $name ($self->keys())
+        {
+            assert(
+                ($name =~ /^$restore_prefix/),
+                "restore_prefix is $restore_prefix but Parameters name $name does not start ".
+                "with $restore_prefix"
+            );
+        }
+    }
+    my $lprefix  = length $restore_prefix;
+    my %orig_load = %{ AI::MXNet::NDArray->load($filename) };
+    my %arg_dict  = map { ($restore_prefix.$_, $orig_load{$_}) } keys %orig_load;
+    if(not $allow_missing)
+    {
+        for my $name ($self->keys())
+        {
+            assert(
+                (exists $arg_dict{ $name }),
+                sprintf("Parameter %s is missing in file %s", substr($name, $lprefix), $filename)
+            );
+        }
+    }
+    for my $name (keys %arg_dict)
+    {
+        if(not $self->_params->exists($name))
+        {
+            assert(
+                $ignore_extra,
+                sprintf(
+                    "Parameter %s loaded from file %s is not present in ParameterDict",
+                    substr($name, $lprefix),
+                    $filename
+                )
+            );
+            next;
+        }
+        @{$self}{$name}->_load_init($arg_dict{$name}, $ctx);
+    }
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN.pm
new file mode 100644
index 000000000000..6a5122713091
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN.pm
@@ -0,0 +1,42 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::RNN;
+use strict;
+use warnings;
+use AI::MXNet::Gluon::RNN::Layer;
+use AI::MXNet::Gluon::RNN::Cell;
+
+sub import
+{
+    my ($class, $short_name) = @_;
+    if($short_name)
+    {
+        $short_name =~ s/[^\w:]//g;
+        if(length $short_name)
+        {
+            my $short_name_package =<<"EOP";
+            package $short_name;
+            \@${short_name}::ISA = ('AI::MXNet::Gluon::RNN_');;
+            1;
+EOP
+            eval $short_name_package;
+        }
+    }
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Cell.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Cell.pm
new file mode 100644
index 000000000000..d2e7db280aaa
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Cell.pm
@@ -0,0 +1,1225 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::RNN::RecurrentCell;
+use Mouse::Role;
+use AI::MXNet::Base;
+use AI::MXNet::Function::Parameters;
+
+method _cells_state_info($cells, $batch_size)
+{
+    return [map { @{ $_->state_info($batch_size) } } @{ $cells }];
+}
+
+method _cells_begin_state($cells, %kwargs)
+{
+    return [map { @{ $_->begin_state(%kwargs) } } @{ $cells }];
+}
+
+method _get_begin_state(GluonClass $F, $begin_state, GluonInput $inputs, $batch_size)
+{
+    if(not defined $begin_state)
+    {
+        if($F =~ /AI::MXNet::NDArray/)
+        {
+            my $ctx = blessed $inputs ? $inputs->context : $inputs->[0]->context;
+            {
+                local($AI::MXNet::current_ctx) = $ctx;
+                my $func = sub {
+                    my %kwargs = @_;
+                    my $shape = delete $kwargs{shape};
+                    return AI::MXNet::NDArray->zeros($shape, %kwargs);
+                };
+                $begin_state = $self->begin_state(batch_size => $batch_size, func => $func);
+            }
+        }
+        else
+        {
+            $begin_state = $self->begin_state(batch_size => $batch_size, func => sub { return $F->zeros(@_) });
+        }
+    }
+    return $begin_state;
+}
+
+method _format_sequence($length, $inputs, $layout, $merge, $in_layout=)
+{
+    assert(
+        (defined $inputs),
+        "unroll(inputs=None) has been deprecated. ".
+        "Please create input variables outside unroll."
+    );
+
+    my $axis = index($layout, 'T');
+    my $batch_axis = index($layout, 'N');
+    my $batch_size = 0;
+    my $in_axis = defined $in_layout ? index($in_layout, 'T') : $axis;
+    my $F;
+    if(blessed $inputs and $inputs->isa('AI::MXNet::Symbol'))
+    {
+        $F = 'AI::MXNet::Symbol';
+        if(not $merge)
+        {
+            assert(
+                (@{ $inputs->list_outputs() } == 1),
+                "unroll doesn't allow grouped symbol as input. Please convert ".
+                "to list with list(inputs) first or let unroll handle splitting"
+            );
+            $inputs = [
+                AI::MXNet::Symbol->split(
+                    $inputs, axis => $in_axis, num_outputs => $length, squeeze_axis => 1
+                )
+            ];
+        }
+    }
+    elsif(blessed $inputs and $inputs->isa('AI::MXNet::NDArray'))
+    {
+        $F = 'AI::MXNet::NDArray';
+        $batch_size = $inputs->shape->[$batch_axis];
+        if(not $merge)
+        {
+            assert(not defined $length or $length == $inputs->shape->[$in_axis]);
+            $inputs = as_array(
+                AI::MXNet::NDArray->split(
+                    $inputs, axis=>$in_axis,
+                    num_outputs => $inputs->shape->[$in_axis],
+                    squeeze_axis => 1
+                )
+            );
+        }
+    }
+    else
+    {
+        assert(not defined $length or @{ $inputs } == $length);
+        if($inputs->[0]->isa('AI::MXNet::Symbol'))
+        {
+            $F = 'AI::MXNet::Symbol';
+        }
+        else
+        {
+            $F = 'AI::MXNet::NDArray';
+            $batch_size = $inputs->[0]->shape->[$batch_axis];
+        }
+        if($merge)
+        {
+            $inputs  = [map { $F->expand_dims($_, axis => $axis) } @{ $inputs }];
+            $inputs  = $F->concat(@{ $inputs }, dim => $axis);
+            $in_axis = $axis;
+        }
+    }
+    if(blessed $inputs and $axis != $in_axis)
+    {
+        $inputs = $F->swapaxes($inputs, dim1=>$axis, dim2=>$in_axis);
+    }
+    return ($inputs, $axis, $F, $batch_size);
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::RecurrentCell
+=cut
+
+=head1 DESCRIPTION
+
+    Abstract role for RNN cells
+
+    Parameters
+    ----------
+    prefix : str, optional
+        Prefix for names of `Block`s
+        (this prefix is also used for names of weights if `params` is `None`
+        i.e. if `params` are being created and not reused)
+    params : Parameter or None, optional
+        Container for weight sharing between cells.
+        A new Parameter container is created if `params` is `None`.
+=cut
+
+=head2 reset
+
+    Reset before re-using the cell for another graph.
+=cut
+
+method reset()
+{
+    $self->init_counter(-1);
+    $self->counter(-1);
+    $_->reset for @{ $self->_children };
+}
+
+=head2 state_info
+
+    Shape and layout information of states
+=cut
+method state_info(Int $batch_size=0)
+{
+    confess('Not Implemented');
+}
+
+=head2 begin_state
+
+        Initial state for this cell.
+
+        Parameters
+        ----------
+        $func : CodeRef, default sub { AI::MXNet::Symbol->zeros(@_) }
+            Function for creating initial state.
+
+            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,
+            `symbol.var etc`. Use `symbol.var` if you want to directly
+            feed input as states.
+
+            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.
+        $batch_size: int, default 0
+            Only required for NDArray API. Size of the batch ('N' in layout)
+            dimension of input.
+
+        %kwargs :
+            Additional keyword arguments passed to func. For example
+            `mean`, `std`, `dtype`, etc.
+
+        Returns
+        -------
+        states : nested array ref of Symbol
+            Starting states for the first RNN step.
+=cut
+
+method begin_state(Int :$batch_size=0, CodeRef :$func=, %kwargs)
+{
+    $func //= sub {
+        my %kwargs = @_;
+        my $shape = delete $kwargs{shape};
+        return AI::MXNet::NDArray->zeros($shape, %kwargs);
+    };
+    assert(
+        (not $self->modified),
+        "After applying modifier cells (e.g. ZoneoutCell) the base ".
+        "cell cannot be called directly. Call the modifier cell instead."
+    );
+    my @states;
+    for my $info (@{ $self->state_info($batch_size) })
+    {
+        $self->init_counter($self->init_counter + 1);
+        if(defined $info)
+        {
+            %$info = (%$info, %kwargs);
+        }
+        else
+        {
+            $info = \%kwargs;
+        }
+        my $state = $func->(
+            name => "${\ $self->_prefix }begin_state_${\ $self->init_counter }",
+            %$info
+        );
+        push @states, $state;
+    }
+    return \@states;
+}
+
+=head2 unroll
+
+        Unrolls an RNN cell across time steps.
+
+        Parameters
+        ----------
+        $length : int
+            Number of steps to unroll.
+        $inputs : Symbol, list of Symbol, or None
+            If `inputs` is a single Symbol (usually the output
+            of Embedding symbol), it should have shape
+            (batch_size, length, ...) if `layout` is 'NTC',
+            or (length, batch_size, ...) if `layout` is 'TNC'.
+
+            If `inputs` is a list of symbols (usually output of
+            previous unroll), they should all have shape
+            (batch_size, ...).
+        :$begin_state : nested list of Symbol, optional
+            Input states created by `begin_state()`
+            or output state of another cell.
+            Created from `begin_state()` if `None`.
+        :$layout : str, optional
+            `layout` of input symbol. Only used if inputs
+            is a single Symbol.
+        :$merge_outputs : bool, optional
+            If `False`, returns outputs as a list of Symbols.
+            If `True`, concatenates output across time steps
+            and returns a single symbol with shape
+            (batch_size, length, ...) if layout is 'NTC',
+            or (length, batch_size, ...) if layout is 'TNC'.
+            If `None`, output whatever is faster.
+
+        Returns
+        -------
+        outputs : list of Symbol or Symbol
+            Symbol (if `merge_outputs` is True) or list of Symbols
+            (if `merge_outputs` is False) corresponding to the output from
+            the RNN from this unrolling.
+
+        states : list of Symbol
+            The new state of this RNN after this unrolling.
+            The type of this symbol is same as the output of `begin_state()`.
+=cut
+
+method unroll(
+    Int $length,
+    Maybe[GluonInput] $inputs,
+    Maybe[GluonInput] :$begin_state=,
+    Str :$layout='NTC',
+    Maybe[Bool] :$merge_outputs=
+)
+{
+    $self->reset();
+    my ($F, $batch_size);
+    ($inputs, undef, $F, $batch_size) = $self->_format_sequence($length, $inputs, $layout, 0);
+    $begin_state //= $self->_get_begin_state($F, $begin_state, $inputs, $batch_size);
+
+    my $states = $begin_state;
+    my $outputs = [];
+    use Data::Dumper;
+    for my $i (0..$length-1)
+    {
+        my $output;
+        ($output, $states) = $self->($inputs->[$i], $states);
+        push @$outputs, $output;
+    }
+    ($outputs) = $self->_format_sequence($length, $outputs, $layout, $merge_outputs);
+    return ($outputs, $states);
+}
+
+method _get_activation(GluonClass $F, GluonInput $inputs, Activation $activation, %kwargs)
+{
+    if(not blessed $activation)
+    {
+        return $F->Activation($inputs, act_type=>$activation, %kwargs);
+    }
+    else
+    {
+        return $activation->($inputs, %kwargs);
+    }
+}
+
+=head2 forward
+
+        Unrolls the recurrent cell for one time step.
+
+        Parameters
+        ----------
+        inputs : sym.Variable
+            Input symbol, 2D, of shape (batch_size * num_units).
+        states : list of sym.Variable
+            RNN state from previous step or the output of begin_state().
+
+        Returns
+        -------
+        output : Symbol
+            Symbol corresponding to the output from the RNN when unrolling
+            for a single time step.
+        states : list of Symbol
+            The new state of this RNN after this unrolling.
+            The type of this symbol is same as the output of `begin_state()`.
+            This can be used as an input state to the next time step
+            of this RNN.
+
+        See Also
+        --------
+        begin_state: This function can provide the states for the first time step.
+        unroll: This function unrolls an RNN for a given number of (>=1) time steps.
+=cut
+
+package AI::MXNet::Gluon::RNN::HybridRecurrentCell;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::HybridBlock';
+with 'AI::MXNet::Gluon::RNN::RecurrentCell';
+has 'modified'      => (is => 'rw', isa => 'Bool', default => 0);
+has [qw/counter
+     init_counter/] => (is => 'rw', isa => 'Int', default => -1);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->reset;
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    my $s = '%s(%s';
+    if($self->can('activation'))
+    {
+        $s .= ", ${\ $self->activation }";
+    }
+    $s .= ')';
+    my $mapping = $self->input_size ? $self->input_size . " -> " . $self->hidden_size : $self->hidden_size;
+    return sprintf($s, $self->_class_name, $mapping);
+};
+
+method forward(GluonInput $inputs, Maybe[GluonInput|ArrayRef[GluonInput]] $states)
+{
+    $self->counter($self->counter + 1);
+    $self->SUPER::forward($inputs, $states);
+}
+
+package AI::MXNet::Gluon::RNN::RNNCell;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::RNNCell
+=cut
+
+=head1 DESCRIPTION
+
+    Simple recurrent neural network cell.
+
+    Parameters
+    ----------
+    hidden_size : int
+        Number of units in output symbol
+    activation : str or Symbol, default 'tanh'
+        Type of activation function.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    prefix : str, default 'rnn_'
+        Prefix for name of `Block`s
+        (and name of weight if params is `None`).
+    params : Parameter or None
+        Container for weight sharing between cells.
+        Created if `None`.
+=cut
+
+has 'hidden_size' => (is => 'rw', isa => 'Int', required => 1);
+has 'activation'  => (is => 'rw', isa => 'Activation', default => 'tanh');
+has [qw/
+    i2h_weight_initializer
+    h2h_weight_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]');
+has [qw/
+    i2h_bias_initializer
+    h2h_bias_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]', default => 'zeros');
+has 'input_size'  => (is => 'rw', isa => 'Int', default => 0);
+has [qw/
+        i2h_weight
+        h2h_weight
+        i2h_bias
+        h2h_bias
+    /]            => (is => 'rw', init_arg => undef);
+
+method python_constructor_arguments()
+{
+    [qw/
+        hidden_size activation 
+        i2h_weight_initializer h2h_weight_initializer
+        i2h_bias_initializer h2h_bias_initializer
+        input_size
+    /];
+}
+
+sub BUILD
+{
+    my $self = shift;
+    $self->i2h_weight($self->params->get(
+        'i2h_weight', shape=>[$self->hidden_size, $self->input_size],
+        init => $self->i2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_weight($self->params->get(
+        'h2h_weight', shape=>[$self->hidden_size, $self->hidden_size],
+        init => $self->h2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->i2h_bias($self->params->get(
+        'i2h_bias', shape=>[$self->hidden_size],
+        init => $self->i2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_bias($self->params->get(
+        'h2h_bias', shape=>[$self->hidden_size],
+        init => $self->h2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+}
+
+method state_info(Int $batch_size=0)
+{
+    return [{ shape => [$batch_size, $self->hidden_size], __layout__ => 'NC' }];
+}
+
+method _alias() { 'rnn' }
+
+method hybrid_forward(
+    GluonClass $F, GluonInput $inputs, GluonInput $states,
+    GluonInput :$i2h_weight, GluonInput :$h2h_weight, GluonInput :$i2h_bias, GluonInput :$h2h_bias
+)
+{
+    my $prefix = "t${\ $self->counter}_";
+    my $i2h = $F->FullyConnected(
+        $inputs, $i2h_weight, $i2h_bias,
+        num_hidden => $self->hidden_size,
+        name => "${prefix}i2h"
+    );
+    my $h2h = $F->FullyConnected(
+        $states->[0], $h2h_weight, $h2h_bias,
+        num_hidden => $self->hidden_size,
+        name => "${prefix}h2h"
+    );
+    my $output = $self->_get_activation($F, $i2h + $h2h, $self->activation, name => "${prefix}out");
+    return ($output, [$output]);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::LSTMCell;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::LSTMCell
+=cut
+
+=head1 DESCRIPTION
+
+    Long-Short Term Memory (LSTM) network cell.
+
+    Parameters
+    ----------
+    hidden_size : int
+        Number of units in output symbol.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer, default 'lstmbias'
+        Initializer for the bias vector. By default, bias for the forget
+        gate is initialized to 1 while all other biases are initialized
+        to zero.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    prefix : str, default 'lstm_'
+        Prefix for name of `Block`s
+        (and name of weight if params is `None`).
+    params : Parameter or None
+        Container for weight sharing between cells.
+        Created if `None`.
+=cut
+
+has 'hidden_size' => (is => 'rw', isa => 'Int', required => 1);
+has [qw/
+    i2h_weight_initializer
+    h2h_weight_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]');
+has [qw/
+    i2h_bias_initializer
+    h2h_bias_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]', default => 'zeros');
+has 'input_size'  => (is => 'rw', isa => 'Int', default => 0);
+has [qw/
+        i2h_weight
+        h2h_weight
+        i2h_bias
+        h2h_bias
+    /]            => (is => 'rw', init_arg => undef);
+
+method python_constructor_arguments()
+{
+    [qw/
+        hidden_size
+        i2h_weight_initializer h2h_weight_initializer
+        i2h_bias_initializer h2h_bias_initializer
+        input_size
+    /];
+}
+
+sub BUILD
+{
+    my $self = shift;
+    $self->i2h_weight($self->params->get(
+        'i2h_weight', shape=>[4*$self->hidden_size, $self->input_size],
+        init => $self->i2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_weight($self->params->get(
+        'h2h_weight', shape=>[4*$self->hidden_size, $self->hidden_size],
+        init => $self->h2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->i2h_bias($self->params->get(
+        'i2h_bias', shape=>[4*$self->hidden_size],
+        init => $self->i2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_bias($self->params->get(
+        'h2h_bias', shape=>[4*$self->hidden_size],
+        init => $self->h2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+}
+
+method state_info(Int $batch_size=0)
+{
+    return [
+        { shape => [$batch_size, $self->hidden_size], __layout__ => 'NC' },
+        { shape => [$batch_size, $self->hidden_size], __layout__ => 'NC' }
+    ];
+}
+
+method _alias() { 'lstm' }
+
+method hybrid_forward(
+    GluonClass $F, GluonInput $inputs, GluonInput $states,
+    GluonInput :$i2h_weight, GluonInput :$h2h_weight, GluonInput :$i2h_bias, GluonInput :$h2h_bias
+)
+{
+    my $prefix = "t${\ $self->counter}_";
+    my $i2h = $F->FullyConnected(
+        $inputs, $i2h_weight, $i2h_bias,
+        num_hidden => $self->hidden_size*4,
+        name => "${prefix}i2h"
+    );
+    my $h2h = $F->FullyConnected(
+        $states->[0], $h2h_weight, $h2h_bias,
+        num_hidden => $self->hidden_size*4,
+        name => "${prefix}h2h"
+    );
+    my $gates = $i2h + $h2h;
+    my @slice_gates = @{ $F->SliceChannel($gates, num_outputs => 4, name => "${prefix}slice") };
+    my $in_gate = $F->Activation($slice_gates[0], act_type=>"sigmoid", name => "${prefix}i");
+    my $forget_gate = $F->Activation($slice_gates[1], act_type=>"sigmoid", name => "${prefix}f");
+    my $in_transform = $F->Activation($slice_gates[2], act_type=>"tanh", name => "${prefix}c");
+    my $out_gate = $F->Activation($slice_gates[3], act_type=>"sigmoid", name => "${prefix}o");
+    my $next_c = $F->_plus($forget_gate * $states->[1], $in_gate * $in_transform, name => "${prefix}state");
+    my $next_h = $F->_mul($out_gate, $F->Activation($next_c, act_type=>"tanh"), name => "${prefix}out");
+    return ($next_h, [$next_h, $next_c]);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::GRUCell;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::GRUCell
+=cut
+
+=head1 DESCRIPTION
+
+    Gated Rectified Unit (GRU) network cell.
+    Note: this is an implementation of the cuDNN version of GRUs
+    (slight modification compared to Cho et al. 2014).
+
+    Parameters
+    ----------
+    hidden_size : int
+        Number of units in output symbol.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    prefix : str, default 'gru_'
+        prefix for name of `Block`s
+        (and name of weight if params is `None`).
+    params : Parameter or None
+        Container for weight sharing between cells.
+        Created if `None`.
+=cut
+
+has 'hidden_size' => (is => 'rw', isa => 'Int', required => 1);
+has [qw/
+    i2h_weight_initializer
+    h2h_weight_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]');
+has [qw/
+    i2h_bias_initializer
+    h2h_bias_initializer
+    /]            => (is => 'rw', isa => 'Maybe[Initializer]', default => 'zeros');
+has 'input_size'  => (is => 'rw', isa => 'Int', default => 0);
+has [qw/
+        i2h_weight
+        h2h_weight
+        i2h_bias
+        h2h_bias
+    /]            => (is => 'rw', init_arg => undef);
+
+method python_constructor_arguments()
+{
+    [qw/
+        hidden_size
+        i2h_weight_initializer h2h_weight_initializer
+        i2h_bias_initializer h2h_bias_initializer
+        input_size
+    /];
+}
+
+sub BUILD
+{
+    my $self = shift;
+    $self->i2h_weight($self->params->get(
+        'i2h_weight', shape=>[3*$self->hidden_size, $self->input_size],
+        init => $self->i2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_weight($self->params->get(
+        'h2h_weight', shape=>[3*$self->hidden_size, $self->hidden_size],
+        init => $self->h2h_weight_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->i2h_bias($self->params->get(
+        'i2h_bias', shape=>[3*$self->hidden_size],
+        init => $self->i2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+    $self->h2h_bias($self->params->get(
+        'h2h_bias', shape=>[3*$self->hidden_size],
+        init => $self->h2h_bias_initializer,
+        allow_deferred_init => 1
+    ));
+}
+
+method state_info(Int $batch_size=0)
+{
+    return [{ shape => [$batch_size, $self->hidden_size], __layout__ => 'NC' }];
+}
+
+method _alias() { 'gru' }
+
+method hybrid_forward(
+    GluonClass $F, GluonInput $inputs, GluonInput $states,
+    GluonInput :$i2h_weight, GluonInput :$h2h_weight, GluonInput :$i2h_bias, GluonInput :$h2h_bias
+)
+{
+    my $prefix = "t${\ $self->counter}_";
+    my $prev_state_h = $states->[0];
+    my $i2h = $F->FullyConnected(
+        $inputs, $i2h_weight, $i2h_bias,
+        num_hidden => $self->hidden_size*3,
+        name => "${prefix}i2h"
+    );
+    my $h2h = $F->FullyConnected(
+        $states->[0], $h2h_weight, $h2h_bias,
+        num_hidden => $self->hidden_size*3,
+        name => "${prefix}h2h"
+    );
+    my ($i2h_r, $i2h_z, $h2h_r, $h2h_z);
+    ($i2h_r, $i2h_z, $i2h) = @{ $F->SliceChannel($i2h, num_outputs => 3, name => "${prefix}i2h_slice") };
+    ($h2h_r, $h2h_z, $h2h) = @{ $F->SliceChannel($h2h, num_outputs => 3, name => "${prefix}h2h_slice") };
+    my $reset_gate  = $F->Activation($i2h_r + $h2h_r, act_type=>"sigmoid", name => "${prefix}r_act");
+    my $update_gate = $F->Activation($i2h_z + $h2h_z, act_type=>"sigmoid", name => "${prefix}z_act");
+    my $next_h_tmp = $F->Activation($i2h + $reset_gate * $h2h, act_type => "tanh", name => "${prefix}h_act");
+    my $next_h = $F->_plus((1 - $update_gate) * $next_h_tmp, $update_gate * $prev_state_h, name => "${prefix}out");
+    return ($next_h, [$next_h]);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::SequentialRNNCell;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+no warnings 'redefine';
+extends 'AI::MXNet::Gluon::Block';
+with 'AI::MXNet::Gluon::RNN::RecurrentCell';
+has 'modified'      => (is => 'rw', isa => 'Bool', default => 0);
+has [qw/counter
+     init_counter/] => (is => 'rw', isa => 'Int', default => -1);
+
+sub BUILD
+{
+    my $self = shift;
+    $self->reset;
+}
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::SequentialRNNCell
+=cut
+
+=head1 DESCRIPTION
+
+    Sequentially stacking multiple RNN cells.
+=cut
+
+=head2 add
+
+    Appends a cell into the stack.
+
+    Parameters
+    ----------
+        cell : rnn cell
+=cut
+
+method add(AI::MXNet::Gluon::Block $cell)
+{
+    $self->register_child($cell);
+}
+
+method state_info(Int $batch_size=0)
+{
+    return $self->_cells_state_info($self->_children, $batch_size);
+}
+
+method begin_state(%kwargs)
+{
+    assert(
+        (not $self->modified),
+        "After applying modifier cells (e.g. ZoneoutCell) the base ".
+        "cell cannot be called directly. Call the modifier cell instead."
+    );
+    return $self->_cells_begin_state($self->_children, %kwargs);
+}
+
+method unroll(Int $length, GluonInput $inputs, Maybe[GluonInput] :$begin_state=, Str :$layout='NTC', Maybe[Bool] :$merge_outputs=)
+{
+    $self->reset();
+    my ($F, $batch_size);
+    ($inputs, undef, $F, $batch_size) = $self->_format_sequence($length, $inputs, $layout, undef);
+    my $num_cells = @{ $self->_children };
+    $begin_state = $self->_get_begin_state($F, $begin_state, $inputs, $batch_size);
+    my $p = 0;
+    my @next_states;
+    my $states;
+    enumerate(sub {
+        my ($i, $cell) = @_;
+        my $n = @{ $cell->state_info() };
+        $states = [@{ $begin_state }[$p..$p+$n-1]];
+        $p += $n;
+        ($inputs, $states) = $cell->unroll(
+            $length, $inputs, begin_state => $states, layout => $layout,
+            merge_outputs => ($i < ($num_cells - 1)) ? undef : $merge_outputs
+        );
+        push @next_states, @{ $states };
+    }, $self->_children);
+    return ($inputs, \@next_states);
+}
+
+method call($inputs, $states)
+{
+    $self->counter($self->counter + 1);
+    my @next_states;
+    my $p = 0;
+    for my $cell (@{ $self->_children })
+    {
+        assert(not $cell->isa('AI::MXNet::Gluon::RNN::BidirectionalCell'));
+        my $n = @{ $cell->state_info() };
+        my $state = [@{ $states }[$p,$p+$n-1]];
+        $p += $n;
+        ($inputs, $state) = $cell->($inputs, $state);
+        push @next_states, @{ $state };
+    }
+    return ($inputs, \@next_states);
+}
+
+use overload '@{}' => sub { shift->_children };
+use overload '""'  => sub {
+    my $self = shift;
+    my $s = "%s(\n%s\n)";
+    my @children;
+    enumerate(sub {
+        my ($i, $m) = @_;
+        push @children, "($i): ". AI::MXNet::Base::_indent("$m", 2);
+    }, $self->_children);
+    return sprintf($s, $self->_class_name, join("\n", @children));
+};
+
+method hybrid_forward(@args)
+{
+    confess('Not Implemented');
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::DropoutCell;
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::DropoutCell
+=cut
+
+=head1 DESCRIPTION
+
+    Applies dropout on input.
+
+    Parameters
+    ----------
+    rate : float
+        Percentage of elements to drop out, which
+        is 1 - percentage to retain.
+=cut
+
+has 'rate' => (is => 'ro', isa => 'Num', required => 1);
+method python_constructor_arguments() { ['rate'] }
+
+method state_info(Int $batch_size=0) { [] }
+
+method _alias() { 'dropout' }
+
+method hybrid_forward(GluonClass $F, GluonInput $inputs, GluonInput $states)
+{
+    if($self->rate > 0)
+    {
+        $inputs = $F->Dropout($inputs, p => $self->rate, name => "t${\ $self->counter }_fwd");
+    }
+    return ($inputs, $states);
+}
+
+method unroll(Int $length, GluonInput $inputs, Maybe[GluonInput] :$begin_state=, Str :$layout='NTC', Maybe[Bool] :$merge_outputs=)
+{
+    $self->reset;
+    my $F;
+    ($inputs, undef, $F) = $self->_format_sequence($length, $inputs, $layout, $merge_outputs);
+    if(blessed $inputs)
+    {
+        return $self->hybrid_forward($F, $inputs, $begin_state//[]);
+    }
+    else
+    {
+        return $self->SUPER::unroll(
+            $length, $inputs, begin_state => $begin_state, layout => $layout,
+            merge_outputs => $merge_outputs
+        );
+    }
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    return $self->_class_name.'(rate ='.$self->rate.')';
+};
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::ModifierCell;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+has 'base_cell' => (is => 'rw', isa => 'AI::MXNet::Gluon::RNN::HybridRecurrentCell', required => 1);
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::ModifierCell
+=cut
+
+=head1 DESCRIPTION
+
+    Base class for modifier cells. A modifier
+    cell takes a base cell, apply modifications
+    on it (e.g. Zoneout), and returns a new cell.
+
+    After applying modifiers the base cell should
+    no longer be called directly. The modifier cell
+    should be used instead.
+=cut
+
+
+sub BUILD
+{
+    my $self = shift;
+    assert(
+        (not $self->base_cell->modified),
+        "Cell ${\ $self->base_cell->name } is already modified. One cell cannot be modified twice"
+    );
+    $self->base_cell->modified(1);
+}
+
+method params()
+{
+    return $self->base_cell->params;
+}
+
+method state_info(Int $batch_size=0)
+{
+    return $self->base_cell->state_info($batch_size);
+
+}
+
+method begin_state(CodeRef :$func=sub{ AI::MXNet::Symbol->zeros(@_) }, %kwargs)
+{
+    assert(
+        (not $self->modified),
+        "After applying modifier cells (e.g. DropoutCell) the base ".
+        "cell cannot be called directly. Call the modifier cell instead."
+    );
+    $self->base_cell->modified(0);
+    my $begin = $self->base_cell->begin_state(func => $func, %kwargs);
+    $self->base_cell->modified(1);
+    return $begin;
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $inputs, GluonInput $states)
+{
+    confess('Not Implemented');
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    return $self->_class_name.'('.$self->base_cell.')';
+};
+
+package AI::MXNet::Gluon::RNN::ZoneoutCell;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::RNN::ModifierCell';
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::ZoneoutCell
+=cut
+
+=head1 DESCRIPTION
+
+    Applies Zoneout on base cell.
+=cut
+has [qw/zoneout_outputs
+        zoneout_states/] => (is => 'ro', isa => 'Num', default => 0);
+has 'prev_output' => (is => 'rw', init_arg => undef);
+method python_constructor_arguments() { ['base_cell', 'zoneout_outputs', 'zoneout_states'] }
+
+sub BUILD
+{
+    my $self = shift;
+    assert(
+        (not $self->base_cell->isa('AI::MXNet::Gluon::RNN::BidirectionalCell')),
+        "BidirectionalCell doesn't support zoneout since it doesn't support step. ".
+        "Please add ZoneoutCell to the cells underneath instead."
+    );
+    assert(
+        (not $self->base_cell->isa('AI::MXNet::Gluon::RNN::SequentialRNNCel') or not $self->base_cell->bidirectional),
+        "Bidirectional SequentialRNNCell doesn't support zoneout. ".
+        "Please add ZoneoutCell to the cells underneath instead."
+    );
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    return $self->_class_name.'(p_out='.$self->zoneout_outputs.', p_state='.$self->zoneout_states.
+           ', '.$self->base_cell.')';
+};
+
+method _alias() { 'zoneout' }
+
+method reset()
+{
+    $self->SUPER::reset();
+    $self->prev_output(undef);
+}
+
+method hybrid_forward(GluonClass $F, GluonInput $inputs, GluonInput $states)
+{
+    my ($cell, $p_outputs, $p_states) = ($self->base_cell, $self->zoneout_outputs, $self->zoneout_states);
+    my ($next_output, $next_states) = $cell->($inputs, $states);
+    my $mask = sub { my ($p, $like) = @_; $F->Dropout($F->ones_like($like), p=>$p) };
+
+    my $prev_output = $self->prev_output//$F->zeros_like($next_output);
+    my $output = $p_outputs != 0 ? $F->where($mask->($p_outputs, $next_output), $next_output, $prev_output) : $next_output;
+    if($p_states != 0)
+    {
+        my @tmp;
+        zip(sub {
+            my ($new_s, $old_s) = @_;
+            push @tmp, $F->where($mask->($p_states, $new_s), $new_s, $old_s);
+        }, $next_states, $states);
+        $states = \@tmp;
+    }
+    else
+    {
+        $states = $next_states;
+    }
+    $self->prev_output($output);
+    return ($output, $states);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::ResidualCell;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::RNN::ModifierCell';
+method python_constructor_arguments() { ['base_cell'] }
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::ResidualCell
+=cut
+
+=head1 DESCRIPTION
+
+    Adds residual connection as described in Wu et al, 2016
+    (https://arxiv.org/abs/1609.08144).
+    Output of the cell is output of the base cell plus input.
+=cut
+
+method hybrid_forward(GluonClas $F, GluonInput $inputs, GluonInput $states)
+{
+    my $output;
+    ($output, $states) = $self->base_cell->($inputs, $states);
+    $output = $F->elemwise_add($output, $inputs, name => "t${\ $self->counter }_fwd");
+    return ($output, $states);
+}
+
+method unroll(Int $length, GluonInput $inputs, Maybe[GluonInput] :$begin_state=, Str :$layout='NTC', Maybe[Bool] :$merge_outputs=)
+{
+    $self->reset();
+
+    $self->base_cell->modified(0);
+    my ($outputs, $states) = $self->base_cell->unroll(
+        $length, $inputs, begin_state => $begin_state, layout => $layout, merge_outputs => $merge_outputs
+    );
+    $self->base_cell->modified(1);
+
+    $merge_outputs //= blessed $outputs ? 1 : 0;
+    my $F;
+    ($inputs, undef, $F) = $self->_format_sequence($length, $inputs, $layout, $merge_outputs);
+    if($merge_outputs)
+    {
+        $outputs = $F->elemwise_add($outputs, $inputs);
+    }
+    else
+    {
+        my @tmp;
+        zip(sub {
+            my ($i, $j) = @_;
+            push @tmp, $F->elemwise_add($i, $j);
+        }, $outputs, $inputs);
+        $outputs = \@tmp;
+    }
+    return ($outputs, $states);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::BidirectionalCell;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+has [qw/l_cell r_cell/] => (is => 'ro', isa => 'AI::MXNet::Gluon::RNN::HybridRecurrentCell', required => 1);
+has 'output_prefix'     => (is => 'ro', isa => 'Str', default => 'bi_');
+method python_constructor_arguments() { ['l_cell', 'r_cell', 'output_prefix'] }
+
+=head1 NAME
+
+    AI::MXNet::Gluon::RNN::BidirectionalCell
+=cut
+
+=head1 DESCRIPTION
+
+    Bidirectional RNN cell.
+
+    Parameters
+    ----------
+    l_cell : RecurrentCell
+        Cell for forward unrolling
+    r_cell : RecurrentCell
+        Cell for backward unrolling
+=cut
+
+method call($inputs, $states)
+{
+    confess("Bidirectional cell cannot be stepped. Please use unroll");
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    "${\ $self->_class_name }(forward=${\ $self->l_cell }, backward=${\ $self->r_cell })";
+};
+
+method state_info(Int $batch_size=0)
+{
+    return $self->_cells_state_info($self->_children, $batch_size);
+}
+
+method begin_state(%kwargs)
+{
+    assert(
+        (not $self->modified),
+        "After applying modifier cells (e.g. DropoutCell) the base ".
+        "cell cannot be called directly. Call the modifier cell instead."
+    );
+    return $self->_cells_begin_state($self->_children, %kwargs);
+}
+
+method unroll(Int $length, GluonInput $inputs, Maybe[GluonInput] :$begin_state=, Str :$layout='NTC', Maybe[Bool] :$merge_outputs=)
+{
+    $self->reset();
+    my ($axis, $F, $batch_size);
+    ($inputs, $axis, $F, $batch_size) = $self->_format_sequence($length, $inputs, $layout, 0);
+    $begin_state //= $self->_get_begin_state($F, $begin_state, $inputs, $batch_size);
+
+    my $states = $begin_state;
+    my ($l_cell, $r_cell) = @{ $self->_children };
+    $l_cell->state_info($batch_size);
+    my ($l_outputs, $l_states) = $l_cell->unroll(
+            $length, $inputs,
+            begin_state => [@{ $states }[0..@{ $l_cell->state_info($batch_size) }-1]],
+            layout => $layout,
+            merge_outputs => $merge_outputs
+    );
+    my ($r_outputs, $r_states) = $r_cell->unroll(
+        $length, [reverse @{$inputs}],
+        begin_state     => [@{$states}[@{ $l_cell->state_info }..@{$states}-1]],
+        layout          => $layout,
+        merge_outputs   => $merge_outputs
+    );
+    if(not defined $merge_outputs)
+    {
+        $merge_outputs = blessed $l_outputs and blessed $r_outputs;
+        ($l_outputs) = $self->_format_sequence(undef, $l_outputs, $layout, $merge_outputs);
+        ($r_outputs) = $self->_format_sequence(undef, $r_outputs, $layout, $merge_outputs);
+    }
+    my $outputs;
+    if($merge_outputs)
+    {
+        $r_outputs = $F->reverse($r_outputs, axis=>$axis);
+        $outputs = $F->concat($l_outputs, $r_outputs, dim=>2, name=>$self->output_prefix.'out');
+    }
+    else
+    {
+        $outputs = [];
+        enumerate(sub {
+            my ($i, $l_o, $r_o) = @_;
+                push @$outputs, $F->concat(
+                    $l_o, $r_o, dim=>1,
+                    name => sprintf('%st%d', $self->output_prefix, $i)
+                );
+            }, [@{ $l_outputs }], [reverse(@{ $r_outputs })]
+        );
+    }
+    $states = [@{ $l_states }, @{ $r_states }];
+    return ($outputs, $states);
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Layer.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Layer.pm
new file mode 100644
index 000000000000..fa850e62a76a
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/RNN/Layer.pm
@@ -0,0 +1,681 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::RNN::Layer;
+use AI::MXNet::Function::Parameters;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::Gluon::Block';
+
+has 'hidden_size'   => (is => 'rw', isa => 'Int');
+has 'num_layers'    => (is => 'rw', isa => 'Int');
+has 'layout'        => (is => 'rw', isa => 'Str');
+has 'dropout'       => (is => 'rw', isa => 'Num');
+has 'bidirectional' => (is => 'rw', isa => 'Bool');
+has 'input_size'    => (is => 'rw', isa => 'Int', default => 0);
+has [qw/
+    i2h_weight_initializer
+    h2h_weight_initializer
+    i2h_bias_initializer
+    h2h_bias_initializer
+    /]              => (is => 'rw', isa => 'Maybe[Initializer]');
+has 'mode'          => (is => 'rw', isa => 'Str');
+has [qw/dir gates
+    i2h_weight
+    h2h_weight
+    i2h_bias
+    h2h_bias
+    unfused/]       => (is => 'rw', init_arg => undef);
+
+method python_constructor_arguments()
+{
+    [qw/
+        hidden_size num_layers layout
+        dropout bidirectional input_size
+        i2h_weight_initializer h2h_weight_initializer
+        i2h_bias_initializer h2h_bias_initializer
+        mode
+    /];
+}
+
+sub BUILD
+{
+    my $self = shift;
+    assert(
+        ($self->layout eq 'TNC' or $self->layout eq 'NTC'),
+        "Invalid layout [${\ $self->layout }]; must be one of ['TNC' or 'NTC']"
+    );
+    $self->i2h_weight([]);
+    $self->h2h_weight([]);
+    $self->i2h_bias([]);
+    $self->h2h_bias([]);
+    $self->dir($self->bidirectional ? 2 : 1);
+    $self->gates({qw/rnn_relu 1 rnn_tanh 1 lstm 4 gru 3/}->{$self->mode});
+    my ($ng, $ni, $nh) = ($self->gates, $self->input_size, $self->hidden_size);
+    for my $i (0..$self->num_layers-1)
+    {
+        for my $j ($self->dir == 2 ? ('l', 'r') : ('l'))
+        {
+            push @{ $self->i2h_weight }, $self->params->get(
+                "$j${i}_i2h_weight", shape=>[$ng*$nh, $ni],
+                init => $self->i2h_weight_initializer,
+                allow_deferred_init => 1
+            );
+            push @{ $self->h2h_weight }, $self->params->get(
+                "$j${i}_h2h_weight", shape=>[$ng*$nh, $nh],
+                init => $self->h2h_weight_initializer,
+                allow_deferred_init => 1
+            );
+            push @{ $self->i2h_bias }, $self->params->get(
+                "$j${i}_i2h_bias", shape=>[$ng*$nh],
+                init => $self->i2h_bias_initializer,
+                allow_deferred_init => 1
+            );
+            push @{ $self->h2h_bias }, $self->params->get(
+                "$j${i}_h2h_bias", shape=>[$ng*$nh],
+                init => $self->h2h_bias_initializer,
+                allow_deferred_init => 1
+            );
+        }
+        $ni = $nh * $self->dir;
+    }
+    $self->unfused($self->_unfuse());
+}
+
+use overload '""' => sub {
+    my $self = shift;
+    my $name = $self->_class_name;
+    my $mapping = $self->input_size ? $self->input_size.' -> '.$self->hidden_size : $self->hidden_size;
+    my $s = "$name($mapping, ${\ $self->layout }";
+    if($self->num_layers != 1)
+    {
+        $s .= ', num_layers='.$self->num_layers;
+    }
+    if($self->dropout != 0)
+    {
+        $s .= ', dropout='.$self->dropout;
+    }
+    if($self->dir == 2)
+    {
+        $s .= ', bidirectional';
+    }
+    $s .= ')';
+    return $s;
+};
+
+method state_info($batch_size=0)
+{
+    confess('NotImplementedError');
+}
+
+# Unfuses the fused RNN in to a stack of rnn cells.
+
+method _unfuse()
+{
+    my $get_cell = {
+        rnn_relu => sub {
+            my %kwargs = @_;
+            AI::MXNet::Gluon::RNN::RNNCell->new(
+                $self->hidden_size,
+                activation => 'relu',
+                %kwargs
+            )
+        },
+        rnn_tanh => sub {
+            my %kwargs = @_;
+            AI::MXNet::Gluon::RNN::RNNCell->new(
+                $self->hidden_size,
+                activation => 'tanh',
+                %kwargs
+            )
+        },
+        lstm => sub {
+            my %kwargs = @_;
+            AI::MXNet::Gluon::RNN::LSTMCell->new(
+                $self->hidden_size,
+                %kwargs
+            )
+        },
+        gru => sub {
+            my %kwargs = @_;
+            AI::MXNet::Gluon::RNN::GRUCell->new(
+                $self->hidden_size,
+                %kwargs
+            )
+        }
+    }->{$self->mode};
+    my $stack = AI::MXNet::Gluon::RNN::SequentialRNNCell->new(prefix => $self->prefix, params => $self->params);
+    $stack->name_scope(sub {
+        my $ni = $self->input_size;
+        for my $i (0..$self->num_layers-1)
+        {
+            my %kwargs = (
+                input_size => $ni,
+                i2h_weight_initializer => $self->i2h_weight_initializer,
+                h2h_weight_initializer => $self->h2h_weight_initializer,
+                i2h_bias_initializer   => $self->i2h_bias_initializer,
+                h2h_bias_initializer   => $self->h2h_bias_initializer
+            );
+            if($self->dir == 2)
+            {
+                $stack->add(
+                    AI::MXNet::Gluon::RNN::BidirectionalCell->new(
+                        $get_cell->(prefix=> "l${i}_", %kwargs),
+                        $get_cell->(prefix=> "r${i}_", %kwargs),
+                    )
+                );
+            }
+            else
+            {
+                $stack->add($get_cell->(prefix=> "l${i}_", %kwargs));
+            }
+            if($self->dropout > 0 and $i != ($self->_num_layers - 1))
+            {
+                $stack->add(AI::MXNet::Gluon::RNN::DropoutCell->new($self->dropout));
+            }
+            $ni = $self->hidden_size * $self->dir;
+        }
+    });
+    return $stack;
+}
+
+method begin_state(
+    $batch_size=0,
+    CodeRef :$func=sub { my %kwargs = @_; my $shape = delete $kwargs{shape}; AI::MXNet::NDArray->zeros($shape, %kwargs) },
+    %kwargs
+)
+{
+    my @states;
+    enumerate(sub {
+        my ($i, $info) = @_;
+        if(defined $info)
+        {
+            %$info = (%$info, %kwargs);
+        }
+        else
+        {
+            %$info = %kwargs;
+        }
+        push @states, $func->(name=> $self->prefix."h0_$i", %$info);
+    }, $self->state_info($batch_size));
+    return \@states;
+}
+
+use Data::Dumper;
+method forward(GluonInput $inputs, Maybe[GluonInput] $states=)
+{
+    my $batch_size = $inputs->shape->[index($self->layout, 'N')];
+    my $skip_states = not defined $states;
+    if($skip_states)
+    {
+        $states = $self->begin_state($batch_size, ctx=>$inputs->context);
+    }
+    if(blessed $states and $states->isa('AI::MXNet::NDArray'))
+    {
+        $states = [$states];
+    }
+    zip(sub {
+        my ($state, $info) = @_;
+        if(Dumper($state->shape) ne Dumper($info->{shape}))
+        {
+            my @state_shape = @{ $state->shape };
+            confess("Invalid recurrent state shape. Expecting @{$info->{shape}}, got @state_shape.");
+        }
+    }, $states, $self->state_info($batch_size));
+    if($self->input_size == 0)
+    {
+        for my $i (0..$self->dir-1)
+        {
+            $self->i2h_weight->[$i]->shape([$self->gates*$self->hidden_size, $inputs->shape->[2]]);
+            $self->i2h_weight->[$i]->_finish_deferred_init();
+        }
+    }
+    my $out;
+    if($inputs->context->device_type eq 'gpu')
+    {
+        $out = $self->_forward_gpu($inputs, $states);
+    }
+    else
+    {
+        $out = $self->_forward_cpu($inputs, $states);
+    }
+
+    # out is (output, state)
+    return $skip_states ? $out->[0] : $out;
+}
+
+method _forward_cpu($inputs, $states)
+{
+    my $ns = @{ $states };
+    my $axis = index($self->layout, 'T');
+    $states = [map { @{$_} } @{ $states }];
+    my $outputs;
+    ($outputs, $states) = $self->unfused->unroll(
+        $inputs->shape->[$axis], $inputs, begin_state => $states,
+        layout => $self->layout, merge_outputs => 1
+    );
+    my @new_states;
+    for my $i (0..$ns-1)
+    {
+        my @tmp;
+        for (my $j = $i; $j < @{ $states }; $j += $ns)
+        {
+            push @tmp, $states->[$j];
+        }
+        my $state = AI::MXNet::NDArray->concat((map { $_->reshape([1, @{ $_->shape }]) } @tmp), dim => 0);
+        push @new_states, $state;
+    }
+    return [$outputs, \@new_states];
+}
+
+method _forward_gpu($inputs, $states)
+{
+    if($self->layout eq 'NTC')
+    {
+        $inputs = $inputs->swapaxes(dim1 => 0, dim2 => 1);
+    }
+    my $ctx = $inputs->context;
+    my @params = map { $_->data($ctx)->reshape([-1]) } map { @{ $_ } } (
+        $self->i2h_weight, $self->h2h_weight,
+        $self->i2h_bias, $self->h2h_bias
+    );
+    my $params = AI::MXNet::NDArray->concat(@params, dim => 0);
+    my $rnn = AI::MXNet::NDArray->RNN(
+        $inputs, $params, @{ $states }, state_size => $self->hidden_size,
+        num_layers => $self->num_layers, bidirectional => $self->dir == 2 ? 1 : 0,
+        p => $self->dropout, state_outputs => 1, mode => $self->mode
+    );
+    my $outputs;
+    my @rnn = @{$rnn};
+    if($self->mode eq 'lstm')
+    {
+        ($outputs, $states) = ($rnn[0], [$rnn[1], $rnn[2]]);
+    }
+    else
+    {
+        ($outputs, $states) = ($rnn[0], [$rnn[1]]);
+    }
+    if($self->layout eq 'NTC')
+    {
+        $outputs = $outputs->swapaxes(dim1 => 0, dim2 => 1);
+    }
+    return [$outputs, $states];
+}
+
+
+package AI::MXNet::Gluon::RNN::RNN;
+
+=head1 NAME
+
+     AI::MXNet::Gluon::RNN::RNN
+=cut
+
+=head1 DESCRIPTION
+
+    Applies a multi-layer Elman RNN with `tanh` or `ReLU` non-linearity to an input sequence.
+
+    For each element in the input sequence, each layer computes the following
+    function:
+
+    .. math::
+        h_t = \tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})
+
+    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the hidden
+    state of the previous layer at time `t` or :math:`input_t` for the first layer.
+    If nonlinearity='relu', then `ReLU` is used instead of `tanh`.
+
+    Parameters
+    ----------
+    hidden_size: int
+        The number of features in the hidden state h.
+    num_layers: int, default 1
+        Number of recurrent layers.
+    activation: {'relu' or 'tanh'}, default 'tanh'
+        The activation function to use.
+    layout : str, default 'TNC'
+        The format of input and output tensors. T, N and C stand for
+        sequence length, batch size, and feature dimensions respectively.
+    dropout: float, default 0
+        If non-zero, introduces a dropout layer on the outputs of each
+        RNN layer except the last layer.
+    bidirectional: bool, default False
+        If `True`, becomes a bidirectional RNN.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    input_size: int, default 0
+        The number of expected features in the input x.
+        If not specified, it will be inferred from input.
+    prefix : str or None
+        Prefix of this `Block`.
+    params : ParameterDict or None
+        Shared Parameters for this `Block`.
+
+
+    Input shapes:
+        The input shape depends on `layout`. For `layout='TNC'`, the
+        input has shape `(sequence_length, batch_size, input_size)`
+
+
+    Output shape:
+        The output shape depends on `layout`. For `layout='TNC'`, the
+        output has shape `(sequence_length, batch_size, num_hidden)`.
+        If `bidirectional` is True, output shape will instead be
+        `(sequence_length, batch_size, 2*num_hidden)`
+
+    Recurrent state:
+        The recurrent state is an NDArray with shape `(num_layers, batch_size, num_hidden)`.
+        If `bidirectional` is True, the recurrent state shape will instead be
+        `(2*num_layers, batch_size, num_hidden)`
+        If input recurrent state is None, zeros are used as default begin states,
+        and the output recurrent state is omitted.
+
+
+    Examples
+    --------
+    >>> layer = mx.gluon.rnn.RNN(100, 3)
+    >>> layer.initialize()
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> # by default zeros are used as begin state
+    >>> output = layer(input)
+    >>> # manually specify begin state.
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> output, hn = layer(input, h0)
+=cut
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::Layer';
+
+has '+num_layers'    => (default => 1);
+has 'activation'     => (is => 'rw', default => 'relu');
+has '+layout'        => (default => 'TNC');
+has '+dropout'       => (default => 0);
+has '+bidirectional' => (default => 0);
+has [qw/
+    +i2h_bias_initializer
+    +h2h_bias_initializer
+    /]               => (default => 'zeros');
+has '+mode'          => (default => sub { 'rnn_' . shift->activation }, lazy => 1);
+method python_constructor_arguments()
+{
+    [qw/
+        hidden_size num_layers activation layout
+        dropout bidirectional input_size
+        i2h_weight_initializer h2h_weight_initializer
+        i2h_bias_initializer h2h_bias_initializer
+    /];
+}
+
+method state_info(DimSize $batch_size=0)
+{
+    return [{
+        shape => [$self->num_layers * $self->dir, $batch_size, $self->hidden_size],
+        __layout__ => 'LNC'
+    }];
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::LSTM;
+
+=head1 NANE
+
+    AI::MXNet::Gluon::RNN::LSTM
+=cut
+
+=head1 DESCRIPTION
+
+    Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.
+
+    For each element in the input sequence, each layer computes the following
+    function:
+
+    .. math::
+        \begin{array}{ll}
+        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\
+        f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\
+        g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\
+        o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\
+        c_t = f_t * c_{(t-1)} + i_t * g_t \\
+        h_t = o_t * \tanh(c_t)
+        \end{array}
+
+    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the
+    cell state at time `t`, :math:`x_t` is the hidden state of the previous
+    layer at time `t` or :math:`input_t` for the first layer, and :math:`i_t`,
+    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and
+    out gates, respectively.
+
+    Parameters
+    ----------
+    hidden_size: int
+        The number of features in the hidden state h.
+    num_layers: int, default 1
+        Number of recurrent layers.
+    layout : str, default 'TNC'
+        The format of input and output tensors. T, N and C stand for
+        sequence length, batch size, and feature dimensions respectively.
+    dropout: float, default 0
+        If non-zero, introduces a dropout layer on the outputs of each
+        RNN layer except the last layer.
+    bidirectional: bool, default False
+        If `True`, becomes a bidirectional RNN.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer, default 'lstmbias'
+        Initializer for the bias vector. By default, bias for the forget
+        gate is initialized to 1 while all other biases are initialized
+        to zero.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    input_size: int, default 0
+        The number of expected features in the input x.
+        If not specified, it will be inferred from input.
+    prefix : str or None
+        Prefix of this `Block`.
+    params : `ParameterDict` or `None`
+        Shared Parameters for this `Block`.
+
+
+    Input shapes:
+        The input shape depends on `layout`. For `layout='TNC'`, the
+        input has shape `(sequence_length, batch_size, input_size)`
+
+    Output shape:
+        The output shape depends on `layout`. For `layout='TNC'`, the
+        output has shape `(sequence_length, batch_size, num_hidden)`.
+        If `bidirectional` is True, output shape will instead be
+        `(sequence_length, batch_size, 2*num_hidden)`
+
+    Recurrent state:
+        The recurrent state is a list of two NDArrays. Both has shape
+        `(num_layers, batch_size, num_hidden)`.
+        If `bidirectional` is True, each recurrent state will instead have shape
+        `(2*num_layers, batch_size, num_hidden)`.
+        If input recurrent state is None, zeros are used as default begin states,
+        and the output recurrent state is omitted.
+
+
+    Examples
+    --------
+    >>> layer = mx.gluon.rnn.LSTM(100, 3)
+    >>> layer.initialize()
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> # by default zeros are used as begin state
+    >>> output = layer(input)
+    >>> # manually specify begin state.
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> c0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> output, hn = layer(input, [h0, c0])
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::Layer';
+
+has '+num_layers'    => (default => 1);
+has '+layout'        => (default => 'TNC');
+has '+dropout'       => (default => 0);
+has '+bidirectional' => (default => 0);
+has [qw/
+    +i2h_bias_initializer
+    +h2h_bias_initializer
+    /]               => (default => 'zeros');
+has '+mode'          => (default => 'lstm');
+
+method state_info(DimSize $batch_size=0)
+{
+    return [
+        {
+            shape => [$self->num_layers * $self->dir, $batch_size, $self->hidden_size],
+            __layout__ => 'LNC'
+        },
+        {
+            shape => [$self->num_layers * $self->dir, $batch_size, $self->hidden_size],
+            __layout__ => 'LNC'
+        }
+    ];
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+package AI::MXNet::Gluon::RNN::GRU;
+
+=head1 NANE
+
+    AI::MXNet::Gluon::RNN::GRU
+=cut
+
+=head1 DESCRIPTION
+
+    Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.
+
+    For each element in the input sequence, each layer computes the following
+    function:
+
+    .. math::
+        \begin{array}{ll}
+        r_t = sigmoid(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
+        i_t = sigmoid(W_{ii} x_t + b_{ii} + W_hi h_{(t-1)} + b_{hi}) \\
+        n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\
+        h_t = (1 - i_t) * n_t + i_t * h_{(t-1)} \\
+        \end{array}
+
+    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the hidden
+    state of the previous layer at time `t` or :math:`input_t` for the first layer,
+    and :math:`r_t`, :math:`i_t`, :math:`n_t` are the reset, input, and new gates, respectively.
+
+    Parameters
+    ----------
+    hidden_size: int
+        The number of features in the hidden state h
+    num_layers: int, default 1
+        Number of recurrent layers.
+    layout : str, default 'TNC'
+        The format of input and output tensors. T, N and C stand for
+        sequence length, batch size, and feature dimensions respectively.
+    dropout: float, default 0
+        If non-zero, introduces a dropout layer on the outputs of each
+        RNN layer except the last layer
+    bidirectional: bool, default False
+        If True, becomes a bidirectional RNN.
+    i2h_weight_initializer : str or Initializer
+        Initializer for the input weights matrix, used for the linear
+        transformation of the inputs.
+    h2h_weight_initializer : str or Initializer
+        Initializer for the recurrent weights matrix, used for the linear
+        transformation of the recurrent state.
+    i2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    h2h_bias_initializer : str or Initializer
+        Initializer for the bias vector.
+    input_size: int, default 0
+        The number of expected features in the input x.
+        If not specified, it will be inferred from input.
+    prefix : str or None
+        Prefix of this `Block`.
+    params : ParameterDict or None
+        Shared Parameters for this `Block`.
+
+
+    Input shapes:
+        The input shape depends on `layout`. For `layout='TNC'`, the
+        input has shape `(sequence_length, batch_size, input_size)`
+
+    Output shape:
+        The output shape depends on `layout`. For `layout='TNC'`, the
+        output has shape `(sequence_length, batch_size, num_hidden)`.
+        If `bidirectional` is True, output shape will instead be
+        `(sequence_length, batch_size, 2*num_hidden)`
+
+    Recurrent state:
+        The recurrent state is an NDArray with shape `(num_layers, batch_size, num_hidden)`.
+        If `bidirectional` is True, the recurrent state shape will instead be
+        `(2*num_layers, batch_size, num_hidden)`
+        If input recurrent state is None, zeros are used as default begin states,
+        and the output recurrent state is omitted.
+
+
+    Examples
+    --------
+    >>> layer = mx.gluon.rnn.GRU(100, 3)
+    >>> layer.initialize()
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> # by default zeros are used as begin state
+    >>> output = layer(input)
+    >>> # manually specify begin state.
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> output, hn = layer(input, h0)
+=cut
+
+use AI::MXNet::Gluon::Mouse;
+extends 'AI::MXNet::Gluon::RNN::Layer';
+
+has '+num_layers'    => (default => 1);
+has '+layout'        => (default => 'TNC');
+has '+dropout'       => (default => 0);
+has '+bidirectional' => (default => 0);
+has [qw/
+    +i2h_bias_initializer
+    +h2h_bias_initializer
+    /]               => (default => 'zeros');
+has '+mode'          => (default => 'gru');
+
+method state_info(DimSize $batch_size=0)
+{
+    return [
+        {
+            shape => [$self->num_layers * $self->dir, $batch_size, $self->hidden_size],
+            __layout__ => 'LNC'
+        }
+    ];
+}
+
+__PACKAGE__->register('AI::MXNet::Gluon::RNN');
+
+1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Trainer.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Trainer.pm
new file mode 100644
index 000000000000..405c6d29aa38
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Trainer.pm
@@ -0,0 +1,334 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+package AI::MXNet::Gluon::Trainer;
+use AI::MXNet::Base;
+use AI::MXNet::Function::Parameters;
+use IO::File;
+use Mouse;
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Trainer
+=cut
+
+=head1 DESCRIPTION
+
+    Applies an `Optimizer` on a set of Parameters. Trainer should
+    be used together with `autograd`.
+
+    Parameters
+    ----------
+    params : ParameterDict
+        The set of parameters to optimize.
+    optimizer : str or Optimizer
+        The optimizer to use. See
+        `help <http://mxnet.io/api/python/optimization.html#the-mxnet-optimizer-package>`_
+        on Optimizer for a list of available optimizers.
+    optimizer_params : dict
+        Key-word arguments to be passed to optimizer constructor. For example,
+        `{'learning_rate': 0.1}`. All optimizers accept learning_rate, wd (weight decay),
+        clip_gradient, and lr_scheduler. See each optimizer's
+        constructor for a list of additional supported arguments.
+    kvstore : str or KVStore
+        kvstore type for multi-gpu and distributed training. See help on
+        :any:`mxnet.kvstore.create` for more information.
+=cut
+
+has '_params'          => (is => 'rw', init_arg => 'params', isa => 'HashRef|ArrayRef|AI::MXNet::Gluon::ParameterDict');
+has 'optimizer'        => (is => 'ro', isa => 'Optimizer');
+has 'optimizer_params' => (is => 'ro', isa => 'Maybe[HashRef]');
+has '_kv_store'        => (is => 'rw', init_arg => 'kvstore', isa => 'Maybe[KVStore]', default => 'device');
+has [qw/_scale _contexts
+    _kv_initialized
+    _update_on_kvstore
+    _updaters
+    _optimizer/]       => (is => 'rw', init_arg => undef);
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { ['params', 'optimizer', 'optimizer_params'] }
+
+sub BUILD
+{
+    my $self = shift;
+    my @params;
+    if(blessed $self->_params)
+    {
+        @params = $self->_params->values;
+    }
+    elsif(ref $self->_params eq 'HASH')
+    {
+        @params = values %{ $self->_params };
+    }
+    else
+    {
+        @params = @{ $self->_params };
+    }
+    $self->_params([]);
+    for my $param (@params)
+    {
+        if(not(blessed $param and $param->isa('AI::MXNet::Gluon::Parameter')))
+        {
+            confess(
+                "First argument must be a array or hash of Parameters, ".
+                "got list of [$param]."
+            );
+        }
+        push @{ $self->_params }, $param;
+    }
+    my $optimizer_params = $self->optimizer_params//{};
+    $self->_scale(delete $optimizer_params->{rescale_grad}//1);
+    $self->_contexts($self->_check_contexts);
+    $self->_init_optimizer($self->optimizer, $optimizer_params);
+    $self->_kv_initialized(0);
+}
+
+method _check_contexts()
+{
+    my $contexts;
+    for my $param (@{ $self->_params })
+    {
+        my $ctx = $param->list_ctx;
+        assert(
+            (not defined $contexts or join('', @{ $contexts }) eq join('', @{ $ctx })),
+            "All Parameters must be initialized on the same set of contexts, ".
+            "but Parameter ${\ $param->name } is initialized on @{ $ctx//[] } while previous Parameters ".
+            "are initialized on @{ $contexts//[] }."
+        );
+        $contexts = $ctx;
+    }
+    return $contexts;
+}
+
+method _init_optimizer($optimizer, $optimizer_params)
+{
+    my %param_dict = map { $_ => $self->_params->[$_] } 0 .. @{ $self->_params } - 1;
+    if(blessed $optimizer and $optimizer->isa('AI::MXNet::Optimizer'))
+    {
+        assert(
+            (not %{ $optimizer_params }),
+            "optimizer_params must be empty if optimizer is an instance of ".
+            "Optimizer instead of str"
+        );
+        $self->_optimizer($optimizer);
+        $self->_optimizer->param_dict(\%param_dict);
+    }
+    else
+    {
+        $self->_optimizer(
+            AI::MXNet::Optimizer->create(
+                $optimizer, param_dict => \%param_dict,
+                %{ $optimizer_params }
+            )
+        );
+    }
+    $self->_updaters([
+        map { AI::MXNet::Optimizer->get_updater($self->_optimizer) } @{ $self->_contexts }
+    ]);
+}
+
+method _init_kvstore()
+{
+    my %arg_arrays = map { $_->name => $_->data($self->_contexts->[0]) } @{ $self->_params };
+    my ($kvstore, $update_on_kvstore) = AI::MXNet::Module::_create_kvstore(
+        $self->_kv_store, scalar(@{$self->_contexts }), \%arg_arrays
+    );
+    if($kvstore)
+    {
+        if($kvstore->type =~ /dist/)
+        {
+            $update_on_kvstore = 0;
+        }
+        enumerate(sub {
+            my ($i, $param) = @_;
+            my $param_arrays = $param->list_data;
+            $kvstore->init($i, $param_arrays->[0]);
+            $kvstore->pull($i, out => $param_arrays, priority => -$i);
+        }, $self->_params);
+        if($update_on_kvstore)
+        {
+            $kvstore->set_optimizer($self->_optimizer);
+        }
+        $self->_kv_store($kvstore);
+        $self->_update_on_kvstore($update_on_kvstore);
+    }
+    else
+    {
+        $self->_kv_store(undef);
+        $self->_update_on_kvstore(undef)
+    }
+    $self->_kv_initialized(1);
+}
+
+=head2 step
+
+        Makes one step of parameter update. Should be called after
+        `autograd.compute_gradient` and outside of `record()` scope.
+
+        Parameters
+        ----------
+        batch_size : int
+            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
+            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.
+        ignore_stale_grad : bool, optional, default=False
+            If true, ignores Parameters with stale gradient (gradient that has not
+            been updated by `backward` after last step) and skip update.
+=cut
+
+method step(Int $batch_size, Bool $ignore_stale_grad=0)
+{
+    if(not $self->_kv_initialized)
+    {
+        $self->_init_kvstore;
+    }
+    $self->_optimizer->rescale_grad($self->_scale/$batch_size);
+    enumerate(sub {
+        my ($i, $param) = @_;
+        return if $param->grad_req eq 'null';
+        if(not $ignore_stale_grad)
+        {
+            for my $data (@{ $param->list_data })
+            {
+                if(not $data->_fresh_grad)
+                {
+                    AI::MXNet::Logging->warning(
+                        "Gradient of Parameter `%s` on context %s has not been updated ".
+                        "by backward since last `step`. This could mean a bug in your ".
+                        "model that maked it only use a subset of the Parameters (Blocks) ".
+                        "for this iteration. If you are intentionally only using a subset, ".
+                        "call step with ignore_stale_grad=True to suppress this ".
+                        "warning and skip updating of Parameters with stale gradient",
+                        $param->name, $data->context
+                    );
+                }
+            }
+        }
+        if($self->_kv_store)
+        {
+            $self->_kv_store->push($i, $param->list_grad, priority => -$i);
+            if($self->_update_on_kvstore)
+            {
+                $self->_kv_store->pull($i, out => $param->list_data, priority => -$i);
+                return;
+            }
+            else
+            {
+                $self->_kv_store->pull($i, out => $param->list_grad, priority => -$i);
+            }
+        }
+        zip(sub {
+            my ($upd, $arr, $grad) = @_;
+            if(not $ignore_stale_grad or $arr->_fresh_grad)
+            {
+                $upd->($i, $grad, $arr);
+                $arr->_fresh_grad(0);
+            }
+        }, $self->_updaters, $param->list_data, $param->list_grad);
+    }, $self->_params);
+}
+
+method learning_rate(Maybe [Num] $lr)
+{
+    if(not blessed $self->_optimizer)
+    {
+        AI::MXNet::Logging->warning(
+            "Optimizer has to be defined before its learning ".
+            "rate can be accessed."
+        );
+        return;
+    }
+    else
+    {
+        if(defined $lr)
+        {
+            $self->_optimizer->lr($lr);
+        }
+        return $self->_optimizer->lr;
+    }
+}
+
+=head2 set_learning_rate
+
+        Sets a new learning rate of the optimizer.
+
+        Parameters
+        ----------
+        lr : float
+            The new learning rate of the optimizer.
+=cut
+
+method set_learning_rate(Num $lr)
+{
+    $self->learning_rate($lr);
+}
+
+=head2 save_states
+
+        Saves trainer states (e.g. optimizer, momentum) to a file.
+
+        Parameters
+        ----------
+        fname : str
+            Path to output states file.
+=cut
+
+method save_states(Str $fname)
+{
+    assert(defined $self->_optimizer);
+    if($self->_update_on_kvstore)
+    {
+        $self->_kv_store->save_optimizer_states($fname, dump_optimizer=>1);
+    }
+    else
+    {
+        open(F, ">$fname") or Carp::confess("can not open $fname: $1");
+        print F $self->_updaters->[0]->get_states(dump_optimizer=>1);
+        close(F);
+    }
+}
+
+=head2 load_states
+
+        Loads trainer states (e.g. optimizer, momentum) from a file.
+
+        Parameters
+        ----------
+        fname : str
+            Path to input states file.
+=cut
+
+method load_states(Str $fname)
+{
+    if($self->_update_on_kvstore)
+    {
+        $self->_kv_store->load_optimizer_states($fname);
+        $self->_optimizer($self->_kv_store->_updater->optimizer);
+    }
+    else
+    {
+        my $states = join('', IO::File->new($fname)->getlines);
+        for my $updater (@{ $self->_updaters })
+        {
+            $updater->set_states($states);
+            $updater->optimizer($self->_updaters->[0]->optimizer);
+        }
+        $self->_optimizer($self->_updaters->[0]->optimizer);
+    }
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Utils.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Utils.pm
new file mode 100644
index 000000000000..eee3cb5a907b
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Gluon/Utils.pm
@@ -0,0 +1,280 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Gluon::Utils;
+use strict;
+use warnings;
+use AI::MXNet::Base;
+use AI::MXNet::Function::Parameters;
+use Digest::SHA qw(sha1_hex);
+use File::Path qw(make_path);
+use HTTP::Tiny;
+use Exporter;
+use base qw(Exporter);
+@AI::MXNet::Gluon::Utils::EXPORT_OK = qw(download);
+
+=head1 NAME
+
+    AI::MXNet::Gluon::Utils
+=cut
+
+=head1 DESCRIPTION
+
+    Miscellaneous utilities.
+=cut
+
+=head2 split_data
+
+    Splits an NDArray into `num_slice` slices along `batch_axis`.
+    Usually used for data parallelism where each slices is sent
+    to one device (i.e. GPU).
+
+    Parameters
+    ----------
+    $data : NDArray
+        A batch of data.
+    $num_slice : int
+        Number of desired slices.
+    $batch_axis=0 : int, default 0
+        The axis along which to slice.
+    :$even_split=1 : bool, default True
+        Whether to force all slices to have the same number of elements.
+        If `True`, an error will be raised when `num_slice` does not evenly
+        divide `data.shape[batch_axis]`.
+
+    Returns
+    -------
+    array ref of NDArray
+        Return value is a array ref even if `num_slice` is 1.
+=cut
+
+
+method split_data(AI::MXNet::NDArray $data, Int $num_slice, Int $batch_axis=0, Bool :$even_split=1)
+{
+    my $size = $data->shape->[$batch_axis];
+    if($size < $num_slice)
+    {
+        Carp::confess(
+            sprintf(
+                "Too many slices for data with shape (%s). Arguments are ".
+                "num_slice=%d and batch_axis=%d.",
+                join(',', @{ $data->shape }), $num_slice, $batch_axis
+            )
+        );
+    }
+    if($even_split and $size % $num_slice != 0)
+    {
+        Carp::confess(
+            sprintf(
+                "data with shape %s cannot be evenly split into %d slices along axis %d. ".
+                "Use a batch size that's multiple of %d or set even_split=False to allow ".
+                "uneven partitioning of data.",
+                join(',', @{ $data->shape }), $num_slice, $batch_axis, $num_slice
+            )
+        );
+    }
+    my $step = int($size/$num_slice);
+    my $slices = [];
+    if($batch_axis == 0)
+    {
+        for my $i (0 .. $num_slice-1)
+        {
+            if($i < $num_slice-1)
+            {
+                push @$slices, $data->slice([$i*$step, ($i+1)*$step-1]);
+            }
+            else
+            {
+                push @$slices, $data->slice([$i*$step, $size-1]);
+            }
+        }
+    }
+    elsif($even_split)
+    {
+        $slices = AI::MXNet::NDArray->split($data, num_outputs => $num_slice, axis => $batch_axis);
+    }
+    else
+    {
+        for my $i (0 .. $num_slice-1)
+        {
+            if($i < $num_slice-1)
+            {
+                push @$slices, $data->slice_axis($batch_axis, $i*$step, ($i+1)*$step);
+            }
+            else
+            {
+                push @$slices, $data->slice_axis($batch_axis, $i*$step, $size);
+            }
+        }
+    }
+    return $slices;
+}
+
+=head2 split_and_load
+
+    Splits an NDArray into `len(ctx_list)` slices along `batch_axis` and loads
+    each slice to one context in `ctx_list`.
+
+    Parameters
+    ----------
+    $data : AcceptableInput
+        A batch of data.
+    :$ctx_list : list of Context
+        A list of Contexts.
+    :$batch_axis : int, default 0
+        The axis along which to slice.
+    :$even_split : bool, default True
+        Whether to force all slices to have the same number of elements.
+
+    Returns
+    -------
+    list of NDArray
+        Each corresponds to a context in `ctx_list`.
+=cut
+
+method split_and_load(
+    PDL|PDL::Matrix|ArrayRef|AI::MXNet::NDArray $data,
+    ArrayRef[AI::MXNet::Context] :$ctx_list,
+    Int :$batch_axis=0,
+    Bool :$even_split=1
+)
+{
+    if(not (blessed $data and $data->isa('AI::MXNet::NDArray')))
+    {
+        $data = AI::MXNet::NDArray->array($data, ctx => $ctx_list->[0])
+    }
+    if(@{ $ctx_list } == 1)
+    {
+        return [$data->as_in_context($ctx_list->[0])];
+    }
+    my $slices = __PACKAGE__->split_data($data, scalar(@$ctx_list), $batch_axis, $even_split);
+    my @ret;
+    zip(sub {
+        my ($i, $ctx) = @_;
+        push @ret, $i->as_in_context($ctx);
+    }, $slices, $ctx_list);
+    return \@ret;
+}
+
+=head2 clip_global_norm
+
+    Rescales NDArrays so that the sum of their 2-norm is smaller than `max_norm`.
+=cut
+
+method clip_global_norm(ArrayRef[AI::MXNet::NDArray] $arrays, Num $max_norm)
+{
+    assert(@$arrays > 0);
+    my $total_norm = 0;
+    for my $arr (@$arrays)
+    {
+        $arr = $arr->reshape([-1]);
+        $total_norm += AI::MXNet::NDArray->dot($arr, $arr);
+    }
+    $total_norm = sqrt($total_norm->asscalar);
+    my $scale = $max_norm / ($total_norm + 1e-8);
+    if($scale < 1)
+    {
+        $_ *= $scale for @{ $arrays };
+    }
+    return $total_norm
+}
+
+=head2 check_sha1
+
+    Check whether the sha1 hash of the file content matches the expected hash.
+
+    Parameters
+    ----------
+    filename : str
+        Path to the file.
+    sha1_hash : str
+        Expected sha1 hash in hexadecimal digits.
+
+    Returns
+    -------
+    bool
+        Whether the file content matches the expected hash.
+=cut
+
+func check_sha1(Str $filename, Str $sha1_hash)
+{
+    local($/) = undef;
+    open(F, $filename) or Carp::confess("can't open $filename $!");
+    my $data = <F>;
+    close(F);
+    return sha1_hex($data) eq $sha1_hash;
+}
+
+=head2 download
+
+    Download an given URL
+
+    Parameters
+    ----------
+    $url : str
+        URL to download
+    :$path : str, optional
+        Destination path to store downloaded file. By default stores to the
+        current directory with same name as in url.
+    :$overwrite : bool, optional
+        Whether to overwrite destination file if already exists.
+    :$sha1_hash : str, optional
+        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified
+        but doesn't match.
+    Returns
+    -------
+    str
+        The file path of the downloaded file.
+=cut
+
+func download(Str $url, Maybe[Str] :$path=, Bool :$overwrite=0, Maybe[Str] :$sha1_hash=)
+{
+    my $fname;
+    $path =~ s/~/$ENV{HOME}/ if defined $path;
+    if(not defined $path)
+    {
+        $fname = (split(m[/], $url))[-1];
+    }
+    elsif(-d $path)
+    {
+        $fname = join('/', $path, (split(m[/], $url))[-1]);
+    }
+    else
+    {
+        $fname = $path;
+    }
+    if($overwrite or not -f $fname or ($sha1_hash and not check_sha1($fname, $sha1_hash)))
+    {
+        $fname =~ s/~/$ENV{HOME}/;
+        my $dirname = $fname;
+        $dirname =~ s/[^\/]+$//;
+        if(not -d $dirname)
+        {
+            make_path($dirname);
+        }
+        warn "Downloading $fname from $url ...\n";
+        my $response = HTTP::Tiny->new->get($url);
+        Carp::confess("download of url failed! ($response->{status} $response->{reason})\n")
+            unless $response->{success};
+        open(F, ">$fname") or Carp::confess("can't open $fname: $!");
+        print F $response->{content};
+        close(F);
+    }
+    return $fname
+}
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/IO.pm b/perl-package/AI-MXNet/lib/AI/MXNet/IO.pm
index 7a61cd9f1f1f..6ca98ca05ab8 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/IO.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/IO.pm
@@ -29,7 +29,7 @@ use Scalar::Util qw/blessed/;
 
 # Convert data into canonical form.
 method init_data(
-    AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]|Undef $data,
+    Maybe[AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]|Hash::Ordered] $data,
     Undef|Int :$allow_empty=,
     Str :$default_name
 )
@@ -37,8 +37,7 @@ method init_data(
     Carp::confess("data must be defined or allow_empty set to true value")
         if(not defined $data and not $allow_empty);
     $data //= [];
-
-    if(ref($data) and ref($data) ne 'ARRAY' and ref($data) ne 'HASH')
+    if(blessed $data and not $data->isa('Hash::Ordered'))
     {
         $data = [$data];
     }
@@ -59,13 +58,24 @@ method init_data(
             @ret = map { $i++; ["_${i}_$default_name", $_] } @{ $data };
         }
     }
-    if(ref($data) eq 'HASH')
+    elsif(ref($data) eq 'HASH')
     {
+        AI::MXNet::Logging->warning(
+            "Use of a raw perl hash as input is obsolete and the behaviour of the iterator is undefined.\n".
+            "Please use Hash::Ordered object instead."
+        );
         while(my ($k, $v) = each %{ $data })
         {
             push @ret, [$k, $v];
         }
     }
+    elsif(blessed $data and $data->isa('Hash::Ordered'))
+    {
+        for my $k ($data->keys)
+        {
+            push @ret, [$k, $data->get($k)];
+        }
+    }
     for my $d (@ret)
     {
         if(not (blessed $d->[1] and $d->[1]->isa('AI::MXNet::NDArray')))
@@ -216,9 +226,11 @@ method reset(){}
 method list()
 {
     my @ret;
-    while(<$self>)
+    while(my $data = <$self>)
     {
-        push @ret, $_;
+        $data->label([map { $_->copy } @{ $data->label }]);
+        $data->data([map { $_->copy } @{ $data->data }]);
+        push @ret, $data;
     }
     return \@ret;
 }
@@ -396,7 +408,7 @@ method getpad()
 package AI::MXNet::NDArrayIter;
 use Mouse;
 use AI::MXNet::Base;
-use List::Util qw(shuffle);
+use List::Util;
 extends 'AI::MXNet::DataIter';
 
 =head1 NAME
@@ -428,27 +440,19 @@ extends 'AI::MXNet::DataIter';
     for training and can cause problems if used for prediction.
 =cut
 
-has 'data'                => (is => 'rw', isa => 'Maybe[AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]]');
+has 'data'                => (is => 'rw', isa => 'Maybe[AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]|Hash::Ordered]');
 has 'data_list'           => (is => 'rw', isa => 'ArrayRef[AI::MXNet::NDArray]');
-has 'label'               => (is => 'rw', isa => 'Maybe[AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]]');
+has 'label'               => (is => 'rw', isa => 'Maybe[AcceptableInput|HashRef[AcceptableInput]|ArrayRef[AcceptableInput]|Hash::Ordered]');
 has 'batch_size'          => (is => 'rw', isa => 'Int', default => 1);
-has '_shuffle'            => (is => 'rw', init_arg => 'shuffle', isa => 'Bool', default => 0);
+has 'shuffle'             => (is => 'rw', isa => 'Bool', default => 0);
 has 'last_batch_handle'   => (is => 'rw', isa => 'Str', default => 'pad');
 has 'label_name'          => (is => 'rw', isa => 'Str', default => 'softmax_label');
 has 'num_source'          => (is => 'rw', isa => 'Int');
 has 'cursor'              => (is => 'rw', isa => 'Int');
 has 'num_data'            => (is => 'rw', isa => 'Int');
 
-around BUILDARGS => sub {
-    my $orig  = shift;
-    my $class = shift;
-    if(@_%2)
-    {
-        my $data  = shift;
-        return $class->$orig(data => $data, @_);
-    }
-    return $class->$orig(@_);
-};
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { ['data', 'label'] };
 
 sub BUILD
 {
@@ -458,9 +462,9 @@ sub BUILD
     my $num_data  = $data->[0][1]->shape->[0];
     confess("size of data dimension 0 $num_data < batch_size ${\ $self->batch_size }")
         unless($num_data >= $self->batch_size);
-    if($self->_shuffle)
+    if($self->shuffle)
     {
-        my @idx = shuffle(0..$num_data-1);
+        my @idx = List::Util::shuffle(0..$num_data-1);
         $_->[1] = AI::MXNet::NDArray->array(pdl_shuffle($_->[1]->aspdl, \@idx)) for @$data;
         $_->[1] = AI::MXNet::NDArray->array(pdl_shuffle($_->[1]->aspdl, \@idx)) for @$label;
     }
@@ -596,7 +600,6 @@ method getpad()
         return 0;
     }
 }
-
 package AI::MXNet::MXDataIter;
 use Mouse;
 use AI::MXNet::Base;
@@ -668,6 +671,7 @@ method reset()
     check_call(AI::MXNetCAPI::DataIterBeforeFirst($self->handle));
 }
 
+
 method next()
 {
     if($self->_debug_skip_load and not $self->_debug_at_begin)
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Image.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Image.pm
index 18ef42af5525..3cc71df7ed01 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Image.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Image.pm
@@ -27,6 +27,49 @@ use AI::MXNet::Function::Parameters;
     AI::MXNet:Image - Read individual image files and perform augmentations.
 =cut
 
+=head2 imread
+
+    Read and decode an image to an NDArray.
+
+    Note: `imread` uses OpenCV.
+    MXNet must have been built with USE_OPENCV=1 for `imdecode` to work.
+
+    Parameters
+    ----------
+    $filename : str
+        Name of the image file to be loaded.
+    :$flag : int
+        0 for grayscale. 1 for colored.
+    :$to_rgb : int
+        0 for BGR format (OpenCV default). 1 for RGB format (MXNet default).
+    :$out : NDArray
+        Output buffer. Do not specify for automatic allocation.
+
+    Returns
+    -------
+    An NDArray containing the image.
+
+    Example
+    -------
+    >>> mx->img->imread("flower.jpg");
+    <NDArray 224x224x3 @cpu(0)>
+
+    Set `flag` parameter to 0 to get grayscale output
+
+    >>> mx->img->imdecode("flower.jpg", flag=>0);
+    <NDArray 224x224x1 @cpu(0)>
+
+    Set `to_rgb` parameter to 0 to get output in OpenCV format (BGR)
+
+    >>> mx->img->imdecode($str_image, to_rgb=>0);
+    <NDArray 224x224x3 @cpu(0)>
+=cut
+
+method imread(Str $filename, Int :$flag=1, Int :$to_rgb=1, Maybe[AI::MXNet::NDArray] :$out=)
+{
+    return AI::MXNet::NDArray->_cvimread($filename, { flag => $flag, to_rgb => $to_rgb, ($out ? (out => $out) : ()) });
+}
+
 =head2 imdecode
 
     Decode an image from string. Requires OpenCV to work.
@@ -396,7 +439,7 @@ method RandomOrderAug(ArrayRef[CodeRef] $ts)
         my @tmp;
         for my $t (@ts)
         {
-            push @tmp, &{$t}($src);
+            push @tmp, $t->($src);
         }
         return \@tmp;
     };
@@ -649,6 +692,11 @@ Int            :$inter_method=2
     return \@auglist;
 }
 
+method imresize(AI::MXNet::NDArray $src, Int $w, Int $h, Int $interp=2)
+{
+    return AI::MXNet::NDArray->_cvimresize($src, $w, $h, { interp=>$interp });
+}
+
 method ImageIter(@args) { AI::MXNet::ImageIter->new(@args) }
 
 package AI::MXNet::ImageIter;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Initializer.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Initializer.pm
index 182327dfccfe..4397b4e39ba5 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Initializer.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Initializer.pm
@@ -151,9 +151,18 @@ method call(Str|AI::MXNet::InitDesc $desc, AI::MXNet::NDArray $arr)
     my $init = $desc->attrs->{ __init__ };
     if($init)
     {
-      my ($klass, $kwargs) = @{ decode_json($init) };
-      $self->get_init_registry->{ lc $klass }->new(%{ $kwargs })->_init_weight("$desc", $arr);
-      $self->_verbose_print($desc, $init, $arr);
+        my ($klass, $kwargs);
+        if(exists $self->get_init_registry->{ lc $init })
+        {
+            $klass = $init;
+            $kwargs = {};
+        }
+        else
+        {
+            ($klass, $kwargs) = @{ decode_json($init) };
+        }
+        $self->get_init_registry->{ lc $klass }->new(%{ $kwargs })->_init_weight("$desc", $arr);
+        $self->_verbose_print($desc, $init, $arr);
     }
     else
     {
@@ -398,7 +407,7 @@ method call(Str $name, AI::MXNet::NDArray $arr)
     {
         if($name =~ /$pattern/)
         {
-            &{$self->map->{$pattern}}($name, $arr);
+            $self->map->{$pattern}->($name, $arr);
             return;
         }
     }
@@ -418,6 +427,12 @@ method _init_weight(Str $name, AI::MXNet::NDArray $arr)
 
 __PACKAGE__->register;
 
+package AI::MXNet::Zeros;
+use Mouse;
+extends 'AI::MXNet::Zero';
+
+__PACKAGE__->register;
+
 package AI::MXNet::One;
 use Mouse;
 extends 'AI::MXNet::Initializer';
@@ -428,6 +443,12 @@ method _init_weight(Str $name, AI::MXNet::NDArray $arr)
 
 __PACKAGE__->register;
 
+package AI::MXNet::Ones;
+use Mouse;
+extends 'AI::MXNet::One';
+
+__PACKAGE__->register;
+
 package AI::MXNet::Constant;
 use Mouse;
 extends 'AI::MXNet::Initializer';
@@ -603,6 +624,8 @@ has "factor_type" => (is => "ro", isa => enum([qw/avg in out/]), default => 'avg
 method _init_weight(Str $name, AI::MXNet::NDArray $arr)
 {
     my @shape = @{ $arr->shape };
+    confess(__PACKAGE__." initializer can not be applied on less than 2D tensor")
+        if @shape < 2;
     my $hw_scale = 1;
     if(@shape > 2)
     {
@@ -717,6 +740,8 @@ package AI::MXNet::LSTMBias;
 use Mouse;
 extends 'AI::MXNet::Initializer';
 has 'forget_bias' => (is => 'ro', isa => 'Num', required => 1);
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
+method python_constructor_arguments() { ['forget_bias'] }
 
 method _init_weight(Str $name, AI::MXNet::NDArray $arr)
 {
@@ -801,7 +826,7 @@ method _init_weight($name, $arr)
         }
         else
         {
-            &{$self->init}($desc, $args->{$name});
+            $self->init->($desc, $args->{$name});
         }
     }
 
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/KVStore.pm b/perl-package/AI-MXNet/lib/AI/MXNet/KVStore.pm
index fa74e4ae4dd1..4410eb3d7a7a 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/KVStore.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/KVStore.pm
@@ -309,14 +309,17 @@ method num_workers()
     ----------
     fname : str
         Path to output states file.
+    dump_optimizer : bool, default False
+            Whether to also save the optimizer itself. This would also save optimizer
+            information such as learning rate and weight decay schedules.
 =cut
 
-method save_optimizer_states(Str $fname)
+method save_optimizer_states(Str $fname, Bool :$dump_optimizer=0)
 {
     confess("Cannot save states for distributed training")
         unless defined $self->_updater;
     open(F, ">:raw", "$fname") or confess("can't open $fname for writing: $!");
-    print F $self->_updater->get_states();
+    print F $self->_updater->get_states($dump_optimizer);
     close(F);
 }
 
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Logging.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Logging.pm
index f3039cc09bfd..839b456e1ef1 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Logging.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Logging.pm
@@ -17,8 +17,11 @@
 
 package AI::MXNet::Logging;
 ## TODO
+use strict;
+use warnings;
 use Mouse;
-sub warning { shift; warn sprintf(shift, @_) . "\n" };
+our $silent = 0;
+sub warning { return if $silent; shift; warn sprintf(shift, @_) . "\n" };
 *debug   = *info = *warning;
 sub get_logger { __PACKAGE__->new }
 
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Metric.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Metric.pm
index c3a3183432d5..a6b440be6eb3 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Metric.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Metric.pm
@@ -20,6 +20,7 @@ use strict;
 use warnings;
 use AI::MXNet::Function::Parameters;
 use Scalar::Util qw/blessed/;
+use JSON::PP;
 
 =head1 NAME
 
@@ -77,10 +78,37 @@ has 'name'       => (is => 'rw', isa => 'Str');
 has 'num'        => (is => 'rw', isa => 'Int');
 has 'num_inst'   => (is => 'rw', isa => 'Maybe[Int|ArrayRef[Int]]');
 has 'sum_metric' => (is => 'rw', isa => 'Maybe[Num|ArrayRef[Num]]');
+has '_kwargs'    => (is => 'rw', init_arg => undef);
+around BUILDARGS => \&AI::MXNet::Base::process_arguments;
 
 sub BUILD
 {
-    shift->reset;
+    my ($self, $kwargs) = @_;
+    $self->reset;
+    $self->_kwargs($kwargs);
+}
+
+method _class_name()
+{
+    my $class = ref $self || $self;
+    $class =~ s/^.+:://;
+    $class;
+}
+
+=head2 get_config
+
+    Save configurations of metric. Can be recreated
+        from configs with mx->metric->create(%{ $config })
+=cut
+
+method get_config()
+{
+    my %config = %{ $self->_kwargs };
+    %config = (%config,
+        metric => $self->_class_name,
+        name   => $self->name
+    );
+    return \%config;
 }
 
 method update($label, $pred)
@@ -151,6 +179,7 @@ use Mouse;
 extends 'AI::MXNet::EvalMetric';
 has 'metrics' => (is => 'rw', isa => 'ArrayRef[AI::MXNet::EvalMetric]', default => sub { [] });
 has '+name'   => (default => 'composite');
+method python_constructor_arguments() { ['metrics'] }
 
 # Add a child metric.
 method add(AI::MXNet::EvalMetric $metric)
@@ -232,6 +261,7 @@ use AI::MXNet::Base;
 extends 'AI::MXNet::EvalMetric';
 has '+name'   => (default => 'top_k_accuracy');
 has 'top_k' => (is => 'rw', isa => 'int', default => 1);
+method python_constructor_arguments() { ['top_k'] }
 
 sub BUILD
 {
@@ -344,6 +374,8 @@ extends 'AI::MXNet::EvalMetric';
 has '+name'        => (default => 'Perplexity');
 has 'ignore_label' => (is => 'ro', isa => 'Maybe[Int]');
 has 'axis'         => (is => 'ro', isa => 'Int', default => -1);
+method python_constructor_arguments() { ['ignore_label', 'axis'] }
+
 around BUILDARGS => sub {
     my $orig  = shift;
     my $class = shift;
@@ -483,13 +515,8 @@ use Mouse;
 use AI::MXNet::Base;
 extends 'AI::MXNet::EvalMetric';
 has '+name'   => (default => 'cross-entropy');
-has 'eps'     => (is => 'ro', isa => 'Num', default => 1e-8);
-around BUILDARGS => sub {
-    my $orig  = shift;
-    my $class = shift;
-    return $class->$orig(eps => $_[0]) if @_ == 1;
-    return $class->$orig(@_);
-};
+has 'eps'     => (is => 'ro', isa => 'Num', default => 1e-12);
+method python_constructor_arguments() { ['eps'] }
 
 method update(ArrayRef[AI::MXNet::NDArray] $labels, ArrayRef[AI::MXNet::NDArray] $preds)
 {
@@ -559,6 +586,140 @@ method update(ArrayRef[AI::MXNet::NDArray] $labels, ArrayRef[AI::MXNet::NDArray]
     }, $labels, $preds);
 }
 
+package AI::MXNet::Loss;
+use Mouse;
+use AI::MXNet::Base;
+extends 'AI::MXNet::EvalMetric';
+has '+name'   => (default => 'loss');
+
+=head1 NAME
+
+    AI::MXNet::Loss
+=cut
+
+=head1 DESCRIPTION
+
+    Dummy metric for directly printing loss.
+
+    Parameters
+    ----------
+    name : str
+        Name of this metric instance for display.
+=cut
+
+method update($labels, ArrayRef[AI::MXNet::NDArray] $preds)
+{
+    for my $pred (@{ $preds })
+    {
+        $self->sum_metric($self->sum_metric + $pred->sum->asscalar);
+        $self->num_inst($self->num_inst + $pred->size);
+    }
+}
+
+package AI::MXNet::Confidence;
+use Mouse;
+
+=head1 NAME
+
+    AI::MXNet::Confidence
+=cut
+
+=head1 DESCRIPTION
+
+    Accuracy by confidence buckets.
+
+    Parameters
+    ----------
+    name : str
+        Name of this metric instance for display.
+    num_classes: Int
+        number of classes
+    confidence_thresholds: ArrayRef[Num]
+        confidence buckets
+    For example
+    my $composite_metric  = AI::MXNet::CompositeEvalMetric->new;
+    $composite_metric->add(mx->metric->create('acc'));
+    $composite_metric->add(
+        AI::MXNet::Confidence->new(
+            num_classes => 2,
+            confidence_thresholds => [ 0.5, 0.7, 0.8, 0.9 ],
+        )
+    );
+=cut
+
+extends 'AI::MXNet::EvalMetric';
+has 'num_classes', is => 'ro', isa => 'Int', required => 1;
+has 'confidence_thresholds', is => 'ro', isa => 'ArrayRef[Num]', required => 1;
+has '+name'   => (default => 'confidence');
+has '+sum_metric', isa => 'PDL';
+has '+num_inst', isa => 'PDL';
+method python_constructor_arguments() { ['num_classes', 'confidence_thresholds'] }
+
+sub _hot
+{
+    my($m, $n) = @_;
+    my $md = $m->dim(-1);
+    my $hot = PDL->zeros($n, $md);
+    $hot->index2d($m->flat(), PDL->sequence($md)) .= 1;
+    return $hot;
+}
+
+sub reset
+{
+    my($self) = @_;
+    my $nt = @{$self->confidence_thresholds};
+    my $n = $self->num_classes;
+    $self->sum_metric(PDL->zeroes($nt, $n));
+    $self->num_inst(PDL->zeroes($nt, $n));
+    return;
+}
+
+sub update
+{
+    my($self, $labels, $preds) = @_;
+    my $n = $self->num_classes;
+    my $ct = PDL->new($self->confidence_thresholds);
+    my $nt = $ct->nelem;
+    for(0 .. @$labels - 1)
+    {
+        my $label = _hot($labels->[$_]->aspdl, $n);
+        my $pred = $preds->[$_]->aspdl;
+        for my $c (0 .. $n - 1)
+        {
+            my $ls = $label->slice($c);
+            my $pm = $pred->slice($c) > $ct;
+            $self->sum_metric->slice(":,$c") += ($pm & $ls);
+            $self->num_inst->slice(":,$c") += $pm;
+        }
+    }
+    return;
+}
+
+sub get
+{
+    my($self) = @_;
+    my(@names, @values);
+    my $val = $self->sum_metric / $self->num_inst;
+    my $ct = $self->confidence_thresholds;
+    my $n = $self->num_classes;
+    for my $c (0 .. $n - 1)
+    {
+        for my $t (0 .. @$ct - 1)
+        {
+            my $sm = $self->sum_metric->at($t, $c);
+            my $ni = $self->num_inst->at($t, $c);
+            push @names, "P(v=$c|Conf>$ct->[$t])=($sm/$ni)";
+            push @values, $val->at($t, $c);
+        }
+    }
+    return(\@names, \@values);
+}
+
+=head1 NAME
+
+    AI::MXNet::CustomMetric
+=cut
+
 =head1 DESCRIPTION
 
     Custom evaluation metric that takes a sub ref.
@@ -582,6 +743,7 @@ use AI::MXNet::Base;
 extends 'AI::MXNet::EvalMetric';
 has 'eval_function'       => (is => 'ro', isa => 'CodeRef');
 has 'allow_extra_outputs' => (is => 'ro', isa => 'Int', default => 0);
+method python_constructor_arguments() { ['eval_function', 'allow_extra_outputs'] }
 
 method update(ArrayRef[AI::MXNet::NDArray] $labels, ArrayRef[AI::MXNet::NDArray] $preds)
 {
@@ -613,22 +775,28 @@ package AI::MXNet::Metric;
 =cut
 
 my %metrics = qw/
-    acc            AI::MXNet::Accuracy
-    accuracy       AI::MXNet::Accuracy
-    ce             AI::MXNet::CrossEntropy
-    f1             AI::MXNet::F1
-    mae            AI::MXNet::MAE
-    mse            AI::MXNet::MSE
-    rmse           AI::MXNet::RMSE
-    top_k_accuracy AI::MXNet::TopKAccuracy
-    Perplexity     AI::MXNet::Perplexity
-    perplexity     AI::MXNet::Perplexity
-    pearsonr       AI::MXNet::PearsonCorrelation
+    acc                 AI::MXNet::Accuracy
+    accuracy            AI::MXNet::Accuracy
+    ce                  AI::MXNet::CrossEntropy
+    crossentropy        AI::MXNet::CrossEntropy
+    f1                  AI::MXNet::F1
+    mae                 AI::MXNet::MAE
+    mse                 AI::MXNet::MSE
+    rmse                AI::MXNet::RMSE
+    top_k_accuracy      AI::MXNet::TopKAccuracy
+    topkaccuracy        AI::MXNet::TopKAccuracy
+    perplexity          AI::MXNet::Perplexity
+    pearsonr            AI::MXNet::PearsonCorrelation
+    pearsoncorrelation  AI::MXNet::PearsonCorrelation
+    loss                AI::MXNet::Loss
+    compositeevalmetric AI::MXNet::CompositeEvalMetric
+    confidence          AI::MXNet::Confidence
 /;
 
-method create(Metric|ArrayRef[Metric] $metric, %kwargs)
+method create(Metric|ArrayRef[Metric] $metric, @kwargs)
 {
     Carp::confess("metric must be defined") unless defined $metric;
+    return $metric if blessed $metric and $metric->isa('AI::MXNet::EvalMetric');
     if(my $ref = ref $metric)
     {
         if($ref eq 'ARRAY')
@@ -636,23 +804,29 @@ method create(Metric|ArrayRef[Metric] $metric, %kwargs)
             my $composite_metric = AI::MXNet::CompositeEvalMetric->new();
             for my $child_metric (@{ $metric })
             {
-                $composite_metric->add(__PACKAGE__->create($child_metric, %kwargs))
+                $composite_metric->add(__PACKAGE__->create($child_metric, @kwargs))
             }
             return $composite_metric;
         }
         else
         {
-            return AI::MXNet::CustomMetric->new(eval_function => $metric, %kwargs);
+            return AI::MXNet::CustomMetric->new(eval_function => $metric, @kwargs);
         }
     }
     else
     {
-        if(not exists $metrics{ lc($metric) })
+        if(not exists $metrics{ lc($metric) } and not $metric =~ /^{/)
         {
             my @metrics = keys %metrics;
             Carp::confess("Metric must be either subref or one of [@metrics]");
         }
-        return $metrics{ lc($metric) }->new(%kwargs);
+        if($metric =~ /^{/ and not @kwargs)
+        {
+            my $config = decode_json($metric);
+            $metric = delete $config->{metric};
+            @kwargs = %{ $config };
+        }
+        return $metrics{ lc($metric) }->new(@kwargs);
     }
 }
 
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Module.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Module.pm
index 3e4d938bf4e9..a1aa1b2f9769 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Module.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Module.pm
@@ -147,7 +147,7 @@ func _update_params(
             # faked an index here, to make optimizer create diff
             # state for the same index but on diff devs, TODO(mli)
             # use a better solution later
-            &{$updater}($index*$num_device+$k, $g, $w);
+            $updater->($index*$num_device+$k, $g, $w);
         }, $arg_list, $grad_list);
     }, $param_arrays, $grad_arrays);
 }
@@ -457,13 +457,13 @@ method init_params(
                     }
                     if(defined $initializer)
                     {
-                        &{$initializer}($name, $arr);
+                        $initializer->($name, $arr);
                     }
                 }
             }
             else
             {
-                &{$initializer}($name, $arr) if defined $initializer;
+                $initializer->($name, $arr) if defined $initializer;
             }
     };
     my $attrs = $self->_symbol->attr_dict;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Module/Base.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Module/Base.pm
index 8df52eb1e98f..4deca466c1a6 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Module/Base.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Module/Base.pm
@@ -15,6 +15,8 @@
 # specific language governing permissions and limitations
 # under the License.
 
+use strict;
+use warnings;
 package AI::MXNet::BatchEndParam;
 use Mouse;
 use AI::MXNet::Function::Parameters;
@@ -81,12 +83,13 @@ method _check_names_match(
 )
 {
     return if (not @$data_shapes and @$data_names == 1 and  $data_names->[0] eq 'softmax_label');
-    my @actual = map { @{$_}[0] } @{ $data_shapes };
-    if("@$data_names" ne "@actual")
+    my @actual = sort map { @{$_}[0] } @{ $data_shapes };
+    my @data_names = sort @$data_names;
+    if("@data_names" ne "@actual")
     {
         my $msg = sprintf(
             "Data provided by %s_shapes don't match names specified by %s_names (%s vs. %s)",
-            $name, $name, "@$data_shapes", "@$data_names"
+            $name, $name, "@actual", "@data_names"
         );
         if($throw)
         {
@@ -283,7 +286,7 @@ method score(
             );
             for my $callback (@{ _as_list($batch_end_callback) })
             {
-                &{$callback}($batch_end_params);
+                $callback->($batch_end_params);
             }
         }
         $actual_num_batch++;
@@ -298,7 +301,7 @@ method score(
         );
         for my $callback (@{ _as_list($score_end_callback) })
         {
-            &{callback}($params);
+            $callback->($params);
         }
     }
     return $eval_metric->get_name_value;
@@ -567,7 +570,7 @@ method fit(
                 );
                 for my $callback (@{ _as_list($batch_end_callback) })
                 {
-                    &{$callback}($batch_end_params);
+                    $callback->($batch_end_params);
                 }
             }
             $nbatch++;
@@ -589,7 +592,7 @@ method fit(
         {
             for my $callback (@{ _as_list($epoch_end_callback) })
             {
-                &{$callback}($epoch, $self->get_symbol, $arg_params, $aux_params);
+                $callback->($epoch, $self->get_symbol, $arg_params, $aux_params);
             }
         }
         #----------------------------------------
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Module/Bucketing.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Module/Bucketing.pm
index 531f41d58a3a..aa495674faa1 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Module/Bucketing.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Module/Bucketing.pm
@@ -156,7 +156,7 @@ sub BUILD
     $self->_fixed_param_names([]) unless defined $original_params->{fixed_param_names};
     $self->_state_names([]) unless defined $original_params->{state_names};
     $self->_params_dirty(0);
-    my ($symbol, $data_names, $label_names) = &{$self->_sym_gen}($self->_default_bucket_key);
+    my ($symbol, $data_names, $label_names) = $self->_sym_gen->($self->_default_bucket_key);
     $self->_check_input_names($symbol, $data_names//[], "data", 1);
     $self->_check_input_names($symbol, $label_names//[], "label", 0);
     $self->_check_input_names($symbol, $self->_state_names, "state", 1);
@@ -179,7 +179,7 @@ method data_names()
     }
     else
     {
-        return (&{$self->_sym_gen}($self->_default_bucket_key))[1];
+        return ($self->_sym_gen->($self->_default_bucket_key))[1];
     }
 }
 
@@ -191,7 +191,7 @@ method output_names()
     }
     else
     {
-        my ($symbol) = &{$self->_sym_gen}($self->_default_bucket_key);
+        my ($symbol) = $self->_sym_gen->($self->_default_bucket_key);
         return $symbol->list_ouputs;
     }
 }
@@ -356,7 +356,7 @@ method bind(
     $self->inputs_need_grad($inputs_need_grad);
     $self->binded(1);
 
-    my ($symbol, $data_names, $label_names) = &{$self->_sym_gen}($bucket_key//$self->_default_bucket_key);
+    my ($symbol, $data_names, $label_names) = $self->_sym_gen->($bucket_key//$self->_default_bucket_key);
     my $module = AI::MXNet::Module->new(
             symbol            => $symbol,
             data_names        => $data_names,
@@ -410,7 +410,7 @@ method switch_bucket(
     assert($self->binded, 'call bind before switching bucket');
     if(not exists $self->_buckets->{ $bucket_key })
     {
-        my ($symbol, $data_names, $label_names) = &{$self->_sym_gen}($bucket_key);
+        my ($symbol, $data_names, $label_names) = $self->_sym_gen->($bucket_key);
         my $module = AI::MXNet::Module->new(
             symbol         => $symbol,
             data_names     => $data_names,
@@ -546,4 +546,29 @@ method install_monitor(AI::MXNet::Monitor $mon)
     }
 }
 
+=head2 save_checkpoint
+
+    Save current progress to a checkpoint.
+    Use mx->callback->module_checkpoint as epoch_end_callback to save during training.
+
+    Parameters
+    ----------
+    prefix : str
+        The file prefix to checkpoint to
+    epoch : int
+        The current epoch number
+    save_optimizer_states : bool
+        Whether to save optimizer states for later training
+=cut
+
+
+method save_checkpoint(Str $prefix, Int $epoch, Bool $save_optimizer_states=0)
+{
+    my %buckets = %{ $self->_buckets };
+    while(my ($key, $module) = each %buckets)
+    {
+        $module->save_checkpoint("${prefix}_$key", $epoch, $save_optimizer_states);
+    }
+}
+
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray.pm b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray.pm
index 36a13147795f..7193f526b892 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray.pm
@@ -49,7 +49,8 @@ use overload
     '<'  => \&lesser,
     '<=' => \&lesser_equal,
     '.=' => \&set,
-    '=' => sub { $_[0] };
+    '@{}'=> \&split_array,
+    '='  => sub { $_[0] };
 
 extends 'AI::MXNet::NDArray::Base';
 has 'writable' => (is => 'rw', isa => 'Int', default => 1, lazy => 1);
@@ -77,6 +78,11 @@ method STORABLE_thaw($cloning, $buf, $writable)
     $self->writable($$writable);
 }
 
+method split_array(@args)
+{
+     $self->shape->[0] > 1 ? $self->split(num_outputs => $self->shape->[0], squeeze_axis => 1, axis => 0) : [$self];
+}
+
 method at(Index @indices)
 {
     confess("No idxs supplied") unless @indices;
@@ -106,9 +112,31 @@ method at(Index @indices)
     return $self->slice(@indices);
 }
 
-method slice(Slice @slices)
+method len() { $self->shape->[0] }
+
+method slice(Slice|AdvancedSlice @slices)
 {
     confess("No slices supplied") unless @slices;
+    if(ref $slices[0] eq 'ARRAY' and ref $slices[0]->[0])
+    {
+        my @indices;
+        my $key = $slices[0];
+        my $dtype = 'int32';
+        for my $idx_i (@{ $key })
+        {
+            if(not (blessed $idx_i and $idx_i->isa(__PACKAGE__)))
+            {
+                $idx_i = __PACKAGE__->array($idx_i, ctx=>$self->context, dtype=>$dtype);
+            }
+            else
+            {
+                $dtype = $idx_i->dtype;
+            }
+            push @indices, $idx_i;
+        }
+        my $indices = __PACKAGE__->stack(@indices);
+        return __PACKAGE__->gather_nd($self, $indices);
+    }
     my $shape = $self->shape;
     my $dsize = @$shape;
     my $isize = @slices;
@@ -121,7 +149,7 @@ method slice(Slice @slices)
     my $i = -1;
     @slices = map {
         ++$i;
-        ref $_ ? (@$_ == 1 ? [$_->[0], $shape->[$i] - 1] : $_) : ($_ eq 'X' ? [0, $shape->[$i] - 1] : [$_, $_]);
+        ref $_ ? (@$_ == 1 ? [$_->[0], $_->[0]] : $_) : ($_ eq 'X' ? [0, $shape->[$i] - 1] : [$_, $_]);
     } @slices;
     zip(sub {
         my ($slice, $dim_size) = @_;
@@ -156,7 +184,7 @@ method set(AcceptableInput $value, $reverse=)
     ## plain number
     if(not ref $value)
     {
-        $self->_set_value($value, { out => $self });
+        $self->_set_value($value, out => $self);
     }
     # ndarray
     elsif(blessed($value) and $value->isa(__PACKAGE__))
@@ -180,6 +208,15 @@ method asscalar()
 {
     confess("ndarray size must be 1") unless $self->size == 1;
     return $self->aspdl->at(0);
+    ## code below works happily on CPU/segfaults on GPU
+    #$self->wait_to_read;
+    #my $perl_pack_type = DTYPE_MX_TO_PERL->{$self->dtype};
+    #my $length = {qw/f 4 d 8 S 2 C 1 l 4/}->{$perl_pack_type};
+    #return
+    #(map {
+    #        $perl_pack_type eq 'S' ? AI::MXNetCAPI::_half_to_float($_) : $_
+    #     } unpack("$perl_pack_type", check_call(AI::MXNetCAPI::NDArrayGetData($self->handle, $length)))
+    #)[0];
 }
 
 method _sync_copyfrom(ArrayRef|PDL|PDL::Matrix $source_array)
@@ -342,9 +379,16 @@ method reshape(ArrayRef[Int] $new_shape)
     my $i = -1;
     my @inferred = map { $i++; $_ == -1 ? ($i) : () } @$new_shape;
     assert((@inferred <= 1), 'Only one dimension can be inferred.');
+    $i = -1;
+    my @keep = map { $i++; $_ == 0 ? ($i) : () } @$new_shape;
+    my $shape = $self->shape;
+    if(@keep)
+    {
+        @{$new_shape}[@keep] = @{$shape}[@keep];
+    }
     if(@inferred)
     {
-        $new_shape->[$inferred[0]] = product(@{ $self->shape })/product(map { abs($_) } @{ $new_shape });
+        $new_shape->[$inferred[0]] = product(@{ $shape })/product(map { abs($_) } @{ $new_shape });
     }
     my $handle = check_call(
                     AI::MXNetCAPI::NDArrayReshape(
@@ -953,7 +997,9 @@ method zeros(
     Shape $shape,
     AI::MXNet::Context :$ctx=AI::MXNet::Context->current_ctx,
     Dtype :$dtype='float32',
-    Maybe[AI::MXNet::NDArray] :$out=
+    Maybe[AI::MXNet::NDArray] :$out=,
+    Maybe[Str] :$name=,
+    Maybe[Str] :$__layout__=
 )
 {
     return __PACKAGE__->_zeros({ shape => $shape, ctx => "$ctx", dtype => $dtype, ($out ? (out => $out) : ())  });
@@ -984,7 +1030,9 @@ method ones(
     Shape $shape,
     AI::MXNet::Context :$ctx=AI::MXNet::Context->current_ctx,
     Dtype :$dtype='float32',
-    Maybe[AI::MXNet::NDArray] :$out=
+    Maybe[AI::MXNet::NDArray] :$out=,
+    Maybe[Str] :$name=,
+    Maybe[Str] :$__layout__=
 )
 {
     return __PACKAGE__->_ones({ shape => $shape, ctx => "$ctx", dtype => $dtype, ($out ? (out => $out) : ()) });
@@ -1017,7 +1065,9 @@ method ones(
 method full(
     Shape $shape, Num $val,
     AI::MXNet::Context :$ctx=AI::MXNet::Context->current_ctx,
-    Dtype :$dtype='float32', Maybe[AI::MXNet::NDArray] :$out=
+    Dtype :$dtype='float32', Maybe[AI::MXNet::NDArray] :$out=,
+    Maybe[Str] :$name=,
+    Maybe[Str] :$__layout__=
 )
 {
     return __PACKAGE__->_set_value({ src => $val, out => $out ? $out : __PACKAGE__->empty($shape, ctx => $ctx, dtype => $dtype) });
@@ -1151,7 +1201,7 @@ method concatenate(ArrayRef[AI::MXNet::NDArray] $arrays, Index :$axis=0, :$alway
     ----------
     :$start=0 : number, optional
         Start of interval. The interval includes this value. The default start value is 0.
-    $stop= : number, optional
+    :$stop= : number, optional
         End of interval. The interval does not include this value.
     :$step=1 : number, optional
         Spacing between the values
@@ -1169,7 +1219,7 @@ method concatenate(ArrayRef[AI::MXNet::NDArray] $arrays, Index :$axis=0, :$alway
         The created NDArray
 =cut
 
-method arange(Index :$start=0, Index :$stop=, Index :$step=1, Index :$repeat=1,
+method arange(Index :$start=0, Maybe[Index] :$stop=, Index :$step=1, Index :$repeat=1,
               AI::MXNet::Context :$ctx=AI::MXNet::Context->current_ctx, Dtype :$dtype='float32')
 {
     return __PACKAGE__->_arange({
@@ -1397,14 +1447,84 @@ method detach()
     return __PACKAGE__->new(handle => $handle);
 }
 
-method backward(Maybe[AI::MXNet::NDArray] $out_grad=, Bool $retain_graph=0)
+=head2 attach_grad
+
+        Attach a gradient buffer to this NDArray, so that `backward`
+        can compute gradient with respect to it.
+
+        Parameters
+        ----------
+        GradReq :$grad_req='write' : {'write', 'add', 'null'}
+            How gradient will be accumulated.
+            - 'write': gradient will be overwritten on every backward.
+            - 'add': gradient will be added to existing value on every backward.
+            - 'null': do not compute gradient for this NDArray.
+        Maybe[Str] :$stype= : str, optional
+            The storage type of the gradient array. Defaults to the same stype of this NDArray.
+=cut
+
+method attach_grad(GradReq :$grad_req='write', Maybe[Str] :$stype=)
 {
+    my $grad;
+    if(defined $stype)
+    {
+        $grad = __PACKAGE__->_zeros($self->shape, stype=>$stype);
+    }
+    else
+    {
+        $grad = $self->zeros_like;
+    }
+    $grad_req = GRAD_REQ_MAP->{$grad_req};
     check_call(
-        AI::MXNetCAPI::AutogradBackward(
+        AI::MXNetCAPI::AutogradMarkVariables(
+            1,
+            [$self->handle],
+            [$grad_req],
+            [$grad->handle]
+        )
+    );
+}
+
+=head2 grad
+
+    Returns gradient buffer attached to this NDArray.
+=cut
+
+method grad()
+{
+    my $handle = check_call(AI::MXNetCAPI::NDArrayGetGrad($self->handle));
+    return undef unless defined $handle;
+    return __PACKAGE__->new(handle => $handle);
+}
+
+=head2 backward
+
+    Compute the gradients of this NDArray w.r.t variables.
+
+    Parameters
+    ----------
+    :$out_grad= : NDArray, optional
+        Gradient with respect to head.
+    :$retain_graph=0 : bool, optional
+        Whether to retain the computaion graph for another backward
+        pass on the same graph. By default the computaion history
+        is cleared.
+    :$train_mode=1 : bool, optional
+        Whether to compute gradient for training or inference.
+=cut
+
+method backward(Maybe[AI::MXNet::NDArray] :$out_grad=, Bool :$retain_graph=0, Bool :$train_mode=1)
+{
+    check_call(
+        AI::MXNetCAPI::AutogradBackwardEx(
             1,
             [$self->handle],
             [defined $out_grad ? $out_grad->handle : undef],
-            $retain_graph
+            0,
+            [],
+            $retain_graph,
+            0,
+            $train_mode
         )
     )
 }
@@ -1421,4 +1541,7 @@ eval << "EOV" if ($^V and $^V >= 5.006007);
 }
 EOV
 
+sub contrib { 'AI::MXNet::Contrib::NDArray' }
+sub random  { 'AI::MXNet::Random' }
+
 __PACKAGE__->meta->make_immutable;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Base.pm b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Base.pm
index b51436157a82..8efe9d740cc3 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Base.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Base.pm
@@ -60,33 +60,66 @@ func _make_ndarray_function($handle, $func_name)
                             $key_var_num_args,
                             $ret_type
     );
+    my %ndarguments;
     my @arguments;
+    my %arguments = (out => 1, name => 1, ctx => 1, shape => 1);
+    my $j = 0;
     for my $i (0..(@$arg_names-1))
     {
         if(not $arg_types->[$i] =~ /^(?:NDArray|Symbol|ndarray\-or\-symbol)/)
         {
             push @arguments, $arg_names->[$i];
+            $arguments{ $arg_names->[$i] } = 1;
+        }
+        else
+        {
+            $ndarguments{ $arg_names->[$i] } = $j++;
         }
     }
     my $generic_ndarray_function = sub
     {
         my $class = shift;
-        my (@args, %kwargs);
+        my (@args, %kwargs, %ndkwargs, @tmp);
         if(@_ and ref $_[-1] eq 'HASH')
         {
             %kwargs = %{ pop(@_) };
         }
-        @args = @_;
-        if(ref $class)
+        else
         {
-            @args = ($class) if not @args;
-            $class = ref $class;
+            while(@_ >= 2 and not ref $_[-2])
+            {
+                if(exists $arguments{ $_[-2] })
+                {
+                    my $v = pop(@_);
+                    my $k = pop(@_);
+                    $kwargs{ $k } = $v;
+                }
+                elsif(exists $ndarguments{ $_[-2] })
+                {
+                    my $v = pop(@_);
+                    my $k = pop(@_);
+                    $ndkwargs{ $k } = $v;
+                }
+                else
+                {
+                    unshift(@tmp, pop(@_));
+                    unshift(@tmp, pop(@_));
+                }
+            }
+        }
+        @args = (@_, @tmp);
+        if(%ndkwargs)
+        {
+            for my $k (keys %ndkwargs)
+            {
+                $args[$ndarguments{$k}] = $ndkwargs{$k};
+            }
         }
         my @ndargs;
         my @pos_args;
         for my $i (@args)
         {
-            if(blessed($i) and $i->isa($class))
+            if(blessed($i) and $i->isa(__PACKAGE__))
             {
                 push @ndargs, $i->handle;
             }
@@ -96,12 +129,13 @@ func _make_ndarray_function($handle, $func_name)
             }
             if(@pos_args > @arguments)
             {
-                die "Too many positional arguments";
+                confess("Too many positional arguments");
             }
         }
         @kwargs{ @arguments[0..$#pos_args] } = @pos_args;
         my $original_output;
         my $output_vars;
+        delete $kwargs{name};
         if(grep { $_ eq 'out' } keys %kwargs)
         {
             $output_vars = delete $kwargs{out};
@@ -115,6 +149,11 @@ func _make_ndarray_function($handle, $func_name)
         {
             $output_vars = [];
         }
+        if(blessed($class) and $class->isa(__PACKAGE__) and not @{ $output_vars })
+        {
+            @ndargs = ($class->handle) if not @ndargs;
+            $class = ref $class;
+        }
         for my $key (keys %kwargs)
         {
             $kwargs{ $key } = "(" .join(", ", @{ $kwargs{ $key } }) .")"
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Slice.pm b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Slice.pm
index 40312ebaa24f..ea49ac5960a4 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Slice.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/NDArray/Slice.pm
@@ -33,22 +33,23 @@ has end    => (is => 'ro', isa => 'Shape', required => 1);
 use overload
     '.=' => \&set,
     '='  => sub { $_[0] },
-    '""' => \&notsupported,
-    '+'  => \&notsupported,
-    '+=' => \&notsupported,
-    '-'  => \&notsupported,
-    '-=' => \&notsupported,
-    '*'  => \&notsupported,
-    '*=' => \&notsupported,
-    '/'  => \&notsupported,
-    '/=' => \&notsupported,
-    '**' => \&notsupported,
-    '==' => \&notsupported,
-    '!=' => \&notsupported,
-    '>'  => \&notsupported,
-    '>=' => \&notsupported,
-    '<'  => \&notsupported,
-    '<=' => \&notsupported;
+    '""' => sub { my $self = $_[0]->sever; "$self" },
+    '**' => sub { my $self = $_[0]->sever; $self ** $_[1] },
+    '==' => sub { my $self = $_[0]->sever; $self == $_[1] },
+    '!=' => sub { my $self = $_[0]->sever; $self != $_[1] },
+    '+'  => sub { my $self = $_[0]->sever; $self +  $_[1] },
+    '*'  => sub { my $self = $_[0]->sever; $self *  $_[1] },
+    '-'  => sub { my $self = $_[0]->sever; $_[2] ? $_[1] - $self : $self - $_[1] },
+    '/'  => sub { my $self = $_[0]->sever; $_[2] ? $_[1] / $self : $self / $_[1] },
+    '+=' => sub { my ($self, $other) = @_; my $in = $self->sever; $self .= ($in+$_[1]) },
+    '-=' => sub { my ($self, $other) = @_; my $in = $self->sever; $self .= ($in-$_[1]) },
+    '*=' => sub { my ($self, $other) = @_; my $in = $self->sever; $self .= ($in*$_[1]) },
+    '/=' => sub { my ($self, $other) = @_; my $in = $self->sever; $self .= ($in/$_[1]) },
+    '**='=> sub { my ($self, $other) = @_; my $in = $self->sever; $self .= ($in**$_[1]) },
+    '>'  => sub { my $self = $_[0]->sever; return $_[2] ? $_[1] >  $self : $self >  $_[1] },
+    '>=' => sub { my $self = $_[0]->sever; return $_[2] ? $_[1] >= $self : $self >= $_[1] },
+    '<'  => sub { my $self = $_[0]->sever; return $_[2] ? $_[1] <  $self : $self <  $_[1] },
+    '<=' => sub { my $self = $_[0]->sever; return $_[2] ? $_[1] <= $self : $self <= $_[1] };
 
 method set(AcceptableInput $value, $reverse=)
 {
@@ -113,7 +114,13 @@ method sever()
     no warnings 'misc';
     use attributes 'AI::MXNet::NDArray::Slice', \&AI::MXNet::NDArray::Slice::sever, 'lvalue';
 }
+
 sub notsupported  { confess("NDArray only support continuous slicing on axis 0"); }
-sub AUTOLOAD { notsupported() }
+sub AUTOLOAD {
+    my $sub = $AI::MXNet::NDArray::Slice::AUTOLOAD;
+    $sub =~ s/.*:://;
+    my $self = shift;
+    return $self->sever->$sub(@_);
+}
 
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Optimizer.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Optimizer.pm
index c6f682253833..2894b1e9179f 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Optimizer.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Optimizer.pm
@@ -108,6 +108,7 @@ has 'clip_gradient'       => (is => "rw", isa => "Maybe[Num]");
 has 'param_idx2name'      => (is => "rw", isa => "HashRef[Str]", default => sub { +{} });
 has 'idx2name'            => (is => "rw", isa => "HashRef[Str]");
 has 'sym'                 => (is => "rw", isa => "Maybe[AI::MXNet::Symbol]");
+has 'param_dict'          => (is => "rw", isa => "HashRef", default => sub { +{} });
 
 sub BUILD
 {
@@ -217,14 +218,18 @@ method _get_lr(Index $index)
     my $lr;
     if($self->lr_scheduler)
     {
-        $lr = &{$self->lr_scheduler}($self->num_update);
+        $lr = $self->lr_scheduler->($self->num_update);
     }
     else
     {
         $lr = $self->lr;
     }
 
-    if(exists $self->lr_mult->{ $index })
+    if(exists $self->param_dict->{ $index })
+    {
+        $lr *= $self->param_dict->{ $index }->lr_mult;
+    }
+    elsif(exists $self->lr_mult->{ $index })
     {
         $lr *= $self->lr_mult->{ $index };
     }
@@ -238,7 +243,11 @@ method _get_lr(Index $index)
 method _get_wd(Index $index)
 {
     my $wd = $self->wd;
-    if(exists $self->wd_mult->{ $index })
+    if(exists $self->param_dict->{ $index })
+    {
+        $wd *= $self->param_dict->{ $index }->wd_mult;
+    }
+    elsif(exists $self->wd_mult->{ $index })
     {
         $wd *= $self->wd_mult->{ $index };
     }
@@ -297,7 +306,7 @@ has 'multi_precision' => (is => "ro", isa => "Bool", default => 0);
 sub BUILD
 {
     my $self = shift;
-    $self->kwargs({ rescale_grad => $self->rescale_grad });
+    $self->kwargs({});
     if($self->momentum)
     {
         $self->kwargs->{momentum} = $self->momentum;
@@ -351,6 +360,7 @@ method update(
         out => $weight,
         lr  => $lr,
         wd  => $wd,
+        rescale_grad => $self->rescale_grad,
         %{ $self->kwargs }
     };
     my $use_multi_precision = ref($state) eq 'ARRAY';
@@ -452,7 +462,7 @@ method update(
     Index                     $index,
     AI::MXNet::NDArray        $weight,
     AI::MXNet::NDArray        $grad,
-    Maybe[AI::MXNet::NDArray] $state
+    Maybe[AI::MXNet::NDArray|ArrayRef[Maybe[AI::MXNet::NDArray]]] $state
 )
 {
     my $lr = $self->_get_lr($index);
@@ -668,7 +678,6 @@ sub BUILD
 {
     my $self = shift;
     $self->kwargs({
-        rescale_grad => $self->rescale_grad,
         beta1   => $self->beta1,
         beta2   => $self->beta2,
         epsilon => $self->epsilon
@@ -715,6 +724,7 @@ method update(
             out => $weight,
             lr  => $lr,
             wd  => $wd,
+            rescale_grad => $self->rescale_grad,
             %{ $self->kwargs }
         }
     );
@@ -867,7 +877,6 @@ sub BUILD
 {
     my $self = shift;
     $self->kwargs({
-        rescale_grad => $self->rescale_grad,
         gamma1       => $self->gamma1,
         epsilon      => $self->epsilon
     });
@@ -933,6 +942,7 @@ method update(
                 out => $weight,
                 lr  => $lr,
                 wd  => $wd,
+                rescale_grad => $self->rescale_grad,
                 %{ $self->kwargs }
             }
         );
@@ -945,6 +955,7 @@ method update(
                 out => $weight,
                 lr  => $lr,
                 wd  => $wd,
+                rescale_grad => $self->rescale_grad,
                 %{ $self->kwargs }
             }
         );
@@ -1351,16 +1362,38 @@ method sync_state_context(Maybe[AI::MXNet::NDArray|ArrayRef[AI::MXNet::NDArray]]
     return $state;
 }
 
+=head2 set_states
+
+    Sets updater states.
+=cut
+
 method set_states($states)
 {
     my $thawed_states = thaw($states);
+    my ($optimizer);
+    if(ref $thawed_states eq 'ARRAY')
+    {
+        ($thawed_states, $optimizer) = @{ $thawed_states };
+        $self->optimizer($optimizer);
+    }
     $self->states($thawed_states);
     %{ $self->states_synced } = map { $_ => 0 } keys %{ $thawed_states };
 }
 
-method get_states()
+=head2 get_states
+
+        Gets updater states.
+
+        Parameters
+        ----------
+        dump_optimizer : bool, default False
+            Whether to also save the optimizer itself. This would also save optimizer
+            information such as learning rate and weight decay schedules.
+=cut
+
+method get_states(Bool $dump_optimizer=0)
 {
-    return freeze($self->states);
+    return freeze($dump_optimizer ? [$self->states, $self->optimizer] : $self->states);
 }
 
 package AI::MXNet::Optimizer;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/RNN/Cell.pm b/perl-package/AI-MXNet/lib/AI/MXNet/RNN/Cell.pm
index 08c3094aa9c7..38db4090556e 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/RNN/Cell.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/RNN/Cell.pm
@@ -247,7 +247,7 @@ method begin_state(CodeRef :$func=AI::MXNet::Symbol->can('zeros'), @kwargs)
             }
         }
         my %kwargs = (@kwargs, %info);
-        my $state = &{$func}(
+        my $state = $func->(
             'AI::MXNet::Symbol',
             @name,
             %kwargs
@@ -425,7 +425,7 @@ method unroll(
     for my $i (0..$length-1)
     {
         my $output;
-        ($output, $states) = &{$self}(
+        ($output, $states) = $self->(
             $inputs[$i],
             $states
         );
@@ -447,7 +447,7 @@ method _get_activation($inputs, $activation, @kwargs)
     }
     else
     {
-        return &{$activation}($inputs, @kwargs);
+        return $activation->($inputs, @kwargs);
     }
 }
 
@@ -1190,7 +1190,7 @@ method call($inputs, $states)
         my $n = scalar(@{ $cell->state_info });
         my $state = [@{ $states }[$p..$p+$n-1]];
         $p += $n;
-        ($inputs, $state) = &{$cell}($inputs, $state);
+        ($inputs, $state) = $cell->($inputs, $state);
         push @next_states, $state;
     }
     return ($inputs, [map { @$_} @next_states]);
@@ -1828,7 +1828,7 @@ has [qw/dropout_outputs dropout_states/] => (is => 'ro', isa => 'Num', default =
 
 method call(AI::MXNet::Symbol $inputs, SymbolOrArrayOfSymbols $states)
 {
-    my ($output, $states) = &{$self->base_cell}($inputs, $states);
+    my ($output, $states) = $self->base_cell->($inputs, $states);
     if($self->dropout_outputs > 0)
     {
         $output = AI::MXNet::Symbol->Dropout(data => $output, p => $self->dropout_outputs);
@@ -1886,7 +1886,7 @@ method reset()
 method call(AI::MXNet::Symbol $inputs, SymbolOrArrayOfSymbols $states)
 {
     my ($cell, $p_outputs, $p_states) = ($self->base_cell, $self->zoneout_outputs, $self->zoneout_states);
-    my ($next_output, $next_states) = &{$cell}($inputs, $states);
+    my ($next_output, $next_states) = $cell->($inputs, $states);
     my $mask = sub {
         my ($p, $like) = @_;
         AI::MXNet::Symbol->Dropout(
@@ -1899,7 +1899,7 @@ method call(AI::MXNet::Symbol $inputs, SymbolOrArrayOfSymbols $states)
     my $prev_output = $self->prev_output // AI::MXNet::Symbol->zeros(shape => [0, 0]);
     my $output = $p_outputs != 0
         ? AI::MXNet::Symbol->where(
-            &{$mask}($p_outputs, $next_output),
+            $mask->($p_outputs, $next_output),
             $next_output,
             $prev_output
         )
@@ -1910,7 +1910,7 @@ method call(AI::MXNet::Symbol $inputs, SymbolOrArrayOfSymbols $states)
         zip(sub {
             my ($new_s, $old_s) = @_;
             push @states, AI::MXNet::Symbol->where(
-                &{$mask}($p_states, $new_s),
+                $mask->($p_states, $new_s),
                 $new_s,
                 $old_s
             );
@@ -1940,7 +1940,7 @@ extends 'AI::MXNet::RNN::ModifierCell';
 method call(AI::MXNet::Symbol $inputs, SymbolOrArrayOfSymbols $states)
 {
     my $output;
-    ($output, $states) = &{$self->base_cell}($inputs, $states);
+    ($output, $states) = $self->base_cell->($inputs, $states);
     $output = AI::MXNet::Symbol->elemwise_add($output, $inputs, name => $output->name.'_plus_residual');
     return ($output, $states)
 }
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Random.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Random.pm
index 8f7b6d3f8d4e..dcd765c0f166 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Random.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Random.pm
@@ -18,6 +18,7 @@
 package AI::MXNet::Random;
 use strict;
 use warnings;
+use Scalar::Util qw/blessed/;
 use AI::MXNet::Base;
 use AI::MXNet::NDArray::Base;
 use AI::MXNet::Function::Parameters;
@@ -57,23 +58,78 @@ method seed(Int $seed_state)
     check_call(AI::MXNetCAPI::RandomSeed($seed_state));
 }
 
-for my $method (
-        [qw/_random_uniform uniform/],
-        [qw/_random_normal normal/],
-        [qw/_random_gamma gamma/],
-        [qw/_random_exponential exponential/],
-        [qw/_random_poisson poisson/],
-        [qw/_random_negbinomial negative_binomial/],
-        [qw/_random_gennegbinomial generalized_negative_binomial/],
-)
-{
-    my ($nd_method_name, $rnd_method_name) = @{$method};
+sub AUTOLOAD {
+    my $sub = $AI::MXNet::Random::AUTOLOAD;
+    $sub =~ s/.*:://;
+    shift;
+    my %updated;
+    my %defaults = (
+        ctx   => AI::MXNet::Context->current_ctx,
+        shape => 1,
+        out   => 1
+    );
+    my @args;
+    my @tmp = @_;
+    if(ref $tmp[-1] eq 'HASH')
+    {
+        my @kwargs = %{ pop(@tmp) };
+        push @tmp, @kwargs;
+    }
+    while(@tmp >= 2 and not ref $tmp[-2])
+    {
+        if(exists $defaults{$tmp[-2]})
+        {
+            my $v = pop(@tmp);
+            my $k = pop(@tmp);
+            if(defined $v)
+            {
+                $updated{$k} = 1;
+                $defaults{$k} = $v;
+            }
+        }
+        else
+        {
+            unshift @args, pop(@tmp);
+            unshift @args, pop(@tmp);
+        }
+    }
+    unshift @args, @tmp;
+    if(blessed($defaults{out}) and not exists $updated{shape})
+    {
+        delete $defaults{shape};
+    }
+    delete $defaults{out} unless blessed $defaults{out};
+    if($sub eq 'exponential')
+    {
+        my $changed = 0;
+        for my $i (0..@args-1)
+        {
+            if(not ref $args[$i] and $args[$i] eq 'scale')
+            {
+                $args[$i] = 'lam';
+                $args[$i+1] = 1/$args[$i+1];
+                $changed = 1;
+            }
+        }
+        $args[0] = 1/$args[0] unless $changed;
+    }
+    if(grep { blessed($_) and $_->isa('AI::MXNet::NDArray') } @args)
+    {
+        if($sub eq 'normal')
+        {
+            my %mapping = qw/loc mu scale sigma/;
+            @args = map { (not ref $_ and exists $mapping{$_}) ? $mapping{$_} : $_ } @args
+        }
+        $sub = "_sample_$sub";
+        delete $defaults{shape} if not exists $updated{shape};
+        delete $defaults{ctx};
+        return AI::MXNet::NDArray->$sub(@args, %defaults);
+    }
+    else
     {
-        no strict 'refs';
-        *{__PACKAGE__."::$rnd_method_name"} = sub { shift;
-            return AI::MXNet::NDArray->$nd_method_name(@_);
-        };
+        $sub = "_random_$sub";
     }
+    return AI::MXNet::NDArray->$sub(@args, %defaults);
 }
 
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/RecordIO.pm b/perl-package/AI-MXNet/lib/AI/MXNet/RecordIO.pm
index 2027a901ec10..f22e2ce92789 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/RecordIO.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/RecordIO.pm
@@ -25,7 +25,7 @@ use Mouse;
 
 =head1 NAME
 
-    AI::MXNet::Function::Parameters - Read/write RecordIO format data
+    AI::MXNet::RecordIO - Read/write RecordIO format data
 =cut
 
 =head2 new
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Rtc.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Rtc.pm
deleted file mode 100644
index 09dc66200322..000000000000
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Rtc.pm
+++ /dev/null
@@ -1,144 +0,0 @@
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#   http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-package AI::MXNet::Rtc;
-use strict;
-use warnings;
-use AI::MXNet::Base;
-use Mouse;
-use AI::MXNet::Function::Parameters;
-
-=head1 DESCRIPTION
-
-    Interface to runtime cuda kernel compile module.
-=cut
-
-=head2 Constructor
-
-    MXRtc object in mxnet.
-    This class allow you to write cuda kernel in perl
-    and call them with NDArray.
-
-    Parameters
-    ----------
-    name : str
-        name of the kernel
-    inputs : tuple of (str, mxnet.ndarray)
-        list of input names and ndarray
-    outputs : tuple of (str, mxnet.ndarray)
-        list of output names and ndarray
-    kernel : str
-        the actual kernel code.
-        Note that this is only the body of the kernel, i.e.
-        after { and before }. Rtc will decorate the kernel.
-        For example, if name = "mykernel" and
-        inputs = [('x', mx.nd.zeros((10,)))]
-        outputs = [('y', mx.nd.zeros((10,)))]
-        kernel = "y[threadIdx.x] = x[threadIdx.x];",
-        the kernel that is compile will be:
-        extern "C" __global__ mykernel(float *x, float *y) {
-            const int x_ndim = 1;
-            const int x_dims = { 10 };
-            const int y_ndim = 1;
-            const int y_dims = { 10 };
-
-            y[threadIdx.x] = x[threadIdx.x];
-        }
-=cut
-
-has 'handle'              => (is => 'rw', isa => 'RtcHandle', init_arg => undef);
-has [qw/name kernel/]     => (is => 'ro', isa => 'Str', required => 1);
-has [qw/inputs outputs/]  => (is => 'ro', isa => 'HashRef[AI::MXNet::NDArray]', required => 1);
-
-sub BUILD
-{
-    my $self = shift;
-    my (@input_names, @output_names, @input_nds, @output_nds);
-    while(my ($name, $arr) = each %{ $self->inputs })
-    {
-        push @input_names, $name;
-        push @input_nds, $arr->handle;
-    }
-    while(my ($name, $arr) = each %{ $self->outputs })
-    {
-        push @output_names, $name;
-        push @output_nds, $arr->handle;
-    }
-    my $handle = check_call(
-        AI::MXNetCAPI::RtcCreate(
-            $self->name,
-            scalar(@input_names),
-            scalar(@output_names),
-            \@input_names,
-            \@output_names,
-            \@input_nds,
-            \@output_nds,
-            $self->kernel
-        )
-    );
-    $self->handle($handle);
-}
-
-sub DEMOLISH
-{
-    check_call(AI::MXNetCAPI::MXRtcFree(shift->handle));
-}
-
-=head2 push
-
-        run the kernel.
-
-        Parameters
-        ----------
-        inputs : list of ndarray
-            list of input. Can be different ndarray then uses for constructor,
-            but must have the same shape and in the same order.
-        outputs : list of ndarray
-            list of out. Can be different ndarray then uses for constructor,
-            but must have the same shape and in the same order.
-        grid_dims : tuple of 3 uint
-            grid dimension for kernel launch
-        block_dims : tuple of 3 uint
-            block dimension for kernel launch
-=cut
-
-
-method push(
-    ArrayRef[AI::MXNet::NDArray] $inputs,
-    ArrayRef[AI::MXNet::NDArray] $outputs,
-    ArrayRef[DimSize] $grid_dims,
-    ArrayRef[DimSize] $block_dims
-)
-{
-    confess("grid_dims must be size of 3")
-        unless @{ $grid_dims } == 3;
-    confess("block_dims must be size of 3")
-        unless @{ $block_dims } == 3;
-    check_call(
-        AI::MXNetCAPI::RtcPush(
-            $self->handle,
-            scalar(@$inputs),
-            scalar(@$outputs),
-            [map { $_->handle } @$inputs],
-            [map { $_->handle } @$outputs],
-            @{ $grid_dims },
-            @{ $block_dims }
-        )
-    );
-}
-
-1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol.pm
index eed6e93f568b..d35bdaea62cd 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol.pm
@@ -26,6 +26,7 @@ use strict;
 use warnings;
 use AI::MXNet::Base;
 use AI::MXNet::Symbol::Base;
+use AI::MXNet::Symbol::Random;
 use AI::MXNet::Types;
 use Mouse;
 use AI::MXNet::Function::Parameters;
@@ -73,7 +74,11 @@ method STORABLE_thaw($cloning, $json)
 method stringify($other=, $reverse=)
 {
     my $name = $self->name;
-    sprintf("<%s %s>", ref($self), $name ? $name : 'Grouped');
+    sprintf(
+        "<%s %s%s>",
+        ref($self),
+        $name ? ($name, '') : ('group [', join(', ', map { $_->name } @{ $self }) . ']')
+    );
 }
 
 method add(AI::MXNet::Symbol|Num $other, $reverse=)
@@ -1463,4 +1468,7 @@ sub  _ufunc_helper
     }
 }
 
+sub contrib { 'AI::MXNet::Contrib::Symbol' }
+sub random  { 'AI::MXNet::Symbol::Random' }
+
 1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Base.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Base.pm
index 4282f124a34b..8d3d069c82e3 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Base.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Base.pm
@@ -90,6 +90,14 @@ func _make_atomic_symbol_function($handle, $name)
         $ret_type) = @{ check_call(AI::MXNetCAPI::SymbolGetAtomicSymbolInfo($handle)) };
     $ret_type //= '';
     my $func_name = $name;
+    my @arguments;
+    my %arguments = map { $_ => 1 } qw/name attr lr_mult wd_mult
+                                       init __layout__ dtype shape/;
+    for my $i (0..@{ $arg_names }-1)
+    {
+        push @arguments, $arg_names->[$i];
+        $arguments{ $arg_names->[$i] } = 1;
+    }
     my $doc_str = build_doc($func_name,
                             $desc,
                             $arg_names,
@@ -99,7 +107,7 @@ func _make_atomic_symbol_function($handle, $name)
                             $ret_type
     );
     my $creator = sub {
-        my $class = shift;
+        my $class = ref($_[0]) || shift;
         my (@args, %kwargs);
         if(
             @_
@@ -114,6 +122,7 @@ func _make_atomic_symbol_function($handle, $name)
         }
         elsif(blessed $_[0] and $_[0]->isa(__PACKAGE__))
         {
+
             while(blessed $_[0] and $_[0]->isa(__PACKAGE__))
             {
                 push @args, shift(@_);
@@ -122,7 +131,18 @@ func _make_atomic_symbol_function($handle, $name)
         }
         else
         {
-            %kwargs = @_;
+            while(@_ >= 2 and not ref $_[-2]
+                    and (exists $arguments{ $_[-2] } or (blessed $_[-1] and $_[-1]->isa(__PACKAGE__))))
+            {
+                my $v = pop(@_);
+                my $k = pop(@_);
+                $kwargs{ $k } = $v;
+            }
+            @kwargs{ @arguments[0..@args-1] } = @args;
+        }
+        if(blessed $class and $class->isa(__PACKAGE__))
+        {
+            $kwargs{data} = $class;
         }
         my $params = {};
         my $symbol_kwargs = {};
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/NameManager.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/NameManager.pm
index 109949c79078..95ea8a6f49ea 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/NameManager.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/NameManager.pm
@@ -75,10 +75,15 @@ method get(Maybe[Str] $name, Str $hint)
 
 method current()
 {
-    $AI::MXNet::current_nm_ldr;
+    $AI::MXNet::Symbol::NameManager;
 }
 
-$AI::MXNet::current_nm_ldr = __PACKAGE__->new;
+method set_current(AI::MXNet::Symbol::NameManager $new)
+{
+    $AI::MXNet::Symbol::NameManager = $new;
+}
+
+$AI::MXNet::Symbol::NameManager = __PACKAGE__->new;
 
 package AI::MXNet::Symbol::Prefix;
 use Mouse;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Random.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Random.pm
new file mode 100644
index 000000000000..795fe44825e4
--- /dev/null
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Symbol/Random.pm
@@ -0,0 +1,58 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+package AI::MXNet::Symbol::Random;
+use strict;
+use warnings;
+use Scalar::Util qw/blessed/;
+
+sub AUTOLOAD {
+    my $sub = $AI::MXNet::Symbol::Random::AUTOLOAD;
+    $sub =~ s/.*:://;
+    shift;
+    my @args = @_;
+    if($sub eq 'exponential')
+    {
+        my $changed = 0;
+        for my $i (0..@args-1)
+        {
+            if(not ref $args[$i] and $args[$i] eq 'scale')
+            {
+                $args[$i] = 'lam';
+                $args[$i+1] = 1/$args[$i+1];
+                $changed = 1;
+            }
+        }
+        $args[0] = 1/$args[0] unless $changed;
+    }
+    if(grep { blessed($_) and $_->isa('AI::MXNet::Symbol') } @args)
+    {
+        if($sub eq 'normal')
+        {
+            my %mapping = qw/loc mu scale sigma/;
+            @args = map { (not ref $_ and exists $mapping{$_}) ? $mapping{$_} : $_ } @args
+        }
+        $sub = "_sample_$sub";
+    }
+    else
+    {
+        $sub = "_random_$sub";
+    }
+    return AI::MXNet::Symbol->$sub(@args);
+}
+
+1;
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/TestUtils.pm b/perl-package/AI-MXNet/lib/AI/MXNet/TestUtils.pm
index ea918c0cddf3..af9b27dce96c 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/TestUtils.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/TestUtils.pm
@@ -26,7 +26,7 @@ use Exporter;
 use base qw(Exporter);
 @AI::MXNet::TestUtils::EXPORT_OK = qw(same reldiff almost_equal GetMNIST_ubyte
                                       GetCifar10 pdl_maximum pdl_minimum mlp2 conv
-                                      check_consistency zip assert enumerate same_array dies_like);
+                                      check_consistency zip assert enumerate same_array dies_like allclose);
 use constant default_numerical_threshold => 1e-6;
 =head1 NAME
 
@@ -47,6 +47,21 @@ func same(PDL $a, PDL $b)
     return ($a != $b)->sum == 0;
 }
 
+=head2 allclose
+
+    Test if all elements of two pdl arrays are almost equal
+
+    Parameters
+    ----------
+    a : pdl
+    b : pdl
+=cut
+
+func allclose(PDL $a, PDL $b)
+{
+    return (($a - $b)->abs <= default_numerical_threshold)->all;
+}
+
 =head2 reldiff
 
     Calculate the relative difference between two input arrays
diff --git a/perl-package/AI-MXNet/lib/AI/MXNet/Types.pm b/perl-package/AI-MXNet/lib/AI/MXNet/Types.pm
index 27dd013f27b6..e9b1e5522eb4 100644
--- a/perl-package/AI-MXNet/lib/AI/MXNet/Types.pm
+++ b/perl-package/AI-MXNet/lib/AI/MXNet/Types.pm
@@ -35,22 +35,31 @@ class_type 'AI::MXNet::EvalMetric';
 class_type 'AI::MXNet::DataParallelExecutorGroup';
 class_type 'AI::MXNet::Optimizer';
 class_type 'AI::MXNet::Initializer';
+class_type 'AI::MXNet::KVStore';
 class_type 'AI::MXNet::InitDesc';
 class_type 'AI::MXNet::IRHeader';
 class_type 'AI::MXNet::Updater';
 class_type 'AI::MXNet::KVStore';
+class_type 'AI::MXNet::Gluon::Block';
+class_type 'AI::MXNet::Gluon::Data::Set';
+class_type 'AI::MXNet::Gluon::RNN::HybridRecurrentCell';
+class_type 'AI::MXNet::Symbol::NameManager';
 subtype "AcceptableInput" => as "Num|PDL|PDL::Matrix|AI::MXNet::NDArray|AI::MXNet::NDArray::Slice|ArrayRef";
 subtype "Index"           => as "Int";
 subtype "DimSize"         => as "Int" => where { $_ >= 0 };
+subtype "Dropout"         => as "Num" => where { $_ >= 0 and $_ <= 1 };
 subtype "Shape"           => as "ArrayRef[DimSize]";
+subtype "CudaKernelShape" => as "Shape" => where { @$_ == 3 };
 subtype "WholeDim"        => as "Str" => where { $_ eq 'X' };
 subtype "Slice"           => as "ArrayRef[Index]|WholeDim|Index" => where { ref $_ ? @$_ > 0 : 1 };
 subtype "Dtype"           => as enum([qw[float32 float64 float16 uint8 int32]]);
-subtype "Metric"          => as "Maybe[CodeRef|Str]";
 subtype "ProfilerMode"    => as enum([qw[symbolic all]]);
+subtype "GluonClass"      => as enum([qw[AI::MXNet::NDArray AI::MXNet::Symbol]]);
+subtype "GluonInput"      => as "AI::MXNet::NDArray|AI::MXNet::Symbol|ArrayRef[AI::MXNet::NDArray|AI::MXNet::Symbol]";
 subtype "ProfilerState"   => as enum([qw[stop run]]);
 subtype "GradReq"         => as enum([qw[add write null]]);
 subtype "KVStoreStr"      => as enum([qw[local device dist dist_sync dist_async]]);
+subtype "PoolType"        => as enum([qw[max avg sum]]);
 subtype "NameShape"       => as "ArrayRef" => where {
     find_type_constraint("Str")->check($_->[0])
         and
@@ -58,6 +67,7 @@ subtype "NameShape"       => as "ArrayRef" => where {
 };
 subtype "Callback"        => as "CodeRef|ArrayRef[Coderef]|AI::MXNet::Callback|ArrayRef[AI::MXNet::Callback]";
 subtype "EvalMetric"      => as "AI::MXNet::EvalMetric|Str|CodeRef";
+subtype "Metric"          => as "Maybe[EvalMetric]";
 subtype "Optimizer"       => as "AI::MXNet::Optimizer|Str";
 subtype "Initializer"     => as "AI::MXNet::Initializer|Str";
 subtype "Updater"         => as "AI::MXNet::Updater|CodeRef";
@@ -65,3 +75,6 @@ subtype "KVStore"         => as "AI::MXNet::KVStore|KVStoreStr";
 subtype "Activation"      => as "AI::MXNet::Symbol|Str|CodeRef";
 subtype "SymbolOrArrayOfSymbols" => as "AI::MXNet::Symbol|ArrayRef[AI::MXNet::Symbol]";
 subtype "NameShapeOrDataDesc" => as "NameShape|AI::MXNet::DataDesc";
+subtype "AdvancedSlice"   => as "ArrayRef[ArrayRef|PDL|PDL::Matrix|AI::MXNet::NDArray]";
+
+1;
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/t/test_autograd.t b/perl-package/AI-MXNet/t/test_autograd.t
new file mode 100644
index 000000000000..32225bfd2728
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_autograd.t
@@ -0,0 +1,370 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use AI::MXNet qw(mx);
+use AI::MXNet::AutoGrad qw(autograd);
+use AI::MXNet::TestUtils qw(same);
+use AI::MXNet::Base;
+use Test::More tests => 74;
+
+sub autograd_assert
+{
+    my $kwargs = {};
+    if(ref $_[-1] eq 'HASH') { $kwargs = pop(@_) };
+    my @args = @_;
+    my $func   = $kwargs->{func};
+    my $grad_f = $kwargs->{grad_func};
+    my $argnum = $kwargs->{argnum};
+    my $grad_func = autograd->grad_and_loss($func, $argnum);
+    my ($grad_vals, $output) = $grad_func->(@args);
+    my $res = $func->(@args);
+    ok(same($output->aspdl, $res->aspdl));
+    my $grad_res = $grad_f->(@args);
+    ok(@$grad_vals == @$grad_res);
+    zip(sub {
+        my ($a, $b) = @_;
+        ok(same($a->aspdl, $b->aspdl));
+    }, $grad_vals, $grad_res);
+}
+
+sub test_unary_func
+{
+    my $check_unary_func = sub {
+        my ($x) = @_;
+        my $f_exp       = sub { $_[0]->exp };
+        my $f_exp_grad  = sub { [$_[0]->exp] };
+        autograd_assert($x, { func => $f_exp, grad_func => $f_exp_grad });
+        my $f_half      = sub { $_[0]/2 };
+        my $f_half_grad = sub { [mx->nd->ones($_[0]->shape) * 0.5] };
+        autograd_assert($x, { func => $f_half, grad_func => $f_half_grad });
+        my $f_square    = sub { $_[0]**2 };
+        my $f_square_grad = sub { [2*$_[0]] };
+        autograd_assert($x, { func => $f_square, grad_func => $f_square_grad });
+    };
+    my $uniform = mx->nd->uniform(shape=>[4, 5]);
+    $check_unary_func->($uniform);
+    # sparse support
+    #my $stypes = ['row_sparse', 'csr', 'default'];
+    #for my $stype (@$stypes)
+    #{
+    #    $check_unary_func->($uniform->tostype($stype));
+    #}
+}
+
+test_unary_func();
+
+sub test_binary_func
+{
+    my $check_binary_func = sub {
+        my ($x, $y) = @_;
+        my $f_add      = sub { $_[0]+$_[1] };
+        my $f_add_grad = sub { [map { mx->nd->ones($_->shape) } @_] };
+        autograd_assert($x, $y, { func => $f_add, grad_func => $f_add_grad });
+        my $f_mul      = sub { $_[0]*$_[1] };
+        my $f_mul_grad = sub { [reverse(@_)] };
+        autograd_assert($x, $y, { func => $f_mul, grad_func => $f_mul_grad });
+        my $f_compose  = sub { $_[0]+$_[0]*$_[1] };
+        my $f_compose_grad = sub { [mx->nd->ones($_[0]->shape) + $y, $x] };
+        autograd_assert($x, $y, { func => $f_compose, grad_func => $f_compose_grad });
+    };
+    my $uniform_x = mx->nd->uniform(shape=>[4, 5]);
+    my $uniform_y = mx->nd->uniform(shape=>[4, 5]);
+    $check_binary_func->($uniform_x, $uniform_y);
+    # sparse support
+    #my $stypes = ['row_sparse', 'csr', 'default'];
+    #for my $stype_x (@$stypes)
+    #{
+    #    for my $stype_y (@$stypes)
+    #    {
+    #        my $x = $uniform_x->tostype($stype_x);
+    #        my $y = $uniform_y->tostype($stype_y);
+    #        $check_binary_func->($x, $y);
+    #    }
+    #}
+}
+
+test_binary_func();
+
+sub test_operator_with_state
+{
+    my $f_fc = sub {
+        my ($a, $b, $weight, $bias) = @_;
+        my $x = $a*$b;
+        my $fc = mx->nd->FullyConnected(
+            $x, $weight, $bias, num_hidden=>32);
+        return $fc;
+    };
+
+    my $a = mx->nd->uniform(shape=>[64, 50]);
+    my $b = mx->nd->uniform(shape=>[64, 50]);
+    my $weight = mx->nd->uniform(shape=>[32, 50]);
+    my $bias = mx->nd->uniform(shape=>[32]);
+
+    my $grad_func = autograd->grad_and_loss($f_fc);
+    my ($grad_vals, $outputs) = $grad_func->($a, $b, $weight, $bias);
+}
+
+test_operator_with_state();
+
+sub test_argnum
+{
+    my $f_with_mode = sub {
+        my ($a, $b, $mode) = @_;
+        if($mode)
+        {
+            return $a+$b;
+        }
+        else
+        {
+            return $a*$b;
+        }
+    };
+    my $a = mx->nd->uniform(shape=>[3, 2]);
+    my $b = mx->nd->uniform(shape=>[3, 2]);
+    my $f_add_grad = sub { [map { mx->nd->ones($_->shape) } @_[0,1]] };
+    my $f_mul_grad = sub { [reverse(@_[0,1])] };
+    autograd_assert($a, $b, 1,
+        { argnum=>[0, 1], func=>$f_with_mode, grad_func=>$f_add_grad });
+    autograd_assert($a, $b, 0,
+        { argnum=>[0, 1], func=>$f_with_mode, grad_func=>$f_mul_grad });
+}
+
+test_argnum();
+
+sub test_training
+{
+    my $x = mx->nd->ones([10, 10]);
+    autograd->record(sub {
+        my $y = mx->nd->Dropout($x, p=>0.5);
+        ok(not ($y->aspdl == $x->aspdl)->all);
+        autograd->pause(sub {
+            my $y = mx->nd->Dropout($x, p=>0.5);
+            ok(($y->aspdl == $x->aspdl)->all);
+        });
+    });
+}
+
+test_training();
+
+sub test_out_grads
+{
+    my $x = mx->nd->ones([3, 5]);
+    my $dx = mx->nd->zeros_like($x);
+    autograd->mark_variables([$x], [$dx]);
+    my $da;
+    my $db = mx->nd->array([1,2,3,4,5]);
+    my $dc = mx->nd->array([5,4,3,2,1]);
+
+    autograd->record(sub {
+        my ($a, $b, $c) = @{ $x };
+        autograd->backward([$a, $b, $c], head_grads => [$da, $db, $dc]);
+    });
+    ok(($dx->aspdl == pdl(
+        [[1,1,1,1,1],
+         [1,2,3,4,5],
+         [5,4,3,2,1]]))->all);
+}
+
+test_out_grads();
+
+sub test_detach_updated_grad
+{
+    my $x = mx->nd->ones([2, 2]);
+    my $dx = mx->nd->zeros_like($x);
+    my $y = mx->nd->ones_like($x);
+    my $dy = mx->nd->zeros_like($x);
+    autograd->mark_variables([$x, $y], [$dx, $dy]);
+    ok($x->_fresh_grad == 0);
+    ok($y->_fresh_grad == 0);
+
+    autograd->record(sub {
+        my $x2 = $x + 2;
+        my $y2  = $x2 + $y;
+        $y2->backward();
+    });
+    ok(($dx->aspdl == 1)->all);
+    ok($x->_fresh_grad == 1);
+    ok($y->_fresh_grad == 1);
+
+    $dx .= 0;
+    $x->_fresh_grad(0);
+    $y->_fresh_grad(0);
+    ok($x->_fresh_grad == 0);
+    ok($y->_fresh_grad == 0);
+
+    autograd->record(sub {
+        my $x2 = $x + 2;
+        $x2 = $x2->detach;
+        my $y2  = $x2 + $y;
+        $y2->backward();
+    });
+    ok(($dx->aspdl == 0)->all);
+    ok($x->_fresh_grad == 0);
+    ok($y->_fresh_grad == 1);
+}
+
+test_detach_updated_grad();
+
+sub test_retain_grad
+{
+    my $x = mx->nd->ones([2, 2]);
+    my $dx = mx->nd->zeros([2, 2]);
+    autograd->mark_variables([$x], [$dx], grad_reqs=>'add');
+    autograd->record(sub {
+        my $y = $x + 1;
+        $y->backward(retain_graph=>0);
+    });
+    ok(($dx->aspdl == 1)->all);
+
+    $dx .= 0;
+    autograd->record(sub {
+        my $y = $x + 1;
+        $y->backward(retain_graph=>1);
+        $y->backward(retain_graph=>0);
+    });
+    ok(($dx->aspdl == 2)->all);
+    no warnings;
+    open(CPERR, ">&STDERR");
+    open(STDERR, ">/dev/null");
+    eval {
+        autograd->record(sub {
+            my $y = $x + 1;
+            $y->backward();
+            $y->backward();
+        });
+    };
+    open(STDERR, ">&CPERR");
+    ok($@);
+}
+
+test_retain_grad();
+
+sub test_attach_grad
+{
+    my $check_attach_grad = sub {
+        my ($x) = @_;
+        ok(not defined $x->grad);
+        $x->attach_grad();
+        autograd->record(sub {
+            my $y = $x * 2;
+            ok(not defined $y->grad);
+            $y->backward;
+        });
+        ok(($x->grad->aspdl == 2)->all);
+    };
+    my $zeros = mx->nd->zeros([10, 10]);
+    $check_attach_grad->($zeros);
+    # sparse support
+    #stypes = ['default', 'row_sparse', 'csr']
+    #for stype in stypes:
+    #    x = zeros.tostype(stype)
+    #    check_attach_grad(x)
+}
+
+test_attach_grad();
+
+sub test_is_train
+{
+    my $x = mx->nd->ones([10, 10]);
+    $x->attach_grad();
+    autograd->record(sub {
+        ok(autograd->is_recording());
+        ok(autograd->is_training());
+        my $y = mx->nd->Dropout($x, p=>0.5);
+        ok($y->aspdl->max == 2 and $y->aspdl->min == 0);
+        $y->backward();
+        ok(($x->grad->aspdl == $y->aspdl)->all);
+        autograd->predict_mode(sub {
+            ok(autograd->is_recording());
+            ok(not autograd->is_training());
+            my $y = mx->nd->Dropout($x, p=>0.5);
+            ok(($y->aspdl == $x->aspdl)->all);
+            $y->backward(train_mode=>0);
+            ok(($x->grad->aspdl == $x->aspdl)->all);
+        });
+    }, train_mode => 1);
+
+    autograd->record(sub {
+        ok(autograd->is_recording());
+        ok(not autograd->is_training());
+        my $y = mx->nd->Dropout($x, p=>0.5);
+        ok(($y->aspdl == $x->aspdl)->all);
+        $y->backward(train_mode=>0);
+        ok(($x->grad->aspdl == $x->aspdl)->all);
+
+        autograd->train_mode(sub {
+            ok(autograd->is_recording);
+            ok(autograd->is_training);
+            my $y = mx->nd->Dropout($x, p=>0.5);
+            ok($y->aspdl->max == 2 and $y->aspdl->min == 0);
+            $y->backward;
+            ok(($x->grad->aspdl == $y->aspdl)->all);
+        });
+    }, train_mode => 0);
+
+    ok(not autograd->is_recording);
+    ok(not autograd->is_training);
+    my $y = mx->nd->Dropout($x, p=>0.5);
+    ok(($y->aspdl == $x->aspdl)->all);
+
+    autograd->train_mode(sub {
+        ok(not autograd->is_recording);
+        ok(autograd->is_training);
+        my $y = mx->nd->Dropout($x, p=>0.5);
+        ok($y->aspdl->max == 2 and $y->aspdl->min == 0);
+    });
+}
+
+test_is_train();
+
+sub test_get_symbol
+{
+    my $x = mx->nd->ones([1]);
+    $x->attach_grad;
+    my $y;
+    autograd->record(sub {
+        $y = $x*$x + 2*$x - 1;
+    });
+    ok(@{ autograd->get_symbol($y)->list_arguments } == 1);
+
+    my $z = mx->nd->ones([1]);
+    $z->attach_grad;
+    autograd->record(sub {
+        $y = $x*$x + 2*$z - 1;
+    });
+    ok(@{ autograd->get_symbol($y)->list_arguments } == 2);
+}
+
+test_get_symbol();
+
+sub test_gradient
+{
+    my $x = mx->nd->ones([1]);
+    $x->attach_grad;
+    my $z;
+    mx->autograd->record(sub {
+        $z = mx->nd->elemwise_add($x->exp, $x);
+    });
+    my $dx = mx->autograd->grad($z, $x, create_graph=>1);
+    ok(abs($dx->asscalar - 3.71828175) < 1e-7);
+    $dx->backward;
+    ok(abs($x->grad->asscalar - 2.71828175) < 1e-7);
+}
+
+test_gradient();
diff --git a/perl-package/AI-MXNet/t/test_cuda_module.t b/perl-package/AI-MXNet/t/test_cuda_module.t
new file mode 100644
index 000000000000..ce0e511cfb49
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_cuda_module.t
@@ -0,0 +1,40 @@
+use strict;
+use warnings;
+use AI::MXNet qw(mx);
+use Test::More tests => 3;
+my $gpu_present = (`perl -e 'use AI::MXNet qw(mx); print mx->nd->ones([1], ctx => mx->gpu(0))->asscalar' 2>/dev/null` eq '1');
+
+sub test_cuda_rtc
+{
+    my $source = '
+    extern "C" __global__ void axpy(const float *x, float *y, float alpha) {
+        int i = threadIdx.x + blockIdx.x * blockDim.x;
+        y[i] += alpha * x[i];
+    }
+
+    extern "C" __global__ void saxpy(const float *x, float *y, float alpha) {
+        extern __shared__ float smem[];
+        int i = threadIdx.x + blockIdx.x * blockDim.x;
+        smem[threadIdx.x] = x[i];
+        y[i] += alpha * smem[threadIdx.x];
+    }
+    ';
+    my $module = mx->rtc->CudaModule($source);
+    my $axpy = $module->get_kernel("axpy", "const float *x, float *y, float alpha");
+    my $x = mx->nd->ones([10], ctx=>mx->gpu(0));
+    my $y = mx->nd->zeros([10], ctx=>mx->gpu(0));
+    $axpy->launch([$x, $y, 3], mx->gpu(0), [1, 1, 1], [10, 1, 1]);
+    ok(($y->aspdl == 3)->all);
+
+    my $saxpy = $module->get_kernel("saxpy", "const float *x, float *y, float alpha");
+    $saxpy->launch([$x, $y, 4], mx->gpu(0), [1, 1, 1], [10, 1, 1], 10);
+    ok(($y->aspdl == 7)->all);
+
+    $saxpy->launch([$x, $y, 5], mx->gpu(0), [2, 1, 1], [5, 1, 1], 5);
+    ok(($y->aspdl == 12)->all);
+}
+
+SKIP: {
+    skip("GPU is not avalilable", 3) unless $gpu_present;
+    test_cuda_rtc();
+}
\ No newline at end of file
diff --git a/perl-package/AI-MXNet/t/test_gluon.t b/perl-package/AI-MXNet/t/test_gluon.t
new file mode 100644
index 000000000000..ff7e2a604d64
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_gluon.t
@@ -0,0 +1,587 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use Test::More tests => 119;
+use AI::MXNet qw(mx);
+use AI::MXNet::Gluon qw(gluon);
+use AI::MXNet::Gluon::NN qw(nn);
+use AI::MXNet::TestUtils qw(almost_equal);
+use Scalar::Util qw(refaddr);
+use AI::MXNet::Base;
+
+sub test_parameter
+{
+    my $p = gluon->Parameter('weight', shape=>[10, 10]);
+    $p->initialize(init=>'xavier', ctx=>[mx->cpu(0), mx->cpu(1)]);
+    ok(@{$p->list_data} == 2);
+    ok(@{$p->list_grad} == 2);
+    ok($p->data(mx->cpu(1))->context eq mx->cpu(1));
+    is_deeply($p->data(mx->cpu(0))->shape, [10, 10]);
+    ok($p->var->name eq  'weight');
+
+    $p->reset_ctx(ctx=>[mx->cpu(1), mx->cpu(2)]);
+    is_deeply($p->list_ctx, [mx->cpu(1), mx->cpu(2)]);
+}
+
+test_parameter();
+
+sub test_paramdict
+{
+    my $params = gluon->ParameterDict('net_');
+    $params->get('weight', shape=>[10, 10]);
+    is_deeply([$params->keys], ['net_weight']);
+    $params->initialize(ctx=>mx->cpu());
+    $params->save('test.params');
+    $params->load('test.params', ctx => mx->cpu());
+}
+
+test_paramdict();
+
+package Net;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Function::Parameters;
+extends 'AI::MXNet::Gluon::Block';
+
+sub BUILD
+{
+    my $self = shift;
+    $self->name_scope(sub {
+        $self->dense0(nn->Dense(5, in_units=>5));
+        $self->dense1(nn->Dense(5, in_units=>5));
+    });
+}
+
+method forward($x)
+{
+    return $self->dense1->($self->dense0->($x));
+}
+
+package main;
+
+sub test_parameter_sharing
+{
+    my $net1 = Net->new(prefix=>'net1_');
+    my $net2 = Net->new(prefix=>'net2_', params=>$net1->collect_params());
+    $net1->collect_params()->initialize();
+    $net2->(mx->nd->zeros([3, 5]));
+    $net1->save_params('net1.params');
+    my $net3 = Net->new(prefix=>'net3_');
+    $net3->load_params('net1.params', ctx => mx->cpu());
+}
+
+test_parameter_sharing();
+
+sub test_basic
+{
+    my $model = nn->Sequential();
+    $model->add(nn->Dense(128, activation=>'tanh', in_units=>10, flatten=>0));
+    $model->add(nn->Dropout(0.5));
+    $model->add(nn->Dense(64, activation=>'tanh', in_units=>256));
+    $model->add(nn->Dense(32, in_units=>64));
+    $model->add(nn->Activation('relu'));
+
+    # symbol
+    my $x = mx->sym->var('data');
+    my $y = $model->($x);
+    ok(@{ $y->list_arguments } == 7);
+
+    # ndarray
+    $model->collect_params()->initialize(init => mx->init->Xavier(magnitude=>2.24));
+    $x = $model->(mx->nd->zeros([32, 2, 10]));
+    is_deeply($x->shape, [32, 32]);
+    $x->wait_to_read;
+
+    $model->collect_params()->setattr(grad_req => 'null');
+    ok(not defined( ($model->collect_params()->values())[0]->_grad));
+    $model->collect_params()->setattr(grad_req => 'write');
+    ok(defined (($model->collect_params()->values())[0]->_grad));
+}
+
+test_basic();
+
+sub test_dense
+{
+    my $model = nn->Dense(128, activation=>'tanh', in_units=>10, flatten=>0, prefix=>'test_');
+    my $inputs = mx->sym->Variable('data');
+    my $outputs = $model->($inputs);
+    is_deeply({map { $_ => 1 } $model->collect_params()->keys()}, {'test_weight', 1, 'test_bias', 1});
+    is_deeply($outputs->list_outputs(), ['test_tanh_fwd_output']);
+    my ($args, $outs, $auxs) = $outputs->infer_shape(data=>[2, 3, 10]);
+    is_deeply($outs, [[2, 3, 128]]);
+
+    $model = nn->Dense(128, activation=>'relu', in_units=>30, flatten=>1, prefix=>'test2_');
+    $inputs = mx->sym->Variable('data');
+    $outputs = $model->($inputs);
+    is_deeply({map { $_ => 1 } $model->collect_params()->keys()}, {'test2_weight', 1, 'test2_bias', 1});
+    is_deeply($outputs->list_outputs(), ['test2_relu_fwd_output']);
+    ($args, $outs, $auxs) = $outputs->infer_shape(data=>[17, 2, 5, 3]);
+    is_deeply($outs, [[17, 128]]);
+}
+
+test_dense();
+
+package Net2;
+use AI::MXNet::Gluon::Mouse;
+use AI::MXNet::Function::Parameters;
+extends 'AI::MXNet::Gluon::HybridBlock';
+has 'model' => (is => 'rw');
+
+method hybrid_forward($F, $x)
+{
+    my $out = $self->model->($x);
+    return $F->add_n(map { $_->sum } @{ $out });
+}
+
+package main;
+
+sub test_symbol_block
+{
+    my $model = nn->HybridSequential();
+    $model->add(nn->Dense(128, activation=>'tanh'));
+    $model->add(nn->Dropout(0.5));
+    $model->add(nn->Dense(64, activation=>'tanh'));
+    $model->add(nn->Dense(32, in_units=>64));
+    $model->add(nn->Activation('relu'));
+
+    $model->initialize();
+
+    my $inputs = mx->sym->var('data');
+    my $outputs = $model->($inputs)->get_internals();
+    my $smodel = gluon->SymbolBlock($outputs, $inputs, params=>$model->collect_params);
+
+    ok(@{ $smodel->(mx->nd->zeros([16, 10])) } == 14);
+    my $out = $smodel->(mx->sym->var('in'));
+    ok(@{ $out } == @{ $outputs->list_outputs() });
+
+    my $net = Net2->new(model => $smodel);
+    $net->hybridize();
+    ok(ref $net->(mx->nd->zeros([16, 10])) eq 'AI::MXNet::NDArray');
+
+    $inputs = mx->sym->var('data');
+    $outputs = $model->($inputs);
+    $smodel = gluon->SymbolBlock($outputs, $inputs, params=>$model->collect_params);
+    $net = Net2->new(model => $smodel);
+    $net->hybridize();
+    ok(ref $net->(mx->nd->zeros([16, 10])) eq 'AI::MXNet::NDArray');
+}
+
+test_symbol_block();
+
+sub check_layer_forward
+{
+    my ($layer, $dshape) = @_;
+    $layer->collect_params()->initialize();
+    my $x = mx->nd->ones($dshape);
+    $x->attach_grad();
+    my $out;
+    mx->autograd->record(sub {
+        $out = $layer->($x);
+    });
+    $out->backward();
+    my $pdl_out = $out->aspdl;
+    my $pdl_dx  = $x->grad->aspdl;
+
+    $layer->hybridize();
+
+    $x = mx->nd->ones($dshape);
+    $x->attach_grad();
+    mx->autograd->record(sub {
+        $out = $layer->($x);
+    });
+    $out->backward();
+
+    ok(almost_equal($pdl_out, $out->aspdl, 1e-5));
+    ok(almost_equal($pdl_dx, $x->grad->aspdl, 1e-5));
+}
+
+sub test_conv
+{
+    my @layers1d = (
+        nn->Conv1D(16, 3, in_channels=>4),
+        nn->Conv1D(16, 3, groups=>2, in_channels=>4),
+        nn->Conv1D(16, 3, strides=>3, groups=>2, in_channels=>4),
+    );
+    for my $layer (@layers1d)
+    {
+        check_layer_forward($layer, [1, 4, 10]);
+    }
+
+    my @layers2d = (
+        nn->Conv2D(16, [3, 4], in_channels=>4),
+        nn->Conv2D(16, [5, 4], in_channels=>4),
+        nn->Conv2D(16, [3, 4], groups=>2, in_channels=>4),
+        nn->Conv2D(16, [3, 4], strides=>4, in_channels=>4),
+        nn->Conv2D(16, [3, 4], dilation=>4, in_channels=>4),
+        nn->Conv2D(16, [3, 4], padding=>4, in_channels=>4),
+    );
+    for my $layer (@layers2d)
+    {
+        check_layer_forward($layer, [1, 4, 20, 20]);
+    }
+
+    my @layers3d = (
+        nn->Conv3D(16, [1, 8, 4], in_channels=>4, activation=>'relu'),
+        nn->Conv3D(16, [5, 4, 3], in_channels=>4),
+        nn->Conv3D(16, [3, 3, 3], groups=>2, in_channels=>4),
+        nn->Conv3D(16, 4, strides=>4, in_channels=>4),
+        nn->Conv3D(16, [3, 3, 3], padding=>4, in_channels=>4),
+    );
+    for my $layer (@layers3d)
+    {
+        check_layer_forward($layer, [1, 4, 10, 10, 10]);
+    }
+
+    # These layouts only supported on GPU for now
+    my $layer = nn->Conv2D(16, [3, 3], layout=>'NHWC', in_channels=>4);
+    #check_layer_forward($layer, [1, 10, 10, 4]);
+
+    $layer = nn->Conv3D(16, [3, 3, 3], layout=>'NDHWC', in_channels=>4);
+    # check_layer_forward(layer, (1, 10, 10, 10, 4))
+}
+
+test_conv();
+
+
+sub test_deconv
+{
+    # commented out code is only supported on GPU for now
+    # my @layers1d = (
+    #     nn->Conv1DTranspose(16, 3, in_channels=>4),
+    #     nn->Conv1DTranspose(16, 3, groups=>2, in_channels=>4),
+    #     nn->Conv1DTranspose(16, 3, strides=>3, groups=>2, in_channels=>4),
+    # );
+    # for my $layer (@layers1d)
+    # {
+    #     check_layer_forward($layer, [1, 4, 10]);
+    # }
+
+
+    my @layers2d = (
+        nn->Conv2DTranspose(16, [3, 4], in_channels=>4),
+        nn->Conv2DTranspose(16, [5, 4], in_channels=>4),
+        nn->Conv2DTranspose(16, [3, 4], groups=>2, in_channels=>4),
+        nn->Conv2DTranspose(16, [3, 4], strides=>4, in_channels=>4),
+        nn->Conv2DTranspose(16, [3, 4], dilation=>4, in_channels=>4),
+        nn->Conv2DTranspose(16, [3, 4], padding=>4, in_channels=>4),
+        nn->Conv2DTranspose(16, [3, 4], strides=>4, output_padding=>3, in_channels=>4),
+    );
+    for my $layer (@layers2d)
+    {
+        check_layer_forward($layer, [1, 4, 20, 20]);
+    }
+
+    # @layers3d = (
+    #     nn->Conv3DTranspose(16, [1, 8, 4], in_channels=>4),
+    #     nn->Conv3DTranspose(16, [5, 4, 3], in_channels=>4),
+    #     nn->Conv3DTranspose(16, [3, 3, 3], groups=>2, in_channels=>4),
+    #     nn->Conv3DTranspose(16, 4, strides=>4, in_channels=>4),
+    #     nn->Conv3DTranspose(16, [3, 3, 3], padding=>4, in_channels=>4),
+    # );
+    # for my $layer (@layers3d)
+    # {
+    #     check_layer_forward($layer, [1, 4, 10, 10, 10]);
+    # }
+    #
+    my $layer = nn->Conv2DTranspose(16, [3, 3], layout=>'NHWC', in_channels=>4);
+    # check_layer_forward($layer, [1, 10, 10, 4]);
+    #
+    # $layer = nn->Conv3DTranspose(16, [3, 3, 3], layout=>'NDHWC', in_channels=>4);
+    # check_layer_forward(layer, [1, 10, 10, 10, 4]);
+}
+
+test_deconv();
+
+sub test_pool
+{
+    my @layers1d = (
+        nn->MaxPool1D(),
+        nn->MaxPool1D(3),
+        nn->MaxPool1D(3, 2),
+        nn->AvgPool1D(),
+        nn->GlobalAvgPool1D(),
+    );
+    for my $layer (@layers1d)
+    {
+        check_layer_forward($layer, [1, 2, 10]);
+    }
+
+    my @layers2d = (
+        nn->MaxPool2D(),
+        nn->MaxPool2D([3, 3]),
+        nn->MaxPool2D(3, 2),
+        nn->AvgPool2D(),
+        nn->GlobalAvgPool2D(),
+    );
+    for my $layer (@layers2d)
+    {
+        check_layer_forward($layer, [1, 2, 10, 10]);
+    }
+
+    my @layers3d = (
+        nn->MaxPool3D(),
+        nn->MaxPool3D([3, 3, 3]),
+        nn->MaxPool3D(3, 2),
+        nn->AvgPool3D(),
+        nn->GlobalAvgPool3D(),
+    );
+    for my $layer (@layers3d)
+    {
+        check_layer_forward($layer, [1, 2, 10, 10, 10]);
+    }
+
+    # test ceil_mode
+    my $x = mx->nd->zeros([2, 2, 10, 10]);
+
+    my $layer = nn->MaxPool2D(3, ceil_mode=>0);
+    $layer->collect_params()->initialize();
+    is_deeply($layer->($x)->shape, [2, 2, 3, 3]);
+
+    $layer = nn->MaxPool2D(3, ceil_mode=>1);
+    $layer->collect_params()->initialize();
+    is_deeply($layer->($x)->shape, [2, 2, 4, 4]);
+}
+
+test_pool();
+
+sub test_batchnorm
+{
+    my $layer = nn->BatchNorm(in_channels=>10);
+    check_layer_forward($layer, [2, 10, 10, 10]);
+}
+
+test_batchnorm();
+
+sub test_reshape
+{
+    my $x = mx->nd->ones([2, 4, 10, 10]);
+    my $layer = nn->Conv2D(10, 2, in_channels=>4);
+    $layer->collect_params()->initialize();
+    mx->autograd->record(sub {
+        $x = $layer->($x);
+        $x = $x->reshape([-1]);
+        $x = $x + 10;
+    });
+    $x->backward();
+}
+
+test_reshape();
+
+sub test_slice
+{
+    my $x = mx->nd->ones([5, 4, 10, 10]);
+    my $layer = nn->Conv2D(10, 2, in_channels=>4);
+    $layer->collect_params()->initialize();
+    mx->autograd->record(sub {
+        $x = $layer->($x);
+        $x = $x->slice([1,3]);
+        $x = $x + 10;
+    });
+    $x->backward();
+}
+
+test_slice();
+
+sub test_at
+{
+    my $x = mx->nd->ones([5, 4, 10, 10]);
+    my $layer = nn->Conv2D(10, 2, in_channels=>4);
+    $layer->collect_params()->initialize();
+    mx->autograd->record(sub {
+        $x = $layer->($x);
+        $x = $x->at(1);
+        $x = $x + 10;
+    });
+    $x->backward();
+}
+
+test_at();
+
+sub test_deferred_init
+{
+    my $x = mx->nd->ones([5, 4, 10, 10]);
+    my $layer = nn->Conv2D(10, 2);
+    $layer->collect_params()->initialize();
+    $layer->($x);
+}
+
+test_deferred_init();
+
+
+sub check_split_data
+{
+    my ($x, $num_slice, $batch_axis, %kwargs) = @_;
+    my $res = gluon->utils->split_data($x, $num_slice, $batch_axis, %kwargs);
+    ok(@{ $res } == $num_slice);
+    ok(almost_equal(mx->nd->concat(@$res, dim=>$batch_axis)->aspdl(), $x->aspdl()));
+}
+
+sub test_split_data
+{
+    my $x = mx->nd->random->uniform(shape=>[128, 33, 64]);
+
+    check_split_data($x, 8, 0);
+    check_split_data($x, 3, 1);
+    check_split_data($x, 4, 1, even_split=>0);
+    check_split_data($x, 15, 1, even_split=>0);
+    eval {
+        check_split_data($x, 4, 1);
+    };
+    ok($@);
+}
+
+test_split_data();
+
+sub test_flatten
+{
+    my $flatten = nn->Flatten();
+    my $x = mx->nd->zeros([3,4,5,6]);
+    is_deeply($flatten->($x)->shape, [3, 4*5*6]);
+    $x = mx->nd->zeros([3,6]);
+    is_deeply($flatten->($x)->shape, [3, 6]);
+    $x = mx->nd->zeros([3]);
+    is_deeply($flatten->($x)->shape, [3, 1]);
+}
+
+test_flatten();
+
+sub test_trainer
+{
+    my $dict_equ = sub { my ($a, $b) = @_;
+        is_deeply({ map { $_ => 1 } keys %$a }, { map { $_ => 1 } keys %$b });
+        for my $k (keys %$a)
+        {
+            ok(($a->{$k}->aspdl == $b->{$k}->aspdl)->all);
+        }
+    };
+    my $x = gluon->Parameter('x', shape=>[10]);
+    $x->initialize(ctx=>[mx->cpu(0), mx->cpu(1)], init=>'zeros');
+    my $trainer = gluon->Trainer([$x], 'sgd', {'learning_rate'=> 1.0, 'momentum'=> 0.5});
+    my $y;
+    mx->autograd->record(sub {
+        for my $w (@{ $x->list_data() })
+        {
+            $y = $w + 1;
+            $y->backward();
+        }
+    });
+    $trainer->step(1);
+
+    ok(($x->data(mx->cpu(1))->aspdl == -2)->all);
+
+    $x->lr_mult(0.5);
+
+    mx->autograd->record(sub {
+        for my $w (@{ $x->list_data() })
+        {
+            $y = $w + 1;
+            $y->backward();
+        }
+    });
+    $trainer->step(1);
+
+    ok(($x->data(mx->cpu(1))->aspdl == -4)->all);
+
+    $trainer->save_states('test.states');
+    my $states;
+    if($trainer->_update_on_kvstore)
+    {
+        $states = { %{ $trainer->_kv_store->_updater->states } };
+    }
+    else
+    {
+        $states = { %{ $trainer->_updaters->[0]->states } };
+    }
+    $trainer->load_states('test.states');
+    if($trainer->_update_on_kvstore)
+    {
+        $dict_equ->($trainer->_kv_store->_updater->states, $states);
+        ok($trainer->_optimizer eq $trainer->_kv_store->_updater->optimizer);
+    }
+    else
+    {
+        for my $updater (@{ $trainer->_updaters })
+        {
+            $dict_equ->($updater->states, $states);
+        }
+        ok($trainer->_optimizer eq $trainer->_updaters->[0]->optimizer);
+    }
+}
+
+test_trainer();
+
+sub test_block_attr_hidden
+{
+    my $b = gluon->Block();
+    # regular attributes can change types
+    $b->a(undef);
+    $b->a(1);
+}
+
+test_block_attr_hidden();
+
+sub test_block_attr_block
+{
+    my $b = gluon->Block();
+    # regular variables can't change types
+    $b->b(gluon->Block());
+    eval { $b->b([2]); };
+    ok($@ =~ /not allowed/i);
+}
+
+test_block_attr_block();
+
+sub test_block_attr_param
+{
+    my $b = gluon->Block();
+    # regular variables can't change types
+    $b->b(gluon->Parameter(name => 'test'));
+    eval { $b->b([2]); };
+    ok($@ =~ /not allowed/i);
+}
+
+test_block_attr_param();
+
+sub test_block_attr_regular
+{
+    my $b = gluon->Block();
+
+    # set block attribute also sets _children
+    $b->c(gluon->Block());
+    my $c2 = gluon->Block();
+    $b->c($c2);
+    ok(refaddr($b->c) == refaddr($c2) and refaddr($b->_children->[0]) == refaddr($c2));
+}
+
+test_block_attr_regular();
+
+sub test_embedding
+{
+    my $layer = gluon->nn->Embedding(10, 100);
+    $layer->initialize();
+    my $x = mx->nd->array([3,4,2,0,1]);
+    my $y;
+    mx->autograd->record(sub {
+        $y = $layer->($x);
+        $y->backward();
+    });
+    ok(($layer->weight->grad->slice([0,4]) == 1)->aspdl->all);
+    ok(($layer->weight->grad->slice([5, -1]) == 0)->aspdl->all);
+}
+
+test_embedding();
diff --git a/perl-package/AI-MXNet/t/test_gluon_data.t b/perl-package/AI-MXNet/t/test_gluon_data.t
new file mode 100644
index 000000000000..92e83b968d23
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_gluon_data.t
@@ -0,0 +1,128 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use AI::MXNet qw(mx);
+use AI::MXNet::Gluon qw(gluon);
+use AI::MXNet::Gluon::Utils qw(download);
+use Archive::Tar;
+use AI::MXNet::TestUtils qw(almost_equal);
+use AI::MXNet::Base;
+use File::Path qw(make_path);
+use IO::File;
+use Test::More tests => 52;
+
+sub test_array_dataset
+{
+    my $X = mx->nd->random->uniform(shape=>[10, 20]);
+    my $Y = mx->nd->random->uniform(shape=>[10]);
+    my $dataset = gluon->data->ArrayDataset($X, $Y);
+    my $loader = gluon->data->DataLoader($dataset, 2);
+    enumerate(sub {
+        my ($i, $d) = @_;
+        my ($x, $y) = @$d;
+        ok(almost_equal($x->aspdl, $X->slice([$i*2,($i+1)*2-1])->aspdl));
+        ok(almost_equal($y->aspdl, $Y->slice([$i*2,($i+1)*2-1])->aspdl));
+    }, \@{ $loader });
+}
+
+test_array_dataset();
+
+sub prepare_record
+{
+    my ($copy) = @_;
+    if(not -d "data/test_images")
+    {
+        make_path('data/test_images');
+    }
+    if(not -d "data/test_images/test_images")
+    {
+        download("http://data.mxnet.io/data/test_images.tar.gz", path => "data/test_images.tar.gz");
+        my $f = Archive::Tar->new('data/test_images.tar.gz');
+        chdir('data');
+        $f->extract;
+        chdir('..');
+    }
+    if(not -f 'data/test.rec')
+    {
+        my @imgs = glob('data/test_images/*');
+        my $record = mx->recordio->MXIndexedRecordIO('data/test.idx', 'data/test.rec', 'w');
+        enumerate(sub {
+            my ($i, $img) = @_;
+            my $str_img = join('',IO::File->new("./$img")->getlines);
+            my $s = mx->recordio->pack([0, $i, $i, 0], $str_img);
+            $record->write_idx($i, $s);
+        }, \@imgs);
+    }
+    if($copy)
+    {
+        make_path('data/images/test_images');
+        `cp  data/test_images/* data/images/test_images`;
+    }
+    return 'data/test.rec';
+}
+
+sub test_recordimage_dataset
+{
+    my $recfile = prepare_record();
+    my $dataset = gluon->data->vision->ImageRecordDataset($recfile);
+    my $loader = gluon->data->DataLoader($dataset, 1);
+    enumerate(sub {
+        my ($i, $d) = @_;
+        my ($x, $y) = @$d;
+        ok($x->shape->[0] == 1 and $x->shape->[3] == 3);
+        ok($y->asscalar == $i);
+    }, \@{ $loader });
+}
+
+test_recordimage_dataset();
+
+sub test_sampler
+{
+    my $seq_sampler = gluon->data->SequentialSampler(10);
+    is_deeply(\@{ $seq_sampler }, [0..9]);
+    my $rand_sampler = gluon->data->RandomSampler(10);
+    is_deeply([sort { $a <=> $b } @{ $rand_sampler }], [0..9]);
+    my $seq_batch_keep = gluon->data->BatchSampler($seq_sampler, 3, 'keep');
+    is_deeply([map { @$_ } @{ $seq_batch_keep }], [0..9]);
+    my $seq_batch_discard = gluon->data->BatchSampler($seq_sampler, 3, 'discard');
+    is_deeply([map { @$_ } @{ $seq_batch_discard }], [0..8]);
+    my $rand_batch_keep = gluon->data->BatchSampler($rand_sampler, 3, 'keep');
+    is_deeply([sort { $a <=> $b } map { @$_ } @{ $rand_batch_keep }], [0..9]);
+}
+
+test_sampler();
+
+sub test_datasets
+{
+    ok(gluon->data->vision->MNIST(root=>'data/mnist')->len == 60000);
+    ok(gluon->data->vision->FashionMNIST(root=>'data/fashion-mnist')->len == 60000);
+    ok(gluon->data->vision->CIFAR10(root=>'data/cifar10', train=>0)->len == 10000);
+}
+
+test_datasets();
+
+sub test_image_folder_dataset
+{
+    prepare_record(1);
+    my $dataset = gluon->data->vision->ImageFolderDataset('data/images');
+    is_deeply($dataset->synsets, ['test_images']);
+    ok(@{ $dataset->items } == 16);
+}
+
+test_image_folder_dataset();
diff --git a/perl-package/AI-MXNet/t/test_gluon_rnn.t b/perl-package/AI-MXNet/t/test_gluon_rnn.t
new file mode 100644
index 000000000000..13f22931468c
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_gluon_rnn.t
@@ -0,0 +1,334 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+use strict;
+use warnings;
+use Test::More tests => 77;
+use AI::MXNet 'mx';
+use AI::MXNet::Gluon 'gluon';
+use AI::MXNet::TestUtils qw/allclose almost_equal/;
+use AI::MXNet::Base;
+use Scalar::Util 'blessed';
+
+sub test_rnn
+{
+    my $cell = gluon->rnn->RNNCell(100, prefix=>'rnn_');
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply([sort $cell->collect_params()->keys()], ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']);
+    is_deeply($outputs->list_outputs(), ['rnn_t0_out_output', 'rnn_t1_out_output', 'rnn_t2_out_output']);
+
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 100], [10, 100], [10, 100]]);
+}
+
+test_rnn();
+
+sub test_lstm
+{
+    my $cell = gluon->rnn->LSTMCell(100, prefix=>'rnn_');
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply([sort $cell->collect_params()->keys()], ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']);
+    is_deeply($outputs->list_outputs(), ['rnn_t0_out_output', 'rnn_t1_out_output', 'rnn_t2_out_output']);
+
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 100], [10, 100], [10, 100]]);
+}
+
+test_lstm();
+
+sub test_lstm_forget_bias
+{
+    my $forget_bias = 2;
+    my $stack = gluon->rnn->SequentialRNNCell();
+    $stack->add(gluon->rnn->LSTMCell(100, i2h_bias_initializer=>mx->init->LSTMBias($forget_bias), prefix=>'l0_'));
+    $stack->add(gluon->rnn->LSTMCell(100, i2h_bias_initializer=>mx->init->LSTMBias($forget_bias), prefix=>'l1_'));
+
+    my $dshape = [32, 1, 200];
+    my $data = mx->sym->Variable('data');
+
+    my ($sym) = $stack->unroll(1, $data, merge_outputs=>1);
+    my $mod = mx->mod->Module($sym, context=>mx->cpu(0));
+    $mod->bind(data_shapes=>[['data', $dshape]]);
+
+    $mod->init_params();
+
+    my ($bias_argument) = grep { /i2h_bias$/ } @{ $sym->list_arguments() };
+    my $expected_bias = pdl((0)x100, ($forget_bias)x100, (0)x200);
+    ok(allclose(($mod->get_params())[0]->{$bias_argument}->aspdl, $expected_bias));
+}
+
+test_lstm_forget_bias();
+
+sub test_gru
+{
+    my $cell = gluon->rnn->GRUCell(100, prefix=>'rnn_');
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply([sort $cell->collect_params()->keys()], ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']);
+    is_deeply($outputs->list_outputs(), ['rnn_t0_out_output', 'rnn_t1_out_output', 'rnn_t2_out_output']);
+
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 100], [10, 100], [10, 100]]);
+}
+
+test_gru();
+
+sub test_residual
+{
+    my $cell = gluon->rnn->ResidualCell(gluon->rnn->GRUCell(50, prefix=>'rnn_'));
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..1];
+    my ($outputs) = $cell->unroll(2, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply([sort $cell->collect_params()->keys()], ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']);
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50]);
+    is_deeply($outs, [[10, 50], [10, 50]]);
+    $outputs = $outputs->eval(args => { rnn_t0_data=>mx->nd->ones([10, 50]),
+                           rnn_t1_data=>mx->nd->ones([10, 50]),
+                           rnn_i2h_weight=>mx->nd->zeros([150, 50]),
+                           rnn_i2h_bias=>mx->nd->zeros([150]),
+                           rnn_h2h_weight=>mx->nd->zeros([150, 50]),
+                           rnn_h2h_bias=>mx->nd->zeros([150]) });
+    my $expected_outputs = mx->nd->ones([10, 50]);
+    ok(($outputs->[0] == $expected_outputs)->aspdl->all);
+    ok(($outputs->[1] == $expected_outputs)->aspdl->all);
+}
+
+test_residual();
+
+sub test_residual_bidirectional
+{
+    my $cell = gluon->rnn->ResidualCell(
+        gluon->rnn->BidirectionalCell(
+            gluon->rnn->GRUCell(25, prefix=>'rnn_l_'),
+            gluon->rnn->GRUCell(25, prefix=>'rnn_r_')
+        )
+    );
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..1];
+    my ($outputs) = $cell->unroll(2, $inputs, merge_outputs => 0);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply([sort $cell->collect_params()->keys()],
+                ['rnn_l_h2h_bias', 'rnn_l_h2h_weight', 'rnn_l_i2h_bias', 'rnn_l_i2h_weight',
+                'rnn_r_h2h_bias', 'rnn_r_h2h_weight', 'rnn_r_i2h_bias', 'rnn_r_i2h_weight']);
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50]);
+    is_deeply($outs, [[10, 50], [10, 50]]);
+    $outputs = $outputs->eval(args => { rnn_t0_data=>mx->nd->ones([10, 50])+5,
+                           rnn_t1_data=>mx->nd->ones([10, 50])+5,
+                           rnn_l_i2h_weight=>mx->nd->zeros([75, 50]),
+                           rnn_l_i2h_bias=>mx->nd->zeros([75]),
+                           rnn_l_h2h_weight=>mx->nd->zeros([75, 25]),
+                           rnn_l_h2h_bias=>mx->nd->zeros([75]),
+                           rnn_r_i2h_weight=>mx->nd->zeros([75, 50]),
+                           rnn_r_i2h_bias=>mx->nd->zeros([75]),
+                           rnn_r_h2h_weight=>mx->nd->zeros([75, 25]),
+                           rnn_r_h2h_bias=>mx->nd->zeros([75]),
+    });
+    my $expected_outputs = mx->nd->ones([10, 50])+5;
+    ok(($outputs->[0] == $expected_outputs)->aspdl->all);
+    ok(($outputs->[1] == $expected_outputs)->aspdl->all);
+}
+
+test_residual_bidirectional();
+
+sub test_stack
+{
+    my $cell = gluon->rnn->SequentialRNNCell();
+    for my $i (0..4)
+    {
+        if($i == 1)
+        {
+            $cell->add(gluon->rnn->ResidualCell(gluon->rnn->LSTMCell(100, prefix=>"rnn_stack${i}_")));
+        }
+        else
+        {
+            $cell->add(gluon->rnn->LSTMCell(100, prefix=>"rnn_stack${i}_"));
+        }
+    }
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    my %keys = map { $_ => 1 } $cell->collect_params()->keys();
+    for my $i (0..4)
+    {
+        ok($keys{"rnn_stack${i}_h2h_weight"});
+        ok($keys{"rnn_stack${i}_h2h_bias"});
+        ok($keys{"rnn_stack${i}_i2h_weight"});
+        ok($keys{"rnn_stack${i}_i2h_bias"});
+    }
+    is_deeply($outputs->list_outputs(), ['rnn_stack4_t0_out_output', 'rnn_stack4_t1_out_output', 'rnn_stack4_t2_out_output']);
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 100], [10, 100], [10, 100]]);
+}
+
+test_stack();
+
+sub test_bidirectional
+{
+    my $cell = gluon->rnn->BidirectionalCell(
+            gluon->rnn->LSTMCell(100, prefix=>'rnn_l0_'),
+            gluon->rnn->LSTMCell(100, prefix=>'rnn_r0_'),
+            output_prefix=>'rnn_bi_');
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    is_deeply($outputs->list_outputs(), ['rnn_bi_t0_output', 'rnn_bi_t1_output', 'rnn_bi_t2_output']);
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 200], [10, 200], [10, 200]]);
+}
+
+test_bidirectional();
+
+sub test_zoneout
+{
+    my $cell = gluon->rnn->ZoneoutCell(gluon->rnn->RNNCell(100, prefix=>'rnn_'), zoneout_outputs=>0.5,
+                              zoneout_states=>0.5);
+    my $inputs = [map { mx->sym->Variable("rnn_t${_}_data") } 0..2];
+    my ($outputs) = $cell->unroll(3, $inputs);
+    $outputs = mx->sym->Group($outputs);
+    my (undef, $outs) = $outputs->infer_shape(rnn_t0_data=>[10,50], rnn_t1_data=>[10,50], rnn_t2_data=>[10,50]);
+    is_deeply($outs, [[10, 100], [10, 100], [10, 100]]);
+}
+
+test_zoneout();
+
+sub check_rnn_forward
+{
+    my ($layer, $inputs, $deterministic) = @_;
+    $deterministic //= 1;
+    $inputs->attach_grad();
+    $layer->collect_params()->initialize();
+    my $out;
+    mx->autograd->record(sub {
+        $out = ($layer->unroll(3, $inputs, merge_outputs=>0))[0];
+        mx->autograd->backward($out);
+        $out = ($layer->unroll(3, $inputs, merge_outputs=>1))[0];
+        $out->backward;
+    });
+
+    my $pdl_out = $out->aspdl;
+    my $pdl_dx = $inputs->grad->aspdl;
+
+    $layer->hybridize;
+
+    mx->autograd->record(sub {
+        $out = ($layer->unroll(3, $inputs, merge_outputs=>0))[0];
+        mx->autograd->backward($out);
+        $out = ($layer->unroll(3, $inputs, merge_outputs=>1))[0];
+        $out->backward;
+    });
+
+    if($deterministic)
+    {
+        ok(almost_equal($pdl_out, $out->aspdl, 1e-3));
+        ok(almost_equal($pdl_dx, $inputs->grad->aspdl, 1e-3));
+    }
+}
+
+sub test_rnn_cells
+{
+    check_rnn_forward(gluon->rnn->LSTMCell(100, input_size=>200), mx->nd->ones([8, 3, 200]));
+    check_rnn_forward(gluon->rnn->RNNCell(100, input_size=>200), mx->nd->ones([8, 3, 200]));
+    check_rnn_forward(gluon->rnn->GRUCell(100, input_size=>200), mx->nd->ones([8, 3, 200]));
+    my $bilayer = gluon->rnn->BidirectionalCell(
+        gluon->rnn->LSTMCell(100, input_size=>200),
+        gluon->rnn->LSTMCell(100, input_size=>200)
+    );
+    check_rnn_forward($bilayer, mx->nd->ones([8, 3, 200]));
+    check_rnn_forward(gluon->rnn->DropoutCell(0.5), mx->nd->ones([8, 3, 200]), 0);
+    check_rnn_forward(
+        gluon->rnn->ZoneoutCell(
+            gluon->rnn->LSTMCell(100, input_size=>200),
+            0.5, 0.2
+        ),
+        mx->nd->ones([8, 3, 200]),
+        0
+    );
+    my $net = gluon->rnn->SequentialRNNCell();
+    $net->add(gluon->rnn->LSTMCell(100, input_size=>200));
+    $net->add(gluon->rnn->RNNCell(100, input_size=>100));
+    $net->add(gluon->rnn->GRUCell(100, input_size=>100));
+    check_rnn_forward($net, mx->nd->ones([8, 3, 200]));
+}
+
+test_rnn_cells();
+
+sub check_rnn_layer_forward
+{
+    my ($layer, $inputs, $states) = @_;
+    $layer->collect_params()->initialize();
+    $inputs->attach_grad;
+    my $out;
+    mx->autograd->record(sub {
+        $out = $layer->($inputs, $states);
+        if(defined $states)
+        {
+            ok(@$out == 2);
+            $out = $out->[0];
+        }
+        else
+        {
+            ok(blessed $out and $out->isa('AI::MXNet::NDArray'));
+        }
+        $out->backward();
+    });
+
+    my $pdl_out = $out->aspdl;
+    my $pdl_dx = $inputs->grad->aspdl;
+    $layer->hybridize;
+
+    mx->autograd->record(sub {
+        $out = $layer->($inputs, $states);
+        if(defined $states)
+        {
+            ok(@$out == 2);
+            $out = $out->[0]
+        }
+        else
+        {
+            ok(blessed $out and $out->isa('AI::MXNet::NDArray'));
+        }
+        $out->backward();
+    });
+
+    ok(almost_equal($pdl_out, $out->aspdl, 1e-3));
+    ok(almost_equal($pdl_dx, $inputs->grad->aspdl, 1e-3));
+}
+
+sub test_rnn_layers
+{
+    check_rnn_layer_forward(gluon->rnn->RNN(10, 2), mx->nd->ones([8, 3, 20]));
+    check_rnn_layer_forward(gluon->rnn->RNN(10, 2), mx->nd->ones([8, 3, 20]), mx->nd->ones([2, 3, 10]));
+    check_rnn_layer_forward(gluon->rnn->LSTM(10, 2), mx->nd->ones([8, 3, 20]));
+    check_rnn_layer_forward(gluon->rnn->LSTM(10, 2), mx->nd->ones([8, 3, 20]), [mx->nd->ones([2, 3, 10]), mx->nd->ones([2, 3, 10])]);
+    check_rnn_layer_forward(gluon->rnn->GRU(10, 2), mx->nd->ones([8, 3, 20]));
+    check_rnn_layer_forward(gluon->rnn->GRU(10, 2), mx->nd->ones([8, 3, 20]), mx->nd->ones([2, 3, 10]));
+
+    my $net = gluon->nn->Sequential();
+    $net->add(gluon->rnn->LSTM(10, 2, bidirectional=>1));
+    $net->add(gluon->nn->BatchNorm(axis=>2));
+    $net->add(gluon->nn->Flatten());
+    $net->add(gluon->nn->Dense(3, activation=>'relu'));
+    $net->collect_params()->initialize();
+    mx->autograd->record(sub {
+        $net->(mx->nd->ones([2, 3, 10]))->backward();
+    });
+}
+
+test_rnn_layers();
diff --git a/perl-package/AI-MXNet/t/test_loss.t b/perl-package/AI-MXNet/t/test_loss.t
new file mode 100644
index 000000000000..bfdf3049950a
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_loss.t
@@ -0,0 +1,304 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use Test::More tests => 24;
+use AI::MXNet 'mx';
+use AI::MXNet::Gluon 'gluon';
+use AI::MXNet::TestUtils 'almost_equal';
+use Hash::Ordered;
+
+sub test_loss_ndarray
+{
+    my $output     = mx->nd->array([1, 2, 3, 4]);
+    my $label      = mx->nd->array([1, 3, 5, 7]);
+    my $weighting  = mx->nd->array([0.5, 1, 0.5, 1]);
+
+    my $loss = gluon->loss->L1Loss();
+    ok(mx->nd->sum($loss->($output, $label))->asscalar() == 6);
+    $loss = gluon->loss->L1Loss(weight=>0.5);
+    ok(mx->nd->sum($loss->($output, $label))->asscalar() == 3);
+    $loss = gluon->loss->L1Loss();
+    ok(mx->nd->sum($loss->($output, $label, $weighting))->asscalar() == 5);
+
+    $loss = gluon->loss->L2Loss();
+    ok(mx->nd->sum($loss->($output, $label))->asscalar() == 7);
+    $loss = gluon->loss->L2Loss(weight=>0.25);
+    ok(mx->nd->sum($loss->($output, $label))->asscalar() == 1.75);
+    $loss = gluon->loss->L2Loss();
+    ok(mx->nd->sum($loss->($output, $label, $weighting))->asscalar() == 6);
+
+    $output    = mx->nd->array([[0, 2], [1, 4]]);
+    $label     = mx->nd->array([0, 1]);
+    $weighting = mx->nd->array([[0.5], [1.0]]);
+
+    $loss = gluon->loss->SoftmaxCrossEntropyLoss();
+    my $L = $loss->($output, $label)->aspdl();
+    ok(almost_equal($L, mx->nd->array([ 2.12692809,  0.04858733])->aspdl));
+
+    $L = $loss->($output, $label, $weighting)->aspdl();
+    ok(almost_equal($L, mx->nd->array([ 1.06346405,  0.04858733])->aspdl));
+}
+
+test_loss_ndarray();
+
+sub get_net
+{
+    my ($num_hidden, $flatten) = @_;
+    $flatten //= 1;
+    my $data = mx->symbol->Variable('data');
+    my $fc1 = mx->symbol->FullyConnected($data, name=>'fc1', num_hidden=>128, flatten=>$flatten);
+    my $act1 = mx->symbol->Activation($fc1, name=>'relu1', act_type=>"relu");
+    my $fc2 = mx->symbol->FullyConnected($act1, name => 'fc2', num_hidden => 64, flatten=>$flatten);
+    my $act2 = mx->symbol->Activation($fc2, name=>'relu2', act_type=>"relu");
+    my $fc3 = mx->symbol->FullyConnected($act2, name=>'fc3', num_hidden=>$num_hidden, flatten=>$flatten);
+    return $fc3;
+}
+
+sub test_ce_loss
+{
+    my $nclass = 10;
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, $nclass]);
+    my $label = mx->nd->array([qw/3 6 5 4 8 9 1 7 9 6 8 0 5 0 9 6 2 0 5 2/], dtype=>'int32');
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label');
+    my $output = get_net($nclass);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->SoftmaxCrossEntropyLoss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    local($AI::MXNet::Logging::silent) = 1;
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam');
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.1);
+}
+
+test_ce_loss();
+
+sub test_bce_loss
+{
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, 20]);
+    my $label = mx->nd->array([qw/1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0/], dtype=>'float32');
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label');
+    my $output = get_net(1);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->SigmoidBinaryCrossEntropyLoss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    local($AI::MXNet::Logging::silent) = 1;
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam',
+            initializer=>mx->init->Xavier(magnitude=>2));
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.01);
+}
+
+test_bce_loss();
+
+sub test_bce_equal_ce2
+{
+    my $N = 100;
+    my $loss1 = gluon->loss->SigmoidBCELoss(from_sigmoid=>1);
+    my $loss2 = gluon->loss->SoftmaxCELoss(from_logits=>1);
+    my $out1 = mx->random->uniform(0, 1, shape=>[$N, 1]);
+    my $out2 = mx->nd->log(mx->nd->concat(1-$out1, $out1, dim=>1) + 1e-8);
+    my $label = mx->nd->round(mx->random->uniform(0, 1, shape=>[$N, 1]));
+    ok(almost_equal($loss1->($out1, $label)->aspdl, $loss2->($out2, $label)->aspdl));
+}
+
+test_bce_equal_ce2();
+
+sub test_kl_loss
+{
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, 10]);
+    my $label = mx->nd->softmax(mx->random->uniform(0, 1, shape=>[$N, 2]));
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label');
+    my $output = mx->sym->log_softmax(get_net(2));
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->KLDivLoss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam');
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.05);
+}
+
+test_kl_loss();
+
+sub test_l2_loss
+{
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, 10]);
+    my $label = mx->nd->softmax(mx->random->uniform(-1, 1, shape=>[$N, 1]));
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label', shuffle=>1);
+    my $output = get_net(1);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->L2Loss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam');
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.1);
+}
+
+test_l2_loss();
+
+sub test_l1_loss
+{
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, 10]);
+    my $label = mx->nd->softmax(mx->random->uniform(-1, 1, shape=>[$N, 1]));
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label', shuffle=>1);
+    my $output = get_net(1);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->L1Loss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam');
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.1);
+}
+
+test_l1_loss();
+
+sub test_ctc_loss
+{
+    my $loss = gluon->loss->CTCLoss();
+    my $l = $loss->(mx->nd->ones([2,20,4]), mx->nd->array([[1,0,-1,-1],[2,1,1,-1]]));
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+
+    $loss = gluon->loss->CTCLoss(layout=>'TNC');
+    $l = $loss->(mx->nd->ones([20,2,4]), mx->nd->array([[1,0,-1,-1],[2,1,1,-1]]));
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+
+    $loss = gluon->loss->CTCLoss(layout=>'TNC', label_layout=>'TN');
+    $l = $loss->(mx->nd->ones([20,2,4]), mx->nd->array([[1,0,-1,-1],[2,1,1,-1]])->T);
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+
+    $loss = gluon->loss->CTCLoss();
+    $l = $loss->(mx->nd->ones([2,20,4]), mx->nd->array([[2,1,2,2],[3,2,2,2]]), undef, mx->nd->array([2,3]));
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+
+    $loss = gluon->loss->CTCLoss();
+    $l = $loss->(mx->nd->ones([2,25,4]), mx->nd->array([[2,1,-1,-1],[3,2,2,-1]]), mx->nd->array([20,20]));
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+
+    $loss = gluon->loss->CTCLoss();
+    $l = $loss->(mx->nd->ones([2,25,4]), mx->nd->array([[2,1,3,3],[3,2,2,3]]), mx->nd->array([20,20]), mx->nd->array([2,3]));
+    ok(almost_equal($l->aspdl, mx->nd->array([18.82820702, 16.50581741])->aspdl));
+}
+
+test_ctc_loss();
+
+sub test_ctc_loss_train
+{
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, 20, 10]);
+    my $label = mx->nd->arange(start => 4, repeat=>$N)->reshape([$N, 4]);
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label', shuffle=>1);
+    my $output = get_net(5, 0);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->CTCLoss(layout=>'NTC', label_layout=>'NT');
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 1},
+            initializer=>mx->init->Xavier(magnitude=>2), eval_metric=>mx->metric->Loss(),
+            optimizer=>'adam');
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 10);
+}
+
+test_ctc_loss_train();
+
+sub test_sample_weight_loss
+{
+    my $nclass = 10;
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, $nclass]);
+    my $label = mx->nd->array([qw/2 0 8 4 3 4 2 5 5 7 2 3 7 1 2 6 4 2 8 0/], dtype=>'int32');
+    my $weight = mx->nd->array([(1)x10,(0)x10]);
+    my $data_iter = mx->io->NDArrayIter(
+        $data,
+        Hash::Ordered->new(label => $label, w => $weight),
+        batch_size=>10
+    );
+    my $output = get_net($nclass);
+    my $l = mx->symbol->Variable('label');
+    my $w = mx->symbol->Variable('w');
+    my $Loss = gluon->loss->SoftmaxCrossEntropyLoss();
+    my $loss = $Loss->($output, $l, $w);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label', 'w']);
+    $mod->fit($data_iter, num_epoch=>50, optimizer_params=>{learning_rate => 0.01},
+            eval_metric=>mx->metric->Loss(), optimizer=>'adam');
+    $data_iter = mx->io->NDArrayIter(
+        $data->slice([10,$data->len-1]),
+        Hash::Ordered->new(label => $label, w => $weight),
+        batch_size=>10
+    );
+    my $score =  $mod->score($data_iter, mx->metric->Loss())->{loss};
+    ok($score > 1);
+    $data_iter = mx->io->NDArrayIter(
+        $data->slice([0,9]),
+        Hash::Ordered->new(label => $label, w => $weight),
+        batch_size=>10
+    );
+    $score =  $mod->score($data_iter, mx->metric->Loss())->{loss};
+    ok($score < 0.05);
+}
+
+test_sample_weight_loss();
+
+sub test_saveload
+{
+    mx->random->seed(1234);
+    my $nclass = 10;
+    my $N = 20;
+    my $data = mx->random->uniform(-1, 1, shape=>[$N, $nclass]);
+    my $label = mx->nd->array([qw/2 0 8 4 3 4 2 5 5 7 2 3 7 1 2 6 4 2 8 0/], dtype=>'int32');
+    my $data_iter = mx->io->NDArrayIter($data, $label, batch_size=>10, label_name=>'label');
+    my $output = get_net($nclass);
+    my $l = mx->symbol->Variable('label');
+    my $Loss = gluon->loss->SoftmaxCrossEntropyLoss();
+    my $loss = $Loss->($output, $l);
+    $loss = mx->sym->make_loss($loss);
+    local($AI::MXNet::Logging::silent) = 1;
+    my $mod = mx->mod->Module($loss, data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>100, optimizer_params=>{learning_rate => 1},
+            eval_metric=>mx->metric->Loss());
+    $mod->save_checkpoint('test', 100, 1);
+    $mod = mx->mod->Module->load('test', 100, 1,
+                             data_names=>['data'], label_names=>['label']);
+    $mod->fit($data_iter, num_epoch=>100, optimizer_params=>{learning_rate => 1},
+            eval_metric=>mx->metric->Loss()
+    );
+    ok($mod->score($data_iter, mx->metric->Loss())->{loss} < 0.05);
+}
+
+test_saveload();
diff --git a/perl-package/AI-MXNet/t/test_metric.t b/perl-package/AI-MXNet/t/test_metric.t
new file mode 100644
index 000000000000..031f2052b780
--- /dev/null
+++ b/perl-package/AI-MXNet/t/test_metric.t
@@ -0,0 +1,45 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+use strict;
+use warnings;
+use Test::More tests => 6;
+use JSON::PP;
+use AI::MXNet 'mx';
+
+sub check_metric
+{
+    my ($metric, @args) = @_;
+    $metric = mx->metric->create($metric, @args);
+    my $str_metric = encode_json($metric->get_config());
+    my $metric2 = mx->metric->create($str_metric);
+    is_deeply($metric->get_config(), $metric2->get_config());
+}
+
+
+sub test_metrics
+{
+    check_metric('acc', axis=>0);
+    check_metric('f1');
+    check_metric('perplexity', -1);
+    check_metric('pearsonr');
+    check_metric('confidence', 2, [0.5, 0.9]);
+    my $composite = mx->metric->create(['acc', 'f1']);
+    check_metric($composite);
+}
+
+test_metrics();
diff --git a/perl-package/AI-MXNet/t/test_module.t b/perl-package/AI-MXNet/t/test_module.t
index 4d19a8e7d5df..7c5690a68b15 100644
--- a/perl-package/AI-MXNet/t/test_module.t
+++ b/perl-package/AI-MXNet/t/test_module.t
@@ -4,7 +4,6 @@ use Test::More tests => 257;
 use AI::MXNet qw(mx);
 use AI::MXNet::Base;
 use AI::MXNet::TestUtils qw(almost_equal enumerate same_array dies_like);
-use Data::Dumper;
 
 sub test_module_layout
 {
diff --git a/perl-package/AI-MXNet/t/test_ndarray.t b/perl-package/AI-MXNet/t/test_ndarray.t
index 4faf464d3b56..b40ed8fa78ab 100644
--- a/perl-package/AI-MXNet/t/test_ndarray.t
+++ b/perl-package/AI-MXNet/t/test_ndarray.t
@@ -1,8 +1,8 @@
 use strict;
 use warnings;
 use AI::MXNet qw(mx);
-use AI::MXNet::TestUtils qw(almost_equal);
-use Test::More tests => 10;
+use AI::MXNet::TestUtils qw(almost_equal same);
+use Test::More tests => 17;
 
 sub test_ndarray_reshape
 {
@@ -58,15 +58,71 @@ sub test_cached
     my $data = mx->nd->ones([3, 4, 10, 10]);
     my $weight = mx->nd->ones([10, 4, 3, 3]);
     my $bias = mx->nd->ones([10]);
-    my $o1 = &{$op}($data, $weight, $bias);
+    my $o1 = $op->($data, $weight, $bias);
     $bias .= 2;
-    my $o2 = &{$op}($data, $weight, $bias);
+    my $o2 = $op->($data, $weight, $bias);
     ok(almost_equal($o2->aspdl, $o1->aspdl+1));
     $o2 .= 0;
-    &{$op}($data, $weight, $bias, out=>$o2);
+    $op->($data, $weight, $bias, out=>$o2);
     ok(almost_equal($o2->aspdl, $o1->aspdl+1));
+
+    $weight->attach_grad();
+    $bias->attach_grad();
+    my $o;
+    mx->autograd->record(sub {
+        $bias = $bias + 1;
+        $o = $op->($data, $weight, $bias);
+        $o = $o * 2;
+        $o->backward();
+    });
+
+    mx->autograd->record(sub {
+        $bias = $bias + 1;
+        $o = $op->($data, $weight, $bias);
+        $o = $o * 2;
+        $o->backward(retain_graph=>1);
+        $o->backward();
+    });
+
+    # try a different shape
+    $data = mx->nd->ones([5, 2, 10, 10]);
+    $weight = mx->nd->ones([10, 2, 3, 3]);
+    $bias = mx->nd->ones([10]);
+    $data->attach_grad;
+
+    mx->autograd->record(sub {
+        $bias = $bias + 1;
+        $o = $op->($data, $weight, $bias);
+        $o = $o * 2;
+        $o->backward();
+    });
+}
+
+sub test_ndarray_slice
+{
+    my $shape = [10];
+    my $A = mx->random->uniform(-10, 10, $shape);
+    my $A2 = $A->aspdl;
+    ok(same($A->slice([3,7])->aspdl, $A2->slice([3, 7])));
+    $A2->slice([3, 7]) *= 10;
+    $A->slice([3,7]) .= $A2->slice([3, 7]);
+    ok(same($A->slice([3,7])->aspdl, $A2->slice([3, 7])));
+
+    $shape = [3,4,5,6,7];
+    $A = mx->nd->random->uniform(shape=>$shape);
+    $A2 = $A->aspdl;
+
+    ok(same($A->slice([1], [3,3], 'X', [1,4], 'X')->aspdl, $A2->slice('X', [1,4], 'X', [3,3], [1])));
+    ok(($A->slice([1], [3,3], 'X', [1,4], 'X') == mx->nd->array($A2->slice('X', [1,4], 'X', [3,3], [1])))->aspdl->all);
+
+    ok($A->slice(1,2,3,4,5)->asscalar() == $A2->at(5, 4, 3, 2, 1));
+
+    my $a = mx->nd->array([[0, 1], [2, 3]]);
+    ok(($a->slice([[1, 1, 0], [0, 1, 0]])->aspdl == mx->nd->array([2, 3, 0])->aspdl)->all);
+    ok(($a->slice([mx->nd->array([1, 1, 0]), mx->nd->array([0, 1, 0])])->aspdl == mx->nd->array([2, 3, 0])->aspdl)->all);
 }
 
+test_ndarray_slice();
 test_ndarray_reshape();
 test_moveaxis();
 test_output();
diff --git a/perl-package/AI-MXNet/t/test_optimizers.t b/perl-package/AI-MXNet/t/test_optimizers.t
index 52ff3072d9eb..d87dd0d566a7 100644
--- a/perl-package/AI-MXNet/t/test_optimizers.t
+++ b/perl-package/AI-MXNet/t/test_optimizers.t
@@ -62,13 +62,13 @@ method update($index, $weight, $grad, $state)
     $grad = $grad * $self->rescale_grad + $wd * $weight;
     if($self->clip_gradient)
     {
-        mx->nd->clip($grad, -$self->clip_gradient, $self->clip_gradient, { out => $grad });
+        mx->nd->clip($grad, -$self->clip_gradient, $self->clip_gradient, out => $grad);
     }
     $mean *= $self->beta1;
     $mean += $grad * (1 - $self->beta1);
 
     $variance *= $self->beta2;
-    $variance += (1 - $self->beta2) * mx->nd->square($grad, { out => $grad });
+    $variance += (1 - $self->beta2) * mx->nd->square($grad, out => $grad);
 
     my $coef1 = 1 - $self->beta1**$t;
     my $coef2 = 1 - $self->beta2**$t;
@@ -182,7 +182,7 @@ method update($index, $weight, $grad, $state)
     }
     if($self->clip_weights)
     {
-        mx->nd->clip($weight, -$self->clip_weights, $self->clip_weights, { out => $weight });
+        mx->nd->clip($weight, -$self->clip_weights, $self->clip_weights, out => $weight);
     }
 }
 
diff --git a/perl-package/AI-MXNet/t/test_random.t b/perl-package/AI-MXNet/t/test_random.t
index 82175948efc4..c95a199f2104 100644
--- a/perl-package/AI-MXNet/t/test_random.t
+++ b/perl-package/AI-MXNet/t/test_random.t
@@ -1,55 +1,210 @@
 use strict;
 use warnings;
-use Test::More tests => 8;
+use Test::More tests => 505;
 use AI::MXNet qw(mx);
-use AI::MXNet::TestUtils qw(same);
+use AI::MXNet::TestUtils qw(same enumerate);
 
 sub check_with_device
 {
-    my ($device)     = @_;
-    my ($a, $b)      = (-10, 10);
-    my ($mu, $sigma) = (10, 2);
-    my $shape        = [100, 100];
-    mx->random->seed(128);
-    my $ret1 = mx->random->normal($mu, $sigma, $shape, { ctx => $device });
-    my $un1  = mx->random->uniform($a, $b, $shape, { ctx => $device });
-    mx->random->seed(128);
-    my $ret2 = mx->random->normal($mu, $sigma, $shape, { ctx => $device });
-    my $un2  = mx->random->uniform($a, $b, $shape, { ctx => $device });
-    ok(same($ret1->aspdl, $ret2->aspdl));
-    ok(same($un1->aspdl, $un2->aspdl));
-    ok(abs($ret1->aspdl->avg - $mu) < 0.1);
-    ok(abs(($ret1->aspdl->stats)[6] - $sigma) < 0.1);
-    ok(abs($un1->aspdl->avg - ($a+$b)/2) < 0.1);
-}
-
-sub check_symbolic_random
-{
-    my ($dev) = @_;
-    my ($a, $b) = (-10, 10);
-    my ($mu, $sigma) = (10, 2);
+    my ($device, $dtype) = @_;
+    my $tol = 0.1;
+    my @symbols = (
+        {
+            name   => 'normal',
+            symbol => sub { mx->sym->random->normal(@_) },
+            ndop   => sub { mx->nd->random->normal(@_)  },
+            params => { loc => 10.0, scale => 0.5 },
+            inputs => [ [loc => [ [ 0.0, 2.5 ], [ -9.75, -7.0 ] ]] , [scale => [ [ 1.0, 3.7 ], [ 4.2, 1.5 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{loc} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - $params->{scale} }, $tol]
+            ]
+        },
+        {
+            name   => 'uniform',
+            symbol => sub { mx->sym->random->uniform(@_) },
+            ndop   => sub { mx->nd->random->uniform(@_)  },
+            params => { low => -1.5, high => 3 },
+            inputs => [ [low => [ [ 0.0, 2.5 ], [ -9.75, -1.0 ] ]] , [high => [ [ 1.0, 3.7 ], [ 4.2, 10.5 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - ($params->{low} + $params->{high})/2 }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - sqrt(1/12) * ($params->{high} - $params->{low}) }, $tol]
+            ]
+        },
+        {
+            name   => 'gamma',
+            symbol => sub { mx->sym->random->gamma(@_) },
+            ndop   => sub { mx->nd->random->gamma(@_)  },
+            params => { alpha => 9, beta => 0.5 },
+            inputs => [ [alpha => [ [ 0.0, 2.5 ], [ 9.75, 11 ] ]] , [beta => [ [ 1, 0.7 ], [ 0.5, 0.3 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{alpha} * $params->{beta} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - sqrt($params->{alpha} * $params->{beta}**2) }, $tol]
+            ]
+        },
+        {
+            name   => 'exponential',
+            symbol => sub { mx->sym->random->exponential(@_) },
+            ndop   => sub { mx->nd->random->exponential(@_)  },
+            params => { scale => 1/4 },
+            inputs => [ [scale => [ [ 1/1, 1/8.5 ], [ 1/2.7, 1/0.5 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{scale} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - $params->{scale} }, $tol]
+            ]
+        },
+        {
+            name   => 'poisson',
+            symbol => sub { mx->sym->random->poisson(@_) },
+            ndop   => sub { mx->nd->random->poisson(@_)  },
+            params => { lam => 4 },
+            inputs => [ [lam => [ [ 1, 8.5 ], [ 2.7, 0.5 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{lam} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - sqrt($params->{lam}) }, $tol]
+            ]
+        },
+        {
+            name   => 'neg-binomial',
+            symbol => sub { mx->sym->random->negative_binomial(@_) },
+            ndop   => sub { mx->nd->random->negative_binomial(@_)  },
+            params => { k => 3, p => 0.4 },
+            inputs => [ [k => [ [ 3, 4 ], [ 5, 6 ] ]] , [p => [ [ 0.4, 0.77 ], [ 0.5, 0.84 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{k}*(1-$params->{p})/$params->{p} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - sqrt($params->{k}*(1-$params->{p}))/$params->{p} }, $tol]
+            ]
+        },
+        {
+            name   => 'gen-neg-binomial',
+            symbol => sub { mx->sym->random->generalized_negative_binomial(@_) },
+            ndop   => sub { mx->nd->random->generalized_negative_binomial(@_)  },
+            params => { mu => 2, alpha => 0.3 },
+            inputs => [ [mu => [ [ 2, 2.5 ], [ 1.3, 1.9 ] ]] , [alpha => [ [ 1.0, 0.1 ], [ 0.2, 0.5 ] ]] ],
+            checks => [
+                [mean => sub { my ($x, $params) = @_; $x->astype('float64')->aspdl->avg - $params->{mu} }, $tol],
+                [std  => sub { my ($x, $params) = @_; ($x->astype('float64')->aspdl->stats)[6] - sqrt($params->{mu}+$params->{alpha}*$params->{mu}**2) }, $tol]
+            ]
+        },
+    );
     my $shape = [100, 100];
-    my $X = mx->sym->Variable("X");
-    my $Y = mx->sym->uniform(low=>$a, high=>$b, shape=>$shape) + $X;
-    my $x = mx->nd->zeros($shape, ctx=>$dev);
-    my $xgrad = mx->nd->zeros($shape, ctx=>$dev);
-    my $yexec = $Y->bind(ctx => $dev, args => {X => $x}, args_grad => {X => $xgrad});
-    mx->random->seed(128);
-    $yexec->forward(1);
-    $yexec->backward($yexec->outputs->[0]);
-    my $un1 = ($yexec->outputs->[0] - $x)->copyto($dev);
-    ok(same($xgrad->aspdl, $un1->aspdl));
-    mx->random->seed(128);
-    $yexec->forward;
-    my $un2 = ($yexec->outputs->[0] - $x)->copyto($dev);
-    ok(same($un1->aspdl, $un2->aspdl));
-    ok(abs($un1->aspdl->avg - ($a+$b)/2) < 0.1);
+    for my $symbdic (@symbols)
+    {
+        my $name = $symbdic->{name};
+        my $ndop = $symbdic->{ndop};
+
+        # check directly
+        my %params = %{ $symbdic->{params} };
+        %params = (%params, shape=>$shape, dtype=>$dtype, ctx=>$device);
+        mx->random->seed(128);
+        my $ret1 = $ndop->(%params);
+        mx->random->seed(128);
+        my $ret2 = $ndop->(%params);
+        ok(same($ret1->aspdl, $ret2->aspdl), "simple $name");
+
+        for my $d (@{ $symbdic->{checks} })
+        {
+            my ($check_name, $check_func, $tol) = @$d;
+            ok((abs($check_func->($ret1, \%params)) < $tol), "simple $name, $check_name");
+        }
+
+        # check multi-distribution sampling, only supports cpu for now
+        %params = (shape=>$shape, dtype=>$dtype, ctx=>$device);
+        %params = (%params, map { $_->[0] => mx->nd->array($_->[1], ctx=>$device, dtype=>$dtype) } @{ $symbdic->{inputs} });
+        mx->random->seed(128);
+        $ret1 = $ndop->(%params);
+        mx->random->seed(128);
+        $ret2 = $ndop->(%params);
+        ok(same($ret1->aspdl, $ret2->aspdl), "advanced $name");
+
+        for my $i (0,1)
+        {
+            for my $j (0,1)
+            {
+                my %stats = map { $_->[0] => $_->[1][$i][$j] } @{ $symbdic->{inputs} };
+                for my $d (@{ $symbdic->{checks} })
+                {
+                    my ($check_name, $check_func, $tol) = @$d;
+                    ok((abs($check_func->($ret2->at($i)->at($j), \%stats)) < $tol), "advanced $name, $check_name");
+                }
+            }
+        }
+
+        # check symbolic
+        my $symbol = $symbdic->{symbol};
+        my $X = mx->sym->Variable("X");
+        %params = %{ $symbdic->{params} };
+        %params = (%params, shape=>$shape, dtype=>$dtype);
+        my $Y = $symbol->(%params) + $X;
+        my $x = mx->nd->zeros($shape, dtype=>$dtype, ctx=>$device);
+        my $xgrad = mx->nd->zeros($shape, dtype=>$dtype, ctx=>$device);
+        my $yexec = $Y->bind(ctx => $device, args => { X => $x }, args_grad => { X => $xgrad });
+        mx->random->seed(128);
+        $yexec->forward(1);
+        $yexec->backward($yexec->outputs->[0]);
+        my $un1 = ($yexec->outputs->[0] - $x)->copyto($device);
+        ok(same($xgrad->aspdl, $un1->aspdl), "symbolic simple");
+        mx->random->seed(128);
+        $yexec->forward();
+        my $un2 = ($yexec->outputs->[0] - $x)->copyto($device);
+        ok(same($un1->aspdl, $un2->aspdl), "symbolic simple $name");
+
+        for my $d (@{ $symbdic->{checks} })
+        {
+            my ($check_name, $check_func, $tol) = @$d;
+            ok((abs($check_func->($un1, \%params)) < $tol), "symbolic $name, $check_name");
+        }
+
+        # check multi-distribution sampling, only supports cpu for now
+        $symbol = $symbdic->{symbol};
+        %params = (shape=>$shape, dtype=>$dtype);
+        my $single_param = @{ $symbdic->{inputs} } == 1;
+        my $v1 = mx->sym->Variable('v1');
+        my $v2 = mx->sym->Variable('v2');
+        $Y = $symbol->($single_param ? ($v1) : ($v1, $v2), %params);
+        my $bindings = { v1 => mx->nd->array($symbdic->{inputs}[0][1]) };
+        if(not $single_param)
+        {
+            $bindings->{v2} = mx->nd->array($symbdic->{inputs}[1][1]);
+        }
+        $yexec = $Y->bind(ctx=>$device, args=>$bindings);
+        $yexec->forward();
+        $un1 = $yexec->outputs->[0]->copyto($device);
+        %params = ();
+        enumerate(sub {
+            my ($i, $r) = @_;
+            enumerate(sub {
+                my ($j, $p1) = @_;
+                $params{ $symbdic->{inputs}[0][0] } = $p1;
+                if(not $single_param)
+                {
+                    $params{ $symbdic->{inputs}[1][0] } = $symbdic->{inputs}[1][1][$i][$j];
+                }
+                my $samples = $un1->at($i)->at($j);
+                for my $d (@{ $symbdic->{checks} })
+                {
+                    my ($check_name, $check_func, $tol) = @$d;
+                    ok((abs($check_func->($samples, \%params)) < $tol), "symbolic advanced $name, $check_name");
+                }
+            }, $r);
+        }, $symbdic->{inputs}[0][1]);
+    }
 }
 
 sub test_random
 {
-    check_with_device(mx->cpu);
-    check_symbolic_random(mx->cpu);
+    check_with_device(mx->context->current_context(), 'float16');
+    check_with_device(mx->context->current_context(), 'float32');
+    check_with_device(mx->context->current_context(), 'float64');
 }
 
 test_random();
+
+sub test_sample_multinomial
+{
+    my $x = mx->nd->array([[0,1,2,3,4],[4,3,2,1,0]])/10.0;
+    ok(@{ mx->nd->random->multinomial($x, shape=>1000, get_prob=>1) }, "multiminomial");
+}
+
+test_sample_multinomial();
+
diff --git a/perl-package/AI-MXNetCAPI/Changes b/perl-package/AI-MXNetCAPI/Changes
index 1a6356c0333d..e8736800453c 100644
--- a/perl-package/AI-MXNetCAPI/Changes
+++ b/perl-package/AI-MXNetCAPI/Changes
@@ -1,6 +1,9 @@
 Revision history for Perl extension AI::MXNetCAPI
 
-1.0102 Sun Aug  6 16:55:08 PDT 2017
+1.1     Sun Oct  1 10:19:08 PDT 2017
+        - support for perl 5.14, Gluon, cuda kernels
+
+1.0102  Sun Aug  6 16:55:08 PDT 2017
         - updated autograd calls.
 
 1.0101  Sun Jul  2 17:16:01 PDT 2017
diff --git a/perl-package/AI-MXNetCAPI/META.json b/perl-package/AI-MXNetCAPI/META.json
index a6d65fd2d73a..92864bcc7071 100644
--- a/perl-package/AI-MXNetCAPI/META.json
+++ b/perl-package/AI-MXNetCAPI/META.json
@@ -37,5 +37,5 @@
       }
    },
    "release_status" : "stable",
-   "version" : "1.0102"
+   "version" : "1.1"
 }
diff --git a/perl-package/AI-MXNetCAPI/META.yml b/perl-package/AI-MXNetCAPI/META.yml
index 0e3bb53c475c..99ffd0718b99 100644
--- a/perl-package/AI-MXNetCAPI/META.yml
+++ b/perl-package/AI-MXNetCAPI/META.yml
@@ -19,4 +19,4 @@ no_index:
     - inc
 requires:
   Test::More: '0'
-version: '1.0102'
+version: '1.1'
diff --git a/perl-package/AI-MXNetCAPI/README b/perl-package/AI-MXNetCAPI/README
index 5c531463e83b..c71ca554780b 100644
--- a/perl-package/AI-MXNetCAPI/README
+++ b/perl-package/AI-MXNetCAPI/README
@@ -1,4 +1,4 @@
-AI-MXNetCAPI version 1.0102
+AI-MXNetCAPI version 1.1
 =====================
 
 Swig interface to MXNet c api.
diff --git a/perl-package/AI-MXNetCAPI/lib/AI/MXNetCAPI.pm b/perl-package/AI-MXNetCAPI/lib/AI/MXNetCAPI.pm
index 0a93d71916f8..8c1499d8d558 100644
--- a/perl-package/AI-MXNetCAPI/lib/AI/MXNetCAPI.pm
+++ b/perl-package/AI-MXNetCAPI/lib/AI/MXNetCAPI.pm
@@ -18,7 +18,7 @@
 package AI::MXNetCAPI;
 use base qw(DynaLoader);
 bootstrap AI::MXNetCAPI;
-our $VERSION = '1.0102';
+our $VERSION = '1.1';
 1;
 __END__
 
diff --git a/perl-package/AI-MXNetCAPI/mxnet.i b/perl-package/AI-MXNetCAPI/mxnet.i
index b4c1336de624..e466e98b7842 100644
--- a/perl-package/AI-MXNetCAPI/mxnet.i
+++ b/perl-package/AI-MXNetCAPI/mxnet.i
@@ -120,6 +120,8 @@ static void ExecutorMonitor_callback(const char* name, NDArrayHandle handle, voi
     SWIG_TypeClientData(SWIGTYPE_p_MXRecordIO, (void *)"RecordIOHandle");
     SWIG_TypeClientData(SWIGTYPE_p_MXRtc, (void *)"RtcHandle");
     SWIG_TypeClientData(SWIGTYPE_p_MXCachedOp, (void *)"CachedOpHandle");
+    SWIG_TypeClientData(SWIGTYPE_p_MXCudaModuleHandle, (void *)"CudaModuleHandle");
+    SWIG_TypeClientData(SWIGTYPE_p_MXCudaKernelHandle, (void *)"CudaKernelHandle");
 %}
 
 /*! \brief manually define unsigned int */
@@ -153,6 +155,10 @@ typedef MXRecordIO *RecordIOHandle;
 typedef MXRtc *RtcHandle;
 /*! \brief handle to cached operator */
 typedef MXCachedOp *CachedOpHandle;
+/*! \brief handle to rtc cuda module*/
+typedef MXCudaModuleHandle *CudaModuleHandle;
+/*! \brief handle to rtc cuda kernel*/
+typedef MXCudaKernelHandle *CudaKernelHandle;
 
 typedef void (*ExecutorMonitorCallback)(const char*,
                                                        NDArrayHandle,
@@ -240,6 +246,13 @@ int MXDumpProfile();
 /*! \brief Set the number of OMP threads to use */
 int MXSetNumOMPThreads(int thread_num);
 
+/*!
+ * \brief get the MXNet library version as an integer
+ * \param pointer to the integer holding the version number
+ * \return 0 when success, -1 when failure happens
+ */
+int MXGetVersion(int *out);
+
 //-------------------------------------
 // Part 1: NDArray creation and deletion
 //-------------------------------------
@@ -597,6 +610,14 @@ int MXImperativeInvoke(AtomicSymbolCreator in,
                                  int num_params,
                                  const char **keys,
                                  const char **vals);
+/*!
+  * \brief set whether to record operator for autograd
+ * \param is_recording 1 when recording, 0 when not recording.
+ * \param prev returns the previous status before this set.
+ * \return 0 when success, -1 when failure happens
+ */
+int MXAutogradSetIsRecording(int is_recording, int* out);
+
 /*!
  * \brief set whether to record operator for autograd
  * \param is_train 1 when training, 0 when testing
@@ -604,6 +625,21 @@ int MXImperativeInvoke(AtomicSymbolCreator in,
  * \return 0 when success, -1 when failure happens
  */
 int MXAutogradSetIsTraining(int is_training, int* out);
+
+/*!
+ * \brief get whether autograd recording is on
+ * \param curr returns the current status.
+ * \return 0 when success, -1 when failure happens
+ */
+int MXAutogradIsRecording(bool* out);
+
+/*!
+ * \brief get whether training mode is on
+ * \param curr returns the current status.
+ * \return 0 when success, -1 when failure happens
+ */
+int MXAutogradIsTraining(bool* out);
+
 /*!
  * \brief mark NDArrays as variables to compute gradient for autograd
  * \param num_var number of variable NDArrays
@@ -635,6 +671,33 @@ int MXAutogradBackward(mx_uint num_output,
                                  NDArrayHandle* in,
                                  int retain_graph);
 
+/*!
+ * \brief compute the gradient of outputs w.r.t variabels
+ * \param num_output number of output NDArray
+ * \param output_handles output NDArrays
+ * \param ograd_handles head gradient for NDArrays
+ * \param retain_graph whether to keep the graph after backward
+ * \param is_train whether to do backward for training or inference
+ * \return 0 when success, -1 when failure happens
+ */
+int MXAutogradBackwardEx(mx_uint num_output,
+                                   NDArrayHandle *in,
+                                   NDArrayHandle *in,
+                                   mx_uint num_variables,
+                                   NDArrayHandle *in,
+                                   int retain_graph,
+                                   int create_graph,
+                                   int is_train,
+                                   NDArrayHandle **out_grad,
+                                   int **out_stype);
+
+/*
+ * \brief get the graph constructed by autograd.
+ * \param handle ndarray handle
+ * \param out output symbol handle
+ */
+int MXAutogradGetSymbol(NDArrayHandle handle, SymbolHandle *out);
+
  /*!
   * \brief create cached operator
   */
@@ -1076,6 +1139,21 @@ int MXExecutorBackward(ExecutorHandle handle,
                                  mx_uint len,
                                  NDArrayHandle *in);
 
+/*!
+ * \brief Excecutor run backward
+ *
+ * \param handle execute handle
+ * \param len lenth
+ * \param head_grads NDArray handle for heads' gradient
+ * \param is_train int value to indicate whether the backward pass is for evaluation
+ *
+ * \return 0 when success, -1 when failure happens
+ */
+int MXExecutorBackwardEx(ExecutorHandle handle,
+                                   mx_uint len,
+                                   NDArrayHandle *in,
+                                   int is_train);
+
 /*!
  * \brief Get executor's head NDArray
  *
@@ -1349,7 +1427,6 @@ int MXInitPSEnv(mx_uint num_vars,
                           const char **keys,
                           const char **vals);
 
-
 /*!
  * \brief Create a kvstore
  * \param type the type of KVStore
@@ -1642,4 +1719,57 @@ int MXRtcPush(RtcHandle handle, mx_uint num_input, mx_uint num_output,
 */
 int MXRtcFree(RtcHandle handle);
 
-int MXCustomOpRegister(const char* op_type, CustomOpPropCreator creator);
+/*
+ * \brief create cuda rtc module
+ * \param source cuda source code
+ * \param num_options number of compiler flags
+ * \param options compiler flags
+ * \param num_exports number of exported function names
+ * \param exported function names
+ * \param out handle to created module
+ */
+int MXRtcCudaModuleCreate(const char* source, int num_options,
+                                    const char** in, int num_exports,
+                                    const char** in, CudaModuleHandle *out);
+/*
+ * \brief delete cuda rtc module
+ * \param handle handle to cuda module
+ */
+int MXRtcCudaModuleFree(CudaModuleHandle handle);
+/*
+ * \brief get kernel from module
+ * \param handle handle to cuda module
+ * \param name name of kernel function
+ * \param num_args number of arguments
+ * \param is_ndarray whether argument is ndarray
+ * \param is_const whether argument is constant
+ * \param arg_types data type of arguments
+ * \param out created kernel
+ */
+int MXRtcCudaKernelCreate(CudaModuleHandle handle, const char* name,
+                                    int num_args, int* in, int* in,
+                                    int* in, CudaKernelHandle *out);
+/*
+ * \brief delete kernel
+ * \param handle handle to previously created kernel
+ */
+int MXRtcCudaKernelFree(CudaKernelHandle handle);
+/*
+ * \brief launch cuda kernel
+ * \param handle handle to kernel
+ * \param dev_id (GPU) device id
+ * \param args pointer to arguments
+ * \param grid_dim_x grid dimension x
+ * \param grid_dim_y grid dimension y
+ * \param grid_dim_z grid dimension z
+ * \param block_dim_x block dimension x
+ * \param block_dim_y block dimension y
+ * \param block_dim_z block dimension z
+ * \param shared_mem size of dynamically allocated shared memory
+ */
+int MXRtcCudaKernelCall(CudaKernelHandle handle, int dev_id, void** cuda_kernel_args,
+                                  mx_uint grid_dim_x, mx_uint grid_dim_y,
+                                  mx_uint grid_dim_z, mx_uint block_dim_x,
+                                  mx_uint block_dim_y, mx_uint block_dim_z,
+                                  mx_uint shared_mem);
+
diff --git a/perl-package/AI-MXNetCAPI/mxnet_typemaps.i b/perl-package/AI-MXNetCAPI/mxnet_typemaps.i
index 586907e7216a..fb794de6e8d5 100644
--- a/perl-package/AI-MXNetCAPI/mxnet_typemaps.i
+++ b/perl-package/AI-MXNetCAPI/mxnet_typemaps.i
@@ -4,19 +4,18 @@
     I32 len;
     int i;
     SV  **tv;
-    STRLEN len2;
     if (!SvROK($input))
         croak("Argument $argnum is not a reference.");
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    len = av_top_index(tempav) + 1;
+    len = av_len(tempav) + 1;
     if(len!=0) 
     {
         $1 = (char **) safemalloc((len)*sizeof(char *));
         for (i = 0; i < len; i++) {
             tv = av_fetch(tempav, i, 0);
-            $1[i] = (char *) SvPV(*tv,len2);
+            $1[i] = (char *) SvPV_nolen(*tv);
         }
     }
     else
@@ -34,7 +33,6 @@
     char *key;
     SV *val;
     I32 len;
-    STRLEN len2;
     int hash_len;
     int i = 0;
     if (!SvROK($input))
@@ -50,7 +48,7 @@
         while ((val = hv_iternextsv(temphv, &key, &len)))
         {
             $1[i] = key;
-            $2[i] = SvPV(val, len2);
+            $2[i] = SvPV_nolen(val);
             ++i;
         }
     }
@@ -82,13 +80,29 @@
     }
 }
 
-%typemap(in,numinputs=0) (int *out) (int temp)
+%typemap(in) (void **out_pdata) (void *temp)
+{
+    temp = NULL;
+    $1 = &temp;
+}
+
+%typemap(argout) (void **out_pdata)
+{
+    if(!result)
+    {
+        $result = newSVpvn((char*)(*$1), SvIV(ST(1)));
+        sv_2mortal($result);
+        argvi++;
+    }
+}
+
+%typemap(in,numinputs=0) (int *out) (int temp), (bool *out) (bool temp)
 {
     temp = 0;
     $1 = &temp;
 }
 
-%typemap(argout) (int *out)
+%typemap(argout) (int *out), (bool *out)
 {
     if(!result)
     {
@@ -172,14 +186,12 @@
 
 %typemap(in) (const void *in), (void *in)
 {
-    STRLEN len;
-    $1 = (void *)SvPV($input, len);
+    $1 = (void *)SvPV_nolen($input);
 }
 
 %typemap(in) (const char *in)
 {
-    STRLEN len;
-    $1 = SvPV($input, len);
+    $1 = SvPV_nolen($input);
 }
 
 %typemap(in) (const mx_uint *in), (mx_uint *in)
@@ -187,18 +199,18 @@
     AV *tempav;
     int i;
     SV  **tv;
-    int av_len; 
+    int av_len;
     if (!SvROK($input))
         croak("Argument $argnum is not a reference.");
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    av_len = av_top_index(tempav) + 1;
+    av_len = av_len(tempav) + 1;
     if(av_len)
     {
         $1 = (mx_uint *)safemalloc(av_len*sizeof(mx_uint));
         for (i = 0; i < av_len; i++) {
-            tv = av_fetch(tempav, i, 0);    
+            tv = av_fetch(tempav, i, 0);
             $1[i] = (mx_uint)SvIV(*tv);
         }
     }
@@ -223,12 +235,12 @@
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    av_len = av_top_index(tempav) + 1;
+    av_len = av_len(tempav) + 1;
     if(av_len)
     {
         $1 = (int *)safemalloc(av_len*sizeof(int));
         for (i = 0; i < av_len; i++) {
-            tv = av_fetch(tempav, i, 0);    
+            tv = av_fetch(tempav, i, 0);
             $1[i] = (int)SvIV(*tv);
         }
     }
@@ -255,12 +267,12 @@
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    av_len = av_top_index(tempav) + 1;
+    av_len = av_len(tempav) + 1;
     if(av_len)
     {
         $1 = ($1_type)safemalloc(av_len*sizeof($*1_type));
         for (i = 0; i < av_len; i++) {
-            tv = av_fetch(tempav, i, 0);    
+            tv = av_fetch(tempav, i, 0);
             res = SWIG_ConvertPtr(*tv,SWIG_as_voidptrptr(&$1[i]), $*1_descriptor, 0);
             if (!SWIG_IsOK(res)) {
                 SWIG_exception_fail(SWIG_ArgError(res), "in method '" "$symname" "', argument " "$argnum"" of type '" "$*1_type""'"); 
@@ -276,6 +288,39 @@
     Safefree($1);
 }
 
+%typemap(in) (void** cuda_kernel_args)
+{
+    AV *tempav;
+    int i;
+    SV  **tv;
+    int res;
+    int av_len;
+    if (!SvROK($input))
+        croak("Argument $argnum is not a reference.");
+        if (SvTYPE(SvRV($input)) != SVt_PVAV)
+        croak("Argument $argnum is not an array.");
+        tempav = (AV*)SvRV($input);
+    av_len = av_len(tempav) + 1;
+    if(av_len)
+    {
+        $1 = ($1_type)safemalloc(av_len*sizeof($*1_type));
+        for (i = 0; i < av_len; i++) {
+            tv = av_fetch(tempav, i, 0);
+            res = SWIG_ConvertPtr(*tv,SWIG_as_voidptrptr(&$1[i]), SWIGTYPE_p_MXNDArray, 0);
+            if (!SWIG_IsOK(res)) {
+                $1[i] = (void*)SvPV_nolen(*tv);
+            }
+        }
+    }
+    else
+    {
+       $1 = NULL;
+    }
+}
+%typemap(freearg) (void** cuda_kernel_args) {
+    Safefree($1);
+}
+
 %typemap(in) (mx_float *in)
 {
     AV *tempav;
@@ -286,7 +331,7 @@
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    len = av_top_index(tempav) + 1;
+    len = av_len(tempav) + 1;
     if(len)
     {
         $1 = (mx_float *)safemalloc(len*sizeof(mx_float));
@@ -306,19 +351,23 @@
 }
 
 %typemap(in,numinputs=0) (NDArrayHandle *out) (NDArrayHandle temp),
-                         (FunctionHandle* out) (FunctionHandle temp), 
+                         (FunctionHandle* out) (FunctionHandle temp),
                          (SymbolHandle *out) (SymbolHandle temp),
                          (ExecutorHandle *out) (ExecutorHandle temp),
                          (DataIterHandle *out) (ExecutorHandle temp),
                          (KVStoreHandle *out) (KVStoreHandle temp),
                          (RecordIOHandle *out) (RecordIOHandle temp),
                          (RtcHandle *out) (RtcHandle temp),
-                         (CachedOpHandle *out) (CachedOpHandle temp)
+                         (CachedOpHandle *out) (CachedOpHandle temp),
+                         (CudaModuleHandle *out) (CudaModuleHandle temp),
+                         (CudaKernelHandle *out) (CudaKernelHandle temp)
 {
     $1 = &temp;
 }
 %typemap(argout) (NDArrayHandle *out), (FunctionHandle* out), (SymbolHandle *out), (ExecutorHandle *out), (DataIterHandle *out),
-                 (KVStoreHandle *out), (RecordIOHandle *out), (RtcHandle *out) (RtcHandle temp), (CachedOpHandle *out) (CachedOpHandle temp)
+                 (KVStoreHandle *out), (RecordIOHandle *out), (RtcHandle *out) (RtcHandle temp), (CachedOpHandle *out) (CachedOpHandle temp),
+                 (CudaModuleHandle *out) (CudaModuleHandle temp), (CudaKernelHandle *out) (CudaKernelHandle temp)
+
 {
     if(!result)
     {
@@ -522,6 +571,72 @@
     }
 }
 
+%typemap(in,numinputs=0) (NDArrayHandle **out_grad) (NDArrayHandle* temp)
+{
+    int vars = SvIV(ST(3));
+    if(vars)
+    {
+        $1 = &temp;
+    }
+    else
+    {
+        $1 = NULL;
+    }
+}
+
+%typemap(argout) (NDArrayHandle** out_grad)
+{
+    if(!result)
+    {
+        AV *myav;
+        SV **svs;
+        int i = 0;
+        int len = SvIV(ST(3));
+        svs = (SV **)safemalloc(len*sizeof(SV *));
+        for (i = 0; i < len ; i++) {
+            svs[i] = SWIG_NewPointerObj(SWIG_as_voidptr((*$1)[i]), SWIGTYPE_p_MXNDArray, 0);
+        }
+        myav = av_make(len,svs);
+        Safefree(svs);
+        $result = newRV_noinc((SV*)myav);
+        sv_2mortal($result);
+        argvi++;
+    }
+}
+
+%typemap(in,numinputs=0) (int **out_stype) (int *temp)
+{
+    int vars = SvIV(ST(3));
+    if(vars)
+    {
+        $1 = &temp;
+    }
+    else
+    {
+        $1 = NULL;
+    }
+}
+
+%typemap(argout) (int** out_stype)
+{
+    if(!result)
+    {
+        AV *myav;
+        SV **svs;
+        int i = 0;
+        int len = SvIV(ST(3));
+        svs = (SV **)safemalloc(len*sizeof(SV *));
+        for (i = 0; i < len ; i++) {
+            svs[i] = newSViv((*$1)[i]);
+        }
+        myav = av_make(len,svs);
+        Safefree(svs);
+        $result = newRV_noinc((SV*)myav);
+        sv_2mortal($result);
+        argvi++;
+    }
+}
+
 %typemap(in) (int *out_size, NDArrayHandle** out_array) (int temp, NDArrayHandle* temp_array)
 {
     AV *tempav;
@@ -534,7 +649,7 @@
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    av_len = av_top_index(tempav) + 1;
+    av_len = av_len(tempav) + 1;
     temp_array = NULL;
     if(av_len)
     {
@@ -553,7 +668,7 @@
 }
 
 %typemap(freearg) (int *out_size, NDArrayHandle** out_array) {
-    if(av_top_index((AV*)SvRV(ST(3))) > -1)
+    if(av_len((AV*)SvRV(ST(3))) > -1)
     {
         Safefree(*$2);
     }
@@ -563,7 +678,7 @@
 {
     SV **svs;
     int i = 0;
-    if(av_top_index((AV*)SvRV(ST(3))) == -1)
+    if(av_len((AV*)SvRV(ST(3))) == -1)
     {
         if(!result)
         {
diff --git a/perl-package/AI-NNVMCAPI/Changes b/perl-package/AI-NNVMCAPI/Changes
index 09395184e3c6..480090de451b 100644
--- a/perl-package/AI-NNVMCAPI/Changes
+++ b/perl-package/AI-NNVMCAPI/Changes
@@ -1,5 +1,8 @@
 Revision history for Perl extension AI::NNVMCAPI.
 
+1.1     Sun Sep 24 10:26:54 PDT 2017
+        - support for perl 5.14
+
 1.01    Sat Jun 10 23:57:27 PDT 2017
         - sync with python.
 
diff --git a/perl-package/AI-NNVMCAPI/META.json b/perl-package/AI-NNVMCAPI/META.json
index 42247c6b98ff..e65204307ebe 100644
--- a/perl-package/AI-NNVMCAPI/META.json
+++ b/perl-package/AI-NNVMCAPI/META.json
@@ -37,5 +37,5 @@
       }
    },
    "release_status" : "stable",
-   "version" : "1.01"
+   "version" : "1.1"
 }
diff --git a/perl-package/AI-NNVMCAPI/META.yml b/perl-package/AI-NNVMCAPI/META.yml
index 6d48cc7b8578..446c9df952f3 100644
--- a/perl-package/AI-NNVMCAPI/META.yml
+++ b/perl-package/AI-NNVMCAPI/META.yml
@@ -19,4 +19,4 @@ no_index:
     - inc
 requires:
   Test::More: '0'
-version: '1.01'
+version: '1.1'
diff --git a/perl-package/AI-NNVMCAPI/README b/perl-package/AI-NNVMCAPI/README
index 50579140de82..9e23d93b7ef8 100644
--- a/perl-package/AI-NNVMCAPI/README
+++ b/perl-package/AI-NNVMCAPI/README
@@ -1,4 +1,4 @@
-AI-NNVMCAPI version 1.01
+AI-NNVMCAPI version 1.1
 =====================
 
 Swig interface to MXNet c api.
diff --git a/perl-package/AI-NNVMCAPI/lib/AI/NNVMCAPI.pm b/perl-package/AI-NNVMCAPI/lib/AI/NNVMCAPI.pm
index 134d922b4d8d..df87d4780a7c 100644
--- a/perl-package/AI-NNVMCAPI/lib/AI/NNVMCAPI.pm
+++ b/perl-package/AI-NNVMCAPI/lib/AI/NNVMCAPI.pm
@@ -18,7 +18,7 @@
 package AI::NNVMCAPI;
 use base qw(DynaLoader);
 bootstrap AI::NNVMCAPI;
-our $VERSION = '1.01';
+our $VERSION = '1.1';
 1;
 __END__
 
diff --git a/perl-package/AI-NNVMCAPI/nnvm_typemaps.i b/perl-package/AI-NNVMCAPI/nnvm_typemaps.i
index 19baf804e187..ccfa8d05de5e 100644
--- a/perl-package/AI-NNVMCAPI/nnvm_typemaps.i
+++ b/perl-package/AI-NNVMCAPI/nnvm_typemaps.i
@@ -4,24 +4,23 @@
     I32 len;
     int i;
     SV  **tv;
-    STRLEN len2;
     if (!SvROK($input))
         croak("Argument $argnum is not a reference.");
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    len = av_top_index(tempav) + 1;
+    len = av_len(tempav) + 1;
     if(len!=0) 
     {
         $1 = (char **) safemalloc((len)*sizeof(char *));
         for (i = 0; i < len; i++) {
-            tv = av_fetch(tempav, i, 0);    
-            $1[i] = (char *) SvPV(*tv,len2);
+            tv = av_fetch(tempav, i, 0);
+            $1[i] = (char *) SvPV_nolen(*tv);
         }
     }
     else
     {
-       $1 = NULL;     
+       $1 = NULL;
     }
 }
 %typemap(freearg) (const char** in), (char** in)  {
@@ -34,7 +33,6 @@
     char *key;
     SV *val;
     I32 len;
-    STRLEN len2;
     int hash_len;
     int i = 0;
     if (!SvROK($input))
@@ -50,7 +48,7 @@
         while ((val = hv_iternextsv(temphv, &key, &len))) 
         {
             $1[i] = key;
-            $2[i] = SvPV(val, len2);
+            $2[i] = SvPV_nolen(val);
             ++i;
         }
     }
@@ -156,7 +154,7 @@
         if (SvTYPE(SvRV($input)) != SVt_PVAV)
         croak("Argument $argnum is not an array.");
         tempav = (AV*)SvRV($input);
-    len = av_top_index(tempav) + 1;
+    len = av_len(tempav) + 1;
     if(len)
     {
         $1 = ($1_type)safemalloc(len*sizeof($*1_type));
diff --git a/perl-package/README.md b/perl-package/README.md
index 81cb5a017373..d25f322f2717 100644
--- a/perl-package/README.md
+++ b/perl-package/README.md
@@ -1,5 +1,6 @@
-[![Build Status](https://travis-ci.org/dmlc/mxnet.svg?branch=master)](https://travis-ci.org/dmlc/mxnet)
-[![Documentation Status](https://readthedocs.org/projects/mxnet/badge/?version=latest)](http://mxnet.io/api/perl/index.html)
+[![Build Status](https://builds.apache.org/job/incubator-mxnet/job/master/badge/icon)](https://builds.apache.org/job/incubator-mxnet/job/master/)
+[![Documentation Status](https://builds.apache.org/job/incubator-mxnet-build-site/badge/icon)](https://mxnet.incubator.apache.org/api/perl/index.html)
+[![GitHub license](http://dmlc.github.io/img/apache2.svg)](../LICENSE)
 
 You have found MXNet Perl Package! The MXNet Perl packages brings flexible and efficient GPU
 computing and state-of-art deep learning to Perl.
@@ -10,7 +11,7 @@ computing and state-of-art deep learning to Perl.
 
 Installation
 ---------
-* [MXNet Setup Document](http://mxnet.io/get_started/ubuntu_setup.html)
+* [MXNet Setup Document](https://mxnet.incubator.apache.org/get_started/ubuntu_setup.html)
   - Check this out for detailed documents, examples and installation guides.
 
 License
diff --git a/tests/ci_build/install/ubuntu_install_perl.sh b/tests/ci_build/install/ubuntu_install_perl.sh
index a981746bc18d..af49952f97d6 100755
--- a/tests/ci_build/install/ubuntu_install_perl.sh
+++ b/tests/ci_build/install/ubuntu_install_perl.sh
@@ -19,4 +19,4 @@
 
 # install libraries for mxnet's perl package on ubuntu
 apt-get update && apt-get install -y libmouse-perl pdl cpanminus swig libgraphviz-perl
-cpanm -q Function::Parameters
+cpanm -q Function::Parameters Hash::Ordered
diff --git a/tests/travis/setup.sh b/tests/travis/setup.sh
index f479306a31a8..c6e3bf829917 100755
--- a/tests/travis/setup.sh
+++ b/tests/travis/setup.sh
@@ -51,9 +51,9 @@ fi
 
 if [ ${TASK} == "perl_test" ]; then
     if [ ${TRAVIS_OS_NAME} == "linux" ]; then
-       cpanm -q -L "${HOME}/perl5" Function::Parameters
+       cpanm -q -L "${HOME}/perl5" Function::Parameters Hash::Ordered
     else
        sudo sh -c 'curl -L https://cpanmin.us | perl - App::cpanminus'
-       sudo cpanm -q -n PDL Mouse Function::Parameters
+       sudo cpanm -q -n PDL Mouse Function::Parameters Hash::Ordered
     fi
 fi
