diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
index aa25fef452c5..d2d5dd5ef18f 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java
@@ -25,7 +25,7 @@
 import java.util.NavigableSet;
 import java.util.TreeMap;
 import java.util.TreeSet;
-
+import org.apache.hadoop.fs.StorageType;
 import org.apache.yetus.audience.InterfaceAudience;
 
 
@@ -53,23 +53,28 @@ public static class HostAndWeight {
 
     private String host;
     private long weight;
+    private long weightForSsd;
 
     /**
      * Constructor
      * @param host the host name
      * @param weight the weight
+     * @param weightForSsd the weight for ssd
      */
-    public HostAndWeight(String host, long weight) {
+    public HostAndWeight(String host, long weight, long weightForSsd) {
       this.host = host;
       this.weight = weight;
+      this.weightForSsd = weightForSsd;
     }
 
     /**
      * add weight
      * @param weight the weight
+     * @param weightForSsd the weight for ssd
      */
-    public void addWeight(long weight) {
+    public void addWeight(long weight, long weightForSsd) {
       this.weight += weight;
+      this.weightForSsd += weightForSsd;
     }
 
     /**
@@ -86,6 +91,13 @@ public long getWeight() {
       return weight;
     }
 
+    /**
+     * @return the weight for ssd
+     */
+    public long getWeightForSsd() {
+      return weightForSsd;
+    }
+
     /**
      * comparator used to sort hosts based on weight
      */
@@ -122,14 +134,27 @@ public synchronized String toString() {
    * @param weight the weight
    */
   public void addHostsAndBlockWeight(String[] hosts, long weight) {
+    addHostsAndBlockWeight(hosts, weight, null);
+  }
+  /**
+   * add some weight to a list of hosts, update the value of unique block weight
+   * @param hosts the list of the host
+   * @param weight the weight
+   */
+  public void addHostsAndBlockWeight(String[] hosts, long weight, StorageType[] storageTypes) {
     if (hosts == null || hosts.length == 0) {
       // erroneous data
       return;
     }
 
     addUniqueWeight(weight);
-    for (String hostname : hosts) {
-      addHostAndBlockWeight(hostname, weight);
+    for (int i = 0; i < hosts.length; i++) {
+      long weightForSsd = 0;
+      if (storageTypes != null && storageTypes.length == hosts.length
+        && storageTypes[i] == StorageType.SSD) {
+        weightForSsd = weight;
+      }
+      addHostAndBlockWeight(hosts[i], weight, weightForSsd);
     }
   }
 
@@ -146,8 +171,9 @@ private void addUniqueWeight(long weight) {
    * add some weight to a specific host
    * @param host the host name
    * @param weight the weight
+   * @param weightForSsd the weight for ssd
    */
-  private void addHostAndBlockWeight(String host, long weight) {
+  private void addHostAndBlockWeight(String host, long weight, long weightForSsd) {
     if (host == null) {
       // erroneous data
       return;
@@ -155,10 +181,10 @@ private void addHostAndBlockWeight(String host, long weight) {
 
     HostAndWeight hostAndWeight = this.hostAndWeights.get(host);
     if(hostAndWeight == null) {
-      hostAndWeight = new HostAndWeight(host, weight);
+      hostAndWeight = new HostAndWeight(host, weight, weightForSsd);
       this.hostAndWeights.put(host, hostAndWeight);
     } else {
-      hostAndWeight.addWeight(weight);
+      hostAndWeight.addWeight(weight, weightForSsd);
     }
   }
 
@@ -199,15 +225,28 @@ public long getUniqueBlocksTotalWeight() {
    * @return the locality index of the given host
    */
   public float getBlockLocalityIndex(String host) {
+    return getBlockLocalityIndex(host, false);
+  }
+
+  /**
+   * return the locality index of a given host
+   * @param host the host name
+   * @param forSsd only calc ssd type
+   * @return the locality index of the given host
+   */
+  public float getBlockLocalityIndex(String host, boolean forSsd) {
     float localityIndex = 0;
     HostAndWeight hostAndWeight = this.hostAndWeights.get(host);
     if (hostAndWeight != null && uniqueBlocksTotalWeight != 0) {
-      localityIndex=(float)hostAndWeight.weight/(float)uniqueBlocksTotalWeight;
+      if (forSsd) {
+        localityIndex = (float) hostAndWeight.weightForSsd / (float) uniqueBlocksTotalWeight;
+      } else {
+        localityIndex = (float) hostAndWeight.weight / (float) uniqueBlocksTotalWeight;
+      }
     }
     return localityIndex;
   }
 
-
   /**
    * This will add the distribution from input to this object
    * @param otherBlocksDistribution the other hdfs blocks distribution
@@ -218,7 +257,7 @@ public void add(HDFSBlocksDistribution otherBlocksDistribution) {
     for (Map.Entry<String, HostAndWeight> otherHostAndWeight:
       otherHostAndWeights.entrySet()) {
       addHostAndBlockWeight(otherHostAndWeight.getValue().host,
-        otherHostAndWeight.getValue().weight);
+        otherHostAndWeight.getValue().weight, otherHostAndWeight.getValue().weightForSsd);
     }
     addUniqueWeight(otherBlocksDistribution.getUniqueBlocksTotalWeight());
   }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
index 7d05c41e0e05..9af2b661d0cb 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/BaseLoadBalancer.java
@@ -125,6 +125,7 @@ private void createRegionFinder() {
    *
    * Cluster tracks a list of unassigned regions, region assignments, and the server
    * topology in terms of server names, hostnames and racks.
+   *
    */
   protected static class Cluster {
     ServerName[] servers;
@@ -184,6 +185,8 @@ protected static class Cluster {
     private float[][] rackLocalities;
     // Maps localityType -> region -> [server|rack]Index with highest locality
     private int[][] regionsToMostLocalEntities;
+    // Maps region -> server index with highest ssd locality
+    private int[] regionsToMostSsdLocalServer;
 
     protected Cluster(
         Map<ServerName, List<RegionInfo>> clusterState,
@@ -536,10 +539,10 @@ public int[] getOrComputeRegionsToMostLocalEntities(LocalityType type) {
      * Looks up locality from cache of localities. Will create cache if it does
      * not already exist.
      */
-    public float getOrComputeLocality(int region, int entity, LocalityType type) {
+    public float getOrComputeLocality(int region, int entity, LocalityType type, IncludeStorageType includeStorageType) {
       switch (type) {
         case SERVER:
-          return getLocalityOfRegion(region, entity);
+          return getLocalityOfRegion(region, entity, includeStorageType);
         case RACK:
           return getOrComputeRackLocalities()[region][entity];
         default:
@@ -551,8 +554,8 @@ public float getOrComputeLocality(int region, int entity, LocalityType type) {
      * Returns locality weighted by region size in MB. Will create locality cache
      * if it does not already exist.
      */
-    public double getOrComputeWeightedLocality(int region, int server, LocalityType type) {
-      return getRegionSizeMB(region) * getOrComputeLocality(region, server, type);
+    public double getOrComputeWeightedLocality(int region, int server, LocalityType type, IncludeStorageType includeStorageType) {
+      return getRegionSizeMB(region) * getOrComputeLocality(region, server, type, includeStorageType);
     }
 
     /**
@@ -575,14 +578,19 @@ public int getRegionSizeMB(int region) {
     private void computeCachedLocalities() {
       rackLocalities = new float[numRegions][numRacks];
       regionsToMostLocalEntities = new int[LocalityType.values().length][numRegions];
+      regionsToMostSsdLocalServer = new int[numRegions];
 
       // Compute localities and find most local server per region
       for (int region = 0; region < numRegions; region++) {
         int serverWithBestLocality = 0;
         float bestLocalityForRegion = 0;
+
+        int serverWithBestSsdLocality = 0;
+        float bestSsdLocalityForRegion = 0;
+
         for (int server = 0; server < numServers; server++) {
           // Aggregate per-rack locality
-          float locality = getLocalityOfRegion(region, server);
+          float locality = getLocalityOfRegion(region, server, IncludeStorageType.ALL);
           int rack = serverIndexToRackIndex[server];
           int numServersInRack = serversPerRack[rack].length;
           rackLocalities[region][rack] += locality / numServersInRack;
@@ -591,8 +599,15 @@ private void computeCachedLocalities() {
             serverWithBestLocality = server;
             bestLocalityForRegion = locality;
           }
+
+          float localityForSsd = getLocalityOfRegion(region, server, IncludeStorageType.SSD);
+          if (localityForSsd > bestSsdLocalityForRegion) {
+            serverWithBestSsdLocality = server;
+            bestSsdLocalityForRegion = localityForSsd;
+          }
         }
         regionsToMostLocalEntities[LocalityType.SERVER.ordinal()][region] = serverWithBestLocality;
+        regionsToMostSsdLocalServer[region] = serverWithBestSsdLocality;
 
         // Find most local rack per region
         int rackWithBestLocality = 0;
@@ -621,6 +636,11 @@ enum LocalityType {
       RACK
     }
 
+    enum IncludeStorageType {
+      ALL,
+      SSD
+    }
+
     /** An action to move or swap a region */
     public static class Action {
       public enum Type {
@@ -978,10 +998,11 @@ int getLowestLocalityRegionOnServer(int serverIndex) {
       }
     }
 
-    float getLocalityOfRegion(int region, int server) {
+    float getLocalityOfRegion(int region, int server, IncludeStorageType includeStorageType) {
       if (regionFinder != null) {
         HDFSBlocksDistribution distribution = regionFinder.getBlockDistribution(regions[region]);
-        return distribution.getBlockLocalityIndex(servers[server].getHostname());
+        return distribution.getBlockLocalityIndex(servers[server].getHostname()
+          , includeStorageType == IncludeStorageType.SSD);
       } else {
         return 0f;
       }
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/LocalityBasedCandidateGenerator.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/LocalityBasedCandidateGenerator.java
index 6afb86ff9e0f..260860289cbf 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/LocalityBasedCandidateGenerator.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/LocalityBasedCandidateGenerator.java
@@ -83,7 +83,7 @@ private Optional<BaseLoadBalancer.Cluster.Action> tryMoveOrSwap(BaseLoadBalancer
 
   private double getWeightedLocality(BaseLoadBalancer.Cluster cluster, int region, int server) {
     return cluster.getOrComputeWeightedLocality(region, server,
-      BaseLoadBalancer.Cluster.LocalityType.SERVER);
+      BaseLoadBalancer.Cluster.LocalityType.SERVER, BaseLoadBalancer.Cluster.IncludeStorageType.ALL);
   }
 
   void setServices(MasterServices services) {
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
index 56b7ae461683..33266d4b7209 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.Action.Type;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.AssignRegionAction;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.LocalityType;
+import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.IncludeStorageType;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.MoveRegionAction;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster.SwapRegionsAction;
 import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
@@ -53,6 +54,7 @@
 import org.slf4j.LoggerFactory;
 
 import org.apache.hbase.thirdparty.com.google.common.annotations.VisibleForTesting;
+import org.apache.hbase.thirdparty.com.google.common.base.Optional;
 import org.apache.hbase.thirdparty.com.google.common.collect.Lists;
 
 
@@ -154,6 +156,7 @@ public class StochasticLoadBalancer extends BaseLoadBalancer {
   private LocalityBasedCandidateGenerator localityCandidateGenerator;
   private ServerLocalityCostFunction localityCost;
   private RackLocalityCostFunction rackLocalityCost;
+  private ServerSsdLocalityCostFunction ssdLocalityCost;
   private RegionReplicaHostCostFunction regionReplicaHostCostFunction;
   private RegionReplicaRackCostFunction regionReplicaRackCostFunction;
 
@@ -185,6 +188,7 @@ public synchronized void setConf(Configuration conf) {
     }
     localityCost = new ServerLocalityCostFunction(conf, services);
     rackLocalityCost = new RackLocalityCostFunction(conf, services);
+    ssdLocalityCost = new ServerSsdLocalityCostFunction(conf, services);
 
     if (this.candidateGenerators == null) {
       candidateGenerators = Lists.newArrayList();
@@ -209,6 +213,7 @@ public synchronized void setConf(Configuration conf) {
     costFunctions.add(new MoveCostFunction(conf));
     costFunctions.add(localityCost);
     costFunctions.add(rackLocalityCost);
+    costFunctions.add(ssdLocalityCost);
     costFunctions.add(new TableSkewCostFunction(conf));
     costFunctions.add(regionReplicaHostCostFunction);
     costFunctions.add(regionReplicaRackCostFunction);
@@ -986,6 +991,7 @@ protected double cost() {
   static abstract class LocalityBasedCostFunction extends CostFunction {
 
     private final LocalityType type;
+    private final IncludeStorageType includeStorageType;
 
     private double bestLocality; // best case locality across cluster weighted by local data size
     private double locality; // current locality across cluster weighted by local data size
@@ -995,10 +1001,12 @@ static abstract class LocalityBasedCostFunction extends CostFunction {
     LocalityBasedCostFunction(Configuration conf,
                               MasterServices srv,
                               LocalityType type,
+                              IncludeStorageType includeStorageType,
                               String localityCostKey,
                               float defaultLocalityCost) {
       super(conf);
       this.type = type;
+      this.includeStorageType = includeStorageType;
       this.setMultiplier(conf.getFloat(localityCostKey, defaultLocalityCost));
       this.services = srv;
       this.locality = 0.0;
@@ -1058,7 +1066,7 @@ private int getMostLocalEntityForRegion(int region) {
     }
 
     private double getWeightedLocality(int region, int entity) {
-      return cluster.getOrComputeWeightedLocality(region, entity, type);
+      return cluster.getOrComputeWeightedLocality(region, entity, type, includeStorageType);
     }
 
   }
@@ -1069,13 +1077,7 @@ static class ServerLocalityCostFunction extends LocalityBasedCostFunction {
     private static final float DEFAULT_LOCALITY_COST = 25;
 
     ServerLocalityCostFunction(Configuration conf, MasterServices srv) {
-      super(
-          conf,
-          srv,
-          LocalityType.SERVER,
-          LOCALITY_COST_KEY,
-          DEFAULT_LOCALITY_COST
-      );
+      super(conf, srv, LocalityType.SERVER, IncludeStorageType.ALL, LOCALITY_COST_KEY, DEFAULT_LOCALITY_COST);
     }
 
     @Override
@@ -1090,13 +1092,8 @@ static class RackLocalityCostFunction extends LocalityBasedCostFunction {
     private static final float DEFAULT_RACK_LOCALITY_COST = 15;
 
     public RackLocalityCostFunction(Configuration conf, MasterServices services) {
-      super(
-          conf,
-          services,
-          LocalityType.RACK,
-          RACK_LOCALITY_COST_KEY,
-          DEFAULT_RACK_LOCALITY_COST
-      );
+      super(conf, services, LocalityType.RACK, IncludeStorageType.ALL, RACK_LOCALITY_COST_KEY,
+        DEFAULT_RACK_LOCALITY_COST);
     }
 
     @Override
@@ -1105,6 +1102,23 @@ int regionIndexToEntityIndex(int region) {
     }
   }
 
+  // Hdfs read local replica first, no matter it is ssd or not.
+  // So it is better to make the local replica stored on ssd to get its benefits.
+  static class ServerSsdLocalityCostFunction extends LocalityBasedCostFunction {
+
+    private static final String LOCALITY_COST_KEY = "hbase.master.balancer.stochastic.ssdLocalityCost";
+    private static final float DEFAULT_LOCALITY_COST = 25;
+
+    ServerSsdLocalityCostFunction(Configuration conf, MasterServices srv) {
+      super(conf, srv, LocalityType.SERVER, IncludeStorageType.SSD, LOCALITY_COST_KEY, DEFAULT_LOCALITY_COST);
+    }
+
+    @Override
+    int regionIndexToEntityIndex(int region) {
+      return cluster.regionIndexToServerIndex[region];
+    }
+  }
+
   /**
    * Base class the allows writing costs functions from rolling average of some
    * number from RegionLoad.
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
index 4b6c8af4c53c..c2704371eb90 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSUtils.java
@@ -61,6 +61,7 @@
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hbase.ClusterId;
 import org.apache.hadoop.hbase.HConstants;
@@ -729,12 +730,7 @@ static public HDFSBlocksDistribution computeHDFSBlocksDistribution(
     HDFSBlocksDistribution blocksDistribution = new HDFSBlocksDistribution();
     BlockLocation [] blockLocations =
       fs.getFileBlockLocations(status, start, length);
-    for(BlockLocation bl : blockLocations) {
-      String [] hosts = bl.getHosts();
-      long len = bl.getLength();
-      blocksDistribution.addHostsAndBlockWeight(hosts, len);
-    }
-
+    addToHDFSBlocksDistribution(blocksDistribution, blockLocations);
     return blocksDistribution;
   }
 
@@ -749,7 +745,8 @@ static public void addToHDFSBlocksDistribution(
     for (BlockLocation bl : blockLocations) {
       String[] hosts = bl.getHosts();
       long len = bl.getLength();
-      blocksDistribution.addHostsAndBlockWeight(hosts, len);
+      StorageType[] storageTypes = bl.getStorageTypes();
+      blocksDistribution.addHostsAndBlockWeight(hosts, len ,storageTypes);
     }
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
index e9ec333e31ed..d69290d0bc45 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/TestHDFSBlocksDistribution.java
@@ -21,6 +21,7 @@
 
 import java.util.HashMap;
 import java.util.Map;
+import org.apache.hadoop.fs.StorageType;
 import org.apache.hadoop.hbase.testclassification.MiscTests;
 import org.apache.hadoop.hbase.testclassification.SmallTests;
 import org.junit.ClassRule;
@@ -50,13 +51,19 @@ public void testAddHostsAndBlockWeight() throws Exception {
     distribution.addHostsAndBlockWeight(new String[] {"testTwo"}, 222);
     assertEquals("Should be two hosts", 2, distribution.getHostAndWeights().size());
     assertEquals("Total weight should be 525", 525, distribution.getUniqueBlocksTotalWeight());
+    distribution.addHostsAndBlockWeight(new String[] {"test"}, 100
+      , new StorageType[] { StorageType.SSD});
+    assertEquals("test host should have weight 403", 403
+      , distribution.getHostAndWeights().get("test").getWeight());
+    assertEquals("test host should have weight for ssd 100", 100
+      , distribution.getHostAndWeights().get("test").getWeightForSsd());
   }
 
   public class MockHDFSBlocksDistribution extends HDFSBlocksDistribution {
     @Override
     public Map<String,HostAndWeight> getHostAndWeights() {
       HashMap<String, HostAndWeight> map = new HashMap<>();
-      map.put("test", new HostAndWeight(null, 100));
+      map.put("test", new HostAndWeight(null, 100, 0));
       return map;
     }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
index 9b8a7b92ef6d..88d306dca689 100644
--- a/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/master/balancer/TestStochasticLoadBalancer.java
@@ -49,6 +49,7 @@
 import org.apache.hadoop.hbase.master.RegionPlan;
 import org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.Cluster;
 import org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.ServerLocalityCostFunction;
+import org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.ServerSsdLocalityCostFunction;
 import org.apache.hadoop.hbase.testclassification.MasterTests;
 import org.apache.hadoop.hbase.testclassification.MediumTests;
 import org.apache.hadoop.hbase.util.Bytes;
@@ -271,9 +272,11 @@ public void testNeedBalance() {
   public void testLocalityCost() throws Exception {
     Configuration conf = HBaseConfiguration.create();
     MockNoopMasterServices master = new MockNoopMasterServices();
-    StochasticLoadBalancer.CostFunction
-        costFunction = new ServerLocalityCostFunction(conf, master);
+    testLocalityCost(new ServerLocalityCostFunction(conf, master));
+    testLocalityCost(new ServerSsdLocalityCostFunction(conf, master));
+  }
 
+  private void testLocalityCost(StochasticLoadBalancer.CostFunction costFunction) {
     for (int test = 0; test < clusterRegionLocationMocks.length; test++) {
       int[][] clusterRegionLocations = clusterRegionLocationMocks[test];
       MockCluster cluster = new MockCluster(clusterRegionLocations);
@@ -511,7 +514,7 @@ public MockCluster(int[][] regions) {
     }
 
     @Override
-    float getLocalityOfRegion(int region, int server) {
+    float getLocalityOfRegion(int region, int server, IncludeStorageType includeStorageType) {
       // convert the locality percentage to a fraction
       return localities[region][server] / 100.0f;
     }
