diff --git a/core/src/main/scala/org/apache/spark/SparkConf.scala b/core/src/main/scala/org/apache/spark/SparkConf.scala
index 605df0e929faa..05d1a1d380e68 100644
--- a/core/src/main/scala/org/apache/spark/SparkConf.scala
+++ b/core/src/main/scala/org/apache/spark/SparkConf.scala
@@ -187,8 +187,8 @@ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging {
   /** Get all executor environment variables set on this SparkConf */
   def getExecutorEnv: Seq[(String, String)] = {
     val prefix = "spark.executorEnv."
-    getAll.filter{case (k, v) => k.startsWith(prefix)}
-          .map{case (k, v) => (k.substring(prefix.length), v)}
+    getAll.filter {case (k, v) => k.startsWith(prefix)}
+          .map {case (k, v) => (k.substring(prefix.length), v)}
   }
 
   /** Get all akka conf variables set on this SparkConf */
@@ -311,7 +311,7 @@ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging {
    * configuration out for debugging.
    */
   def toDebugString: String = {
-    settings.toArray.sorted.map{case (k, v) => k + "=" + v}.mkString("\n")
+    settings.toArray.sorted.map {case (k, v) => k + "=" + v}.mkString("\n")
   }
 }
 
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
index 0846225e4f992..f23d9254a2b55 100644
--- a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala
@@ -454,7 +454,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def leftOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner)
   : JavaPairRDD[K, (V, Optional[W])] = {
     val joinResult = rdd.leftOuterJoin(other, partitioner)
-    fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))})
+    fromRDD(joinResult.mapValues {case (v, w) => (v, JavaUtils.optionToOptional(w))})
   }
 
   /**
@@ -466,7 +466,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def rightOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner)
   : JavaPairRDD[K, (Optional[V], W)] = {
     val joinResult = rdd.rightOuterJoin(other, partitioner)
-    fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)})
+    fromRDD(joinResult.mapValues {case (v, w) => (JavaUtils.optionToOptional(v), w)})
   }
 
   /**
@@ -480,8 +480,8 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def fullOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner)
   : JavaPairRDD[K, (Optional[V], Optional[W])] = {
     val joinResult = rdd.fullOuterJoin(other, partitioner)
-    fromRDD(joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    fromRDD(joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     })
   }
 
@@ -541,7 +541,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    */
   def leftOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, Optional[W])] = {
     val joinResult = rdd.leftOuterJoin(other)
-    fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))})
+    fromRDD(joinResult.mapValues {case (v, w) => (v, JavaUtils.optionToOptional(w))})
   }
 
   /**
@@ -553,7 +553,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def leftOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int)
   : JavaPairRDD[K, (V, Optional[W])] = {
     val joinResult = rdd.leftOuterJoin(other, numPartitions)
-    fromRDD(joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))})
+    fromRDD(joinResult.mapValues {case (v, w) => (v, JavaUtils.optionToOptional(w))})
   }
 
   /**
@@ -564,7 +564,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    */
   def rightOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], W)] = {
     val joinResult = rdd.rightOuterJoin(other)
-    fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)})
+    fromRDD(joinResult.mapValues {case (v, w) => (JavaUtils.optionToOptional(v), w)})
   }
 
   /**
@@ -576,7 +576,7 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def rightOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int)
   : JavaPairRDD[K, (Optional[V], W)] = {
     val joinResult = rdd.rightOuterJoin(other, numPartitions)
-    fromRDD(joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)})
+    fromRDD(joinResult.mapValues {case (v, w) => (JavaUtils.optionToOptional(v), w)})
   }
 
   /**
@@ -590,8 +590,8 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
    */
   def fullOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], Optional[W])] = {
     val joinResult = rdd.fullOuterJoin(other)
-    fromRDD(joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    fromRDD(joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     })
   }
 
@@ -606,8 +606,8 @@ class JavaPairRDD[K, V](val rdd: RDD[(K, V)])
   def fullOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int)
   : JavaPairRDD[K, (Optional[V], Optional[W])] = {
     val joinResult = rdd.fullOuterJoin(other, numPartitions)
-    fromRDD(joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    fromRDD(joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     })
   }
 
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
index 49dc95f349eac..6b3ed9339fff3 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala
@@ -160,7 +160,7 @@ private[python] object PythonHadoopUtil {
   def mapToConf(map: java.util.Map[String, String]): Configuration = {
     import collection.JavaConversions._
     val conf = new Configuration()
-    map.foreach{ case (k, v) => conf.set(k, v) }
+    map.foreach { case (k, v) => conf.set(k, v) }
     conf
   }
 
diff --git a/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala b/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala
index d11db978b842e..7e7ad7e7fa2fb 100644
--- a/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala
@@ -134,14 +134,14 @@ object WriteInputFormatTestDataGenerator {
      */
     val intKeys = Seq((1, "aa"), (2, "bb"), (2, "aa"), (3, "cc"), (2, "bb"), (1, "aa"))
     sc.parallelize(intKeys).saveAsSequenceFile(intPath)
-    sc.parallelize(intKeys.map{ case (k, v) => (k.toDouble, v) }).saveAsSequenceFile(doublePath)
-    sc.parallelize(intKeys.map{ case (k, v) => (k.toString, v) }).saveAsSequenceFile(textPath)
-    sc.parallelize(intKeys.map{ case (k, v) => (k, v.getBytes(Charset.forName("UTF-8"))) }
+    sc.parallelize(intKeys.map { case (k, v) => (k.toDouble, v) }).saveAsSequenceFile(doublePath)
+    sc.parallelize(intKeys.map { case (k, v) => (k.toString, v) }).saveAsSequenceFile(textPath)
+    sc.parallelize(intKeys.map { case (k, v) => (k, v.getBytes(Charset.forName("UTF-8"))) }
       ).saveAsSequenceFile(bytesPath)
     val bools = Seq((1, true), (2, true), (2, false), (3, true), (2, false), (1, false))
     sc.parallelize(bools).saveAsSequenceFile(boolPath)
-    sc.parallelize(intKeys).map{ case (k, v) =>
-      (new IntWritable(k), NullWritable.get())
+    sc.parallelize(intKeys).map {
+      case (k, v) => (new IntWritable(k), NullWritable.get())
     }.saveAsSequenceFile(nullPath)
 
     // Create test data for ArrayWritable
@@ -150,8 +150,8 @@ object WriteInputFormatTestDataGenerator {
       (2, Array(3.0, 4.0, 5.0)),
       (3, Array(4.0, 5.0, 6.0))
     )
-    sc.parallelize(data, numSlices = 2)
-      .map{ case (k, v) =>
+    sc.parallelize(data, numSlices = 2).map {
+      case (k, v) =>
         val va = new DoubleArrayWritable
         va.set(v.map(new DoubleWritable(_)))
         (new IntWritable(k), va)
@@ -165,12 +165,13 @@ object WriteInputFormatTestDataGenerator {
       (2, Map(1.0 -> "aa")),
       (1, Map(3.0 -> "bb"))
     )
-    sc.parallelize(mapData, numSlices = 2).map{ case (i, m) =>
-      val mw = new MapWritable()
-      m.foreach { case (k, v) =>
-        mw.put(new DoubleWritable(k), new Text(v))
-      }
-      (new IntWritable(i), mw)
+    sc.parallelize(mapData, numSlices = 2).map {
+      case (i, m) =>
+        val mw = new MapWritable()
+        m.foreach { case (k, v) =>
+          mw.put(new DoubleWritable(k), new Text(v))
+        }
+        (new IntWritable(i), mw)
     }.saveAsSequenceFile(mapPath)
 
     // Create test data for arbitrary custom writable TestWritable
@@ -181,7 +182,7 @@ object WriteInputFormatTestDataGenerator {
       ("3", TestWritable("test56", 456, 423.5)),
       ("2", TestWritable("test2", 123, 5435.2))
     )
-    val rdd = sc.parallelize(testClass, numSlices = 2).map{ case (k, v) => (new Text(k), v) }
+    val rdd = sc.parallelize(testClass, numSlices = 2).map { case (k, v) => (new Text(k), v) }
     rdd.saveAsNewAPIHadoopFile(classPath,
       classOf[Text], classOf[TestWritable],
       classOf[SequenceFileOutputFormat[Text, TestWritable]])
diff --git a/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala b/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala
index 9f9911762505a..8184f6a6faa82 100644
--- a/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala
@@ -168,7 +168,7 @@ private[spark] class DriverRunner(
   private def launchDriver(command: Seq[String], envVars: Map[String, String], baseDir: File,
                            supervise: Boolean) {
     val builder = new ProcessBuilder(command: _*).directory(baseDir)
-    envVars.map{ case(k,v) => builder.environment().put(k, v) }
+    envVars.map { case(k,v) => builder.environment().put(k, v) }
 
     def initialize(process: Process) = {
       // Redirect stdout and stderr to files
diff --git a/core/src/main/scala/org/apache/spark/executor/ExecutorURLClassLoader.scala b/core/src/main/scala/org/apache/spark/executor/ExecutorURLClassLoader.scala
index 218ed7b5d2d39..425e220888ac3 100644
--- a/core/src/main/scala/org/apache/spark/executor/ExecutorURLClassLoader.scala
+++ b/core/src/main/scala/org/apache/spark/executor/ExecutorURLClassLoader.scala
@@ -34,7 +34,7 @@ private[spark] trait MutableURLClassLoader extends ClassLoader {
 private[spark] class ChildExecutorURLClassLoader(urls: Array[URL], parent: ClassLoader)
   extends MutableURLClassLoader {
 
-  private object userClassLoader extends URLClassLoader(urls, null){
+  private object userClassLoader extends URLClassLoader(urls, null) {
     override def addURL(url: URL) {
       super.addURL(url)
     }
diff --git a/core/src/main/scala/org/apache/spark/network/nio/Connection.scala b/core/src/main/scala/org/apache/spark/network/nio/Connection.scala
index 4f6f5e235811d..623cc0ab99dca 100644
--- a/core/src/main/scala/org/apache/spark/network/nio/Connection.scala
+++ b/core/src/main/scala/org/apache/spark/network/nio/Connection.scala
@@ -323,7 +323,7 @@ class SendingConnection(val address: InetSocketAddress, selector_ : Selector,
 
   // MUST be called within the selector loop
   def connect() {
-    try{
+    try {
       channel.register(selector, SelectionKey.OP_CONNECT)
       channel.connect(address)
       logInfo("Initiating connection to [" + address + "]")
diff --git a/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala b/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala
index 11ebafbf6d457..cbda76e8c1b50 100644
--- a/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala
@@ -316,7 +316,7 @@ private[spark] class PartitionCoalescer(maxPartitions: Int, prev: RDD[_], balanc
         for(i <- 0 until maxPartitions) {
           val rangeStart = ((i.toLong * prev.partitions.length) / maxPartitions).toInt
           val rangeEnd = (((i.toLong + 1) * prev.partitions.length) / maxPartitions).toInt
-          (rangeStart until rangeEnd).foreach{ j => groupArr(i).arr += prev.partitions(j) }
+          (rangeStart until rangeEnd).foreach(j => groupArr(i).arr += prev.partitions(j))
         }
       }
     } else {
diff --git a/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala b/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala
index 6b63eb23e9ee1..f20fbcbfdeb8c 100644
--- a/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala
@@ -200,7 +200,7 @@ class HadoopRDD[K, V](
       reader = inputFormat.getRecordReader(split.inputSplit.value, jobConf, Reporter.NULL)
 
       // Register an on-task-completion callback to close the input stream.
-      context.addTaskCompletionListener{ context => closeIfNeeded() }
+      context.addTaskCompletionListener(context => closeIfNeeded())
       val key: K = reader.createKey()
       val value: V = reader.createValue()
 
diff --git a/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala b/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala
index 0e38f224ac81d..d5a3164faac6e 100644
--- a/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala
@@ -68,7 +68,7 @@ class JdbcRDD[T: ClassTag](
   }
 
   override def compute(thePart: Partition, context: TaskContext) = new NextIterator[T] {
-    context.addTaskCompletionListener{ context => closeIfNeeded() }
+    context.addTaskCompletionListener(context => closeIfNeeded())
     val part = thePart.asInstanceOf[JdbcPartition]
     val conn = getConnection()
     val stmt = conn.prepareStatement(sql, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)
diff --git a/core/src/main/scala/org/apache/spark/rdd/RDD.scala b/core/src/main/scala/org/apache/spark/rdd/RDD.scala
index 2aba40d152e3e..4e9e8033723c2 100644
--- a/core/src/main/scala/org/apache/spark/rdd/RDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/RDD.scala
@@ -1357,7 +1357,7 @@ abstract class RDD[T: ClassTag](
       val leftOffset = (partitionStr.length - 1) / 2
       val nextPrefix = (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset))
 
-      debugSelf(rdd).zipWithIndex.map{
+      debugSelf(rdd).zipWithIndex.map {
         case (desc: String, 0) => s"$partitionStr $desc"
         case (desc: String, _) => s"$nextPrefix $desc"
       } ++ debugChildren(rdd, nextPrefix)
@@ -1371,7 +1371,7 @@ abstract class RDD[T: ClassTag](
         + (if (isLastChild) "  " else "| ")
         + (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset)))
 
-      debugSelf(rdd).zipWithIndex.map{
+      debugSelf(rdd).zipWithIndex.map {
         case (desc: String, 0) => s"$thisPrefix+-$partitionStr $desc"
         case (desc: String, _) => s"$nextPrefix$desc"
       } ++ debugChildren(rdd, nextPrefix)
diff --git a/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala b/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala
index bac37bfdaa23f..94e2e3bd47f93 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala
@@ -173,7 +173,7 @@ object InputFormatInfo {
     for (inputSplit <- formats) {
       val splits = inputSplit.findPreferredLocations()
 
-      for (split <- splits){
+      for (split <- splits) {
         val location = split.hostLocation
         val set = nodeToSplit.getOrElseUpdate(location, new HashSet[SplitInfo])
         set += split
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
index 6d697e3d003f6..5c0b7b5b963c0 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala
@@ -526,7 +526,7 @@ private[spark] object TaskSchedulerImpl {
         val containerList: ArrayBuffer[T] = map.get(key).getOrElse(null)
         assert(containerList != null)
         // Get the index'th entry for this host - if present
-        if (index < containerList.size){
+        if (index < containerList.size) {
           retval += containerList.apply(index)
           found = true
         }
diff --git a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
index b727438ae7e47..e6622a49850ca 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala
@@ -65,7 +65,7 @@ private[spark] class SortShuffleManager(conf: SparkConf) extends ShuffleManager
   override def unregisterShuffle(shuffleId: Int): Boolean = {
     if (shuffleMapNumber.containsKey(shuffleId)) {
       val numMaps = shuffleMapNumber.remove(shuffleId)
-      (0 until numMaps).map{ mapId =>
+      (0 until numMaps).map { mapId =>
         shuffleBlockManager.removeDataByMap(shuffleId, mapId)
       }
     }
diff --git a/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala b/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala
index 142285094342c..506f73c4e4ebf 100644
--- a/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala
+++ b/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala
@@ -43,7 +43,7 @@ class BlockManagerId private (
 
   def executorId: String = executorId_
 
-  if (null != host_){
+  if (null != host_) {
     Utils.checkHost(host_, "Expected hostname")
     assert (port_ > 0)
   }
diff --git a/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala b/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala
index 18d2b5075aa08..09eb0304ef2de 100644
--- a/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala
+++ b/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala
@@ -64,7 +64,7 @@ private[spark] object UIWorkloadGenerator {
       ("Single Shuffle", baseData.map(x => (x % 10, x)).reduceByKey(_ + _).count),
       ("Entirely failed phase", baseData.map(x => throw new Exception).count),
       ("Partially failed phase", {
-        baseData.map{x =>
+        baseData.map {x =>
           val probFailure = (4.0 / NUM_PARTITIONS)
           if (nextFloat() < probFailure) {
             throw new Exception("This is a task failure")
@@ -73,7 +73,7 @@ private[spark] object UIWorkloadGenerator {
         }.count
       }),
       ("Partially failed phase (longer tasks)", {
-        baseData.map{x =>
+        baseData.map {x =>
           val probFailure = (4.0 / NUM_PARTITIONS)
           if (nextFloat() < probFailure) {
             Thread.sleep(100)
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
index 2414e4c65237e..a2599ade43d38 100644
--- a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala
@@ -292,7 +292,7 @@ private[ui] class StagePage(parent: JobProgressTab) extends WebUIPage("stage") {
         </td>
         <td>
           {Unparsed(
-            info.accumulables.map{acc => s"${acc.name}: ${acc.update.get}"}.mkString("<br/>")
+            info.accumulables.map(acc => s"${acc.name}: ${acc.update.get}").mkString("<br/>")
           )}
         </td>
         <!--
diff --git a/core/src/main/scala/org/apache/spark/util/Distribution.scala b/core/src/main/scala/org/apache/spark/util/Distribution.scala
index a465298c8c5ab..08f59f9ed5f76 100644
--- a/core/src/main/scala/org/apache/spark/util/Distribution.scala
+++ b/core/src/main/scala/org/apache/spark/util/Distribution.scala
@@ -44,7 +44,7 @@ private[spark] class Distribution(val data: Array[Double], val startIdx: Int, va
    */
   def getQuantiles(probabilities: Traversable[Double] = defaultProbabilities)
       : IndexedSeq[Double] = {
-    probabilities.toIndexedSeq.map{p:Double => data(closestIndex(p))}
+    probabilities.toIndexedSeq.map(q => data(closestIndex(q)))
   }
 
   private def closestIndex(p: Double) = {
@@ -53,7 +53,7 @@ private[spark] class Distribution(val data: Array[Double], val startIdx: Int, va
 
   def showQuantiles(out: PrintStream = System.out): Unit = {
     out.println("min\t25%\t50%\t75%\tmax")
-    getQuantiles(defaultProbabilities).foreach{q => out.print(q + "\t")}
+    getQuantiles(defaultProbabilities).foreach(q => out.print(q + "\t"))
     out.println
   }
 
@@ -81,7 +81,7 @@ private[spark] object Distribution {
 
   def showQuantiles(out: PrintStream = System.out, quantiles: Traversable[Double]) {
     out.println("min\t25%\t50%\t75%\tmax")
-    quantiles.foreach{q => out.print(q + "\t")}
+    quantiles.foreach(q => out.print(q + "\t"))
     out.println
   }
 }
diff --git a/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala b/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala
index 65251e93190f0..a27720ab3adcd 100644
--- a/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala
@@ -33,10 +33,10 @@ object DriverSubmissionTest {
     val properties = System.getProperties()
 
     println("Environment variables containing SPARK_TEST:")
-    env.filter{case (k, v) => k.contains("SPARK_TEST")}.foreach(println)
+    env.filter {case (k, v) => k.contains("SPARK_TEST")}.foreach(println)
 
     println("System properties containing spark.test:")
-    properties.filter{case (k, v) => k.toString.contains("spark.test")}.foreach(println)
+    properties.filter {case (k, v) => k.toString.contains("spark.test")}.foreach(println)
 
     for (i <- 1 until numSecondsToSleep) {
       println(s"Alive for $i out of $numSecondsToSleep seconds")
diff --git a/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala b/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala
index 931faac5463c4..6b2a2ac0b7b82 100644
--- a/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala
@@ -55,7 +55,7 @@ object LocalFileLR {
     val ITERATIONS = args(1).toInt
 
     // Initialize w to a random value
-    var w = DenseVector.fill(D){2 * rand.nextDouble - 1}
+    var w = DenseVector.fill(D)(2 * rand.nextDouble - 1)
     println("Initial w: " + w)
 
     for (i <- 1 to ITERATIONS) {
diff --git a/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala b/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala
index 17624c20cff3d..3679d33e49b37 100644
--- a/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala
@@ -42,7 +42,7 @@ object LocalKMeans {
 
   def generateData = {
     def generatePoint(i: Int) = {
-      DenseVector.fill(D){rand.nextDouble * R}
+      DenseVector.fill(D)(rand.nextDouble * R)
     }
     Array.tabulate(N)(generatePoint)
   }
diff --git a/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala b/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala
index 2d75b9d2590f8..f22b49eecf129 100644
--- a/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala
@@ -39,7 +39,7 @@ object LocalLR {
   def generateData = {
     def generatePoint(i: Int) = {
       val y = if(i % 2 == 0) -1 else 1
-      val x = DenseVector.fill(D){rand.nextGaussian + y * R}
+      val x = DenseVector.fill(D)(rand.nextGaussian + y * R)
       DataPoint(x, y)
     }
     Array.tabulate(N)(generatePoint)
@@ -59,7 +59,7 @@ object LocalLR {
 
     val data = generateData
     // Initialize w to a random value
-    var w = DenseVector.fill(D){2 * rand.nextDouble - 1}
+    var w = DenseVector.fill(D)(2 * rand.nextDouble - 1)
     println("Initial w: " + w)
 
     for (i <- 1 to ITERATIONS) {
diff --git a/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala b/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala
index 74620ad007d83..b3e62cc1cf0c3 100644
--- a/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala
@@ -77,7 +77,7 @@ object LogQuery {
 
     dataSet.map(line => (extractKey(line), extractStats(line)))
       .reduceByKey((a, b) => a.merge(b))
-      .collect().foreach{
+      .collect().foreach {
         case (user, query) => println("%s\t%s".format(user, query))}
 
     sc.stop()
diff --git a/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala b/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala
index 3258510894372..27c1bf88965b2 100644
--- a/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala
@@ -80,7 +80,7 @@ object SparkHdfsLR {
     val ITERATIONS = args(1).toInt
 
     // Initialize w to a random value
-    var w = DenseVector.fill(D){2 * rand.nextDouble - 1}
+    var w = DenseVector.fill(D)(2 * rand.nextDouble - 1)
     println("Initial w: " + w)
 
     for (i <- 1 to ITERATIONS) {
diff --git a/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala b/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala
index 48e8d11cdf95b..4ae85a82ed87a 100644
--- a/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala
@@ -79,7 +79,7 @@ object SparkKMeans {
     while(tempDist > convergeDist) {
       val closest = data.map (p => (closestPoint(p, kPoints), (p, 1)))
 
-      val pointStats = closest.reduceByKey{case ((x1, y1), (x2, y2)) => (x1 + x2, y1 + y2)}
+      val pointStats = closest.reduceByKey {case ((x1, y1), (x2, y2)) => (x1 + x2, y1 + y2)}
 
       val newPoints = pointStats.map {pair =>
         (pair._1, pair._2._1 * (1.0 / pair._2._2))}.collectAsMap()
diff --git a/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala b/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala
index fc23308fc4adf..519fac00d61ea 100644
--- a/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala
@@ -44,7 +44,7 @@ object SparkLR {
   def generateData = {
     def generatePoint(i: Int) = {
       val y = if(i % 2 == 0) -1 else 1
-      val x = DenseVector.fill(D){rand.nextGaussian + y * R}
+      val x = DenseVector.fill(D)(rand.nextGaussian + y * R)
       DataPoint(x, y)
     }
     Array.tabulate(N)(generatePoint)
@@ -68,7 +68,7 @@ object SparkLR {
     val points = sc.parallelize(generateData, numSlices).cache()
 
     // Initialize w to a random value
-    var w = DenseVector.fill(D){2 * rand.nextDouble - 1}
+    var w = DenseVector.fill(D)(2 * rand.nextDouble - 1)
     println("Initial w: " + w)
 
     for (i <- 1 to ITERATIONS) {
diff --git a/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala b/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala
index 4c7e006da0618..209eb2271dc7e 100644
--- a/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala
@@ -39,16 +39,17 @@ object SparkPageRank {
     val iters = if (args.length > 0) args(1).toInt else 10
     val ctx = new SparkContext(sparkConf)
     val lines = ctx.textFile(args(0), 1)
-    val links = lines.map{ s =>
+    val links = lines.map { s =>
       val parts = s.split("\\s+")
       (parts(0), parts(1))
     }.distinct().groupByKey().cache()
     var ranks = links.mapValues(v => 1.0)
 
     for (i <- 1 to iters) {
-      val contribs = links.join(ranks).values.flatMap{ case (urls, rank) =>
-        val size = urls.size
-        urls.map(url => (url, rank / size))
+      val contribs = links.join(ranks).values.flatMap {
+        case (urls, rank) =>
+          val size = urls.size
+          urls.map(url => (url, rank / size))
       }
       ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)
     }
diff --git a/examples/src/main/scala/org/apache/spark/examples/SparkTachyonHdfsLR.scala b/examples/src/main/scala/org/apache/spark/examples/SparkTachyonHdfsLR.scala
index 96d13612e46dd..e06512c0c5dfe 100644
--- a/examples/src/main/scala/org/apache/spark/examples/SparkTachyonHdfsLR.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/SparkTachyonHdfsLR.scala
@@ -63,7 +63,7 @@ object SparkTachyonHdfsLR {
     val ITERATIONS = args(1).toInt
 
     // Initialize w to a random value
-    var w = DenseVector.fill(D){2 * rand.nextDouble - 1}
+    var w = DenseVector.fill(D)(2 * rand.nextDouble - 1)
     println("Initial w: " + w)
 
     for (i <- 1 to ITERATIONS) {
diff --git a/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala b/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala
index c4317a6aec798..fcae7dc20b0be 100644
--- a/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala
@@ -107,7 +107,9 @@ object Analytics extends Logging {
 
         if (!outFname.isEmpty) {
           logWarning("Saving pageranks of pages to " + outFname)
-          pr.map{case (id, r) => id + "\t" + r}.saveAsTextFile(outFname)
+          pr.map {
+            case (id, r) => id + "\t" + r
+          }.saveAsTextFile(outFname)
         }
 
         sc.stop()
@@ -129,7 +131,9 @@ object Analytics extends Logging {
         val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_))
 
         val cc = ConnectedComponents.run(graph)
-        println("Components: " + cc.vertices.map{ case (vid,data) => data}.distinct())
+        println("Components: " + cc.vertices.map {
+          case (vid,data) => data
+        }.distinct())
         sc.stop()
 
       case "triangles" =>
diff --git a/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala b/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala
index fc6678013b932..d1907bc70e047 100644
--- a/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala
@@ -188,7 +188,7 @@ object MovieLensALS {
     def mapPredictedRating(r: Double) = if (implicitPrefs) math.max(math.min(r, 1.0), 0.0) else r
 
     val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))
-    val predictionsAndRatings = predictions.map{ x =>
+    val predictionsAndRatings = predictions.map { x =>
       ((x.user, x.product), mapPredictedRating(x.rating))
     }.join(data.map(x => ((x.user, x.product), x.rating))).values
     math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean())
diff --git a/examples/src/main/scala/org/apache/spark/examples/streaming/ActorWordCount.scala b/examples/src/main/scala/org/apache/spark/examples/streaming/ActorWordCount.scala
index b433082dce1a2..b56dd460b49ec 100644
--- a/examples/src/main/scala/org/apache/spark/examples/streaming/ActorWordCount.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/streaming/ActorWordCount.scala
@@ -104,7 +104,7 @@ extends Actor with ActorHelper {
 object FeederActor {
 
   def main(args: Array[String]) {
-    if(args.length < 2){
+    if(args.length < 2) {
       System.err.println(
         "Usage: FeederActor <hostname> <port>\n"
       )
diff --git a/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala b/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala
index f55d23ab3924b..7c9f62c8a3522 100644
--- a/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/streaming/TwitterPopularTags.scala
@@ -58,11 +58,11 @@ object TwitterPopularTags {
     val hashTags = stream.flatMap(status => status.getText.split(" ").filter(_.startsWith("#")))
 
     val topCounts60 = hashTags.map((_, 1)).reduceByKeyAndWindow(_ + _, Seconds(60))
-                     .map{case (topic, count) => (count, topic)}
+                     .map {case (topic, count) => (count, topic)}
                      .transform(_.sortByKey(false))
 
     val topCounts10 = hashTags.map((_, 1)).reduceByKeyAndWindow(_ + _, Seconds(10))
-                     .map{case (topic, count) => (count, topic)}
+                     .map {case (topic, count) => (count, topic)}
                      .transform(_.sortByKey(false))
 
 
@@ -70,13 +70,13 @@ object TwitterPopularTags {
     topCounts60.foreachRDD(rdd => {
       val topList = rdd.take(10)
       println("\nPopular topics in last 60 seconds (%s total):".format(rdd.count()))
-      topList.foreach{case (count, tag) => println("%s (%s tweets)".format(tag, count))}
+      topList.foreach {case (count, tag) => println("%s (%s tweets)".format(tag, count))}
     })
 
     topCounts10.foreachRDD(rdd => {
       val topList = rdd.take(10)
       println("\nPopular topics in last 10 seconds (%s total):".format(rdd.count()))
-      topList.foreach{case (count, tag) => println("%s (%s tweets)".format(tag, count))}
+      topList.foreach {case (count, tag) => println("%s (%s tweets)".format(tag, count))}
     })
 
     ssc.start()
diff --git a/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala b/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala
index d9b886eff77cc..a26662a907a2b 100644
--- a/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala
+++ b/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala
@@ -67,7 +67,7 @@ object PageViewStream {
     val statusesPerZipCode = pageViews.window(Seconds(30), Seconds(2))
                                       .map(view => ((view.zipCode, view.status)))
                                       .groupByKey()
-    val errorRatePerZipCode = statusesPerZipCode.map{
+    val errorRatePerZipCode = statusesPerZipCode.map {
       case(zip, statuses) =>
         val normalCount = statuses.filter(_ == 200).size
         val errorCount = statuses.size - normalCount
diff --git a/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala b/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala
index 8dd958033b338..30aa3372c775c 100644
--- a/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala
+++ b/graphx/src/main/scala/org/apache/spark/graphx/lib/StronglyConnectedComponents.scala
@@ -67,7 +67,7 @@ object StronglyConnectedComponents {
         sccWorkGraph = sccWorkGraph.subgraph(vpred = (vid, data) => !data._2).cache()
       } while (sccWorkGraph.numVertices < numVertices)
 
-      sccWorkGraph = sccWorkGraph.mapVertices{ case (vid, (color, isFinal)) => (vid, isFinal) }
+      sccWorkGraph = sccWorkGraph.mapVertices { case (vid, (color, isFinal)) => (vid, isFinal) }
 
       // collect min of all my neighbor's scc values, update if it's smaller than mine
       // then notify any neighbors with scc values larger than mine
diff --git a/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala b/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala
index 8a13c74221546..638416cd344e7 100644
--- a/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala
+++ b/graphx/src/main/scala/org/apache/spark/graphx/util/GraphGenerators.scala
@@ -146,7 +146,7 @@ object GraphGenerators {
   private def outDegreeFromEdges[ED: ClassTag](edges: RDD[Edge[ED]]): Graph[Int, ED] = {
     val vertices = edges.flatMap { edge => List((edge.srcId, 1)) }
       .reduceByKey(_ + _)
-      .map{ case (vid, degree) => (vid, degree) }
+      .map { case (vid, degree) => (vid, degree) }
     Graph(vertices, edges, 0)
   }
 
@@ -244,10 +244,11 @@ object GraphGenerators {
     val vertices: RDD[(VertexId, (Int,Int))] =
       sc.parallelize(0 until rows).flatMap( r => (0 until cols).map( c => (sub2ind(r,c), (r,c)) ) )
     val edges: RDD[Edge[Double]] =
-      vertices.flatMap{ case (vid, (r,c)) =>
-        (if (r + 1 < rows) { Seq( (sub2ind(r, c), sub2ind(r + 1, c))) } else { Seq.empty }) ++
-        (if (c + 1 < cols) { Seq( (sub2ind(r, c), sub2ind(r, c + 1))) } else { Seq.empty })
-      }.map{ case (src, dst) => Edge(src, dst, 1.0) }
+      vertices.flatMap {
+        case (vid, (r,c)) =>
+          (if (r + 1 < rows) { Seq( (sub2ind(r, c), sub2ind(r + 1, c))) } else { Seq.empty }) ++
+          (if (c + 1 < cols) { Seq( (sub2ind(r, c), sub2ind(r, c + 1))) } else { Seq.empty })
+      }.map { case (src, dst) => Edge(src, dst, 1.0) }
     Graph(vertices, edges)
   } // end of gridGraph
 
diff --git a/mllib/src/main/scala/org/apache/spark/mllib/linalg/BLAS.scala b/mllib/src/main/scala/org/apache/spark/mllib/linalg/BLAS.scala
index 54ee930d61003..67e9168a823ae 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/linalg/BLAS.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/linalg/BLAS.scala
@@ -315,7 +315,7 @@ private[mllib] object BLAS extends Serializable with Logging {
     val Acols = if (!transA) A.colPtrs else A.rowIndices
 
     // Slicing is easy in this case. This is the optimal multiplication setting for sparse matrices
-    if (transA){
+    if (transA) {
       var colCounterForB = 0
       if (!transB) { // Expensive to put the check inside the loop
         while (colCounterForB < nB) {
@@ -357,7 +357,7 @@ private[mllib] object BLAS extends Serializable with Logging {
       }
     } else {
       // Scale matrix first if `beta` is not equal to 0.0
-      if (beta != 0.0){
+      if (beta != 0.0) {
         f2jBLAS.dscal(C.values.length, beta, C.values, 1)
       }
       // Perform matrix multiplication and add to C. The rows of A are multiplied by the columns of
@@ -372,7 +372,7 @@ private[mllib] object BLAS extends Serializable with Logging {
             var i = Acols(colCounterForA)
             val indEnd = Acols(colCounterForA + 1)
             val Bval = B.values(Bstart + colCounterForA) * alpha
-            while (i < indEnd){
+            while (i < indEnd) {
               C.values(Cstart + Arows(i)) += Avals(i) * Bval
               i += 1
             }
@@ -384,11 +384,11 @@ private[mllib] object BLAS extends Serializable with Logging {
         while (colCounterForB < nB) {
           var colCounterForA = 0 // The column of A to multiply with the row of B
           val Cstart = colCounterForB * mA
-          while (colCounterForA < kA){
+          while (colCounterForA < kA) {
             var i = Acols(colCounterForA)
             val indEnd = Acols(colCounterForA + 1)
             val Bval = B(colCounterForB, colCounterForA) * alpha
-            while (i < indEnd){
+            while (i < indEnd) {
               C.values(Cstart + Arows(i)) += Avals(i) * Bval
               i += 1
             }
@@ -492,13 +492,13 @@ private[mllib] object BLAS extends Serializable with Logging {
     val Acols = if (!trans) A.colPtrs else A.rowIndices
 
     // Slicing is easy in this case. This is the optimal multiplication setting for sparse matrices
-    if (trans){
+    if (trans) {
       var rowCounter = 0
-      while (rowCounter < mA){
+      while (rowCounter < mA) {
         var i = Arows(rowCounter)
         val indEnd = Arows(rowCounter + 1)
         var sum = 0.0
-        while(i < indEnd){
+        while(i < indEnd) {
           sum += Avals(i) * x.values(Acols(i))
           i += 1
         }
@@ -507,16 +507,16 @@ private[mllib] object BLAS extends Serializable with Logging {
       }
     } else {
       // Scale vector first if `beta` is not equal to 0.0
-      if (beta != 0.0){
+      if (beta != 0.0) {
         scal(beta, y)
       }
       // Perform matrix-vector multiplication and add to y
       var colCounterForA = 0
-      while (colCounterForA < nA){
+      while (colCounterForA < nA) {
         var i = Acols(colCounterForA)
         val indEnd = Acols(colCounterForA + 1)
         val xVal = x.values(colCounterForA) * alpha
-        while (i < indEnd){
+        while (i < indEnd) {
           val rowIndex = Arows(i)
           y.values(rowIndex) += Avals(i) * xVal
           i += 1
diff --git a/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala b/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala
index 2cc52e94282ba..9e8632f8fb5bb 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala
@@ -190,7 +190,7 @@ class SparseMatrix(
 
   private[mllib] def update(i: Int, j: Int, v: Double): Unit = {
     val ind = index(i, j)
-    if (ind == -1){
+    if (ind == -1) {
       throw new NoSuchElementException("The given row and column indices correspond to a zero " +
         "value. Only non-zero elements in Sparse Matrices can be updated.")
     } else {
@@ -280,7 +280,7 @@ object Matrices {
   def eye(n: Int): Matrix = {
     val identity = Matrices.zeros(n, n)
     var i = 0
-    while (i < n){
+    while (i < n) {
       identity.update(i, i, 1.0)
       i += 1
     }
diff --git a/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala b/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala
index 84d192db53e26..8f28b50fd70b5 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala
@@ -498,12 +498,12 @@ class ALS private (
             toSend(userBlock) += factors(p)
           }
         }
-        toSend.zipWithIndex.map{ case (buf, idx) => (idx, (bid, buf.toArray)) }
+        toSend.zipWithIndex.map { case (buf, idx) => (idx, (bid, buf.toArray)) }
     }.groupByKey(new HashPartitioner(numUserBlocks))
      .join(userInLinks)
-     .mapValues{ case (messages, inLinkBlock) =>
-        updateBlock(messages, inLinkBlock, rank, lambda, alpha, YtY)
-      }
+     .mapValues {
+      case (messages, inLinkBlock) => updateBlock(messages, inLinkBlock, rank, lambda, alpha, YtY)
+    }
   }
 
   /**
diff --git a/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala b/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala
index 66b58ba770160..dcffd8265c8f3 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/recommendation/MatrixFactorizationModel.scala
@@ -54,7 +54,7 @@ class MatrixFactorizationModel private[mllib] (
     * @return RDD of Ratings.
     */
   def predict(usersProducts: RDD[(Int, Int)]): RDD[Rating] = {
-    val users = userFeatures.join(usersProducts).map{
+    val users = userFeatures.join(usersProducts).map {
       case (user, (uFeatures, product)) => (product, (user, uFeatures))
     }
     users.join(productFeatures).map {
diff --git a/mllib/src/main/scala/org/apache/spark/mllib/tree/model/Node.scala b/mllib/src/main/scala/org/apache/spark/mllib/tree/model/Node.scala
index 2179da8dbe03e..3d6219f527668 100644
--- a/mllib/src/main/scala/org/apache/spark/mllib/tree/model/Node.scala
+++ b/mllib/src/main/scala/org/apache/spark/mllib/tree/model/Node.scala
@@ -81,7 +81,7 @@ class Node (
   def predict(features: Vector) : Double = {
     if (isLeaf) {
       predict.predict
-    } else{
+    } else {
       if (split.get.featureType == Continuous) {
         if (features(split.get.feature) <= split.get.threshold) {
           leftNode.get.predict(features)
diff --git a/project/spark-style/src/main/scala/org/apache/spark/scalastyle/SparkSpaceBeforeLeftBraceChecker.scala b/project/spark-style/src/main/scala/org/apache/spark/scalastyle/SparkSpaceBeforeLeftBraceChecker.scala
new file mode 100644
index 0000000000000..8c711565802c8
--- /dev/null
+++ b/project/spark-style/src/main/scala/org/apache/spark/scalastyle/SparkSpaceBeforeLeftBraceChecker.scala
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.scalastyle
+
+import org.scalastyle.{PositionError, ScalariformChecker, ScalastyleError}
+import scala.collection.mutable.{ListBuffer, Queue}
+import scalariform.lexer.{Token, Tokens}
+import scalariform.lexer.Tokens._
+import scalariform.parser.CompilationUnit
+
+class SparkSpaceBeforeLeftBraceChecker extends ScalariformChecker {
+  val errorKey: String = "insert.a.single.space.before.left.brace"
+
+  val rememberQueue: Queue[Token] = Queue[Token]()
+
+  // The list of disallowed tokens before left brace without single space.
+  val disallowedTokensBeforeLBrace = Seq (
+    ARROW, DO, ELSE, FINALLY, MATCH, NEW, OP, RETURN, RPAREN, THROW, TRY, VARID, YIELD
+  )
+
+  override def verify(ast: CompilationUnit): List[ScalastyleError] = {
+
+    var list: ListBuffer[ScalastyleError] = new ListBuffer[ScalastyleError]
+
+    for (token <- ast.tokens) {
+      rememberToken(token)
+      if (isLBrace(token) &&
+          isTokenAfterSpecificTokens(token) &&
+          !hasSingleWhiteSpaceBefore(token)) {
+        list += new PositionError(token.offset)
+      }
+    }
+    list.toList
+  }
+
+  private def rememberToken(x: Token) = {
+    rememberQueue.enqueue(x)
+    if (rememberQueue.size > 2) {
+      rememberQueue.dequeue
+    }
+    x
+  }
+
+  private def isTokenAfterSpecificTokens(x: Token) = {
+    val previousToken = rememberQueue.head
+    disallowedTokensBeforeLBrace.contains(previousToken.tokenType)
+  }
+
+  private def isLBrace(x: Token) =
+    x.tokenType == Tokens.LBRACE
+
+  private def hasSingleWhiteSpaceBefore(x: Token) =
+    x.associatedWhitespaceAndComments.whitespaces.size == 1
+}
diff --git a/repl/src/main/scala/org/apache/spark/repl/SparkRunnerSettings.scala b/repl/src/main/scala/org/apache/spark/repl/SparkRunnerSettings.scala
index 7fd5fbb42468c..305f9e4627f2b 100644
--- a/repl/src/main/scala/org/apache/spark/repl/SparkRunnerSettings.scala
+++ b/repl/src/main/scala/org/apache/spark/repl/SparkRunnerSettings.scala
@@ -23,7 +23,7 @@ import scala.tools.nsc.Settings
  * <i>scala.tools.nsc.Settings</i> implementation adding Spark-specific REPL
  * command line options.
  */
-class SparkRunnerSettings(error: String => Unit) extends Settings(error){
+class SparkRunnerSettings(error: String => Unit) extends Settings(error) {
 
   val loadfiles = MultiStringSetting(
       "-i",
diff --git a/scalastyle-config.xml b/scalastyle-config.xml
index c54f8b72ebf42..1edf8c434933e 100644
--- a/scalastyle-config.xml
+++ b/scalastyle-config.xml
@@ -142,4 +142,5 @@
  <check level="error" class="org.scalastyle.file.NoNewLineAtEofChecker" enabled="false"></check>
  <check level="error" class="org.apache.spark.scalastyle.NonASCIICharacterChecker" enabled="true"></check>
  <check level="error" class="org.apache.spark.scalastyle.SparkSpaceAfterCommentStartChecker" enabled="true"></check>
+ <check level="error" class="org.apache.spark.scalastyle.SparkSpaceBeforeLeftBraceChecker" enabled="true"></check>
 </scalastyle>
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
index 7cc14dc7a9c9e..f5a79cf95a35d 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
@@ -671,7 +671,7 @@ private[hive] object HiveQl {
       }
 
       val (db, tableName) =
-        tableNameParts.getChildren.map{ case Token(part, Nil) => cleanIdentifier(part)} match {
+        tableNameParts.getChildren.map { case Token(part, Nil) => cleanIdentifier(part)} match {
           case Seq(tableOnly) => (None, tableOnly)
           case Seq(databaseName, table) => (Some(databaseName), table)
       }
diff --git a/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala b/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala
index 59d4423086ef0..4e45865def5b7 100644
--- a/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala
+++ b/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala
@@ -588,7 +588,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
   def leftOuterJoin[W](other: JavaPairDStream[K, W]): JavaPairDStream[K, (V, Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.leftOuterJoin(other.dstream)
-    joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}
+    joinResult.mapValues {
+      case (v, w) => (v, JavaUtils.optionToOptional(w))
+    }
   }
 
   /**
@@ -602,7 +604,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (V, Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.leftOuterJoin(other.dstream, numPartitions)
-    joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}
+    joinResult.mapValues {
+      case (v, w) => (v, JavaUtils.optionToOptional(w))
+    }
   }
 
   /**
@@ -616,7 +620,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (V, Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.leftOuterJoin(other.dstream, partitioner)
-    joinResult.mapValues{case (v, w) => (v, JavaUtils.optionToOptional(w))}
+    joinResult.mapValues {
+      case (v, w) => (v, JavaUtils.optionToOptional(w))
+    }
   }
 
   /**
@@ -627,7 +633,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
   def rightOuterJoin[W](other: JavaPairDStream[K, W]): JavaPairDStream[K, (Optional[V], W)] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.rightOuterJoin(other.dstream)
-    joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), w)
+    }
   }
 
   /**
@@ -641,7 +649,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (Optional[V], W)] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.rightOuterJoin(other.dstream, numPartitions)
-    joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), w)
+    }
   }
 
   /**
@@ -655,7 +665,9 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (Optional[V], W)] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.rightOuterJoin(other.dstream, partitioner)
-    joinResult.mapValues{case (v, w) => (JavaUtils.optionToOptional(v), w)}
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), w)
+    }
   }
 
   /**
@@ -667,8 +679,8 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
       : JavaPairDStream[K, (Optional[V], Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.fullOuterJoin(other.dstream)
-    joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     }
   }
 
@@ -683,8 +695,8 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (Optional[V], Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.fullOuterJoin(other.dstream, numPartitions)
-    joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     }
   }
 
@@ -699,8 +711,8 @@ class JavaPairDStream[K, V](val dstream: DStream[(K, V)])(
     ): JavaPairDStream[K, (Optional[V], Optional[W])] = {
     implicit val cm: ClassTag[W] = fakeClassTag
     val joinResult = dstream.fullOuterJoin(other.dstream, partitioner)
-    joinResult.mapValues{ case (v, w) =>
-      (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
+    joinResult.mapValues {
+      case (v, w) => (JavaUtils.optionToOptional(v), JavaUtils.optionToOptional(w))
     }
   }
 
diff --git a/streaming/src/main/scala/org/apache/spark/streaming/receiver/ActorReceiver.scala b/streaming/src/main/scala/org/apache/spark/streaming/receiver/ActorReceiver.scala
index 1868a1ebc7b4a..9b50275484921 100644
--- a/streaming/src/main/scala/org/apache/spark/streaming/receiver/ActorReceiver.scala
+++ b/streaming/src/main/scala/org/apache/spark/streaming/receiver/ActorReceiver.scala
@@ -68,7 +68,7 @@ object ActorSupervisorStrategy {
  *       should be same.
  */
 @DeveloperApi
-trait ActorHelper extends Logging{
+trait ActorHelper extends Logging {
 
   self: Actor => // to ensure that this can be added to Actor classes only
 
diff --git a/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala b/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala
index a73d6f3bf0661..7c97f618c894a 100644
--- a/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala
+++ b/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala
@@ -53,7 +53,7 @@ object RawTextHelper {
         case (k, v) => (k, v)
       }
     }
-    map.toIterator.map{case (k, v) => (k, v)}
+    map.toIterator.map {case (k, v) => (k, v)}
   }
 
   /**
diff --git a/yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala b/yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala
index abd37834ed3cc..349ef2da4fa5f 100644
--- a/yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala
+++ b/yarn/alpha/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala
@@ -146,7 +146,7 @@ private[yarn] class YarnAllocationHandler(
 
     val requestedContainers: ArrayBuffer[ResourceRequest] =
       new ArrayBuffer[ResourceRequest](rackToCounts.size)
-    for ((rack, count) <- rackToCounts){
+    for ((rack, count) <- rackToCounts) {
       requestedContainers +=
         createResourceRequest(AllocationType.RACK, rack, count,
           YarnSparkHadoopUtil.RM_REQUEST_PRIORITY)
@@ -209,7 +209,7 @@ private[yarn] class YarnAllocationHandler(
   private def createReleasedContainerList(): ArrayBuffer[ContainerId] = {
     val retval = new ArrayBuffer[ContainerId](1)
     // Iterator on COW list ...
-    for (container <- releaseList.iterator()){
+    for (container <- releaseList.iterator()) {
       retval += container
     }
     // Remove from the original list.
