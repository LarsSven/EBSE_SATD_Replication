{
  "url": "https://api.github.com/repos/apache/spark/pulls/28661",
  "id": 424249613,
  "node_id": "MDExOlB1bGxSZXF1ZXN0NDI0MjQ5NjEz",
  "html_url": "https://github.com/apache/spark/pull/28661",
  "diff_url": "https://github.com/apache/spark/pull/28661.diff",
  "patch_url": "https://github.com/apache/spark/pull/28661.patch",
  "issue_url": "https://api.github.com/repos/apache/spark/issues/28661",
  "number": 28661,
  "state": "closed",
  "locked": false,
  "title": "[SPARK-31849][PYTHON][SQL] Make PySpark SQL exceptions more Pythonic",
  "user": {
    "login": "HyukjinKwon",
    "id": 6477701,
    "node_id": "MDQ6VXNlcjY0Nzc3MDE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/6477701?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/HyukjinKwon",
    "html_url": "https://github.com/HyukjinKwon",
    "followers_url": "https://api.github.com/users/HyukjinKwon/followers",
    "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}",
    "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions",
    "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs",
    "repos_url": "https://api.github.com/users/HyukjinKwon/repos",
    "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}",
    "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events",
    "type": "User",
    "site_admin": false
  },
  "body": "### What changes were proposed in this pull request?\r\n\r\nThis PR proposes to make PySpark exception more Pythonic by hiding JVM stacktrace by default. It can be enabled by turning on `spark.sql.pyspark.jvmStacktrace.enabled` configuration.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  ...\r\npyspark.sql.utils.PythonException:\r\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\r\nTraceback (most recent call last):\r\n  ...\r\n```\r\n\r\nIf this `spark.sql.pyspark.jvmStacktrace.enabled` is enabled, it appends:\r\n\r\n```\r\nJVM stacktrace:\r\norg.apache.spark.Exception: ...\r\n  ...\r\n```\r\n\r\nFor example, the codes below:\r\n\r\n```python\r\nfrom pyspark.sql.functions import udf\r\n@udf\r\ndef divide_by_zero(v):\r\n    raise v / 0\r\n\r\nspark.range(1).select(divide_by_zero(\"id\")).show()\r\n```\r\n\r\nwill show an error messages that looks like Python exception thrown from the local. \r\n\r\n<details>\r\n<summary>Python exception message when <code>spark.sql.pyspark.jvmStacktrace.enabled</code> is off (default)</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/dataframe.py\", line 427, in show\r\n    print(self._jdf.showString(n, 20, vertical))\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 131, in deco\r\n    raise_from(converted)\r\n  File \"<string>\", line 3, in raise_from\r\npyspark.sql.utils.PythonException:\r\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\r\nTraceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Python exception message when <code>spark.sql.pyspark.jvmStacktrace.enabled</code> is on</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/dataframe.py\", line 427, in show\r\n    print(self._jdf.showString(n, 20, vertical))\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 137, in deco\r\n    raise_from(converted)\r\n  File \"<string>\", line 3, in raise_from\r\npyspark.sql.utils.PythonException:\r\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\r\nTraceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\nJVM stacktrace:\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 192.168.35.193, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2117)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2066)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2065)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2065)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1021)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2297)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2246)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2235)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:823)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3653)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3644)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3642)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Python exception message without this change</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/dataframe.py\", line 427, in show\r\n    print(self._jdf.showString(n, 20, vertical))\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 98, in deco\r\n    return f(*a, **kw)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o160.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 5.0 failed 4 times, most recent failure: Lost task 10.3 in stage 5.0 (TID 37, 192.168.35.193, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2117)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2066)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2065)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2065)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1021)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2297)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2246)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2235)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:823)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3653)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3644)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3642)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n```\r\n\r\n\r\n</details>\r\n\r\n<br/>\r\n\r\nAnother example with Python 3.7:\r\n\r\n```python\r\nsql(\"a\")\r\n```\r\n\r\n<details>\r\n<summary>Python exception message when <code>spark.sql.pyspark.jvmStacktrace.enabled</code> is off (default)</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/session.py\", line 646, in sql\r\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 131, in deco\r\n    raise_from(converted)\r\n  File \"<string>\", line 3, in raise_from\r\npyspark.sql.utils.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Python exception message when <code>spark.sql.pyspark.jvmStacktrace.enabled</code> is on</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/session.py\", line 646, in sql\r\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 131, in deco\r\n    raise_from(converted)\r\n  File \"<string>\", line 3, in raise_from\r\npyspark.sql.utils.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n\r\nJVM stacktrace:\r\norg.apache.spark.sql.catalyst.parser.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n\r\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:133)\r\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:49)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>Python exception message without this change</summary>\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 98, in deco\r\n    return f(*a, **kw)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o26.sql.\r\n: org.apache.spark.sql.catalyst.parser.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n\r\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:133)\r\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:49)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/session.py\", line 646, in sql\r\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 102, in deco\r\n    raise converted\r\npyspark.sql.utils.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Why are the changes needed?\r\n\r\nCurrently, PySpark exceptions are very unfriendly to Python users with causing a bunch of JVM stacktrace. See \"Python exception message without this change\" above.\r\n\r\n### Does this PR introduce _any_ user-facing change?\r\n\r\nYes, it will change the exception message. See the examples above.\r\n\r\n### How was this patch tested?\r\n\r\nManually tested by\r\n\r\n```bash\r\n./bin/pyspark --conf spark.sql.pyspark.jvmStacktrace.enabled=true\r\n```\r\n\r\nand running the examples above.\r\n",
  "created_at": "2020-05-28T05:01:49Z",
  "updated_at": "2020-07-27T07:44:38Z",
  "closed_at": "2020-06-01T00:45:29Z",
  "merged_at": null,
  "merge_commit_sha": "4f1c8ec38eafd6743af4f612382bf6069fa68aa4",
  "assignee": null,
  "assignees": [],
  "requested_reviewers": [
    {
      "login": "BryanCutler",
      "id": 4534389,
      "node_id": "MDQ6VXNlcjQ1MzQzODk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4534389?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/BryanCutler",
      "html_url": "https://github.com/BryanCutler",
      "followers_url": "https://api.github.com/users/BryanCutler/followers",
      "following_url": "https://api.github.com/users/BryanCutler/following{/other_user}",
      "gists_url": "https://api.github.com/users/BryanCutler/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/BryanCutler/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/BryanCutler/subscriptions",
      "organizations_url": "https://api.github.com/users/BryanCutler/orgs",
      "repos_url": "https://api.github.com/users/BryanCutler/repos",
      "events_url": "https://api.github.com/users/BryanCutler/events{/privacy}",
      "received_events_url": "https://api.github.com/users/BryanCutler/received_events",
      "type": "User",
      "site_admin": false
    }
  ],
  "requested_teams": [],
  "labels": [
    {
      "id": 1405794576,
      "node_id": "MDU6TGFiZWwxNDA1Nzk0NTc2",
      "url": "https://api.github.com/repos/apache/spark/labels/SQL",
      "name": "SQL",
      "color": "ededed",
      "default": false,
      "description": null
    },
    {
      "id": 1982260031,
      "node_id": "MDU6TGFiZWwxOTgyMjYwMDMx",
      "url": "https://api.github.com/repos/apache/spark/labels/PYTHON",
      "name": "PYTHON",
      "color": "ededed",
      "default": false,
      "description": null
    }
  ],
  "milestone": null,
  "draft": false,
  "commits_url": "https://api.github.com/repos/apache/spark/pulls/28661/commits",
  "review_comments_url": "https://api.github.com/repos/apache/spark/pulls/28661/comments",
  "review_comment_url": "https://api.github.com/repos/apache/spark/pulls/comments{/number}",
  "comments_url": "https://api.github.com/repos/apache/spark/issues/28661/comments",
  "statuses_url": "https://api.github.com/repos/apache/spark/statuses/28c2a516eec5ffbd527b9e62869b279a162458e4",
  "head": {
    "label": "HyukjinKwon:python-debug",
    "ref": "python-debug",
    "sha": "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "user": {
      "login": "HyukjinKwon",
      "id": 6477701,
      "node_id": "MDQ6VXNlcjY0Nzc3MDE=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6477701?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/HyukjinKwon",
      "html_url": "https://github.com/HyukjinKwon",
      "followers_url": "https://api.github.com/users/HyukjinKwon/followers",
      "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}",
      "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions",
      "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs",
      "repos_url": "https://api.github.com/users/HyukjinKwon/repos",
      "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}",
      "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events",
      "type": "User",
      "site_admin": false
    },
    "repo": {
      "id": 42300713,
      "node_id": "MDEwOlJlcG9zaXRvcnk0MjMwMDcxMw==",
      "name": "spark",
      "full_name": "HyukjinKwon/spark",
      "private": false,
      "owner": {
        "login": "HyukjinKwon",
        "id": 6477701,
        "node_id": "MDQ6VXNlcjY0Nzc3MDE=",
        "avatar_url": "https://avatars.githubusercontent.com/u/6477701?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/HyukjinKwon",
        "html_url": "https://github.com/HyukjinKwon",
        "followers_url": "https://api.github.com/users/HyukjinKwon/followers",
        "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}",
        "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions",
        "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs",
        "repos_url": "https://api.github.com/users/HyukjinKwon/repos",
        "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}",
        "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events",
        "type": "User",
        "site_admin": false
      },
      "html_url": "https://github.com/HyukjinKwon/spark",
      "description": "Mirror of Apache Spark",
      "fork": true,
      "url": "https://api.github.com/repos/HyukjinKwon/spark",
      "forks_url": "https://api.github.com/repos/HyukjinKwon/spark/forks",
      "keys_url": "https://api.github.com/repos/HyukjinKwon/spark/keys{/key_id}",
      "collaborators_url": "https://api.github.com/repos/HyukjinKwon/spark/collaborators{/collaborator}",
      "teams_url": "https://api.github.com/repos/HyukjinKwon/spark/teams",
      "hooks_url": "https://api.github.com/repos/HyukjinKwon/spark/hooks",
      "issue_events_url": "https://api.github.com/repos/HyukjinKwon/spark/issues/events{/number}",
      "events_url": "https://api.github.com/repos/HyukjinKwon/spark/events",
      "assignees_url": "https://api.github.com/repos/HyukjinKwon/spark/assignees{/user}",
      "branches_url": "https://api.github.com/repos/HyukjinKwon/spark/branches{/branch}",
      "tags_url": "https://api.github.com/repos/HyukjinKwon/spark/tags",
      "blobs_url": "https://api.github.com/repos/HyukjinKwon/spark/git/blobs{/sha}",
      "git_tags_url": "https://api.github.com/repos/HyukjinKwon/spark/git/tags{/sha}",
      "git_refs_url": "https://api.github.com/repos/HyukjinKwon/spark/git/refs{/sha}",
      "trees_url": "https://api.github.com/repos/HyukjinKwon/spark/git/trees{/sha}",
      "statuses_url": "https://api.github.com/repos/HyukjinKwon/spark/statuses/{sha}",
      "languages_url": "https://api.github.com/repos/HyukjinKwon/spark/languages",
      "stargazers_url": "https://api.github.com/repos/HyukjinKwon/spark/stargazers",
      "contributors_url": "https://api.github.com/repos/HyukjinKwon/spark/contributors",
      "subscribers_url": "https://api.github.com/repos/HyukjinKwon/spark/subscribers",
      "subscription_url": "https://api.github.com/repos/HyukjinKwon/spark/subscription",
      "commits_url": "https://api.github.com/repos/HyukjinKwon/spark/commits{/sha}",
      "git_commits_url": "https://api.github.com/repos/HyukjinKwon/spark/git/commits{/sha}",
      "comments_url": "https://api.github.com/repos/HyukjinKwon/spark/comments{/number}",
      "issue_comment_url": "https://api.github.com/repos/HyukjinKwon/spark/issues/comments{/number}",
      "contents_url": "https://api.github.com/repos/HyukjinKwon/spark/contents/{+path}",
      "compare_url": "https://api.github.com/repos/HyukjinKwon/spark/compare/{base}...{head}",
      "merges_url": "https://api.github.com/repos/HyukjinKwon/spark/merges",
      "archive_url": "https://api.github.com/repos/HyukjinKwon/spark/{archive_format}{/ref}",
      "downloads_url": "https://api.github.com/repos/HyukjinKwon/spark/downloads",
      "issues_url": "https://api.github.com/repos/HyukjinKwon/spark/issues{/number}",
      "pulls_url": "https://api.github.com/repos/HyukjinKwon/spark/pulls{/number}",
      "milestones_url": "https://api.github.com/repos/HyukjinKwon/spark/milestones{/number}",
      "notifications_url": "https://api.github.com/repos/HyukjinKwon/spark/notifications{?since,all,participating}",
      "labels_url": "https://api.github.com/repos/HyukjinKwon/spark/labels{/name}",
      "releases_url": "https://api.github.com/repos/HyukjinKwon/spark/releases{/id}",
      "deployments_url": "https://api.github.com/repos/HyukjinKwon/spark/deployments",
      "created_at": "2015-09-11T09:47:43Z",
      "updated_at": "2022-01-11T00:22:42Z",
      "pushed_at": "2022-12-26T23:04:44Z",
      "git_url": "git://github.com/HyukjinKwon/spark.git",
      "ssh_url": "git@github.com:HyukjinKwon/spark.git",
      "clone_url": "https://github.com/HyukjinKwon/spark.git",
      "svn_url": "https://github.com/HyukjinKwon/spark",
      "homepage": null,
      "size": 414264,
      "stargazers_count": 1,
      "watchers_count": 1,
      "language": "Scala",
      "has_issues": false,
      "has_projects": true,
      "has_downloads": true,
      "has_wiki": false,
      "has_pages": false,
      "has_discussions": false,
      "forks_count": 2,
      "mirror_url": null,
      "archived": false,
      "disabled": false,
      "open_issues_count": 0,
      "license": {
        "key": "apache-2.0",
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "url": "https://api.github.com/licenses/apache-2.0",
        "node_id": "MDc6TGljZW5zZTI="
      },
      "allow_forking": true,
      "is_template": false,
      "web_commit_signoff_required": false,
      "topics": [],
      "visibility": "public",
      "forks": 2,
      "open_issues": 0,
      "watchers": 1,
      "default_branch": "master"
    }
  },
  "base": {
    "label": "apache:master",
    "ref": "master",
    "sha": "8bbb666622e042c1533da294ac7b504b6aaa694a",
    "user": {
      "login": "apache",
      "id": 47359,
      "node_id": "MDEyOk9yZ2FuaXphdGlvbjQ3MzU5",
      "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/apache",
      "html_url": "https://github.com/apache",
      "followers_url": "https://api.github.com/users/apache/followers",
      "following_url": "https://api.github.com/users/apache/following{/other_user}",
      "gists_url": "https://api.github.com/users/apache/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/apache/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/apache/subscriptions",
      "organizations_url": "https://api.github.com/users/apache/orgs",
      "repos_url": "https://api.github.com/users/apache/repos",
      "events_url": "https://api.github.com/users/apache/events{/privacy}",
      "received_events_url": "https://api.github.com/users/apache/received_events",
      "type": "Organization",
      "site_admin": false
    },
    "repo": {
      "id": 17165658,
      "node_id": "MDEwOlJlcG9zaXRvcnkxNzE2NTY1OA==",
      "name": "spark",
      "full_name": "apache/spark",
      "private": false,
      "owner": {
        "login": "apache",
        "id": 47359,
        "node_id": "MDEyOk9yZ2FuaXphdGlvbjQ3MzU5",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/apache",
        "html_url": "https://github.com/apache",
        "followers_url": "https://api.github.com/users/apache/followers",
        "following_url": "https://api.github.com/users/apache/following{/other_user}",
        "gists_url": "https://api.github.com/users/apache/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/apache/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/apache/subscriptions",
        "organizations_url": "https://api.github.com/users/apache/orgs",
        "repos_url": "https://api.github.com/users/apache/repos",
        "events_url": "https://api.github.com/users/apache/events{/privacy}",
        "received_events_url": "https://api.github.com/users/apache/received_events",
        "type": "Organization",
        "site_admin": false
      },
      "html_url": "https://github.com/apache/spark",
      "description": "Apache Spark - A unified analytics engine for large-scale data processing",
      "fork": false,
      "url": "https://api.github.com/repos/apache/spark",
      "forks_url": "https://api.github.com/repos/apache/spark/forks",
      "keys_url": "https://api.github.com/repos/apache/spark/keys{/key_id}",
      "collaborators_url": "https://api.github.com/repos/apache/spark/collaborators{/collaborator}",
      "teams_url": "https://api.github.com/repos/apache/spark/teams",
      "hooks_url": "https://api.github.com/repos/apache/spark/hooks",
      "issue_events_url": "https://api.github.com/repos/apache/spark/issues/events{/number}",
      "events_url": "https://api.github.com/repos/apache/spark/events",
      "assignees_url": "https://api.github.com/repos/apache/spark/assignees{/user}",
      "branches_url": "https://api.github.com/repos/apache/spark/branches{/branch}",
      "tags_url": "https://api.github.com/repos/apache/spark/tags",
      "blobs_url": "https://api.github.com/repos/apache/spark/git/blobs{/sha}",
      "git_tags_url": "https://api.github.com/repos/apache/spark/git/tags{/sha}",
      "git_refs_url": "https://api.github.com/repos/apache/spark/git/refs{/sha}",
      "trees_url": "https://api.github.com/repos/apache/spark/git/trees{/sha}",
      "statuses_url": "https://api.github.com/repos/apache/spark/statuses/{sha}",
      "languages_url": "https://api.github.com/repos/apache/spark/languages",
      "stargazers_url": "https://api.github.com/repos/apache/spark/stargazers",
      "contributors_url": "https://api.github.com/repos/apache/spark/contributors",
      "subscribers_url": "https://api.github.com/repos/apache/spark/subscribers",
      "subscription_url": "https://api.github.com/repos/apache/spark/subscription",
      "commits_url": "https://api.github.com/repos/apache/spark/commits{/sha}",
      "git_commits_url": "https://api.github.com/repos/apache/spark/git/commits{/sha}",
      "comments_url": "https://api.github.com/repos/apache/spark/comments{/number}",
      "issue_comment_url": "https://api.github.com/repos/apache/spark/issues/comments{/number}",
      "contents_url": "https://api.github.com/repos/apache/spark/contents/{+path}",
      "compare_url": "https://api.github.com/repos/apache/spark/compare/{base}...{head}",
      "merges_url": "https://api.github.com/repos/apache/spark/merges",
      "archive_url": "https://api.github.com/repos/apache/spark/{archive_format}{/ref}",
      "downloads_url": "https://api.github.com/repos/apache/spark/downloads",
      "issues_url": "https://api.github.com/repos/apache/spark/issues{/number}",
      "pulls_url": "https://api.github.com/repos/apache/spark/pulls{/number}",
      "milestones_url": "https://api.github.com/repos/apache/spark/milestones{/number}",
      "notifications_url": "https://api.github.com/repos/apache/spark/notifications{?since,all,participating}",
      "labels_url": "https://api.github.com/repos/apache/spark/labels{/name}",
      "releases_url": "https://api.github.com/repos/apache/spark/releases{/id}",
      "deployments_url": "https://api.github.com/repos/apache/spark/deployments",
      "created_at": "2014-02-25T08:00:08Z",
      "updated_at": "2022-12-26T21:18:49Z",
      "pushed_at": "2022-12-26T23:10:31Z",
      "git_url": "git://github.com/apache/spark.git",
      "ssh_url": "git@github.com:apache/spark.git",
      "clone_url": "https://github.com/apache/spark.git",
      "svn_url": "https://github.com/apache/spark",
      "homepage": "https://spark.apache.org/",
      "size": 451536,
      "stargazers_count": 34617,
      "watchers_count": 34617,
      "language": "Scala",
      "has_issues": false,
      "has_projects": true,
      "has_downloads": true,
      "has_wiki": false,
      "has_pages": false,
      "has_discussions": false,
      "forks_count": 26430,
      "mirror_url": null,
      "archived": false,
      "disabled": false,
      "open_issues_count": 208,
      "license": {
        "key": "apache-2.0",
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "url": "https://api.github.com/licenses/apache-2.0",
        "node_id": "MDc6TGljZW5zZTI="
      },
      "allow_forking": true,
      "is_template": false,
      "web_commit_signoff_required": false,
      "topics": [
        "big-data",
        "java",
        "jdbc",
        "python",
        "r",
        "scala",
        "spark",
        "sql"
      ],
      "visibility": "public",
      "forks": 26430,
      "open_issues": 208,
      "watchers": 34617,
      "default_branch": "master"
    }
  },
  "_links": {
    "self": {
      "href": "https://api.github.com/repos/apache/spark/pulls/28661"
    },
    "html": {
      "href": "https://github.com/apache/spark/pull/28661"
    },
    "issue": {
      "href": "https://api.github.com/repos/apache/spark/issues/28661"
    },
    "comments": {
      "href": "https://api.github.com/repos/apache/spark/issues/28661/comments"
    },
    "review_comments": {
      "href": "https://api.github.com/repos/apache/spark/pulls/28661/comments"
    },
    "review_comment": {
      "href": "https://api.github.com/repos/apache/spark/pulls/comments{/number}"
    },
    "commits": {
      "href": "https://api.github.com/repos/apache/spark/pulls/28661/commits"
    },
    "statuses": {
      "href": "https://api.github.com/repos/apache/spark/statuses/28c2a516eec5ffbd527b9e62869b279a162458e4"
    }
  },
  "author_association": "MEMBER",
  "auto_merge": null,
  "active_lock_reason": null,
  "merged": false,
  "mergeable": null,
  "rebaseable": null,
  "mergeable_state": "unknown",
  "merged_by": null,
  "comments": 22,
  "review_comments": 15,
  "maintainer_can_modify": false,
  "commits": 3,
  "additions": 61,
  "deletions": 17,
  "changed_files": 3
}