diff --git a/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroEncoder.scala b/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroEncoder.scala
new file mode 100644
index 0000000000000..f8291c2951970
--- /dev/null
+++ b/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroEncoder.scala
@@ -0,0 +1,549 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.spark.sql.avro
+
+import java.io._
+import java.util.{Map => JMap}
+
+import scala.collection.JavaConverters._
+import scala.language.existentials
+import scala.reflect.ClassTag
+
+import org.apache.avro.Schema
+import org.apache.avro.Schema.Parser
+import org.apache.avro.Schema.Type._
+import org.apache.avro.generic.{GenericData, IndexedRecord}
+import org.apache.avro.reflect.ReflectData
+import org.apache.avro.specific.SpecificRecord
+
+import org.apache.spark.sql.Encoder
+import org.apache.spark.sql.avro.SchemaConverters._
+import org.apache.spark.sql.catalyst.InternalRow
+import org.apache.spark.sql.catalyst.analysis.{GetColumnByOrdinal, UnresolvedExtractValue}
+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
+import org.apache.spark.sql.catalyst.expressions._
+import org.apache.spark.sql.catalyst.expressions.codegen._
+import org.apache.spark.sql.catalyst.expressions.codegen.Block._
+import org.apache.spark.sql.catalyst.expressions.objects.{LambdaVariable => _, _}
+import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, GenericArrayData}
+import org.apache.spark.sql.types._
+import org.apache.spark.unsafe.types.UTF8String
+
+/**
+ * A Spark-SQL Encoder for Avro objects
+ */
+object AvroEncoder {
+  /**
+   * Provides an Encoder for Avro objects of the given class
+   *
+   * @param avroClass the class of the Avro object for which to generate the Encoder
+   * @tparam T the type of the Avro class, must implement SpecificRecord
+   * @return an Encoder for the given Avro class
+   */
+  def of[T <: SpecificRecord](avroClass: Class[T]): Encoder[T] = {
+    AvroExpressionEncoder.of(avroClass)
+  }
+
+  /**
+   * Provides an Encoder for Avro objects implementing the given schema
+   *
+   * @param avroSchema the Schema of the Avro object for which to generate the Encoder
+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord
+   * @return an Encoder for the given Avro Schema
+   */
+  def of[T <: IndexedRecord](avroSchema: Schema): Encoder[T] = {
+    AvroExpressionEncoder.of(avroSchema)
+  }
+
+  /**
+   * Provides an Encoder for Avro objects implementing the given schema
+   *
+   * @param jsonFormatSchema the json string represented Schema of the Avro object
+   *                         for which to generate the Encoder
+   * @tparam T the type of the Avro class that implements the Schema, must implement IndexedRecord
+   * @return an Encoder for the given Avro Schema
+   */
+  def of[T <: IndexedRecord](jsonFormatSchema: String): Encoder[T] = {
+    val avroSchema = new Schema.Parser().parse(jsonFormatSchema)
+    AvroExpressionEncoder.of(avroSchema)
+  }
+}
+
+class SerializableSchema(@transient var value: Schema) extends Externalizable {
+  def this() = this(null)
+  override def readExternal(in: ObjectInput): Unit = {
+    value = new Parser().parse(in.readObject().asInstanceOf[String])
+  }
+  override def writeExternal(out: ObjectOutput): Unit = out.writeObject(value.toString)
+  def resolveUnion(datum: Any): Int = GenericData.get.resolveUnion(value, datum)
+}
+
+object AvroExpressionEncoder {
+
+  def of[T <: SpecificRecord](avroClass: Class[T]): ExpressionEncoder[T] = {
+    val schema = avroClass.getMethod("getClassSchema").invoke(null).asInstanceOf[Schema]
+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])
+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)
+    val deserializer = AvroTypeInference.deserializerFor(schema)
+    new ExpressionEncoder[T](
+      serializer,
+      deserializer,
+      ClassTag[T](avroClass))
+  }
+
+  def of[T <: IndexedRecord](schema: Schema): ExpressionEncoder[T] = {
+    assert(toSqlType(schema).dataType.isInstanceOf[StructType])
+    val avroClass = Option(ReflectData.get.getClass(schema))
+      .map(_.asSubclass(classOf[SpecificRecord]))
+      .getOrElse(classOf[GenericData.Record])
+    val serializer = AvroTypeInference.serializerFor(avroClass, schema)
+    val deserializer = AvroTypeInference.deserializerFor(schema)
+    new ExpressionEncoder[T](
+      serializer,
+      deserializer,
+      ClassTag[T](avroClass))
+  }
+}
+
+/**
+ * Utilities for providing Avro object serializers and deserializers
+ */
+private object AvroTypeInference {
+
+  /**
+   * Translates an Avro Schema type to a proper SQL DataType. The Java Objects that back data in
+   * generated Generic and Specific records sometimes do not align with those suggested by Avro
+   * ReflectData, so we infer the proper SQL DataType to serialize and deserialize based on
+   * nullability and the wrapping Schema type.
+   */
+  private def inferExternalType(avroSchema: Schema): DataType = {
+    toSqlType(avroSchema) match {
+      // the non-nullable primitive types
+      case SchemaType(BooleanType, false) => BooleanType
+      case SchemaType(IntegerType, false) => IntegerType
+      case SchemaType(LongType, false) =>
+        if (avroSchema.getType == UNION) {
+          ObjectType(classOf[java.lang.Number])
+        } else {
+          LongType
+        }
+      case SchemaType(FloatType, false) => FloatType
+      case SchemaType(DoubleType, false) =>
+        if (avroSchema.getType == UNION) {
+          ObjectType(classOf[java.lang.Number])
+        } else {
+          DoubleType
+        }
+      // the nullable primitive types
+      case SchemaType(BooleanType, true) => ObjectType(classOf[java.lang.Boolean])
+      case SchemaType(IntegerType, true) => ObjectType(classOf[java.lang.Integer])
+      case SchemaType(LongType, true) => ObjectType(classOf[java.lang.Long])
+      case SchemaType(FloatType, true) => ObjectType(classOf[java.lang.Float])
+      case SchemaType(DoubleType, true) => ObjectType(classOf[java.lang.Double])
+      // the binary types
+      case SchemaType(BinaryType, _) =>
+        if (avroSchema.getType == FIXED) {
+          Option(ReflectData.get.getClass(avroSchema))
+            .map(ObjectType(_))
+            .getOrElse(ObjectType(classOf[GenericData.Fixed]))
+        } else {
+          ObjectType(classOf[java.nio.ByteBuffer])
+        }
+      // the referenced types
+      case SchemaType(ArrayType(_, _), _) =>
+        ObjectType(classOf[java.util.List[Object]])
+      case SchemaType(StringType, _) =>
+        avroSchema.getType match {
+          case ENUM =>
+            Option(ReflectData.get.getClass(avroSchema))
+              .map(ObjectType(_))
+              .getOrElse(ObjectType(classOf[GenericData.EnumSymbol]))
+          case _ =>
+            ObjectType(classOf[CharSequence])
+        }
+      case SchemaType(StructType(_), _) =>
+        Option(ReflectData.get.getClass(avroSchema))
+          .map(ObjectType(_))
+          .getOrElse(ObjectType(classOf[GenericData.Record]))
+      case SchemaType(MapType(_, _, _), _) =>
+        ObjectType(classOf[java.util.Map[Object, Object]])
+      case other =>
+        throw new IncompatibleSchemaException(s"Unsupported type $other")
+    }
+  }
+
+  /**
+   * Returns an expression that can be used to deserialize an InternalRow to an Avro object of
+   * type `T` that implements IndexedRecord and is compatible with the given Schema. The Spark SQL
+   * representation is located at ordinal 0 of a row, i.e. `GetColumnByOrdinal(0, _)`. Nested
+   * will have their fields accessed using `UnresolvedExtractValue`.
+   */
+  def deserializerFor[T <: IndexedRecord] (avroSchema: Schema): Expression = {
+    deserializerFor(avroSchema, GetColumnByOrdinal(0, inferExternalType(avroSchema)))
+  }
+
+  private def deserializerFor(avroSchema: Schema, path: Expression): Expression = {
+    /** Returns the current path with a sub-field extracted. */
+    def addToPath(part: String): Expression = UnresolvedExtractValue(path, Literal(part))
+
+    avroSchema.getType match {
+      case BOOLEAN =>
+        NewInstance(
+          classOf[java.lang.Boolean],
+          path :: Nil,
+          ObjectType(classOf[java.lang.Boolean]))
+      case INT =>
+        NewInstance(
+          classOf[java.lang.Integer],
+          path :: Nil,
+          ObjectType(classOf[java.lang.Integer]))
+      case LONG =>
+        NewInstance(
+          classOf[java.lang.Long],
+          path :: Nil,
+          ObjectType(classOf[java.lang.Long]))
+      case FLOAT =>
+        NewInstance(
+          classOf[java.lang.Float],
+          path :: Nil,
+          ObjectType(classOf[java.lang.Float]))
+      case DOUBLE =>
+        NewInstance(
+          classOf[java.lang.Double],
+          path :: Nil,
+          ObjectType(classOf[java.lang.Double]))
+
+      case BYTES =>
+        StaticInvoke(
+          classOf[java.nio.ByteBuffer],
+          ObjectType(classOf[java.nio.ByteBuffer]),
+          "wrap",
+          path :: Nil)
+      case FIXED =>
+        val fixedClass = Option(ReflectData.get.getClass(avroSchema))
+          .getOrElse(classOf[GenericData.Fixed])
+        if (fixedClass == classOf[GenericData.Fixed]) {
+          NewInstance(
+            fixedClass,
+            Invoke(
+              Literal.fromObject(
+                new SerializableSchema(avroSchema),
+                ObjectType(classOf[SerializableSchema])),
+              "value",
+              ObjectType(classOf[Schema]),
+              Nil) ::
+              path ::
+              Nil,
+            ObjectType(fixedClass))
+        } else {
+          NewInstance(
+            fixedClass,
+            path :: Nil,
+            ObjectType(fixedClass))
+        }
+
+      case STRING =>
+        Invoke(path, "toString", ObjectType(classOf[String]))
+
+      case ENUM =>
+        val enumClass = Option(ReflectData.get.getClass(avroSchema))
+          .getOrElse(classOf[GenericData.EnumSymbol])
+        if (enumClass == classOf[GenericData.EnumSymbol]) {
+          NewInstance(
+            enumClass,
+            Invoke(
+              Literal.fromObject(
+                new SerializableSchema(avroSchema),
+                ObjectType(classOf[SerializableSchema])),
+              "value",
+              ObjectType(classOf[Schema]),
+              Nil) ::
+              Invoke(path, "toString", ObjectType(classOf[String])) ::
+              Nil,
+            ObjectType(enumClass))
+        } else {
+          StaticInvoke(
+            enumClass,
+            ObjectType(enumClass),
+            "valueOf",
+            Invoke(path, "toString", ObjectType(classOf[String])) :: Nil)
+        }
+
+      case ARRAY =>
+        val elementSchema = avroSchema.getElementType
+        val elementType = toSqlType(elementSchema).dataType
+        val array = Invoke(
+          MapObjects(element =>
+            deserializerFor(elementSchema, element),
+            path,
+            elementType),
+          "array",
+          ObjectType(classOf[Array[Any]]))
+        StaticInvoke(
+          classOf[java.util.Arrays],
+          ObjectType(classOf[java.util.List[Object]]),
+          "asList",
+          array :: Nil)
+
+      case MAP =>
+        val valueSchema = avroSchema.getValueType
+        val valueType = inferExternalType(valueSchema) match {
+          case t if t == ObjectType(classOf[java.lang.CharSequence]) =>
+            StringType
+          case other => other
+        }
+        val keyData = Invoke(
+          MapObjects(
+            p => deserializerFor(Schema.create(STRING), p),
+            Invoke(path, "keyArray", ArrayType(StringType)),
+            StringType),
+          "array",
+          ObjectType(classOf[Array[Any]]))
+        val valueData = Invoke(
+          MapObjects(
+            p => deserializerFor(valueSchema, p),
+            Invoke(path, "valueArray", ArrayType(valueType)),
+            valueType),
+          "array",
+          ObjectType(classOf[Array[Any]]))
+        StaticInvoke(
+          ArrayBasedMapData.getClass,
+          ObjectType(classOf[JMap[_, _]]),
+          "toJavaMap",
+          keyData :: valueData :: Nil)
+
+      case UNION =>
+        val (resolvedSchema, _) =
+          org.apache.spark.sql.avro.SchemaConverters.resolveUnionType(avroSchema, Set.empty)
+        if (resolvedSchema.getType == RECORD &&
+          avroSchema.getTypes.asScala.filterNot(_.getType == NULL).length > 1) {
+          // A Union resolved to a record that originally had more than 1 type when filtered
+          // of its nulls must be complex
+          val bottom = Literal.create(null, ObjectType(classOf[Object])).asInstanceOf[Expression]
+          resolvedSchema.getFields.asScala.foldLeft(bottom) {
+            (tree: Expression, field: Schema.Field) =>
+              val fieldValue = ObjectCast(
+                deserializerFor(field.schema, addToPath(field.name)),
+                ObjectType(classOf[Object]))
+              If(IsNull(fieldValue), tree, fieldValue)
+          }
+        } else {
+          deserializerFor(resolvedSchema, path)
+        }
+
+      case RECORD =>
+        val args = avroSchema.getFields.asScala.map { field =>
+          val position = Literal(field.pos)
+          val argument = deserializerFor(field.schema, addToPath(field.name))
+          (position, argument)
+        }.toList
+        val recordClass = Option(ReflectData.get.getClass(avroSchema))
+          .getOrElse(classOf[GenericData.Record])
+        val newInstance = if (recordClass == classOf[GenericData.Record]) {
+          NewInstance(
+            recordClass,
+            Invoke(
+              Literal.fromObject(
+                new SerializableSchema(avroSchema),
+                ObjectType(classOf[SerializableSchema])),
+              "value",
+              ObjectType(classOf[Schema]),
+              Nil) :: Nil,
+            ObjectType(recordClass))
+        } else {
+          NewInstance(
+            recordClass,
+            Nil,
+            ObjectType(recordClass))
+        }
+        val result = InitializeAvroObject(newInstance, args)
+
+        If(IsNull(path), Literal.create(null, ObjectType(recordClass)), result)
+
+      case NULL =>
+        // Encountering NULL at this level implies it was the type of a Field, which should never
+        // be the case.
+        throw new IncompatibleSchemaException("Null type should only be used in Union types")
+    }
+  }
+
+  /**
+   * Returns an expression that can be used to serialize an Avro object with a class of type `T`
+   * that is compatible with the given Schema to an InternalRow
+   */
+  def serializerFor[T <: IndexedRecord](
+      avroClass: Class[T], avroSchema: Schema): Expression = {
+    val inputObject = BoundReference(0, ObjectType(avroClass), nullable = true)
+    val nullSafeInput = AssertNotNull(inputObject, Seq("top level"))
+    serializerFor(nullSafeInput, avroSchema)
+  }
+
+  def serializerFor(
+      inputObject: Expression,
+      avroSchema: Schema): Expression = {
+
+    def toCatalystArray(inputObject: Expression, schema: Schema): Expression = {
+      val elementType = inferExternalType(schema)
+      if (elementType.isInstanceOf[ObjectType]) {
+        MapObjects(element =>
+          serializerFor(element, schema),
+          Invoke(
+            inputObject,
+            "toArray",
+            ObjectType(classOf[Array[Object]])),
+          elementType)
+      } else {
+        NewInstance(
+          classOf[GenericArrayData],
+          inputObject :: Nil,
+          dataType = ArrayType(elementType, containsNull = false))
+      }
+    }
+
+    def toCatalystMap(inputObject: Expression, schema: Schema): Expression = {
+      val valueSchema = schema.getValueType
+      val valueType = inferExternalType(valueSchema)
+      ExternalMapToCatalyst(
+        inputObject,
+        ObjectType(classOf[java.lang.CharSequence]),
+        serializerFor(_, Schema.create(STRING)),
+        keyNullable = true,
+        valueType,
+        serializerFor(_, valueSchema),
+        valueNullable = true)
+    }
+
+    if (!inputObject.dataType.isInstanceOf[ObjectType]) {
+      inputObject
+    } else {
+      avroSchema.getType match {
+        case BOOLEAN =>
+          Invoke(inputObject, "booleanValue", BooleanType)
+        case INT =>
+          Invoke(inputObject, "intValue", IntegerType)
+        case LONG =>
+          Invoke(inputObject, "longValue", LongType)
+        case FLOAT =>
+          Invoke(inputObject, "floatValue", FloatType)
+        case DOUBLE =>
+          Invoke(inputObject, "doubleValue", DoubleType)
+
+        case BYTES =>
+          Invoke(inputObject, "array", BinaryType)
+        case FIXED =>
+          Invoke(inputObject, "bytes", BinaryType)
+
+        case STRING =>
+          StaticInvoke(
+            classOf[UTF8String],
+            StringType,
+            "fromString",
+            Invoke(inputObject, "toString", ObjectType(classOf[java.lang.String])) :: Nil)
+
+        case ENUM =>
+          StaticInvoke(
+            classOf[UTF8String],
+            StringType,
+            "fromString",
+            Invoke(inputObject, "toString", ObjectType(classOf[java.lang.String])) :: Nil)
+
+        case ARRAY =>
+          val elementSchema = avroSchema.getElementType
+          toCatalystArray(inputObject, elementSchema)
+
+        case MAP =>
+          toCatalystMap(inputObject, avroSchema)
+
+        case UNION =>
+          val unionWithoutNulls = Schema.createUnion(
+            avroSchema.getTypes.asScala.filterNot(_.getType == NULL).toList.asJava)
+          val (resolvedSchema, nullable) = resolveUnionType(avroSchema, Set.empty)
+          if (resolvedSchema.getType == RECORD && unionWithoutNulls.getTypes.asScala.length > 1) {
+            // A Union resolved to a record that originally had more than 1 type when filtered
+            // of its nulls must be complex
+            val complexStruct = CreateNamedStruct(
+              resolvedSchema.getFields.asScala.zipWithIndex.flatMap { case (field, index) =>
+                val unionIndex = Invoke(
+                  Literal.fromObject(
+                    new SerializableSchema(unionWithoutNulls),
+                    ObjectType(classOf[SerializableSchema])),
+                  "resolveUnion",
+                  IntegerType,
+                  inputObject :: Nil)
+                val fieldValue = If(EqualTo(Literal(index), unionIndex),
+                  serializerFor(
+                    ObjectCast(
+                      inputObject,
+                      inferExternalType(field.schema())),
+                    field.schema),
+                  Literal.create(null, toSqlType(field.schema()).dataType))
+                Literal(field.name) :: serializerFor(fieldValue, field.schema) :: Nil})
+            complexStruct
+          } else {
+            if (nullable) {
+              serializerFor(inputObject, resolvedSchema)
+            } else {
+              serializerFor(
+                AssertNotNull(inputObject, Seq(avroSchema.getTypes.toString)),
+                resolvedSchema)
+            }
+          }
+
+        case RECORD =>
+          val createStruct = CreateNamedStruct(
+            avroSchema.getFields.asScala.flatMap { field =>
+              val fieldValue = Invoke(
+                inputObject,
+                "get",
+                inferExternalType(field.schema),
+                Literal(field.pos) :: Nil)
+              Literal(field.name) :: serializerFor(fieldValue, field.schema) :: Nil})
+          If(IsNull(inputObject), Literal.create(null, createStruct.dataType), createStruct)
+
+        case NULL =>
+          // Encountering NULL at this level implies it was the type of a Field, which should never
+          // be the case
+          throw new IncompatibleSchemaException("Null type should only be used in Union types")
+      }
+    }
+  }
+
+  /**
+   * Casts an expression to another object.
+   *
+   * @param value The value to cast
+   * @param resultType The type the value should be cast to.
+   */
+  private case class ObjectCast(
+      value : Expression,
+      resultType: DataType) extends Expression with NonSQLExpression {
+    override def nullable: Boolean = value.nullable
+    override def dataType: DataType = resultType
+    override def children: Seq[Expression] = value :: Nil
+    override def eval(input: InternalRow): Any =
+      throw new UnsupportedOperationException("Only code-generated evaluation is supported.")
+    override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
+      val javaType = CodeGenerator.javaType(resultType)
+      val obj = value.genCode(ctx)
+      val code = code"""
+         ${obj.code}
+         final $javaType ${ev.value} = ($javaType) ${obj.value};
+       """
+      ev.copy(code = code, isNull = obj.isNull)
+    }
+  }
+}
diff --git a/external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala b/external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala
index 3947d327dfac6..09749a1f51775 100644
--- a/external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala
+++ b/external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala
@@ -18,22 +18,19 @@
 package org.apache.spark.sql.avro
 
 import scala.collection.JavaConverters._
-import scala.util.Random
 
 import org.apache.avro.{LogicalTypes, Schema, SchemaBuilder}
 import org.apache.avro.LogicalTypes.{Date, Decimal, TimestampMicros, TimestampMillis}
 import org.apache.avro.Schema.Type._
 
-import org.apache.spark.sql.catalyst.util.RandomUUIDGenerator
 import org.apache.spark.sql.types._
-import org.apache.spark.sql.types.Decimal.{maxPrecisionForBytes, minBytesForPrecision}
+import org.apache.spark.sql.types.Decimal.minBytesForPrecision
 
 /**
  * This object contains method that are used to convert sparkSQL schemas to avro schemas and vice
  * versa.
  */
 object SchemaConverters {
-  private lazy val uuidGenerator = RandomUUIDGenerator(new Random().nextLong())
 
   private lazy val nullSchema = Schema.create(Schema.Type.NULL)
 
@@ -100,39 +97,57 @@ object SchemaConverters {
           nullable = false)
 
       case UNION =>
-        if (avroSchema.getTypes.asScala.exists(_.getType == NULL)) {
-          // In case of a union with null, eliminate it and make a recursive call
-          val remainingUnionTypes = avroSchema.getTypes.asScala.filterNot(_.getType == NULL)
-          if (remainingUnionTypes.size == 1) {
-            toSqlTypeHelper(remainingUnionTypes.head, existingRecordNames).copy(nullable = true)
-          } else {
-            toSqlTypeHelper(Schema.createUnion(remainingUnionTypes.asJava), existingRecordNames)
-              .copy(nullable = true)
-          }
-        } else avroSchema.getTypes.asScala.map(_.getType) match {
-          case Seq(t1) =>
-            toSqlTypeHelper(avroSchema.getTypes.get(0), existingRecordNames)
-          case Seq(t1, t2) if Set(t1, t2) == Set(INT, LONG) =>
-            SchemaType(LongType, nullable = false)
-          case Seq(t1, t2) if Set(t1, t2) == Set(FLOAT, DOUBLE) =>
-            SchemaType(DoubleType, nullable = false)
-          case _ =>
-            // Convert complex unions to struct types where field names are member0, member1, etc.
-            // This is consistent with the behavior when converting between Avro and Parquet.
-            val fields = avroSchema.getTypes.asScala.zipWithIndex.map {
-              case (s, i) =>
-                val schemaType = toSqlTypeHelper(s, existingRecordNames)
-                // All fields are nullable because only one of them is set at a time
-                StructField(s"member$i", schemaType.dataType, nullable = true)
-            }
-
-            SchemaType(StructType(fields), nullable = false)
+        resolveUnionType(avroSchema, existingRecordNames) match {
+          case (schema, nullable) =>
+            toSqlTypeHelper(schema, existingRecordNames).copy(nullable = nullable)
         }
 
       case other => throw new IncompatibleSchemaException(s"Unsupported type $other")
     }
   }
 
+  /**
+   * Resolves an avro UNION type to an SQL-compatible avro type. Converts complex unions to records
+   * if necessary.
+   */
+  def resolveUnionType(
+      avroSchema: Schema,
+      existingRecordNames: Set[String],
+      nullable: Boolean = false): (Schema, Boolean) = {
+    if (avroSchema.getTypes.asScala.exists(_.getType == NULL)) {
+      // In case of a union with null, eliminate it, and make a recursive call
+      val remainingUnionTypes = avroSchema.getTypes.asScala.filterNot(_.getType == NULL)
+      if (remainingUnionTypes.size == 1) {
+        (remainingUnionTypes.head, true)
+      } else {
+        resolveUnionType(
+          Schema.createUnion(remainingUnionTypes.asJava),
+          existingRecordNames,
+          nullable = true)
+      }
+    } else avroSchema.getTypes.asScala.map(_.getType) match {
+      case Seq(t1) =>
+        (avroSchema.getTypes.get(0), true)
+      case Seq(t1, t2) if Set(t1, t2) == Set(INT, LONG) =>
+        (Schema.create(LONG), false)
+      case Seq(t1, t2) if Set(t1, t2) == Set(FLOAT, DOUBLE) =>
+        (Schema.create(DOUBLE), false)
+      case _ =>
+        // Convert complex unions to records where field names are member0, member1, etc.
+        // This is consistent with the behavior when converting between Avro and Parquet.
+        val record = SchemaBuilder.record(avroSchema.getName).fields()
+        avroSchema.getTypes.asScala.zipWithIndex.foreach {
+          case (s, i) =>
+            // All fields are nullable because only one of them is set at a time
+            record.name(s"member$i").`type`(SchemaBuilder.unionOf()
+              .`type`(Schema.create(NULL)).and
+              .`type`(s).endUnion())
+              .withDefault(null)
+        }
+        (record.endRecord(), false)
+    }
+  }
+
   def toAvroType(
       catalystType: DataType,
       nullable: Boolean = false,
diff --git a/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroEncoderSuite.scala b/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroEncoderSuite.scala
new file mode 100644
index 0000000000000..c5d03561a6697
--- /dev/null
+++ b/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroEncoderSuite.scala
@@ -0,0 +1,350 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.avro
+
+import java.nio.ByteBuffer
+
+import org.apache.avro.{Schema, SchemaBuilder}
+import org.apache.avro.generic.{GenericData, GenericRecordBuilder}
+
+import org.apache.spark.{SparkConf, SparkContext}
+import org.apache.spark.rdd.RDD
+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
+import org.apache.spark.sql.test.SharedSparkSession
+import org.apache.spark.sql.types.{FloatType, IntegerType, StringType}
+
+class AvroEncoderSuite extends SharedSparkSession {
+  import testImplicits._
+
+  test("encoder from json schema") {
+    val jsonFormatSchema =
+      """
+        |{
+        |  "type" : "record",
+        |  "name" : "simple_record",
+        |  "fields" :
+        |   [
+        |    { "name" : "myUInt", "type" : [ "int", "null" ], "default" : 1 },
+        |    { "name" : "myULong", "type" : [ "long", "null" ], "default" : 2 },
+        |    { "name" : "myUBool", "type" : [ "boolean", "null" ], "default" : true },
+        |    { "name" : "myUDouble", "type" : [ "double", "null" ], "default" : 3 },
+        |    { "name" : "myUFloat", "type" : [ "float", "null" ], "default" : 4.5 },
+        |    { "name" : "myUString", "type" : [ "string", "null" ], "default" : "foo" },
+        |
+        |    { "name" : "myInt", "type" : "int", "default" : 10 },
+        |    { "name" : "myLong", "type" : "long", "default" : 11 },
+        |    { "name" : "myBool", "type" : "boolean", "default" : false },
+        |    { "name" : "myDouble", "type" : "double", "default" : 12 },
+        |    { "name" : "myFloat", "type" : "float", "default" : 13.14 },
+        |    { "name" : "myString", "type" : "string", "default" : "bar" },
+        |
+        |    { "name" : "myArray", "type" :
+        |     { "type" : "array", "items" : "bytes" }, "default" : [ "a12b", "cc50" ] },
+        |    { "name" : "myMap", "type" : { "type" : "map", "values" : "string" },
+        |     "default" : {"a":"A", "b":"B"} }
+        |   ]
+        |}
+      """.stripMargin
+    val encoder = AvroEncoder.of(jsonFormatSchema)
+    val expressionEncoder = encoder.asInstanceOf[ExpressionEncoder[GenericData.Record]]
+    val schema = new Schema.Parser().parse(jsonFormatSchema)
+    val record = new GenericRecordBuilder(schema).build
+    val row = expressionEncoder.toRow(record)
+    val recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+    assert(record.toString == recordFromRow.toString)
+  }
+
+  test("generic record converts to row and back") {
+    // complex schema including type of basic type, array with int/string/record/enum,
+    // nested record and map.
+    val jsonFormatSchema =
+      """
+        |{
+        |  "type" : "record",
+        |  "name" : "record",
+        |  "fields" : [ {
+        |    "name" : "boolean",
+        |    "type" : "boolean",
+        |    "default" : false
+        |  }, {
+        |    "name" : "int",
+        |    "type" : "int",
+        |    "default" : 0
+        |  }, {
+        |    "name" : "long",
+        |    "type" : "long",
+        |    "default" : 0
+        |  }, {
+        |    "name" : "float",
+        |    "type" : "float",
+        |    "default" : 0.0
+        |  }, {
+        |    "name" : "double",
+        |    "type" : "double",
+        |    "default" : 0.0
+        |  }, {
+        |    "name" : "string",
+        |    "type" : "string",
+        |    "default" : "string"
+        |  }, {
+        |    "name" : "bytes",
+        |    "type" : "bytes",
+        |    "default" : "bytes"
+        |  }, {
+        |    "name" : "nested",
+        |    "type" : {
+        |      "type" : "record",
+        |      "name" : "simple_record",
+        |      "fields" : [ {
+        |        "name" : "nested1",
+        |        "type" : "int",
+        |        "default" : 0
+        |      }, {
+        |        "name" : "nested2",
+        |        "type" : "string",
+        |        "default" : "string"
+        |      } ]
+        |    },
+        |    "default" : {
+        |      "nested1" : 0,
+        |      "nested2" : "string"
+        |    }
+        |  }, {
+        |    "name" : "enum",
+        |    "type" : {
+        |      "type" : "enum",
+        |      "name" : "simple_enums",
+        |      "symbols" : [ "SPADES", "HEARTS", "CLUBS", "DIAMONDS" ]
+        |    },
+        |    "default" : "SPADES"
+        |  }, {
+        |    "name" : "int_array",
+        |    "type" : {
+        |      "type" : "array",
+        |      "items" : "int"
+        |    },
+        |    "default" : [ 1, 2, 3 ]
+        |  }, {
+        |    "name" : "string_array",
+        |    "type" : {
+        |      "type" : "array",
+        |      "items" : "string"
+        |    },
+        |    "default" : [ "a", "b", "c" ]
+        |  }, {
+        |    "name" : "record_array",
+        |    "type" : {
+        |      "type" : "array",
+        |      "items" : "simple_record"
+        |    },
+        |    "default" : [ {
+        |      "nested1" : 0,
+        |      "nested2" : "string"
+        |    }, {
+        |      "nested1" : 0,
+        |      "nested2" : "string"
+        |    } ]
+        |  }, {
+        |    "name" : "enum_array",
+        |    "type" : {
+        |      "type" : "array",
+        |      "items" : "simple_enums"
+        |    },
+        |    "default" : [ "SPADES", "HEARTS", "SPADES" ]
+        |  }, {
+        |    "name" : "fixed_array",
+        |    "type" : {
+        |      "type" : "array",
+        |      "items" : {
+        |        "type" : "fixed",
+        |        "name" : "simple_fixed",
+        |        "size" : 3
+        |      }
+        |    },
+        |    "default" : [ "foo", "bar", "baz" ]
+        |  }, {
+        |    "name" : "fixed",
+        |    "type" : {
+        |      "type" : "fixed",
+        |      "name" : "simple_fixed_item",
+        |      "size" : 16
+        |    },
+        |    "default" : "string_length_16"
+        |  }, {
+        |    "name" : "map",
+        |    "type" : {
+        |      "type" : "map",
+        |      "values" : "string"
+        |    },
+        |    "default" : {
+        |      "a" : "A"
+        |    }
+        |  } ]
+        |}
+      """.stripMargin
+
+    val schema = new Schema.Parser().parse(jsonFormatSchema)
+    val encoder = AvroEncoder.of[GenericData.Record](schema)
+    val expressionEncoder = encoder.asInstanceOf[ExpressionEncoder[GenericData.Record]]
+    val record = new GenericRecordBuilder(schema).build
+    val row = expressionEncoder.toRow(record)
+    val recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+    assert(record.toString == recordFromRow.toString)
+  }
+
+  test("encoder resolves union types to rows") {
+    val jsonFormatSchema =
+      """
+        |{
+        |  "type" : "record",
+        |  "name" : "record",
+        |  "fields" : [ {
+        |    "name" : "int_null_union",
+        |    "type" : [ "null", "int" ],
+        |    "default" : null
+        |  }, {
+        |    "name" : "string_null_union",
+        |    "type" : [ "null", "string" ],
+        |    "default" : null
+        |  }, {
+        |    "name" : "int_long_union",
+        |    "type" : [ "int", "long" ],
+        |    "default" : 0
+        |  }, {
+        |    "name" : "float_double_union",
+        |    "type" : [ "float", "double" ],
+        |    "default" : 0.0
+        |  } ]
+        |}
+      """.stripMargin
+
+    val schema = new Schema.Parser().parse(jsonFormatSchema)
+    val encoder = AvroEncoder.of[GenericData.Record](schema)
+    val expressionEncoder = encoder.asInstanceOf[ExpressionEncoder[GenericData.Record]]
+    val record = new GenericRecordBuilder(schema).build
+    val row = expressionEncoder.toRow(record)
+    val recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+    assert(record.get(0) == recordFromRow.get(0))
+    assert(record.get(1) == recordFromRow.get(1))
+    assert(record.get(2) == recordFromRow.get(2))
+    assert(record.get(3) == recordFromRow.get(3))
+    record.put(0, 0)
+    record.put(1, "value")
+    val updatedRow = expressionEncoder.toRow(record)
+    val updatedRecordFromRow = expressionEncoder.resolveAndBind().fromRow(updatedRow)
+    assert(record.get(0) == updatedRecordFromRow.get(0))
+    assert(record.get(1) == updatedRecordFromRow.get(1))
+  }
+
+  test("encoder resolves complex unions to rows") {
+    val nested =
+      SchemaBuilder.record("simple_record").fields()
+        .name("nested1").`type`("int").withDefault(0)
+        .name("nested2").`type`("string").withDefault("foo").endRecord()
+    val schema = SchemaBuilder.record("record").fields()
+      .name("int_float_string_record").`type`(
+      SchemaBuilder.unionOf()
+        .`type`("null").and()
+        .`type`("int").and()
+        .`type`("float").and()
+        .`type`("string").and()
+        .`type`(nested).endUnion()
+    ).withDefault(null).endRecord()
+
+    val encoder = AvroEncoder.of[GenericData.Record](schema)
+    val expressionEncoder = encoder.asInstanceOf[ExpressionEncoder[GenericData.Record]]
+    val record = new GenericRecordBuilder(schema).build
+    var row = expressionEncoder.toRow(record)
+    var recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+
+    assert(row.getStruct(0, 4).get(0, IntegerType) == null)
+    assert(row.getStruct(0, 4).get(1, FloatType) == null)
+    assert(row.getStruct(0, 4).get(2, StringType) == null)
+    assert(row.getStruct(0, 4).getStruct(3, 2) == null)
+    assert(record == recordFromRow)
+
+    record.put(0, 1)
+    row = expressionEncoder.toRow(record)
+    recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+
+    assert(row.getStruct(0, 4).get(1, FloatType) == null)
+    assert(row.getStruct(0, 4).get(2, StringType) == null)
+    assert(row.getStruct(0, 4).getStruct(3, 2) == null)
+    assert(record == recordFromRow)
+
+    record.put(0, 1F)
+    row = expressionEncoder.toRow(record)
+    recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+
+    assert(row.getStruct(0, 4).get(0, IntegerType) == null)
+    assert(row.getStruct(0, 4).get(2, StringType) == null)
+    assert(row.getStruct(0, 4).getStruct(3, 2) == null)
+    assert(record == recordFromRow)
+
+    record.put(0, "bar")
+    row = expressionEncoder.toRow(record)
+    recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+
+    assert(row.getStruct(0, 4).get(0, IntegerType) == null)
+    assert(row.getStruct(0, 4).get(1, FloatType) == null)
+    assert(row.getStruct(0, 4).getStruct(3, 2) == null)
+    assert(record == recordFromRow)
+
+    record.put(0, new GenericRecordBuilder(nested).build())
+    row = expressionEncoder.toRow(record)
+    recordFromRow = expressionEncoder.resolveAndBind().fromRow(row)
+
+    assert(row.getStruct(0, 4).get(0, IntegerType) == null)
+    assert(row.getStruct(0, 4).get(1, FloatType) == null)
+    assert(row.getStruct(0, 4).get(2, StringType) == null)
+    assert(record == recordFromRow)
+  }
+
+  test("create Dataset from GenericRecord") {
+    // need a spark context with kryo as serializer
+    spark.stop()
+    val conf = new SparkConf()
+      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
+      .set("spark.driver.allowMultipleContexts", "true")
+      .set("spark.master", "local[2]")
+      .set("spark.app.name", "AvroEncoderSuite")
+    val context = new SparkContext(conf)
+
+    val schema: Schema =
+      SchemaBuilder
+        .record("GenericRecordTest")
+        .fields()
+        .requiredString("field1")
+        .name("enumVal").`type`().enumeration("letters").symbols("a", "b", "c").enumDefault("a")
+        .name("fixedVal").`type`().fixed("MD5").size(16).fixedDefault(ByteBuffer.allocate(16))
+        .endRecord()
+
+    implicit val enc = AvroEncoder.of[GenericData.Record](schema)
+
+    val genericRecords = (1 to 10) map { i =>
+      new GenericRecordBuilder(schema)
+        .set("field1", "field-" + i)
+        .build()
+    }
+
+    val rdd: RDD[GenericData.Record] = context.parallelize(genericRecords)
+    val ds = rdd.toDS()
+    assert(ds.count() == genericRecords.size)
+    context.stop()
+  }
+}
diff --git a/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala b/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala
index b349ac57892db..b4a77b59f4c1d 100644
--- a/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala
+++ b/external/avro/src/test/scala/org/apache/spark/sql/avro/AvroSuite.scala
@@ -1497,30 +1497,6 @@ abstract class AvroSuite extends QueryTest with SharedSparkSession {
       |}
     """.stripMargin)
   }
-
-  test("log a warning of ignoreExtension deprecation") {
-    val logAppender = new LogAppender
-    withTempPath { dir =>
-      Seq(("a", 1, 2), ("b", 1, 2), ("c", 2, 1), ("d", 2, 1))
-        .toDF("value", "p1", "p2")
-        .repartition(2)
-        .write
-        .format("avro")
-        .save(dir.getCanonicalPath)
-      withLogAppender(logAppender) {
-        spark
-          .read
-          .format("avro")
-          .option(AvroOptions.ignoreExtensionKey, false)
-          .load(dir.getCanonicalPath)
-          .count()
-      }
-      val deprecatedEvents = logAppender.loggingEvents
-        .filter(_.getRenderedMessage.contains(
-          s"Option ${AvroOptions.ignoreExtensionKey} is deprecated"))
-      assert(deprecatedEvents.size === 1)
-    }
-  }
 }
 
 class AvroV1Suite extends AvroSuite {
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala
index bd499671d6441..cfe8c4dd1933f 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala
@@ -25,7 +25,7 @@ import org.apache.spark.sql.catalyst.{InternalRow, JavaTypeInference, ScalaRefle
 import org.apache.spark.sql.catalyst.analysis.{Analyzer, GetColumnByOrdinal, SimpleAnalyzer, UnresolvedAttribute, UnresolvedExtractValue}
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection
-import org.apache.spark.sql.catalyst.expressions.objects.{AssertNotNull, InitializeJavaBean, Invoke, NewInstance}
+import org.apache.spark.sql.catalyst.expressions.objects._
 import org.apache.spark.sql.catalyst.optimizer.{ReassignLambdaVariableID, SimplifyCasts}
 import org.apache.spark.sql.catalyst.plans.logical.{CatalystSerde, DeserializeToObject, LeafNode, LocalRelation}
 import org.apache.spark.sql.internal.SQLConf
@@ -237,6 +237,7 @@ case class ExpressionEncoder[T](
           GetColumnByOrdinal(ordinal, dt)
         case If(IsNull(GetColumnByOrdinal(0, _)), _, n: NewInstance) => n
         case If(IsNull(GetColumnByOrdinal(0, _)), _, i: InitializeJavaBean) => i
+        case If(IsNull(GetColumnByOrdinal(0, _)), _, i: InitializeAvroObject) => i
       }
     } else {
       // For other input objects like primitive, array, map, etc., we deserialize the first column
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala
index 54abd09d89ddb..008177e27bdc0 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala
@@ -1581,6 +1581,58 @@ case class InitializeJavaBean(beanInstance: Expression, setters: Map[String, Exp
   }
 }
 
+/**
+ * Initializes an Avro Record instance (that implements the IndexedRecord interface) by calling
+ * the `put` method on a the Record instance with the provided position and value arguments
+ *
+ * @param objectInstance an expression that will evaluate to the Record instance
+ * @param args a sequence of expression pairs that will respectively evaluate to the index of
+ *             the record in which to insert, and the argument value to insert
+ */
+case class InitializeAvroObject(
+    objectInstance: Expression,
+    args: List[(Expression, Expression)]) extends Expression with NonSQLExpression {
+
+  override def nullable: Boolean = objectInstance.nullable
+  override def children: Seq[Expression] = objectInstance +: args.map { case (_, v) => v }
+  override def dataType: DataType = objectInstance.dataType
+
+  override def eval(input: InternalRow): Any =
+    throw new UnsupportedOperationException("Only code-generated evaluation is supported.")
+
+  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
+    val instanceGen = objectInstance.genCode(ctx)
+
+    val avroInstance = ctx.freshName("avroObject")
+    val avroInstanceJavaType =
+      CodeGenerator.javaType(objectInstance.dataType)
+    ctx.addMutableState(
+      avroInstanceJavaType, avroInstance, forceInline = true, useFreshName = false)
+
+    val initialize = args.map {
+      case (posExpr, argExpr) =>
+        val posGen = posExpr.genCode(ctx)
+        val argGen = argExpr.genCode(ctx)
+        s"""
+            ${posGen.code}
+            ${argGen.code}
+            $avroInstance.put(${posGen.value}, ${argGen.value});
+          """
+    }
+    val initExpressions =
+      ctx.splitExpressionsWithCurrentInputs(initialize)
+    val code =
+      code"""
+          ${instanceGen.code}
+          $avroInstance = ${instanceGen.value};
+          if (!${instanceGen.isNull}) {
+            $initExpressions
+          }
+        """
+    ev.copy(code = code, isNull = instanceGen.isNull, value = instanceGen.value)
+  }
+}
+
 /**
  * Asserts that input values of a non-nullable child expression are not null.
  *
