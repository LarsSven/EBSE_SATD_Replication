diff --git a/gobblin-api/src/main/java/gobblin/source/extractor/Extractor.java b/gobblin-api/src/main/java/gobblin/source/extractor/Extractor.java
index 40dd3e81b3..266d3d4333 100644
--- a/gobblin-api/src/main/java/gobblin/source/extractor/Extractor.java
+++ b/gobblin-api/src/main/java/gobblin/source/extractor/Extractor.java
@@ -31,38 +31,37 @@
 public interface Extractor<S, D> extends Closeable {
 
   /**
-   * Get the schema (Metadata) of the extracted data records.
+   * Get the schema (metadata) of the extracted data records.
    *
    * @return schema of the extracted data records
+   * @throws java.io.IOException if there is problem getting the schema
    */
-  public S getSchema();
+  public S getSchema() throws IOException;
 
   /**
-   * Read a data record from the data source.
+   * Read the next data record from the data source.
    *
    * <p>
-   *   This method allows data record object reuse through the one passed in if the
-   *   implementation class decides to do so.
+   *   Reuse of data records has been deprecated and is not executed internally.
    * </p>
    *
-   * @param reuse the data record object to be used
-   * @return a data record
+   * @param reuse the data record object to be reused
+   * @return the next data record extracted from the data source
    * @throws DataRecordException if there is problem with the extracted data record
-   * @throws java.io.IOException if there is problem extract a data record from the source
+   * @throws java.io.IOException if there is problem extracting the next data record from the source
    */
-  public D readRecord(D reuse)
-      throws DataRecordException, IOException;
+  public D readRecord(@Deprecated D reuse) throws DataRecordException, IOException;
 
   /**
    * Get the expected source record count.
    *
-   * @return expected source record count
+   * @return the expected source record count
    */
   public long getExpectedRecordCount();
 
   /**
    * Get the calculated high watermark up to which data records are to be extracted.
-   * @return high watermark
+   * @return the calculated high watermark
    */
   public long getHighWatermark();
 }
diff --git a/gobblin-core/src/main/java/gobblin/source/DatePartitionedDailyAvroSource.java b/gobblin-core/src/main/java/gobblin/source/DatePartitionedDailyAvroSource.java
index 6ac228c3de..d8fcc88b13 100644
--- a/gobblin-core/src/main/java/gobblin/source/DatePartitionedDailyAvroSource.java
+++ b/gobblin-core/src/main/java/gobblin/source/DatePartitionedDailyAvroSource.java
@@ -35,11 +35,11 @@
 import gobblin.configuration.SourceState;
 import gobblin.configuration.State;
 import gobblin.configuration.WorkUnitState;
-import gobblin.source.extractor.DatePartitionedExtractor;
+import gobblin.source.extractor.DatePartitionedAvroFileExtractor;
 import gobblin.source.extractor.Extractor;
 import gobblin.source.extractor.filebased.FileBasedHelperException;
 import gobblin.source.extractor.filebased.FileBasedSource;
-import gobblin.source.extractor.hadoop.HadoopFsHelper;
+import gobblin.source.extractor.hadoop.AvroFsHelper;
 import gobblin.source.workunit.Extract;
 import gobblin.source.workunit.Extract.TableType;
 import gobblin.source.workunit.MultiWorkUnitWeightedQueue;
@@ -139,7 +139,7 @@ private void init(SourceState state) {
       Throwables.propagate(e);
     }
 
-    HadoopFsHelper fsHelper = (HadoopFsHelper) this.fsHelper;
+    AvroFsHelper fsHelper = (AvroFsHelper) this.fsHelper;
     this.fs = fsHelper.getFileSystem();
 
     this.sourceState = state;
@@ -167,13 +167,13 @@ private void init(SourceState state) {
 
   @Override
   public void initFileSystemHelper(State state) throws FileBasedHelperException {
-    this.fsHelper = new HadoopFsHelper(state);
+    this.fsHelper = new AvroFsHelper(state);
     this.fsHelper.connect();
   }
 
   @Override
   public Extractor<Schema, GenericRecord> getExtractor(WorkUnitState state) throws IOException {
-    return new DatePartitionedExtractor(state);
+    return new DatePartitionedAvroFileExtractor(state);
   }
 
   @Override
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedExtractor.java b/gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedAvroFileExtractor.java
similarity index 70%
rename from gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedExtractor.java
rename to gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedAvroFileExtractor.java
index 09f472694a..c59c3891a5 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedExtractor.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/DatePartitionedAvroFileExtractor.java
@@ -11,20 +11,17 @@
 
 package gobblin.source.extractor;
 
-import org.apache.avro.Schema;
-import org.apache.avro.generic.GenericRecord;
-
 import gobblin.configuration.WorkUnitState;
-import gobblin.source.extractor.hadoop.HadoopExtractor;
+import gobblin.source.extractor.hadoop.AvroFileExtractor;
 
 
 /**
- * Extension of {@link HadoopExtractor} where the {@link #getHighWatermark()} method returns the result of the
+ * Extension of {@link AvroFileExtractor} where the {@link #getHighWatermark()} method returns the result of the
  * specified WorkUnit's {@link gobblin.source.workunit.WorkUnit#getHighWaterMark()} method.
  */
-public class DatePartitionedExtractor extends HadoopExtractor<Schema, GenericRecord> {
+public class DatePartitionedAvroFileExtractor extends AvroFileExtractor {
 
-  public DatePartitionedExtractor(WorkUnitState workUnitState) {
+  public DatePartitionedAvroFileExtractor(WorkUnitState workUnitState) {
     super(workUnitState);
   }
 
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/extract/QueryBasedExtractor.java b/gobblin-core/src/main/java/gobblin/source/extractor/extract/QueryBasedExtractor.java
index 6b44597976..c91be25244 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/extract/QueryBasedExtractor.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/extract/QueryBasedExtractor.java
@@ -141,8 +141,7 @@ public QueryBasedExtractor(WorkUnitState workUnitState) {
    * @return record of type D
    */
   @Override
-  public D readRecord(D reuse)
-      throws DataRecordException, IOException {
+  public D readRecord(@Deprecated D reuse) throws DataRecordException, IOException {
     if (!this.isPullRequired()) {
       this.log.info("No more records to read");
       return null;
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/filebased/FileBasedExtractor.java b/gobblin-core/src/main/java/gobblin/source/extractor/filebased/FileBasedExtractor.java
index f96c2a852c..85ab7162f7 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/filebased/FileBasedExtractor.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/filebased/FileBasedExtractor.java
@@ -11,24 +11,23 @@
 
 package gobblin.source.extractor.filebased;
 
-import gobblin.source.extractor.DataRecordException;
-import gobblin.source.extractor.Extractor;
-import java.io.Closeable;
 import java.io.IOException;
 import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.commons.io.IOUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import com.google.common.base.Throwables;
+import com.google.common.collect.Lists;
+import com.google.common.io.Closer;
+
 import gobblin.configuration.ConfigurationKeys;
 import gobblin.configuration.WorkUnitState;
+import gobblin.source.extractor.DataRecordException;
+import gobblin.source.extractor.Extractor;
 import gobblin.source.workunit.WorkUnit;
 
 
@@ -41,32 +40,32 @@
  *            type of schema
  * @param <D>
  *            type of data record
- * @param <K>
- *            key type of the command output
- * @param <V>
- *            value type of the command output
  */
-public class FileBasedExtractor<S, D> implements Extractor<S, D> {
-  private Logger log = LoggerFactory.getLogger(FileBasedExtractor.class);
+public abstract class FileBasedExtractor<S, D> implements Extractor<S, D> {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(FileBasedExtractor.class);
+
+  protected final WorkUnit workUnit;
+  protected final WorkUnitState workUnitState;
+  protected final FileBasedHelper fsHelper;
+  protected final List<String> filesToPull;
+
+  protected final Closer closer = Closer.create();
+
+  private final int statusCount;
+  private long totalRecordCount = 0;
+
   private Iterator<D> currentFileItr;
   private String currentFile;
   private boolean readRecordStart;
-  private boolean supportsReuse = true;
-  private boolean seenFirstRecord = false;
-
-  protected WorkUnit workUnit;
-  protected WorkUnitState workUnitState;
-  protected FileBasedHelper fsHelper;
-  protected List<String> filesToPull;
-  protected Map<String, Closeable> fileHandles;
-  private long totalRecordCount = 0;
 
   public FileBasedExtractor(WorkUnitState workUnitState, FileBasedHelper fsHelper) {
     this.workUnitState = workUnitState;
     this.workUnit = workUnitState.getWorkunit();
     this.filesToPull =
-        new ArrayList<String>(workUnitState.getPropAsList(ConfigurationKeys.SOURCE_FILEBASED_FILES_TO_PULL, ""));
-    this.fileHandles = new HashMap<String, Closeable>();
+        Lists.newArrayList(workUnitState.getPropAsList(ConfigurationKeys.SOURCE_FILEBASED_FILES_TO_PULL, ""));
+    this.statusCount = this.workUnit.getPropAsInt(ConfigurationKeys.FILEBASED_REPORT_STATUS_ON_COUNT,
+        ConfigurationKeys.DEFAULT_FILEBASED_REPORT_STATUS_ON_COUNT);
     this.fsHelper = fsHelper;
     try {
       this.fsHelper.connect();
@@ -82,63 +81,42 @@ public FileBasedExtractor(WorkUnitState workUnitState, FileBasedHelper fsHelper)
    * file
    */
   @Override
-  public D readRecord(D reuse)
-      throws DataRecordException, IOException {
+  public D readRecord(@Deprecated D reuse) throws DataRecordException, IOException {
     this.totalRecordCount++;
-    int statusCount = this.workUnit.getPropAsInt(ConfigurationKeys.FILEBASED_REPORT_STATUS_ON_COUNT,
-        ConfigurationKeys.DEFAULT_FILEBASED_REPORT_STATUS_ON_COUNT);
 
-    if (statusCount > 0 && this.totalRecordCount % statusCount == 0) {
-      this.log.info("Total number of records processed so far: " + this.totalRecordCount);
+    if (this.statusCount > 0 && this.totalRecordCount % this.statusCount == 0) {
+      LOGGER.info("Total number of records processed so far: " + this.totalRecordCount);
     }
 
     if (!readRecordStart) {
-      log.info("Starting to read records");
+      LOGGER.info("Starting to read records");
       if (!filesToPull.isEmpty()) {
         currentFile = filesToPull.remove(0);
         currentFileItr = downloadFile(currentFile);
-        seenFirstRecord = false;
-        log.info("Will start downloading file: " + currentFile);
+        LOGGER.info("Will start downloading file: " + currentFile);
       } else {
-        log.info("Finished reading records from all files");
+        LOGGER.info("Finished reading records from all files");
         return null;
       }
       readRecordStart = true;
     }
 
     while (!currentFileItr.hasNext() && !filesToPull.isEmpty()) {
-      log.info("Finished downloading file: " + currentFile);
-      closeFile(currentFile);
+      LOGGER.info("Finished downloading file: " + currentFile);
+      closeCurrentFile();
       currentFile = filesToPull.remove(0);
       currentFileItr = downloadFile(currentFile);
-      seenFirstRecord = false;
-      log.info("Will start downloading file: " + currentFile);
+      LOGGER.info("Will start downloading file: " + currentFile);
     }
 
     if (currentFileItr.hasNext()) {
-      if (supportsReuse && seenFirstRecord) {
-        try {
-          return (D) currentFileItr.getClass().getMethod("next", reuse.getClass()).invoke(currentFileItr, reuse);
-        } catch (Exception e) {
-          e.printStackTrace();
-          log.info("Object reuse unsupported, continuing without reuse");
-          supportsReuse = false;
-        }
-      }
-      seenFirstRecord = true;
-      return (D) currentFileItr.next();
+      return currentFileItr.next();
     } else {
-      log.info("Finished reading records from all files");
+      LOGGER.info("Finished reading records from all files");
       return null;
     }
   }
 
-  /**
-   * Get a list of commands to execute on the source file system, executes the
-   * commands, and parses the output for the schema
-   *
-   * @return the schema
-   */
   @SuppressWarnings("unchecked")
   @Override
   public S getSchema() {
@@ -164,28 +142,25 @@ public long getExpectedRecordCount() {
    */
   @Override
   public long getHighWatermark() {
-    log.info("High Watermark is -1 for file based extractors");
+    LOGGER.info("High Watermark is -1 for file based extractors");
     return -1;
   }
 
   /**
    * Downloads a file from the source
    *
-   * @param f
+   * @param file
    *            is the file to download
    * @return an iterator over the file
-   * @TODO Add support for different file formats besides text e.g. avro
-   *       iterator, byte iterator, json iterator
+   * TODO Add support for different file formats besides text e.g. avro iterator, byte iterator, json iterator.
    */
   @SuppressWarnings("unchecked")
-  public Iterator<D> downloadFile(String file)
-      throws IOException {
-    log.info("Beginning to download file: " + file);
+  public Iterator<D> downloadFile(String file) throws IOException {
+    LOGGER.info("Beginning to download file: " + file);
 
     try {
-      InputStream inputStream = this.fsHelper.getFileStream(file);
+      InputStream inputStream = this.closer.register(this.fsHelper.getFileStream(file));
       Iterator<D> fileItr = (Iterator<D>) IOUtils.lineIterator(inputStream, ConfigurationKeys.DEFAULT_CHARSET_ENCODING);
-      fileHandles.put(file, inputStream);
       if (workUnitState.getPropAsBoolean(ConfigurationKeys.SOURCE_SKIP_FIRST_RECORD, false) && fileItr.hasNext()) {
         fileItr.next();
       }
@@ -196,17 +171,15 @@ public Iterator<D> downloadFile(String file)
   }
 
   /**
-   * Closes a file from the source
-   *
-   * @param f
-   *            is the file to download
-   * @return an iterator over the file
+   * Closes the current file being read.
    */
-  public void closeFile(String file) {
+  public void closeCurrentFile() {
     try {
-      this.fileHandles.get(file).close();
+      this.closer.close();
     } catch (IOException e) {
-      log.error("Could not successfully close file: " + file + " due to error: " + e.getMessage(), e);
+      if (this.currentFile != null) {
+        LOGGER.error("Failed to close file: " + this.currentFile, e);
+      }
     }
   }
 
@@ -215,7 +188,7 @@ public void close() {
     try {
       this.fsHelper.close();
     } catch (FileBasedHelperException e) {
-      log.error("Could not successfully close file system helper due to error: " + e.getMessage(), e);
+      LOGGER.error("Could not successfully close file system helper due to error: " + e.getMessage(), e);
     }
   }
 }
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopExtractor.java b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileExtractor.java
similarity index 66%
rename from gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopExtractor.java
rename to gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileExtractor.java
index ba4604ba3a..d1ea5b7a21 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopExtractor.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileExtractor.java
@@ -11,33 +11,34 @@
 
 package gobblin.source.extractor.hadoop;
 
-import gobblin.source.extractor.filebased.FileBasedExtractor;
-import gobblin.source.extractor.filebased.FileBasedHelperException;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.avro.file.DataFileReader;
+import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 
 import com.google.common.base.Throwables;
+
 import gobblin.configuration.ConfigurationKeys;
 import gobblin.configuration.WorkUnitState;
+import gobblin.source.extractor.filebased.FileBasedExtractor;
+import gobblin.source.extractor.filebased.FileBasedHelperException;
 
 
-public class HadoopExtractor<S, D> extends FileBasedExtractor<S, D> {
+/**
+ * A custom type of {@link FileBasedExtractor}s for extracting data from Avro files.
+ */
+public class AvroFileExtractor extends FileBasedExtractor<Schema, GenericRecord> {
 
-  public HadoopExtractor(WorkUnitState workUnitState) {
-    super(workUnitState, new HadoopFsHelper(workUnitState));
+  public AvroFileExtractor(WorkUnitState workUnitState) {
+    super(workUnitState, new AvroFsHelper(workUnitState));
   }
 
   @Override
-  public Iterator<D> downloadFile(String file)
+  public Iterator<GenericRecord> downloadFile(String file)
       throws IOException {
-    DataFileReader<GenericRecord> dfr = null;
     try {
-      dfr = ((HadoopFsHelper) this.fsHelper).getAvroFile(file);
-      fileHandles.put(file, dfr);
-      return (Iterator<D>) dfr;
+      return this.closer.register(((AvroFsHelper) this.fsHelper).getAvroFile(file));
     } catch (FileBasedHelperException e) {
       Throwables.propagate(e);
     }
@@ -48,17 +49,17 @@ public Iterator<D> downloadFile(String file)
    * Assumption is that all files in the input directory have the same schema
    */
   @Override
-  public S getSchema() {
+  public Schema getSchema() {
     if (this.workUnit.contains(ConfigurationKeys.SOURCE_SCHEMA)) {
-      return (S) this.workUnit.getProp(ConfigurationKeys.SOURCE_SCHEMA);
+      return new Schema.Parser().parse(this.workUnit.getProp(ConfigurationKeys.SOURCE_SCHEMA));
     }
 
-    HadoopFsHelper hfsHelper = (HadoopFsHelper) this.fsHelper;
+    AvroFsHelper hfsHelper = (AvroFsHelper) this.fsHelper;
     if (this.filesToPull.isEmpty()) {
       return null;
     } else {
       try {
-        return (S) hfsHelper.getAvroSchema(this.filesToPull.get(0));
+        return hfsHelper.getAvroSchema(this.filesToPull.get(0));
       } catch (FileBasedHelperException e) {
         Throwables.propagate(e);
         return null;
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopSource.java b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileSource.java
similarity index 79%
rename from gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopSource.java
rename to gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileSource.java
index dc28c5c0a8..106b1c925a 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopSource.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFileSource.java
@@ -28,19 +28,19 @@
 import gobblin.source.extractor.filebased.FileBasedSource;
 
 
-public class HadoopSource extends FileBasedSource<Schema, GenericRecord> {
-  private Logger log = LoggerFactory.getLogger(HadoopSource.class);
+public class AvroFileSource extends FileBasedSource<Schema, GenericRecord> {
+  private static final Logger LOGGER = LoggerFactory.getLogger(AvroFileSource.class);
 
   @Override
   public Extractor<Schema, GenericRecord> getExtractor(WorkUnitState state)
       throws IOException {
-    return new HadoopExtractor(state);
+    return new AvroFileExtractor(state);
   }
 
   @Override
   public void initFileSystemHelper(State state)
       throws FileBasedHelperException {
-    this.fsHelper = new HadoopFsHelper(state);
+    this.fsHelper = new AvroFsHelper(state);
     this.fsHelper.connect();
   }
 
@@ -50,10 +50,10 @@ public List<String> getcurrentFsSnapshot(State state) {
     String path = state.getProp(ConfigurationKeys.SOURCE_FILEBASED_DATA_DIRECTORY);
 
     try {
-      log.info("Running ls command with input " + path);
+      LOGGER.info("Running ls command with input " + path);
       results = this.fsHelper.ls(path);
     } catch (FileBasedHelperException e) {
-      log.error("Not able to run ls command due to " + e.getMessage() + " will not pull any files", e);
+      LOGGER.error("Not able to run ls command due to " + e.getMessage() + " will not pull any files", e);
     }
     return results;
   }
diff --git a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopFsHelper.java b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFsHelper.java
similarity index 94%
rename from gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopFsHelper.java
rename to gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFsHelper.java
index e70b19fcfb..b29f775446 100644
--- a/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/HadoopFsHelper.java
+++ b/gobblin-core/src/main/java/gobblin/source/extractor/hadoop/AvroFsHelper.java
@@ -40,17 +40,19 @@
 import gobblin.configuration.State;
 
 
-public class HadoopFsHelper implements FileBasedHelper {
-  private static Logger log = LoggerFactory.getLogger(HadoopFsHelper.class);
+public class AvroFsHelper implements FileBasedHelper {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(AvroFsHelper.class);
+
   private State state;
   private final Configuration configuration;
   private FileSystem fs;
 
-  public HadoopFsHelper(State state) {
+  public AvroFsHelper(State state) {
     this(state, HadoopUtils.newConfiguration());
   }
 
-  public HadoopFsHelper(State state, Configuration configuration) {
+  public AvroFsHelper(State state, Configuration configuration) {
     this.state = state;
     this.configuration = configuration;
   }
@@ -147,7 +149,7 @@ public Schema getAvroSchema(String file)
         try {
           dfr.close();
         } catch (IOException e) {
-          log.error("Failed to close avro file " + file, e);
+          LOGGER.error("Failed to close avro file " + file, e);
         }
       }
     }
diff --git a/gobblin-core/src/test/java/gobblin/source/extractor/hadoop/HadoopFsHelperTest.java b/gobblin-core/src/test/java/gobblin/source/extractor/hadoop/AvroFsHelperTest.java
similarity index 91%
rename from gobblin-core/src/test/java/gobblin/source/extractor/hadoop/HadoopFsHelperTest.java
rename to gobblin-core/src/test/java/gobblin/source/extractor/hadoop/AvroFsHelperTest.java
index b85afe2c68..06b91c009c 100644
--- a/gobblin-core/src/test/java/gobblin/source/extractor/hadoop/HadoopFsHelperTest.java
+++ b/gobblin-core/src/test/java/gobblin/source/extractor/hadoop/AvroFsHelperTest.java
@@ -24,14 +24,14 @@
 import org.testng.Assert;
 import org.testng.annotations.Test;
 
-public class HadoopFsHelperTest {
+public class AvroFsHelperTest {
 
   @Test(expectedExceptions = IllegalArgumentException.class)
   public void testConnectFailsWithS3URLWithoutAWSCredentials() throws FileBasedHelperException {
     Configuration conf = new Configuration(); // plain conf, no S3 credentials
     SourceState sourceState = new SourceState();
     sourceState.setProp(ConfigurationKeys.SOURCE_FILEBASED_FS_URI, "s3://support.elasticmapreduce/spark/install-spark/");
-    HadoopFsHelper fsHelper = new HadoopFsHelper(sourceState, conf);
+    AvroFsHelper fsHelper = new AvroFsHelper(sourceState, conf);
     fsHelper.connect();
   }
 
@@ -41,7 +41,7 @@ public void testGetFileStreamSucceedsWithUncompressedFile() throws FileBasedHelp
     URL rootUrl = getClass().getResource("/source/");
     String rootPath = rootUrl.toString();
     sourceState.setProp(ConfigurationKeys.SOURCE_FILEBASED_FS_URI, rootPath);
-    HadoopFsHelper fsHelper = new HadoopFsHelper(sourceState);
+    AvroFsHelper fsHelper = new AvroFsHelper(sourceState);
 
     fsHelper.connect();
     URL url = getClass().getResource("/source/simple.tsv");
@@ -57,7 +57,7 @@ public void testGetFileStreamSucceedsWithGZIPFile() throws FileBasedHelperExcept
     URL rootUrl = getClass().getResource("/source/");
     String rootPath = rootUrl.toString();
     sourceState.setProp(ConfigurationKeys.SOURCE_FILEBASED_FS_URI, rootPath);
-    HadoopFsHelper fsHelper = new HadoopFsHelper(sourceState);
+    AvroFsHelper fsHelper = new AvroFsHelper(sourceState);
 
     fsHelper.connect();
     URL url = getClass().getResource("/source/simple.tsv.gz");
diff --git a/gobblin-example/src/main/java/gobblin/example/simplejson/SimpleJsonExtractor.java b/gobblin-example/src/main/java/gobblin/example/simplejson/SimpleJsonExtractor.java
index 74378da121..2318c7ecaf 100644
--- a/gobblin-example/src/main/java/gobblin/example/simplejson/SimpleJsonExtractor.java
+++ b/gobblin-example/src/main/java/gobblin/example/simplejson/SimpleJsonExtractor.java
@@ -86,8 +86,7 @@ public String getSchema() {
   }
 
   @Override
-  public String readRecord(String reuse)
-      throws DataRecordException, IOException {
+  public String readRecord(@Deprecated String reuse) throws DataRecordException, IOException {
     // Read the next line
     return this.bufferedReader.readLine();
   }
diff --git a/gobblin-example/src/main/java/gobblin/example/wikipedia/WikipediaExtractor.java b/gobblin-example/src/main/java/gobblin/example/wikipedia/WikipediaExtractor.java
index a49e68e7d8..6e1ebd631b 100644
--- a/gobblin-example/src/main/java/gobblin/example/wikipedia/WikipediaExtractor.java
+++ b/gobblin-example/src/main/java/gobblin/example/wikipedia/WikipediaExtractor.java
@@ -227,8 +227,7 @@ public String getSchema() {
   }
 
   @Override
-  public JsonElement readRecord(JsonElement reuse)
-      throws DataRecordException, IOException {
+  public JsonElement readRecord(@Deprecated JsonElement reuse) throws DataRecordException, IOException {
     if (this.reader == null) {
       return null;
     }
diff --git a/gobblin-runtime/src/main/java/gobblin/runtime/ExtractorDecorator.java b/gobblin-runtime/src/main/java/gobblin/runtime/ExtractorDecorator.java
index 8222f4f784..737b4cc7cb 100644
--- a/gobblin-runtime/src/main/java/gobblin/runtime/ExtractorDecorator.java
+++ b/gobblin-runtime/src/main/java/gobblin/runtime/ExtractorDecorator.java
@@ -40,7 +40,7 @@ public ExtractorDecorator(Extractor<S, D> extractor, String taskId, Logger logge
   }
 
   @Override
-  public S getSchema() {
+  public S getSchema() throws IOException {
     try {
       return this.extractor.getSchema();
     } catch (Throwable t) {
@@ -52,8 +52,7 @@ public S getSchema() {
   }
 
   @Override
-  public D readRecord(D reuse)
-      throws DataRecordException, IOException {
+  public D readRecord(@Deprecated D reuse) throws DataRecordException, IOException {
     try {
       return this.extractor.readRecord(reuse);
     } catch (Throwable t) {
@@ -65,8 +64,7 @@ public D readRecord(D reuse)
   }
 
   @Override
-  public void close()
-      throws IOException {
+  public void close() throws IOException {
     this.extractor.close();
   }
 
diff --git a/gobblin-runtime/src/main/java/gobblin/runtime/Task.java b/gobblin-runtime/src/main/java/gobblin/runtime/Task.java
index 38cc05265d..f7aa258e75 100644
--- a/gobblin-runtime/src/main/java/gobblin/runtime/Task.java
+++ b/gobblin-runtime/src/main/java/gobblin/runtime/Task.java
@@ -136,10 +136,6 @@ public void run() {
       if (inMultipleBranches(forkedSchemas) && !(schema instanceof Copyable)) {
         throw new CopyNotSupportedException(schema + " is not copyable");
       }
-      // The schema could still be Copyable even if it is not going to multiple branches
-      if (schema instanceof Copyable) {
-        schema = ((Copyable) schema).copy();
-      }
 
       // Used for the main branch to wait for all forks to finish
       CountDownLatch forkCountDownLatch = new CountDownLatch(branches);
@@ -148,7 +144,9 @@ public void run() {
       for (int i = 0; i < branches; i++) {
         if (forkedSchemas.get(i)) {
           Fork fork = closer.register(
-              new Fork(this.taskContext, this.taskState, schema, branches, i, forkCountDownLatch));
+              new Fork(this.taskContext, this.taskState,
+                  schema instanceof Copyable ? ((Copyable) schema).copy() : schema,
+                  branches, i, forkCountDownLatch));
           // Schedule the fork to run
           this.taskExecutor.submit(fork);
           this.forks.add(Optional.of(fork));
@@ -163,9 +161,9 @@ public void run() {
 
       long pullLimit = this.taskState.getPropAsLong(ConfigurationKeys.EXTRACT_PULL_LIMIT, 0);
       long recordsPulled = 0;
-      Object record = null;
+      Object record;
       // Extract, convert, and fork one source record at a time.
-      while ((pullLimit <= 0 || recordsPulled < pullLimit) && (record = extractor.readRecord(record)) != null) {
+      while ((pullLimit <= 0 || recordsPulled < pullLimit) && (record = extractor.readRecord(null)) != null) {
         recordsPulled++;
         for (Object convertedRecord : converter.convertRecord(schema, record, this.taskState)) {
           processRecord(convertedRecord, forkOperator, rowChecker, rowResults, branches);
@@ -360,10 +358,6 @@ private void processRecord(Object convertedRecord, ForkOperator forkOperator, Ro
     if (inMultipleBranches(forkedRecords) && !(convertedRecord instanceof Copyable)) {
       throw new CopyNotSupportedException(convertedRecord + " is not copyable");
     }
-    // The record could still be Copyable even if it is not going to multiple branches
-    if (convertedRecord instanceof Copyable) {
-      convertedRecord = ((Copyable) convertedRecord).copy();
-    }
 
     // If the record has been successfully put into the queues of every forks
     boolean allPutsSucceeded = false;
@@ -379,7 +373,8 @@ private void processRecord(Object convertedRecord, ForkOperator forkOperator, Ro
           continue;
         }
         if (this.forks.get(i).isPresent() && forkedRecords.get(i)) {
-          boolean succeeded = this.forks.get(i).get().putRecord(convertedRecord);
+          boolean succeeded = this.forks.get(i).get().putRecord(
+              convertedRecord instanceof Copyable ? ((Copyable) convertedRecord).copy() : convertedRecord);
           succeededPuts[i] = succeeded;
           if (!succeeded) {
             allPutsSucceeded = false;
diff --git a/gobblin-runtime/src/test/java/gobblin/test/TestExtractor.java b/gobblin-runtime/src/test/java/gobblin/test/TestExtractor.java
index 3a5b7247c0..bb14b92fb5 100644
--- a/gobblin-runtime/src/test/java/gobblin/test/TestExtractor.java
+++ b/gobblin-runtime/src/test/java/gobblin/test/TestExtractor.java
@@ -81,7 +81,7 @@ public String getSchema() {
   }
 
   @Override
-  public String readRecord(String reuse) {
+  public String readRecord(@Deprecated String reuse) {
     if (this.dataFileReader == null) {
       return null;
     }
