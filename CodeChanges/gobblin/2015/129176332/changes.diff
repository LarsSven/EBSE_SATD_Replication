diff --git a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedCopyableDatasetFinder.java b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedCopyableDatasetFinder.java
index aef52342c4..ea46556087 100644
--- a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedCopyableDatasetFinder.java
+++ b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedCopyableDatasetFinder.java
@@ -18,7 +18,6 @@
 
 
 
-import gobblin.data.management.retention.dataset.ConfigurableCleanableDataset;
 import java.io.IOException;
 import java.net.URI;
 import java.util.Collection;
@@ -29,6 +28,7 @@
 import org.apache.hadoop.fs.FileSystem;
 
 import com.typesafe.config.Config;
+import com.google.common.base.Optional;
 
 import gobblin.config.client.ConfigClient;
 import gobblin.dataset.Dataset;
@@ -39,7 +39,7 @@
  * Based on the ConfigStore object to find all {@link ConfigBasedMultiDatasets} to replicate.
  * Specifically for replication job.
  * Normal DistcpNG Job which doesn'involve Dataflow concepts should not use this DatasetFinder but
- * different implementation of {@link ConfigBasedDatasetsFinder}. 
+ * different implementation of {@link ConfigBasedDatasetsFinder}.
  */
 @Slf4j
 public class ConfigBasedCopyableDatasetFinder extends ConfigBasedDatasetsFinder {
@@ -49,13 +49,14 @@ public ConfigBasedCopyableDatasetFinder(FileSystem fs, Properties jobProps) thro
   }
 
   protected Callable<Void> findDatasetsCallable(final ConfigClient confClient,
-      final URI u, final Properties p, final Collection<Dataset> datasets) {
+      final URI u, final Properties p, Optional<List<String>> blacklistPatterns, final Collection<Dataset> datasets) {
     return new Callable<Void>() {
       @Override
       public Void call() throws Exception {
         // Process each {@link Config}, find dataset and add those into the datasets
         Config c = confClient.getConfig(u);
-        List<Dataset> datasetForConfig = new ConfigBasedMultiDatasets(c, p).getConfigBasedDatasetList();
+        List<Dataset> datasetForConfig =
+            new ConfigBasedMultiDatasets(c, p, blacklistPatterns).getConfigBasedDatasetList();
         datasets.addAll(datasetForConfig);
         return null;
       }
diff --git a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java
index bc892ed1ac..62f646b7c2 100644
--- a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java
+++ b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDataset.java
@@ -81,6 +81,13 @@ public ConfigBasedDataset(ReplicationConfiguration rc, Properties props, CopyRou
     calculateDatasetURN();
   }
 
+  public ConfigBasedDataset(ReplicationConfiguration rc, Properties props, CopyRoute copyRoute, String datasetURN) {
+    this.props = props;
+    this.copyRoute = copyRoute;
+    this.rc = rc;
+    this.datasetURN = datasetURN;
+  }
+
   private void calculateDatasetURN(){
     EndPoint e = this.copyRoute.getCopyTo();
     if (e instanceof HadoopFsEndPoint) {
diff --git a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java
index d8b9355468..754775e3f5 100644
--- a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java
+++ b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinder.java
@@ -17,7 +17,6 @@
 
 package gobblin.data.management.copy.replication;
 
-import gobblin.util.FileListUtils;
 import java.io.IOException;
 import java.net.URI;
 import java.net.URISyntaxException;
@@ -34,9 +33,7 @@
 import java.util.concurrent.Callable;
 import java.util.concurrent.CopyOnWriteArrayList;
 import java.util.concurrent.ExecutionException;
-import java.util.stream.Collectors;
 
-import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 
@@ -64,7 +61,6 @@
 import gobblin.util.ExecutorsUtils;
 import gobblin.util.executors.IteratorExecutor;
 import lombok.extern.slf4j.Slf4j;
-import org.apache.hadoop.fs.PathFilter;
 
 
 /**
@@ -91,17 +87,13 @@ public abstract class ConfigBasedDatasetsFinder implements DatasetsFinder {
   public static final String GOBBLIN_CONFIG_STORE_DATASET_COMMON_ROOT =
       ConfigurationKeys.CONFIG_BASED_PREFIX + ".dataset.common.root";
 
-  // In addition to the white/blacklist tags, this configuration let the user to black/whitelist some datasets
+  // In addition to the white/blacklist tags, this configuration let the user to whitelist some datasets
   // in the job-level configuration, which is not specified in configStore
   // as to have easier approach to black/whitelist some datasets on operation side.
-  // The semantics keep still as tag, which the blacklist override whitelist if any dataset in common.
+  // White job-level blacklist is different from tag-based blacklist since the latter is part of dataset discovery
+  // but the former is filtering process.
+  // Tag-based dataset discover happens at the first, before the job-level glob-pattern based filtering.
   public static final String JOB_LEVEL_BLACKLIST = CopyConfiguration.COPY_PREFIX + ".configBased.blacklist" ;
-  public static final String JOB_LEVEL_WHITELIST = CopyConfiguration.COPY_PREFIX + ".configBased.whitelist" ;
-
-  // There are some cases that WATERMARK checking is desired, like
-  // Unexpected data loss on target while not changing watermark accordingly.
-  // This configuration make WATERMARK checking configurable for operation convenience, default true
-  public static final String WATERMARK_ENABLE = CopyConfiguration.COPY_PREFIX + ".configBased.watermark.enabled" ;
 
 
   protected final String storeRoot;
@@ -113,8 +105,7 @@ public abstract class ConfigBasedDatasetsFinder implements DatasetsFinder {
   private final int threadPoolSize;
   private FileSystem fs;
 
-  private Optional<List<String>> blacklistURNs;
-  private Optional<List<String>> whitelistURNs;
+  public final Optional<List<String>> blacklistPatterns;
 
 
   public ConfigBasedDatasetsFinder(FileSystem fs, Properties jobProps) throws IOException {
@@ -155,45 +146,38 @@ public ConfigBasedDatasetsFinder(FileSystem fs, Properties jobProps) throws IOEx
 
 
     if (props.containsKey(JOB_LEVEL_BLACKLIST)) {
-      this.blacklistURNs = Optional.of(Splitter.on(",").omitEmptyStrings().splitToList(props.getProperty(JOB_LEVEL_BLACKLIST)));
+      this.blacklistPatterns = Optional.of(Splitter.on(",").omitEmptyStrings().splitToList(props.getProperty(JOB_LEVEL_BLACKLIST)));
     } else {
-      this.blacklistURNs = Optional.absent();
+      this.blacklistPatterns = Optional.absent();
     }
 
-    if (props.containsKey(JOB_LEVEL_WHITELIST)) {
-      this.whitelistURNs = Optional.of(Splitter.on(",").omitEmptyStrings().splitToList(props.getProperty(JOB_LEVEL_WHITELIST)));
-    } else {
-      this.whitelistURNs = Optional.absent();
-    }
   }
 
+  /**
+   * Semantic of two-level black/whitelist:
+   * - Whitelist always respect blacklist.
+   * - Job-level black/whitelist is NOT OVERRIDING dataset-level black/whitelist but enhance it.
+   */
   protected Set<URI> getValidDatasetURIs(Path datasetCommonRoot) {
     Collection<URI> allDatasetURIs;
     Set<URI> disabledURISet = new HashSet();
-    if (this.blacklistURNs.isPresent()) {
-      for(String urn : this.blacklistURNs.get()) {
-        disabledURISet.addAll(this.datasetURNtoURI(urn));
-      }
-    }
 
+    // This try block basically populate the Valid dataset URI set.
     try {
-      // get all the URIs which imports {@link #replicationTag} or all from whitelistURNs
-      allDatasetURIs = this.whitelistURNs.isPresent()
-          ? this.whitelistURNs.get().stream().flatMap(u -> this.datasetURNtoURI(u).stream()).collect(Collectors.toList())
-          : configClient.getImportedBy(new URI(whitelistTag.toString()), true);
-      populateDisabledURIs(disabledURISet);
+      allDatasetURIs = configClient.getImportedBy(new URI(whitelistTag.toString()), true);
+      enhanceDisabledURIsWithBlackListTag(disabledURISet);
     } catch ( ConfigStoreFactoryDoesNotExistsException | ConfigStoreCreationException
         | URISyntaxException e) {
       log.error("Caught error while getting all the datasets URIs " + e.getMessage());
       throw new RuntimeException(e);
     }
-    return getValidDatasetURIs(allDatasetURIs, disabledURISet, datasetCommonRoot);
+    return getValidDatasetURIsHelper(allDatasetURIs, disabledURISet, datasetCommonRoot);
   }
 
   /**
    * Extended signature for testing convenience.
    */
-  protected static Set<URI> getValidDatasetURIs(Collection<URI> allDatasetURIs, Set<URI> disabledURISet, Path datasetCommonRoot){
+  protected static Set<URI> getValidDatasetURIsHelper(Collection<URI> allDatasetURIs, Set<URI> disabledURISet, Path datasetCommonRoot){
     if (allDatasetURIs == null || allDatasetURIs.isEmpty()) {
       return ImmutableSet.of();
     }
@@ -243,13 +227,12 @@ public int compare(URI c1, URI c2) {
     return validURISet;
   }
 
-  private void populateDisabledURIs(Set<URI> disabledURIs) throws
+  private void enhanceDisabledURIsWithBlackListTag(Set<URI> disabledURIs) throws
                                                            URISyntaxException,
                                                            ConfigStoreFactoryDoesNotExistsException,
                                                            ConfigStoreCreationException,
                                                            VersionDoesNotExistException {
     if (this.blacklistTags.isPresent()) {
-      disabledURIs = new HashSet<URI>();
       for (Path s : this.blacklistTags.get()) {
         disabledURIs.addAll(configClient.getImportedBy(new URI(s.toString()), true));
       }
@@ -282,7 +265,7 @@ public List<Dataset> findDatasets() throws IOException {
         Iterators.transform(leafDatasets.iterator(), new Function<URI, Callable<Void>>() {
           @Override
           public Callable<Void> apply(final URI datasetURI) {
-            return findDatasetsCallable(configClient, datasetURI, props, result);
+            return findDatasetsCallable(configClient, datasetURI, props, blacklistPatterns, result);
           }
         });
 
@@ -304,35 +287,20 @@ protected void executeItertorExecutor(Iterator<Callable<Void>> callableIterator)
 
   /**
    * Helper funcition for converting datasetURN into URI
-   * Enable Pattern-like support.
+   * Note that here the URN can possibly being specified with pattern, i.e. with wildcards like `*`
+   * It will be resolved by configStore.
    */
-  public List<URI> datasetURNtoURI(String datasetURN) {
-    Path mergedPath = PathUtils.mergePaths(this.commonRoot, new Path(datasetURN));
-    List<URI> resultURIs = new ArrayList<>();
+  private URI datasetURNtoURI(String datasetURN) {
     try {
-      // mergedPath is possibly to have pattern-format characters like '*'
-      FileStatus[] matchedPaths = fs.globStatus(mergedPath);
-      for (FileStatus matchedPath: matchedPaths) {
-        List<FileStatus> fileStatuses = FileListUtils.listFilesRecursively(fs, matchedPath.getPath(), new PathFilter() {
-          @Override
-          public boolean accept(Path path) {
-            return true;
-          }
-        });
-        for (FileStatus fileStatus : fileStatuses) {
-          resultURIs.add(new URI(PathUtils.getPathWithoutSchemeAndAuthority(fileStatus.getPath()).toString()));
-        }
-      }
-      return resultURIs;
+      return new URI(PathUtils.mergePaths(new Path(this.storeRoot), new Path(datasetURN)).toString());
     }catch (URISyntaxException e) {
       log.error("Dataset with URN:" + datasetURN + " cannot be converted into URI. Skip the dataset");
       return null;
-    }catch (IOException ioe){
-      log.error("DatasetURN with pattern:" + mergedPath.toString() + " cannot be resolved");
-      return null;
     }
   }
 
   protected abstract Callable<Void> findDatasetsCallable(final ConfigClient confClient,
-      final URI u, final Properties p, final Collection<Dataset> datasets);
+      final URI u, final Properties p, Optional<List<String>> blacklistPatterns,
+      final Collection<Dataset> datasets);
+
 }
diff --git a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedMultiDatasets.java b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedMultiDatasets.java
index 2461242ca2..50fb2a4141 100644
--- a/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedMultiDatasets.java
+++ b/gobblin-data-management/src/main/java/gobblin/data/management/copy/replication/ConfigBasedMultiDatasets.java
@@ -17,6 +17,7 @@
 
 package gobblin.data.management.copy.replication;
 
+import avro.shaded.com.google.common.annotations.VisibleForTesting;
 import gobblin.dataset.Dataset;
 import java.io.IOException;
 import java.net.URI;
@@ -24,6 +25,7 @@
 import java.util.List;
 import java.util.Properties;
 
+import java.util.regex.Pattern;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 
@@ -52,16 +54,25 @@ public class ConfigBasedMultiDatasets {
 
   private final Properties props;
   private final List<Dataset> datasets = new ArrayList<>();
+  private Optional<List<Pattern>> blacklist = Optional.of(new ArrayList<>());
+
 
   /**
    * if push mode is set in property, only replicate data when
    * 1. Push mode is set in Config store
-   * 2. CopyTo cluster in sync with property with {@link #ConfigurationKeys.WRITER_FILE_SYSTEM_URI}
+   * 2. CopyTo cluster in sync with property with 'writer.fs.uri'
    */
   public static final String REPLICATION_PUSH_MODE = CopyConfiguration.COPY_PREFIX + ".replicationPushMode";
 
-  public ConfigBasedMultiDatasets (Config c, Properties props){
+  // Dummy constructor, return empty datasets.
+  public ConfigBasedMultiDatasets(){
+    this.props = new Properties();
+  }
+
+  public ConfigBasedMultiDatasets (Config c, Properties props,
+      Optional<List<String>> blacklistPatterns){
     this.props = props;
+    blacklist = patternListInitHelper(blacklistPatterns);
 
     try {
       FileSystem executionCluster = FileSystem.get(new Configuration());
@@ -86,6 +97,19 @@ public ConfigBasedMultiDatasets (Config c, Properties props){
     }
   }
 
+  private Optional<List<Pattern>> patternListInitHelper(Optional<List<String>> patterns){
+    if (patterns.isPresent() && patterns.get().size() >= 1) {
+      List<Pattern> tmpPatterns = new ArrayList<>();
+      for (String pattern : patterns.get()){
+        tmpPatterns.add(Pattern.compile(pattern));
+      }
+      return Optional.of(tmpPatterns);
+    }
+    else{
+      return Optional.absent();
+    }
+  }
+
   private void generateDatasetInPushMode(ReplicationConfiguration rc, URI executionClusterURI){
     if(rc.getCopyMode()== ReplicationCopyMode.PULL){
       log.info("Skip process pull mode dataset with meta data{} as job level property specify push mode ", rc.getMetaData());
@@ -116,7 +140,15 @@ private void generateDatasetInPushMode(ReplicationConfiguration rc, URI executio
 
             HadoopFsEndPoint ep = (HadoopFsEndPoint)cr.getCopyTo();
             if(ep.getFsURI().toString().equals(pushModeTargetCluster)){
-              this.datasets.add(new ConfigBasedDataset(rc, this.props, cr));
+              // For a candidate dataset, iterate thru. all available blacklist patterns.
+              ConfigBasedDataset configBasedDataset = new ConfigBasedDataset(rc, this.props, cr);
+              if (blacklistFilteringHelper(configBasedDataset, this.blacklist)){
+                this.datasets.add(configBasedDataset);
+              }
+              else{
+                log.info("Dataset" + configBasedDataset.datasetURN() + " has been filtered out because of blacklist pattern:"
+                    + this.blacklist.get().toString());
+              }
             }
           }
         }// inner for loops ends
@@ -138,12 +170,40 @@ private void generateDatasetInPullMode(ReplicationConfiguration rc, URI executio
       if(needGenerateCopyEntity(replica, executionClusterURI)){
         Optional<CopyRoute> copyRoute = cpGen.getPullRoute(rc, replica);
         if(copyRoute.isPresent()){
-          this.datasets.add(new ConfigBasedDataset(rc, this.props, copyRoute.get()));
+          ConfigBasedDataset configBasedDataset = new ConfigBasedDataset(rc, this.props, copyRoute.get());
+          if (blacklistFilteringHelper(configBasedDataset, this.blacklist)){
+            this.datasets.add(configBasedDataset);
+          }
+          else{
+            log.info("Dataset" + configBasedDataset.datasetURN() + " has been filtered out because of blacklist pattern:"
+                + this.blacklist.get().toString());
+          }
         }
       }
     }
   }
 
+  @VisibleForTesting
+  /**
+   * Return false if the target configBasedDataset should be kept in the blacklist.
+   */
+  public boolean blacklistFilteringHelper(ConfigBasedDataset configBasedDataset, Optional<List<Pattern>> patternList){
+    String datasetURN = configBasedDataset.datasetURN();
+    if (patternList.isPresent()) {
+      for(Pattern pattern: patternList.get()) {
+        if (pattern.matcher(datasetURN).find()){
+          return false;
+        }
+      }
+      // If the dataset get thru. all blacklist check, accept it.
+      return true;
+    }
+    // If blacklist not specified, automatically accept the dataset.
+    else {
+      return true;
+    }
+  }
+
   public List<Dataset> getConfigBasedDatasetList(){
     return this.datasets;
   }
diff --git a/gobblin-data-management/src/main/java/gobblin/data/management/retention/profile/ConfigBasedCleanabledDatasetFinder.java b/gobblin-data-management/src/main/java/gobblin/data/management/retention/profile/ConfigBasedCleanabledDatasetFinder.java
index e008e267cf..7dcf0a1515 100644
--- a/gobblin-data-management/src/main/java/gobblin/data/management/retention/profile/ConfigBasedCleanabledDatasetFinder.java
+++ b/gobblin-data-management/src/main/java/gobblin/data/management/retention/profile/ConfigBasedCleanabledDatasetFinder.java
@@ -16,9 +16,11 @@
  */
 package gobblin.data.management.retention.profile;
 
+import com.google.common.base.Optional;
 import java.io.IOException;
 import java.net.URI;
 import java.util.Collection;
+import java.util.List;
 import java.util.Properties;
 import java.util.concurrent.Callable;
 
@@ -52,7 +54,7 @@ public ConfigBasedCleanabledDatasetFinder(FileSystem fs, Properties jobProps) th
   }
 
   protected Callable<Void> findDatasetsCallable(final ConfigClient confClient,
-      final URI u, final Properties p, final Collection<Dataset> datasets) {
+      final URI u, final Properties p, Optional<List<String>> blacklistURNs, final Collection<Dataset> datasets) {
     return new Callable<Void>() {
       @Override
       public Void call() throws Exception {
diff --git a/gobblin-data-management/src/test/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinderTest.java b/gobblin-data-management/src/test/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinderTest.java
index 57aece76aa..634e34a032 100644
--- a/gobblin-data-management/src/test/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinderTest.java
+++ b/gobblin-data-management/src/test/java/gobblin/data/management/copy/replication/ConfigBasedDatasetsFinderTest.java
@@ -17,7 +17,7 @@
 
 package gobblin.data.management.copy.replication;
 
-import gobblin.configuration.ConfigurationKeys;
+import com.google.common.base.Optional;
 import java.io.IOException;
 import java.net.URI;
 import java.net.URISyntaxException;
@@ -28,14 +28,12 @@
 import java.util.Properties;
 import java.util.Set;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
+import java.util.regex.Pattern;
 import org.apache.hadoop.fs.Path;
+import org.mockito.Mockito;
 import org.testng.Assert;
 import org.testng.annotations.Test;
 
-import static gobblin.data.management.copy.replication.ConfigBasedDatasetsFinder.*;
-
 
 /**
  * Unit test for {@link ConfigBasedDatasetsFinder}
@@ -71,7 +69,7 @@ public void testGetLeafDatasetURIs() throws URISyntaxException, IOException {
     Set<URI> disabled = new HashSet<URI>();
     disabled.add(new URI("/data/derived/gowl/pymk/invitationsCreationsSends/hourly_data/aggregation/daily"));
 
-    Set<URI> validURIs = ConfigBasedDatasetsFinder.getValidDatasetURIs(allDatasetURIs, disabled, new Path("/data/derived"));
+    Set<URI> validURIs = ConfigBasedDatasetsFinder.getValidDatasetURIsHelper(allDatasetURIs, disabled, new Path("/data/derived"));
 
     Assert.assertTrue(validURIs.size() == 3);
     Assert.assertTrue(validURIs.contains(new URI("/data/derived/gowl/pymk/invitationsCreationsSends/hourly_data/aggregation/daily_dedup")));
@@ -80,42 +78,31 @@ public void testGetLeafDatasetURIs() throws URISyntaxException, IOException {
   }
 
   @Test
-  public void testValidURIsWithBlacklist() throws URISyntaxException, IOException {
-    /* tmp ConfigStore structure:
-       /tmp/configStore/test/A
-          /A/A1
-       /tmp/configStore/test/B
-          /B/B1
-       /tmp/configStore/test/C
-          /C/C1/C11
-          /C/C1/C12
-
-       Will finally have A1, B1, C11
-     */
-    FileSystem fs = FileSystem.get(new Configuration()) ;
-    Properties jobProps = new Properties() ;
-    fs.mkdirs(new Path("/tmp/configStore/test"));
-    fs.mkdirs(new Path("/tmp/configStore/test/A"));
-    fs.mkdirs(new Path("/tmp/configStore/test/B"));
-    fs.mkdirs(new Path("/tmp/configStore/test/C"));
-    fs.create(new Path("/tmp/configStore/test/A/A1"));
-    fs.create(new Path("/tmp/configStore/test/B/B1"));
-    fs.mkdirs(new Path("/tmp/configStore/test/C/C1"));
-    fs.create(new Path("/tmp/configStore/test/C/C1/C11"));
-    fs.create(new Path("/tmp/configStore/test/C/C1/C12"));
-    jobProps.setProperty(ConfigurationKeys.CONFIG_MANAGEMENT_STORE_URI, "/tmp/configStore/");
-    jobProps.setProperty(GOBBLIN_CONFIG_STORE_WHITELIST_TAG, "/");
-    jobProps.setProperty(GOBBLIN_CONFIG_STORE_DATASET_COMMON_ROOT, "test");
-
-    ConfigBasedDatasetsFinder configBasedDatasetsFinder = new ConfigBasedCopyableDatasetFinder(fs, jobProps);
-    List<URI> uriList = configBasedDatasetsFinder.datasetURNtoURI("C/*");
-
-    Assert.assertTrue(uriList.size() == 2);
-    Assert.assertTrue(uriList.contains(new URI("/tmp/configStore/test/C/C1/C11")));
-    Assert.assertTrue(uriList.contains(new URI("/tmp/configStore/test/C/C1/C12")));
-
-    // Clean up
-    fs.delete( new Path("/tmp/configStore"), true);
-    return;
+  public void blacklistPatternTest() {
+    Properties properties = new Properties();
+    properties.setProperty("gobblin.selected.policy", "random");
+    properties.setProperty("source","random");
+    properties.setProperty("replicas", "random");
+
+    ConfigBasedMultiDatasets configBasedMultiDatasets = new ConfigBasedMultiDatasets();
+
+    ReplicationConfiguration rc = Mockito.mock(ReplicationConfiguration.class);
+    CopyRoute cr = Mockito.mock(CopyRoute.class);
+    ConfigBasedDataset configBasedDataset = new ConfigBasedDataset(rc, new Properties(), cr, "/test/tmp/word");
+    ConfigBasedDataset configBasedDataset2 = new ConfigBasedDataset(rc, new Properties(), cr, "/test/a_temporary/word");
+    ConfigBasedDataset configBasedDataset3 = new ConfigBasedDataset(rc, new Properties(), cr, "/test/go/word");
+
+
+    Pattern pattern1 = Pattern.compile(".*_temporary.*");
+    Pattern pattern2 = Pattern.compile(".*tmp.*");
+    List<Pattern> patternList = new ArrayList<>();
+    patternList.add(pattern1);
+    patternList.add(pattern2);
+
+    Assert.assertFalse(configBasedMultiDatasets.blacklistFilteringHelper(configBasedDataset, Optional.of(patternList)));
+    Assert.assertFalse(configBasedMultiDatasets.blacklistFilteringHelper(configBasedDataset2, Optional.of(patternList)));
+    Assert.assertTrue(configBasedMultiDatasets.blacklistFilteringHelper(configBasedDataset3, Optional.of(patternList)));
+
+
   }
 }
