diff --git a/.travis.yml b/.travis.yml
index 65ddf08a4c7..6750717c668 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -7,7 +7,13 @@ env:
     - JOBS=1
   matrix:
     - RUN_INTEGRATION_TESTS=true
+      KAFKA_VERSION=0.9
     - RUN_INTEGRATION_TESTS=false
+      KAFKA_VERSION=0.9
+    - RUN_INTEGRATION_TESTS=true
+      KAFKA_VERSION=2.0
+    - RUN_INTEGRATION_TESTS=false
+      KAFKA_VERSION=2.0
 
 jdk:
   - oraclejdk8
diff --git a/.travis_install.sh b/.travis_install.sh
index 995ce1fdab8..f6cd3a2c18d 100755
--- a/.travis_install.sh
+++ b/.travis_install.sh
@@ -38,10 +38,15 @@ if [ $noThirdEyeChange -eq 0 ]; then
   fi
 fi
 
+KAFKA_BUILD_OPTS=""
+if [ "$KAFKA_VERSION" != '0.9' ]; then
+  KAFKA_BUILD_OPTS="-Dkafka.version=${KAFKA_VERSION}"
+fi
+
 if [ $noThirdEyeChange -ne 0 ]; then
   echo "Full Pinot build"
   echo "No ThirdEye changes"
-  mvn clean install -B -DskipTests=true -Dmaven.javadoc.skip=true -Dassembly.skipAssembly=true || exit $?
+  mvn clean install -B -DskipTests=true -Dmaven.javadoc.skip=true -Dassembly.skipAssembly=true ${KAFKA_BUILD_OPTS} || exit $?
 fi
 
 # Build ThirdEye for ThirdEye related changes
diff --git a/.travis_test.sh b/.travis_test.sh
index 2c65f03965b..e3d3ad90144 100755
--- a/.travis_test.sh
+++ b/.travis_test.sh
@@ -56,14 +56,25 @@ if [ "$TRAVIS_JDK_VERSION" != 'oraclejdk8' ]; then
 fi
 
 passed=0
+
+KAFKA_BUILD_OPTS=""
+if [ "$KAFKA_VERSION" != '0.9' ]; then
+  git diff --name-only $TRAVIS_COMMIT_RANGE | egrep '^(pinot-connectors)'
+  if [ $? -ne 0 ]; then
+    echo "No Pinot Connector Changes, Skip tests for Kafka Connector: ${KAFKA_VERSION}."
+    exit 0
+  fi
+  KAFKA_BUILD_OPTS="-Dkafka.version=${KAFKA_VERSION}"
+fi
+
 # Only run integration tests if needed
 if [ "$RUN_INTEGRATION_TESTS" != 'false' ]; then
-  mvn test -B -P travis,travis-integration-tests-only
+  mvn test -B -P travis,travis-integration-tests-only ${KAFKA_BUILD_OPTS}
   if [ $? -eq 0 ]; then
     passed=1
   fi
 else
-  mvn test -B -P travis,travis-no-integration-tests
+  mvn test -B -P travis,travis-no-integration-tests ${KAFKA_BUILD_OPTS}
   if [ $? -eq 0 ]; then
     passed=1
   fi
diff --git a/docs/pluggable_streams.rst b/docs/pluggable_streams.rst
index d3c7a118266..ff27cde581e 100644
--- a/docs/pluggable_streams.rst
+++ b/docs/pluggable_streams.rst
@@ -144,3 +144,51 @@ The properties for the thresholds are as follows:
 
 
 An example of this implementation can be found in the `KafkaConsumerFactory <https://github.com/apache/incubator-pinot/blob/master/pinot-core/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaConsumerFactory.java>`_, which is an implementation for the kafka stream.
+
+
+Kafka 2.x Plugin
+^^^^^^^^^^^^^^^^
+
+Pinot provides stream plugin support for Kafka 2.x version.
+Although the version used in this implementation is kafka 2.0.0, it's possible to compile it with higher kafka lib version, e.g. 2.1.1.
+
+How to build and release Pinot package with Kafka 2.x connector
+---------------------------------------------------------------
+
+.. code-block:: none
+
+  mvn clean package -DskipTests -Pbin-dist -Dkafka.version=2.0
+
+How to use Kafka 2.x connector
+------------------------------
+
+Below is a sample `streamConfigs` used to create a realtime table with Kafka Stream(High) level consumer:
+
+.. code-block:: none
+
+  "streamConfigs": {
+    "streamType": "kafka",
+    "stream.kafka.consumer.type": "highLevel",
+    "stream.kafka.topic.name": "meetupRSVPEvents",
+    "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+    "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+    "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory",
+    "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+    "stream.kafka.hlc.bootstrap.server": "localhost:19092"
+  }
+
+Upgrade from Kafka 0.9 connector to Kafka 2.x connector
+-------------------------------------------------------
+
+* Update  table config:
+Update config: ``stream.kafka.consumer.factory.class.name`` from ``org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory`` to ``org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory``.
+
+* If using Stream(High) level consumer:
+Please also add config ``stream.kafka.hlc.bootstrap.server`` into ``tableIndexConfig.streamConfigs``.
+This config should be the URI of Kafka broker lists, e.g. ``localhost:9092``.
+
+How to use this plugin with higher Kafka version?
+-----------------------------------------
+
+This connector is also suitable for Kafka lib version higher than ``2.0.0``.
+In ``pinot-connector-kafka-2.0/pom.xml`` change the ``kafka.lib.version`` from ``2.0.0`` to ``2.1.1`` will make this Connector working with Kafka ``2.1.1``.
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/pom.xml b/pinot-connectors/pinot-connector-kafka-0.9/pom.xml
index 0450102a076..e78b92809b5 100644
--- a/pinot-connectors/pinot-connector-kafka-0.9/pom.xml
+++ b/pinot-connectors/pinot-connector-kafka-0.9/pom.xml
@@ -35,10 +35,15 @@
   <properties>
     <pinot.root>${basedir}/../..</pinot.root>
     <kafka.lib.version>0.9.0.1</kafka.lib.version>
+    <kafka.scala.version>2.10</kafka.scala.version>
   </properties>
 
   <dependencies>
-
+    <dependency>
+      <groupId>org.apache.pinot</groupId>
+      <artifactId>pinot-connector-kafka-base</artifactId>
+      <version>${project.version}</version>
+    </dependency>
     <!-- Kafka  -->
     <dependency>
       <groupId>org.apache.kafka</groupId>
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory b/pinot-connectors/pinot-connector-kafka-0.9/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory
new file mode 100644
index 00000000000..dac7560fda5
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-0.9/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory
@@ -0,0 +1,19 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory
\ No newline at end of file
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/src/test/java/org/apache/pinot/core/realtime/impl/kafka/KafkaPartitionLevelConsumerTest.java b/pinot-connectors/pinot-connector-kafka-0.9/src/test/java/org/apache/pinot/core/realtime/impl/kafka/KafkaPartitionLevelConsumerTest.java
index 165dcb4d85b..64788b7b6ea 100644
--- a/pinot-connectors/pinot-connector-kafka-0.9/src/test/java/org/apache/pinot/core/realtime/impl/kafka/KafkaPartitionLevelConsumerTest.java
+++ b/pinot-connectors/pinot-connector-kafka-0.9/src/test/java/org/apache/pinot/core/realtime/impl/kafka/KafkaPartitionLevelConsumerTest.java
@@ -16,7 +16,7 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pinot.core.realtime.kafka;
+package org.apache.pinot.core.realtime.impl.kafka;
 
 import com.google.common.base.Preconditions;
 import java.util.Collections;
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/README.md b/pinot-connectors/pinot-connector-kafka-2.0/README.md
new file mode 100644
index 00000000000..b28f8213b2a
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/README.md
@@ -0,0 +1,57 @@
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+# Pinot connector for kafka 2.x
+
+This is an implementation of the kafka stream for kafka versions 2.x The version used in this implementation is kafka 2.0.0.
+
+A stream plugin for another version of kafka, or another stream, can be added in a similar fashion. Refer to documentation on (Pluggable Streams)[https://pinot.readthedocs.io/en/latest/pluggable_streams.html] for the specfic interfaces to implement.
+
+* How to build and release Pinot package with Kafka 2.x connector
+```$xslt
+mvn clean package -DskipTests -Pbin-dist -Dkafka.version=2.0
+```
+
+* How to use Kafka 2.x connector
+Below is a sample `streamConfigs` used to create a realtime table with Kafka Stream(High) level consumer:
+```$xslt
+"streamConfigs": {
+  "streamType": "kafka",
+  "stream.kafka.consumer.type": "highLevel",
+  "stream.kafka.topic.name": "meetupRSVPEvents",
+  "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+  "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+  "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory",
+  "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+  "stream.kafka.hlc.bootstrap.server": "localhost:19092"
+}
+```
+
+* Upgrade from Kafka 0.9 connector to Kafka 2.x connector:
+
+  1. Update  table config:
+ `stream.kafka.consumer.factory.class.name` from `org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory` to `org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory`.
+
+  1. If using Stream(High) level consumer, please also add config `stream.kafka.hlc.bootstrap.server` into `tableIndexConfig.streamConfigs`.
+This config should be the URI of Kafka broker lists, e.g. `localhost:9092`.
+
+* How to upgrade to Kafka version > `2.0.0`
+This connector is also suitable for Kafka lib version higher than `2.0.0`.
+In `pinot-connector-kafka-2.0/pom.xml` change the `kafka.lib.version` from `2.0.0` to `2.1.1` will make this Connector working with Kafka `2.1.1`.
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/pom.xml b/pinot-connectors/pinot-connector-kafka-2.0/pom.xml
new file mode 100644
index 00000000000..1f3bd4184af
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/pom.xml
@@ -0,0 +1,103 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <parent>
+    <artifactId>pinot-connectors</artifactId>
+    <groupId>org.apache.pinot</groupId>
+    <version>0.2.0-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+  <modelVersion>4.0.0</modelVersion>
+  <artifactId>pinot-connector-kafka-2.0</artifactId>
+  <name>Pinot Connector Kafka 2.0</name>
+  <url>https://pinot.apache.org/</url>
+  <properties>
+    <pinot.root>${basedir}/../..</pinot.root>
+    <kafka.lib.version>2.0.0</kafka.lib.version>
+  </properties>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.pinot</groupId>
+      <artifactId>pinot-connector-kafka-base</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <!-- Kafka  -->
+    <dependency>
+      <groupId>org.apache.kafka</groupId>
+      <artifactId>kafka-clients</artifactId>
+      <version>${kafka.lib.version}</version>
+      <exclusions>
+        <exclusion>
+          <groupId>org.slf4j</groupId>
+          <artifactId>slf4j-log4j12</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>net.sf.jopt-simple</groupId>
+          <artifactId>jopt-simple</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.scala-lang</groupId>
+          <artifactId>scala-library</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.zookeeper</groupId>
+          <artifactId>zookeeper</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+    <dependency>
+      <groupId>org.scala-lang</groupId>
+      <artifactId>scala-library</artifactId>
+      <version>2.12.8</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.kafka</groupId>
+      <artifactId>kafka_2.12</artifactId>
+      <version>${kafka.lib.version}</version>
+      <exclusions>
+        <exclusion>
+          <groupId>org.slf4j</groupId>
+          <artifactId>slf4j-log4j12</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.scala-lang</groupId>
+          <artifactId>scala-library</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.scala-lang</groupId>
+          <artifactId>scala-reflect</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>org.apache.zookeeper</groupId>
+          <artifactId>zookeeper</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+    <dependency>
+      <groupId>net.sf.jopt-simple</groupId>
+      <artifactId>jopt-simple</artifactId>
+    </dependency>
+  </dependencies>
+</project>
\ No newline at end of file
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaConsumerFactory.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaConsumerFactory.java
new file mode 100644
index 00000000000..5a30f9299be
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaConsumerFactory.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import org.apache.pinot.common.data.Schema;
+import org.apache.pinot.common.metadata.instance.InstanceZKMetadata;
+import org.apache.pinot.common.metrics.ServerMetrics;
+import org.apache.pinot.core.realtime.stream.PartitionLevelConsumer;
+import org.apache.pinot.core.realtime.stream.StreamConsumerFactory;
+import org.apache.pinot.core.realtime.stream.StreamLevelConsumer;
+import org.apache.pinot.core.realtime.stream.StreamMetadataProvider;
+
+
+public class KafkaConsumerFactory extends StreamConsumerFactory {
+  @Override
+  public PartitionLevelConsumer createPartitionLevelConsumer(String clientId, int partition) {
+    return new KafkaPartitionLevelConsumer(clientId, _streamConfig, partition);
+  }
+
+  @Override
+  public StreamLevelConsumer createStreamLevelConsumer(String clientId, String tableName, Schema schema,
+      InstanceZKMetadata instanceZKMetadata, ServerMetrics serverMetrics) {
+    return new KafkaStreamLevelConsumer(clientId, tableName, _streamConfig, schema, instanceZKMetadata, serverMetrics);
+  }
+
+  @Override
+  public StreamMetadataProvider createPartitionMetadataProvider(String clientId, int partition) {
+    return new KafkaStreamMetadataProvider(clientId, _streamConfig, partition);
+  }
+
+  @Override
+  public StreamMetadataProvider createStreamMetadataProvider(String clientId) {
+    return new KafkaStreamMetadataProvider(clientId, _streamConfig);
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaMessageBatch.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaMessageBatch.java
new file mode 100644
index 00000000000..abd801bc205
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaMessageBatch.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import java.util.ArrayList;
+import java.util.List;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.pinot.core.realtime.impl.kafka.MessageAndOffset;
+import org.apache.pinot.core.realtime.stream.MessageBatch;
+
+
+public class KafkaMessageBatch implements MessageBatch<byte[]> {
+
+  private List<MessageAndOffset> messageList = new ArrayList<>();
+
+  public KafkaMessageBatch(Iterable<ConsumerRecord<String, Bytes>> iterable) {
+    for (ConsumerRecord<String, Bytes> record : iterable) {
+      messageList.add(new MessageAndOffset(record.value().get(), record.offset()));
+    }
+  }
+
+  @Override
+  public int getMessageCount() {
+    return messageList.size();
+  }
+
+  @Override
+  public byte[] getMessageAtIndex(int index) {
+    return messageList.get(index).getMessage().array();
+  }
+
+  @Override
+  public int getMessageOffsetAtIndex(int index) {
+    return messageList.get(index).getMessage().arrayOffset();
+  }
+
+  @Override
+  public int getMessageLengthAtIndex(int index) {
+    return messageList.get(index).payloadSize();
+  }
+
+  @Override
+  public long getNextStreamMessageOffsetAtIndex(int index) {
+    return messageList.get(index).getNextOffset();
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java
new file mode 100644
index 00000000000..0a80a30f38e
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java
@@ -0,0 +1,73 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.annotations.VisibleForTesting;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Properties;
+import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.serialization.BytesDeserializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+
+
+/**
+ * KafkaPartitionLevelConnectionHandler provides low level APIs to access Kafka partition level information.
+ * E.g. partition counts, offsets per partition.
+ *
+ */
+public abstract class KafkaPartitionLevelConnectionHandler {
+
+  protected final KafkaPartitionLevelStreamConfig _config;
+  protected final String _clientId;
+  protected final int _partition;
+  protected final String _topic;
+  protected final Consumer<String, Bytes> _consumer;
+  protected final TopicPartition _topicPartition;
+
+  public KafkaPartitionLevelConnectionHandler(String clientId, StreamConfig streamConfig, int partition) {
+    _config = new KafkaPartitionLevelStreamConfig(streamConfig);
+    _clientId = clientId;
+    _partition = partition;
+    _topic = _config.getKafkaTopicName();
+    Properties consumerProp = new Properties();
+    consumerProp.putAll(streamConfig.getStreamConfigsMap());
+    consumerProp.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, _config.getBootstrapHosts());
+    consumerProp.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
+    consumerProp.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class.getName());
+    _consumer = new KafkaConsumer<>(consumerProp);
+    _topicPartition = new TopicPartition(_topic, _partition);
+    _consumer.assign(Collections.singletonList(_topicPartition));
+  }
+
+  public void close()
+      throws IOException {
+    _consumer.close();
+  }
+
+  @VisibleForTesting
+  protected KafkaPartitionLevelStreamConfig getKafkaPartitionLevelStreamConfig() {
+    return _config;
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumer.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumer.java
new file mode 100644
index 00000000000..014ff88a3f5
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumer.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.collect.Iterables;
+import java.io.IOException;
+import java.time.Duration;
+import java.util.List;
+import java.util.concurrent.TimeoutException;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.ConsumerRecords;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.pinot.core.realtime.stream.MessageBatch;
+import org.apache.pinot.core.realtime.stream.PartitionLevelConsumer;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+public class KafkaPartitionLevelConsumer extends KafkaPartitionLevelConnectionHandler implements PartitionLevelConsumer {
+  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaPartitionLevelConsumer.class);
+
+  public KafkaPartitionLevelConsumer(String clientId, StreamConfig streamConfig, int partition) {
+    super(clientId, streamConfig, partition);
+  }
+
+  @Override
+  public MessageBatch fetchMessages(long startOffset, long endOffset, int timeoutMillis)
+      throws TimeoutException {
+    _consumer.seek(_topicPartition, startOffset);
+    ConsumerRecords<String, Bytes> consumerRecords = _consumer.poll(Duration.ofMillis(timeoutMillis));
+    final Iterable<ConsumerRecord<String, Bytes>> messageAndOffsetIterable =
+        buildOffsetFilteringIterable(consumerRecords.records(_topicPartition), startOffset, endOffset);
+    return new KafkaMessageBatch(messageAndOffsetIterable);
+  }
+
+  private Iterable<ConsumerRecord<String, Bytes>> buildOffsetFilteringIterable(
+      final List<ConsumerRecord<String, Bytes>> messageAndOffsets, final long startOffset, final long endOffset) {
+    return Iterables.filter(messageAndOffsets, input -> {
+      // Filter messages that are either null or have an offset ∉ [startOffset, endOffset]
+      return input != null && input.offset() >= startOffset && (endOffset > input.offset() || endOffset == -1);
+    });
+  }
+
+  @Override
+  public void close()
+      throws IOException {
+    super.close();
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfig.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfig.java
new file mode 100644
index 00000000000..9019e912702
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfig.java
@@ -0,0 +1,147 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.base.Preconditions;
+import java.util.Map;
+import org.apache.commons.lang.StringUtils;
+import org.apache.pinot.common.utils.EqualityUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStreamConfigProperties;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+
+
+/**
+ * Wrapper around {@link StreamConfig} for use in {@link KafkaPartitionLevelConsumer}
+ */
+public class KafkaPartitionLevelStreamConfig {
+
+  private final String _kafkaTopicName;
+  private final String _bootstrapHosts;
+  private final int _kafkaBufferSize;
+  private final int _kafkaSocketTimeout;
+  private final int _kafkaFetcherSizeBytes;
+  private final int _kafkaFetcherMinBytes;
+  private final Map<String, String> _streamConfigMap;
+
+  /**
+   * Builds a wrapper around {@link StreamConfig} to fetch kafka partition level consumer related configs
+   * @param streamConfig
+   */
+  public KafkaPartitionLevelStreamConfig(StreamConfig streamConfig) {
+    _streamConfigMap = streamConfig.getStreamConfigsMap();
+
+    _kafkaTopicName = streamConfig.getTopicName();
+
+    String llcBrokerListKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BROKER_LIST);
+    String llcBufferKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE);
+    String llcTimeoutKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT);
+    String fetcherSizeKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_SIZE_BYTES);
+    String fetcherMinBytesKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_MIN_BYTES);
+    _bootstrapHosts = _streamConfigMap.get(llcBrokerListKey);
+    _kafkaBufferSize = getIntConfigWithDefault(_streamConfigMap, llcBufferKey,
+        KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT);
+    _kafkaSocketTimeout = getIntConfigWithDefault(_streamConfigMap, llcTimeoutKey,
+        KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT_DEFAULT);
+    _kafkaFetcherSizeBytes = getIntConfigWithDefault(_streamConfigMap, fetcherSizeKey, _kafkaBufferSize);
+    _kafkaFetcherMinBytes = getIntConfigWithDefault(_streamConfigMap, fetcherMinBytesKey,
+        KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_MIN_BYTES_DEFAULT);
+    Preconditions.checkNotNull(_bootstrapHosts,
+        "Must specify kafka brokers list " + llcBrokerListKey + " in case of low level kafka consumer");
+  }
+
+  public String getKafkaTopicName() {
+    return _kafkaTopicName;
+  }
+
+  public String getBootstrapHosts() {
+    return _bootstrapHosts;
+  }
+
+  public int getKafkaBufferSize() {
+    return _kafkaBufferSize;
+  }
+
+  public int getKafkaSocketTimeout() {
+    return _kafkaSocketTimeout;
+  }
+
+  public int getKafkaFetcherSizeBytes() {
+    return _kafkaFetcherSizeBytes;
+  }
+
+  public int getKafkaFetcherMinBytes() {
+    return _kafkaFetcherMinBytes;
+  }
+
+  private int getIntConfigWithDefault(Map<String, String> configMap, String key, int defaultValue) {
+    String stringValue = configMap.get(key);
+    try {
+      if (StringUtils.isNotEmpty(stringValue)) {
+        return Integer.parseInt(stringValue);
+      }
+      return defaultValue;
+    } catch (NumberFormatException ex) {
+      return defaultValue;
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "KafkaLowLevelStreamConfig{" + "_kafkaTopicName='" + _kafkaTopicName + '\'' + ", _bootstrapHosts='"
+        + _bootstrapHosts + '\'' + ", _kafkaBufferSize='" + _kafkaBufferSize + '\'' + ", _kafkaSocketTimeout='"
+        + _kafkaSocketTimeout + '\'' + ", _kafkaFetcherSizeBytes='" + _kafkaFetcherSizeBytes + '\''
+        + ", _kafkaFetcherMinBytes='" + _kafkaFetcherMinBytes + '\'' + '}';
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (EqualityUtils.isSameReference(this, o)) {
+      return true;
+    }
+
+    if (EqualityUtils.isNullOrNotSameClass(this, o)) {
+      return false;
+    }
+
+    KafkaPartitionLevelStreamConfig that = (KafkaPartitionLevelStreamConfig) o;
+
+    return EqualityUtils.isEqual(_kafkaTopicName, that._kafkaTopicName) && EqualityUtils
+        .isEqual(_bootstrapHosts, that._bootstrapHosts) && EqualityUtils
+        .isEqual(_kafkaBufferSize, that._kafkaBufferSize) && EqualityUtils
+        .isEqual(_kafkaSocketTimeout, that._kafkaSocketTimeout) && EqualityUtils
+        .isEqual(_kafkaFetcherSizeBytes, that._kafkaFetcherSizeBytes) && EqualityUtils
+        .isEqual(_kafkaFetcherMinBytes, that._kafkaFetcherMinBytes);
+  }
+
+  @Override
+  public int hashCode() {
+    int result = EqualityUtils.hashCodeOf(_kafkaTopicName);
+    result = EqualityUtils.hashCodeOf(result, _bootstrapHosts);
+    result = EqualityUtils.hashCodeOf(result, _kafkaBufferSize);
+    result = EqualityUtils.hashCodeOf(result, _kafkaSocketTimeout);
+    result = EqualityUtils.hashCodeOf(result, _kafkaFetcherSizeBytes);
+    result = EqualityUtils.hashCodeOf(result, _kafkaFetcherMinBytes);
+    return result;
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java
new file mode 100644
index 00000000000..9a0eb958909
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java
@@ -0,0 +1,179 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.yammer.metrics.core.Meter;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import org.apache.kafka.clients.consumer.ConsumerRecord;
+import org.apache.kafka.clients.consumer.ConsumerRecords;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.OffsetAndMetadata;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.utils.Bytes;
+import org.apache.pinot.common.data.Schema;
+import org.apache.pinot.common.metadata.instance.InstanceZKMetadata;
+import org.apache.pinot.common.metrics.ServerMeter;
+import org.apache.pinot.common.metrics.ServerMetrics;
+import org.apache.pinot.core.data.GenericRow;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.apache.pinot.core.realtime.stream.StreamDecoderProvider;
+import org.apache.pinot.core.realtime.stream.StreamLevelConsumer;
+import org.apache.pinot.core.realtime.stream.StreamMessageDecoder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ * An implementation of a {@link StreamLevelConsumer} which consumes from the kafka stream
+ */
+public class KafkaStreamLevelConsumer implements StreamLevelConsumer {
+
+  private StreamMessageDecoder _messageDecoder;
+  private Logger INSTANCE_LOGGER;
+
+  private String _clientId;
+  private String _tableAndStreamName;
+
+  private StreamConfig _streamConfig;
+  private KafkaStreamLevelStreamConfig _kafkaStreamLevelStreamConfig;
+
+  private KafkaConsumer<Bytes, Bytes> consumer;
+  private ConsumerRecords<Bytes, Bytes> consumerRecords;
+  private Iterator<ConsumerRecord<Bytes, Bytes>> kafkaIterator;
+  private Map<Integer, Long> consumerOffsets = new HashMap<>(); // tracking current consumed records offsets.
+
+  private long lastLogTime = 0;
+  private long lastCount = 0;
+  private long currentCount = 0L;
+
+  private ServerMetrics _serverMetrics;
+  private Meter tableAndStreamRowsConsumed = null;
+  private Meter tableRowsConsumed = null;
+
+  public KafkaStreamLevelConsumer(String clientId, String tableName, StreamConfig streamConfig, Schema schema,
+      InstanceZKMetadata instanceZKMetadata, ServerMetrics serverMetrics) {
+    _clientId = clientId;
+    _streamConfig = streamConfig;
+    _kafkaStreamLevelStreamConfig = new KafkaStreamLevelStreamConfig(streamConfig, tableName, instanceZKMetadata);
+    _serverMetrics = serverMetrics;
+
+    _messageDecoder = StreamDecoderProvider.create(streamConfig, schema);
+
+    _tableAndStreamName = tableName + "-" + streamConfig.getTopicName();
+    INSTANCE_LOGGER = LoggerFactory
+        .getLogger(KafkaStreamLevelConsumer.class.getName() + "_" + tableName + "_" + streamConfig.getTopicName());
+    INSTANCE_LOGGER.info("KafkaStreamLevelConsumer: streamConfig : {}", _streamConfig);
+  }
+
+  @Override
+  public void start()
+      throws Exception {
+    consumer = KafkaStreamLevelConsumerManager.acquireKafkaConsumerForConfig(_kafkaStreamLevelStreamConfig);
+  }
+
+  private void updateKafkaIterator() {
+    consumerRecords = consumer.poll(Duration.ofMillis(_streamConfig.getFetchTimeoutMillis()));
+    kafkaIterator = consumerRecords.iterator();
+  }
+
+  private void resetOffsets() {
+    for (int partition : consumerOffsets.keySet()) {
+      long offsetToSeek = consumerOffsets.get(partition);
+      consumer.seek(new TopicPartition(_streamConfig.getTopicName(), partition), offsetToSeek);
+    }
+  }
+
+  @Override
+  public GenericRow next(GenericRow destination) {
+    if (kafkaIterator == null || !kafkaIterator.hasNext()) {
+      updateKafkaIterator();
+    }
+    if (kafkaIterator.hasNext()) {
+      try {
+        final ConsumerRecord<Bytes, Bytes> record = kafkaIterator.next();
+        updateOffsets(record.partition(), record.offset());
+        destination = _messageDecoder.decode(record.value().get(), destination);
+        tableAndStreamRowsConsumed = _serverMetrics
+            .addMeteredTableValue(_tableAndStreamName, ServerMeter.REALTIME_ROWS_CONSUMED, 1L,
+                tableAndStreamRowsConsumed);
+        tableRowsConsumed =
+            _serverMetrics.addMeteredGlobalValue(ServerMeter.REALTIME_ROWS_CONSUMED, 1L, tableRowsConsumed);
+
+        ++currentCount;
+
+        final long now = System.currentTimeMillis();
+        // Log every minute or 100k events
+        if (now - lastLogTime > 60000 || currentCount - lastCount >= 100000) {
+          if (lastCount == 0) {
+            INSTANCE_LOGGER.info("Consumed {} events from kafka stream {}", currentCount, _streamConfig.getTopicName());
+          } else {
+            INSTANCE_LOGGER.info("Consumed {} events from kafka stream {} (rate:{}/s)", currentCount - lastCount,
+                _streamConfig.getTopicName(), (float) (currentCount - lastCount) * 1000 / (now - lastLogTime));
+          }
+          lastCount = currentCount;
+          lastLogTime = now;
+        }
+        return destination;
+      } catch (Exception e) {
+        INSTANCE_LOGGER.warn("Caught exception while consuming events", e);
+        _serverMetrics.addMeteredTableValue(_tableAndStreamName, ServerMeter.REALTIME_CONSUMPTION_EXCEPTIONS, 1L);
+        _serverMetrics.addMeteredGlobalValue(ServerMeter.REALTIME_CONSUMPTION_EXCEPTIONS, 1L);
+        throw e;
+      }
+    }
+    return null;
+  }
+
+  private void updateOffsets(int partition, long offset) {
+    consumerOffsets.put(partition, offset + 1);
+  }
+
+  @Override
+  public void commit() {
+    consumer.commitSync(getOffsetsMap());
+    // Since the lastest batch may not be consumed fully, so we need to reset kafka consumer's offset.
+    resetOffsets();
+    consumerOffsets.clear();
+    _serverMetrics.addMeteredTableValue(_tableAndStreamName, ServerMeter.REALTIME_OFFSET_COMMITS, 1L);
+    _serverMetrics.addMeteredGlobalValue(ServerMeter.REALTIME_OFFSET_COMMITS, 1L);
+  }
+
+  private Map<TopicPartition, OffsetAndMetadata> getOffsetsMap() {
+    Map<TopicPartition, OffsetAndMetadata> offsetsMap = new HashMap<>();
+    for (Integer partition : consumerOffsets.keySet()) {
+      offsetsMap.put(new TopicPartition(_streamConfig.getTopicName(), partition),
+          new OffsetAndMetadata(consumerOffsets.get(partition)));
+    }
+    return offsetsMap;
+  }
+
+  @Override
+  public void shutdown()
+      throws Exception {
+    if (consumer != null) {
+      // If offsets commit is not succeed, then reset the offsets here.
+      resetOffsets();
+      KafkaStreamLevelConsumerManager.releaseKafkaConsumer(consumer);
+      consumer = null;
+    }
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java
new file mode 100644
index 00000000000..9df8d336634
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java
@@ -0,0 +1,194 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.util.concurrent.Uninterruptibles;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Properties;
+import java.util.concurrent.TimeUnit;
+import org.apache.commons.lang3.tuple.ImmutableTriple;
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.common.serialization.BytesDeserializer;
+import org.apache.kafka.common.serialization.StringDeserializer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+/**
+ * Manager for Kafka consumers that reuses consumers and delays their shutdown.
+ *
+ * This is a workaround for the current realtime design flaw where any issue while flushing/committing offsets causes
+ * duplicate or dropped events. Kafka consumption is driven by the controller, which assigns a realtime segment to the
+ * servers; when a server is assigned a new realtime segment, it creates a Kafka consumer, consumes until it reaches a
+ * threshold then flushes to disk, writes metadata to helix indicating the segment is completed, commits Kafka offsets
+ * to ZK and then shuts down the consumer. The controller notices the metadata write and reassigns a segment to the
+ * server, so that it can keep on consuming.
+ *
+ * This logic is flawed if committing Kafka offsets fails, at which time the committed state is unknown. The proper fix
+ * would be to just keep on using that consumer and try committing our offsets later, but we recreate a new Kafka
+ * consumer whenever we get a new segment and also keep the old consumer around, leading to half the events being
+ * assigned, due to Kafka rebalancing the partitions between the two consumers (one of which is not actually reading
+ * anything anymore). Because that logic is stateless and driven by Helix, there's no real clean way to keep the
+ * consumer alive and pass it to the next segment.
+ *
+ * This class and long comment is to work around this issue by keeping the consumer alive for a little bit instead of
+ * shutting it down immediately, so that the next segment assignment can pick up the same consumer. This way, even if
+ * committing the offsets fails, we can still pick up the same consumer the next time we get a segment assigned to us
+ * by the controller and hopefully commit our offsets the next time we flush to disk.
+ *
+ * This temporary code should be completely removed by the time we redesign the consumption to use the lower level
+ * Kafka APIs.
+ */
+public class KafkaStreamLevelConsumerManager {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaStreamLevelConsumerManager.class);
+  private static final Long IN_USE = -1L;
+  private static final long CONSUMER_SHUTDOWN_DELAY_MILLIS = TimeUnit.SECONDS.toMillis(60); // One minute
+  private static final Map<ImmutableTriple<String, String, String>, KafkaConsumer> CONSUMER_FOR_CONFIG_KEY =
+      new HashMap<>();
+  private static final IdentityHashMap<KafkaConsumer, Long> CONSUMER_RELEASE_TIME = new IdentityHashMap<>();
+
+  public static KafkaConsumer acquireKafkaConsumerForConfig(KafkaStreamLevelStreamConfig kafkaStreamLevelStreamConfig) {
+    final ImmutableTriple<String, String, String> configKey =
+        new ImmutableTriple<>(kafkaStreamLevelStreamConfig.getKafkaTopicName(), kafkaStreamLevelStreamConfig.getGroupId(),
+            kafkaStreamLevelStreamConfig.getBootstrapServers());
+
+    synchronized (KafkaStreamLevelConsumerManager.class) {
+      // If we have the consumer and it's not already acquired, return it, otherwise error out if it's already acquired
+      if (CONSUMER_FOR_CONFIG_KEY.containsKey(configKey)) {
+        KafkaConsumer kafkaConsumer = CONSUMER_FOR_CONFIG_KEY.get(configKey);
+        if (CONSUMER_RELEASE_TIME.get(kafkaConsumer).equals(IN_USE)) {
+          throw new RuntimeException("Consumer " + kafkaConsumer + " already in use!");
+        } else {
+          LOGGER.info("Reusing kafka consumer with id {}", kafkaConsumer);
+          CONSUMER_RELEASE_TIME.put(kafkaConsumer, IN_USE);
+          return kafkaConsumer;
+        }
+      }
+
+      LOGGER.info("Creating new kafka consumer and iterator for topic {}",
+          kafkaStreamLevelStreamConfig.getKafkaTopicName());
+
+      // Create the consumer
+
+      Properties consumerProp = new Properties();
+      consumerProp.putAll(kafkaStreamLevelStreamConfig.getKafkaConsumerProperties());
+      consumerProp.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaStreamLevelStreamConfig.getBootstrapServers());
+      consumerProp.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
+      consumerProp.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class.getName());
+      if (consumerProp.containsKey(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG) && consumerProp
+          .getProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).equals("smallest")) {
+        consumerProp.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
+      }
+      KafkaConsumer consumer = new KafkaConsumer<>(consumerProp);
+      consumer.subscribe(Collections.singletonList(kafkaStreamLevelStreamConfig.getKafkaTopicName()));
+
+      // Mark both the consumer and iterator as acquired
+      CONSUMER_FOR_CONFIG_KEY.put(configKey, consumer);
+      CONSUMER_RELEASE_TIME.put(consumer, IN_USE);
+
+      LOGGER.info("Created consumer with id {} for topic {}", consumer, kafkaStreamLevelStreamConfig.getKafkaTopicName());
+
+      return consumer;
+    }
+  }
+
+  public static void releaseKafkaConsumer(final KafkaConsumer kafkaConsumer) {
+    synchronized (KafkaStreamLevelConsumerManager.class) {
+      // Release the consumer, mark it for shutdown in the future
+      final long releaseTime = System.currentTimeMillis() + CONSUMER_SHUTDOWN_DELAY_MILLIS;
+      CONSUMER_RELEASE_TIME.put(kafkaConsumer, releaseTime);
+
+      LOGGER.info("Marking consumer with id {} for release at {}", kafkaConsumer, releaseTime);
+
+      // Schedule the shutdown of the consumer
+      new Thread() {
+        @Override
+        public void run() {
+          try {
+            // Await the shutdown time
+            Uninterruptibles.sleepUninterruptibly(CONSUMER_SHUTDOWN_DELAY_MILLIS, TimeUnit.MILLISECONDS);
+
+            // Shutdown all consumers that have not been re-acquired
+            synchronized (KafkaStreamLevelConsumerManager.class) {
+              LOGGER.info("Executing release check for consumer {} at {}, scheduled at {}", kafkaConsumer,
+                  System.currentTimeMillis(), releaseTime);
+
+              Iterator<Map.Entry<ImmutableTriple<String, String, String>, KafkaConsumer>> configIterator =
+                  CONSUMER_FOR_CONFIG_KEY.entrySet().iterator();
+
+              while (configIterator.hasNext()) {
+                Map.Entry<ImmutableTriple<String, String, String>, KafkaConsumer> entry = configIterator.next();
+                KafkaConsumer kafkaConsumer = entry.getValue();
+
+                final Long releaseTime = CONSUMER_RELEASE_TIME.get(kafkaConsumer);
+                if (!releaseTime.equals(IN_USE) && releaseTime < System.currentTimeMillis()) {
+                  LOGGER.info("Releasing consumer {}", kafkaConsumer);
+
+                  try {
+                    kafkaConsumer.close();
+                  } catch (Exception e) {
+                    LOGGER.warn("Caught exception while shutting down Kafka consumer with id {}", kafkaConsumer, e);
+                  }
+
+                  configIterator.remove();
+                  CONSUMER_RELEASE_TIME.remove(kafkaConsumer);
+                } else {
+                  LOGGER.info("Not releasing consumer {}, it has been reacquired", kafkaConsumer);
+                }
+              }
+            }
+          } catch (Exception e) {
+            LOGGER.warn("Caught exception in release of consumer {}", kafkaConsumer, e);
+          }
+        }
+      }.start();
+    }
+  }
+
+  public static void closeAllConsumers() {
+    try {
+      // Shutdown all consumers
+      synchronized (KafkaStreamLevelConsumerManager.class) {
+        LOGGER.info("Trying to shutdown all the kafka consumers");
+        Iterator<KafkaConsumer> consumerIterator = CONSUMER_FOR_CONFIG_KEY.values().iterator();
+
+        while (consumerIterator.hasNext()) {
+          KafkaConsumer kafkaConsumer = consumerIterator.next();
+          LOGGER.info("Trying to shutdown consumer {}", kafkaConsumer);
+          try {
+            kafkaConsumer.close();
+          } catch (Exception e) {
+            LOGGER.warn("Caught exception while shutting down Kafka consumer with id {}", kafkaConsumer, e);
+          }
+          consumerIterator.remove();
+        }
+        CONSUMER_FOR_CONFIG_KEY.clear();
+        CONSUMER_RELEASE_TIME.clear();
+      }
+    } catch (Exception e) {
+      LOGGER.warn("Caught exception during shutting down all kafka consumers", e);
+    }
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java
new file mode 100644
index 00000000000..062fa326454
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java
@@ -0,0 +1,136 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.base.Preconditions;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Properties;
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.pinot.common.metadata.instance.InstanceZKMetadata;
+import org.apache.pinot.common.utils.EqualityUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStreamConfigProperties;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.apache.pinot.core.realtime.stream.StreamConfigProperties;
+
+
+/**
+ * Wrapper around {@link StreamConfig} for use in the {@link KafkaStreamLevelConsumer}
+ */
+public class KafkaStreamLevelStreamConfig {
+  private static final String DEFAULT_AUTO_COMMIT_ENABLE = "false";
+
+  private static final Map<String, String> defaultProps;
+  private String _kafkaTopicName;
+  private String _groupId;
+  private String _bootstrapServers;
+  private Map<String, String> _kafkaConsumerProperties;
+
+  /**
+   * Builds a wrapper around {@link StreamConfig} to fetch kafka stream level consumer specific configs
+   * @param streamConfig
+   * @param tableName
+   * @param instanceZKMetadata
+   */
+  public KafkaStreamLevelStreamConfig(StreamConfig streamConfig, String tableName,
+      InstanceZKMetadata instanceZKMetadata) {
+    Map<String, String> streamConfigMap = streamConfig.getStreamConfigsMap();
+
+    _kafkaTopicName = streamConfig.getTopicName();
+    String hlcBootstrapBrokerUrlKey = KafkaStreamConfigProperties
+        .constructStreamProperty(KafkaStreamConfigProperties.HighLevelConsumer.KAFKA_HLC_BOOTSTRAP_SERVER);
+    _bootstrapServers = streamConfigMap.get(hlcBootstrapBrokerUrlKey);
+    Preconditions.checkNotNull(_bootstrapServers,
+        "Must specify bootstrap broker connect string " + hlcBootstrapBrokerUrlKey + " in high level kafka consumer");
+    _groupId = instanceZKMetadata.getGroupId(tableName);
+
+    _kafkaConsumerProperties = new HashMap<>();
+    String kafkaConsumerPropertyPrefix =
+        KafkaStreamConfigProperties.constructStreamProperty(KafkaStreamConfigProperties.KAFKA_CONSUMER_PROP_PREFIX);
+    for (String key : streamConfigMap.keySet()) {
+      if (key.startsWith(kafkaConsumerPropertyPrefix)) {
+        _kafkaConsumerProperties
+            .put(StreamConfigProperties.getPropertySuffix(key, kafkaConsumerPropertyPrefix), streamConfigMap.get(key));
+      }
+    }
+  }
+
+  public String getKafkaTopicName() {
+    return _kafkaTopicName;
+  }
+
+  public String getGroupId() {
+    return _groupId;
+  }
+
+  public Properties getKafkaConsumerProperties() {
+    Properties props = new Properties();
+    for (String key : defaultProps.keySet()) {
+      props.put(key, defaultProps.get(key));
+    }
+    for (String key : _kafkaConsumerProperties.keySet()) {
+      props.put(key, _kafkaConsumerProperties.get(key));
+    }
+    props.put("group.id", _groupId);
+    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, _bootstrapServers);
+    return props;
+  }
+
+  @Override
+  public String toString() {
+    return "KafkaStreamLevelStreamConfig{" + "_kafkaTopicName='" + _kafkaTopicName + '\'' + ", _groupId='" + _groupId
+        + '\'' + ", _bootstrapServers='" + _bootstrapServers + '\'' + ", _kafkaConsumerProperties="
+        + _kafkaConsumerProperties + '}';
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (EqualityUtils.isSameReference(this, o)) {
+      return true;
+    }
+
+    if (EqualityUtils.isNullOrNotSameClass(this, o)) {
+      return false;
+    }
+
+    KafkaStreamLevelStreamConfig that = (KafkaStreamLevelStreamConfig) o;
+
+    return EqualityUtils.isEqual(_kafkaTopicName, that._kafkaTopicName) && EqualityUtils
+        .isEqual(_groupId, that._groupId) && EqualityUtils.isEqual(_bootstrapServers, that._bootstrapServers)
+        && EqualityUtils.isEqual(_kafkaConsumerProperties, that._kafkaConsumerProperties);
+  }
+
+  @Override
+  public int hashCode() {
+    int result = EqualityUtils.hashCodeOf(_kafkaTopicName);
+    result = EqualityUtils.hashCodeOf(result, _groupId);
+    result = EqualityUtils.hashCodeOf(result, _bootstrapServers);
+    result = EqualityUtils.hashCodeOf(result, _kafkaConsumerProperties);
+    return result;
+  }
+
+  public String getBootstrapServers() {
+    return _bootstrapServers;
+  }
+
+  static {
+    defaultProps = new HashMap<>();
+    defaultProps.put(KafkaStreamConfigProperties.HighLevelConsumer.AUTO_COMMIT_ENABLE, DEFAULT_AUTO_COMMIT_ENABLE);
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamMetadataProvider.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamMetadataProvider.java
new file mode 100644
index 00000000000..7d106ed8c0f
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamMetadataProvider.java
@@ -0,0 +1,67 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import com.google.common.base.Preconditions;
+import java.io.IOException;
+import java.time.Duration;
+import java.util.Collections;
+import java.util.concurrent.TimeoutException;
+import javax.annotation.Nonnull;
+import org.apache.pinot.core.realtime.stream.OffsetCriteria;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.apache.pinot.core.realtime.stream.StreamMetadataProvider;
+
+
+public class KafkaStreamMetadataProvider extends KafkaPartitionLevelConnectionHandler implements StreamMetadataProvider {
+
+  public KafkaStreamMetadataProvider(String clientId, StreamConfig streamConfig) {
+    this(clientId, streamConfig, Integer.MIN_VALUE);
+  }
+
+  public KafkaStreamMetadataProvider(String clientId, StreamConfig streamConfig, int partition) {
+    super(clientId, streamConfig, partition);
+  }
+
+  @Override
+  public int fetchPartitionCount(long timeoutMillis) {
+    return _consumer.partitionsFor(_topic, Duration.ofMillis(timeoutMillis)).size();
+  }
+
+  @Override
+  public long fetchPartitionOffset(@Nonnull OffsetCriteria offsetCriteria, long timeoutMillis)
+      throws TimeoutException {
+    Preconditions.checkNotNull(offsetCriteria);
+    if (offsetCriteria.isLargest()) {
+      return _consumer.endOffsets(Collections.singletonList(_topicPartition), Duration.ofMillis(timeoutMillis))
+          .get(_topicPartition);
+    } else if (offsetCriteria.isSmallest()) {
+      return _consumer.beginningOffsets(Collections.singletonList(_topicPartition), Duration.ofMillis(timeoutMillis))
+          .get(_topicPartition);
+    } else {
+      throw new IllegalArgumentException("Unknown initial offset value " + offsetCriteria.toString());
+    }
+  }
+
+  @Override
+  public void close()
+      throws IOException {
+    super.close();
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataProducer.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataProducer.java
new file mode 100644
index 00000000000..eebd89729ba
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataProducer.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2.server;
+
+import java.util.Properties;
+import org.apache.kafka.clients.producer.KafkaProducer;
+import org.apache.kafka.clients.producer.Producer;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.pinot.core.realtime.stream.StreamDataProducer;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+public class KafkaDataProducer implements StreamDataProducer {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaDataProducer.class);
+  private Producer<byte[], byte[]> producer;
+
+  @Override
+  public void init(Properties props) {
+    if (!props.containsKey("bootstrap.servers")) {
+      props.put("bootstrap.servers", props.get("metadata.broker.list"));
+    }
+    if (!props.containsKey("key.serializer")) {
+      props.put("key.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
+    }
+    if (!props.containsKey("value.serializer")) {
+      props.put("value.serializer", "org.apache.kafka.common.serialization.ByteArraySerializer");
+    }
+    if (props.containsKey("partitioner.class")) {
+      props.remove("partitioner.class");
+    }
+    props.remove("metadata.broker.list");
+    props.remove("request.required.acks");
+    props.remove("serializer.class");
+    try {
+      this.producer = new KafkaProducer<>(props);
+    } catch (Exception e) {
+      LOGGER.error("Failed to create a Kafka 2 Producer.", e);
+    }
+  }
+
+  @Override
+  public void produce(String topic, byte[] payload) {
+    ProducerRecord<byte[], byte[]> record = new ProducerRecord(topic, payload);
+    producer.send(record);
+    producer.flush();
+  }
+
+  @Override
+  public void produce(String topic, byte[] key, byte[] payload) {
+    ProducerRecord<byte[], byte[]> record = new ProducerRecord(topic, key, payload);
+    producer.send(record);
+    producer.flush();
+  }
+
+  @Override
+  public void close() {
+    producer.close();
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataServerStartable.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataServerStartable.java
new file mode 100644
index 00000000000..c666b960623
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/server/KafkaDataServerStartable.java
@@ -0,0 +1,98 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2.server;
+
+import java.io.File;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Properties;
+import java.util.UUID;
+import kafka.server.KafkaConfig;
+import kafka.server.KafkaServerStartable;
+import org.I0Itec.zkclient.ZkClient;
+import org.apache.commons.io.FileUtils;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.AdminClientConfig;
+import org.apache.kafka.clients.admin.KafkaAdminClient;
+import org.apache.kafka.clients.admin.NewTopic;
+import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+public class KafkaDataServerStartable implements StreamDataServerStartable {
+  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaDataServerStartable.class);
+
+  private static final String ZOOKEEPER_CONNECT = "zookeeper.connect";
+  private static final String LOG_DIRS = "log.dirs";
+  private static final String PORT = "port";
+
+  private KafkaServerStartable serverStartable;
+  private int port;
+  private String zkStr;
+  private String logDirPath;
+  private AdminClient adminClient;
+
+  public void init(Properties props) {
+    port = (int) props.get(PORT);
+    zkStr = props.getProperty(ZOOKEEPER_CONNECT);
+    logDirPath = props.getProperty(LOG_DIRS);
+
+    // Create the ZK nodes for Kafka, if needed
+    int indexOfFirstSlash = zkStr.indexOf('/');
+    if (indexOfFirstSlash != -1) {
+      String bareZkUrl = zkStr.substring(0, indexOfFirstSlash);
+      String zkNodePath = zkStr.substring(indexOfFirstSlash);
+      ZkClient client = new ZkClient(bareZkUrl);
+      client.createPersistent(zkNodePath, true);
+      client.close();
+    }
+
+    File logDir = new File(logDirPath);
+    logDir.mkdirs();
+
+    props.put("zookeeper.session.timeout.ms", "60000");
+    serverStartable = new KafkaServerStartable(new KafkaConfig(props));
+    final Map<String, Object> config = new HashMap<>();
+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:" + port);
+    config.put(AdminClientConfig.CLIENT_ID_CONFIG, "Kafka2AdminClient-" + UUID.randomUUID().toString());
+    config.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 15000);
+    adminClient = KafkaAdminClient.create(config);
+  }
+
+  @Override
+  public void start() {
+    serverStartable.startup();
+  }
+
+  @Override
+  public void stop() {
+    serverStartable.shutdown();
+    FileUtils.deleteQuietly(new File(serverStartable.staticServerConfig().logDirs().apply(0)));
+  }
+
+  @Override
+  public void createTopic(String topic, Properties props) {
+    int partition = (Integer) props.get("partition");
+    Collection<NewTopic> topicList = Arrays.asList(new NewTopic(topic, partition, (short) 1));
+    adminClient.createTopics(topicList);
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory b/pinot-connectors/pinot-connector-kafka-2.0/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory
new file mode 100644
index 00000000000..ef64dd48cc2
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/resources/META-INF/services/org.apache.pinot.core.realtime.stream.StreamConsumerFactory
@@ -0,0 +1,19 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory
\ No newline at end of file
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumerTest.java b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumerTest.java
new file mode 100644
index 00000000000..6d9055821ca
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConsumerTest.java
@@ -0,0 +1,293 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Properties;
+import java.util.concurrent.TimeoutException;
+import org.apache.kafka.clients.producer.KafkaProducer;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.clients.producer.ProducerRecord;
+import org.apache.kafka.common.serialization.StringSerializer;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStreamConfigProperties;
+import org.apache.pinot.core.realtime.impl.kafka2.utils.MiniKafkaCluster;
+import org.apache.pinot.core.realtime.stream.MessageBatch;
+import org.apache.pinot.core.realtime.stream.OffsetCriteria;
+import org.apache.pinot.core.realtime.stream.PartitionLevelConsumer;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.apache.pinot.core.realtime.stream.StreamConsumerFactory;
+import org.apache.pinot.core.realtime.stream.StreamConsumerFactoryProvider;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.testng.Assert;
+import org.testng.annotations.AfterClass;
+import org.testng.annotations.BeforeClass;
+import org.testng.annotations.Test;
+
+
+/**
+ * Tests for the KafkaPartitionLevelConsumer.
+ */
+public class KafkaPartitionLevelConsumerTest {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(KafkaPartitionLevelConsumerTest.class);
+  private static final long STABILIZE_SLEEP_DELAYS = 3000;
+  private static final String TEST_TOPIC_1 = "foo";
+  private static final String TEST_TOPIC_2 = "bar";
+  private static final int NUM_MSG_PRODUCED_PER_PARTITION = 1000;
+
+  private static MiniKafkaCluster kafkaCluster;
+  private static String brokerAddress;
+
+  @BeforeClass
+  public static void setup()
+      throws Exception {
+    kafkaCluster = new MiniKafkaCluster.Builder().newServer("0").build();
+    LOGGER.info("Trying to start MiniKafkaCluster");
+    kafkaCluster.start();
+    brokerAddress = getKakfaBroker();
+    kafkaCluster.createTopic(TEST_TOPIC_1, 1, 1);
+    kafkaCluster.createTopic(TEST_TOPIC_2, 2, 1);
+    Thread.sleep(STABILIZE_SLEEP_DELAYS);
+    produceMsgToKafka();
+    Thread.sleep(STABILIZE_SLEEP_DELAYS);
+  }
+
+  private static void produceMsgToKafka() {
+    Properties props = new Properties();
+    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, getKakfaBroker());
+    props.put(ProducerConfig.CLIENT_ID_CONFIG, "clientId");
+    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
+    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
+    KafkaProducer p = new KafkaProducer<>(props);
+    for (int i = 0; i < NUM_MSG_PRODUCED_PER_PARTITION; i++) {
+      p.send(new ProducerRecord(TEST_TOPIC_1, "sample_msg_" + i));
+      // TEST_TOPIC_2 has 2 partitions
+      p.send(new ProducerRecord(TEST_TOPIC_2, "sample_msg_" + i));
+      p.send(new ProducerRecord(TEST_TOPIC_2, "sample_msg_" + i));
+    }
+  }
+
+  private static String getKakfaBroker() {
+    return "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+  }
+
+  @AfterClass
+  public static void shutDown()
+      throws Exception {
+    kafkaCluster.deleteTopic(TEST_TOPIC_1);
+    kafkaCluster.deleteTopic(TEST_TOPIC_2);
+    kafkaCluster.close();
+  }
+
+  @Test
+  public void testBuildConsumer()
+      throws Exception {
+    String streamType = "kafka";
+    String streamKafkaTopicName = "theTopic";
+    String streamKafkaBrokerList = "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+    String streamKafkaConsumerType = "simple";
+    String clientId = "clientId";
+
+    Map<String, String> streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", streamKafkaTopicName);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    streamConfigMap.put("stream.kafka.fetcher.size", "10000");
+    streamConfigMap.put("stream.kafka.fetcher.minBytes", "20000");
+    StreamConfig streamConfig = new StreamConfig(streamConfigMap);
+
+    KafkaStreamMetadataProvider streamMetadataProvider =
+        new KafkaStreamMetadataProvider(clientId, streamConfig);
+
+    // test default value
+    KafkaPartitionLevelConsumer kafkaSimpleStreamConsumer =
+        new KafkaPartitionLevelConsumer(clientId, streamConfig, 0);
+    kafkaSimpleStreamConsumer.fetchMessages(12345L, 23456L, 10000);
+
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT,
+        kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaBufferSize());
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT_DEFAULT,
+        kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaSocketTimeout());
+
+    // test parsing values
+    Assert.assertEquals(10000,
+        kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaFetcherSizeBytes());
+    Assert
+        .assertEquals(20000, kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaFetcherMinBytes());
+
+    // test user defined values
+    streamConfigMap.put("stream.kafka.buffer.size", "100");
+    streamConfigMap.put("stream.kafka.socket.timeout", "1000");
+    streamConfig = new StreamConfig(streamConfigMap);
+    kafkaSimpleStreamConsumer = new KafkaPartitionLevelConsumer(clientId, streamConfig, 0);
+    kafkaSimpleStreamConsumer.fetchMessages(12345L, 23456L, 10000);
+    Assert.assertEquals(100, kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaBufferSize());
+    Assert.assertEquals(1000, kafkaSimpleStreamConsumer.getKafkaPartitionLevelStreamConfig().getKafkaSocketTimeout());
+  }
+
+  @Test
+  public void testGetPartitionCount() {
+    String streamType = "kafka";
+    String streamKafkaBrokerList = "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+    String streamKafkaConsumerType = "simple";
+    String clientId = "clientId";
+
+    Map<String, String> streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", TEST_TOPIC_1);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    StreamConfig streamConfig = new StreamConfig(streamConfigMap);
+
+    KafkaStreamMetadataProvider streamMetadataProvider =
+        new KafkaStreamMetadataProvider(clientId, streamConfig);
+    Assert.assertEquals(streamMetadataProvider.fetchPartitionCount(10000L), 1);
+
+    streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", TEST_TOPIC_2);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    streamConfig = new StreamConfig(streamConfigMap);
+
+    streamMetadataProvider = new KafkaStreamMetadataProvider(clientId, streamConfig);
+    Assert.assertEquals(streamMetadataProvider.fetchPartitionCount(10000L), 2);
+  }
+
+  @Test
+  public void testFetchMessages()
+      throws Exception {
+    String streamType = "kafka";
+    String streamKafkaTopicName = "theTopic";
+    String streamKafkaBrokerList = "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+    String streamKafkaConsumerType = "simple";
+    String clientId = "clientId";
+
+    Map<String, String> streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", streamKafkaTopicName);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    StreamConfig streamConfig = new StreamConfig(streamConfigMap);
+
+    int partition = 0;
+    KafkaPartitionLevelConsumer kafkaSimpleStreamConsumer =
+        new KafkaPartitionLevelConsumer(clientId, streamConfig, partition);
+    kafkaSimpleStreamConsumer.fetchMessages(12345L, 23456L, 10000);
+  }
+
+  @Test
+  public void testFetchOffsets()
+      throws Exception {
+    testFetchOffsets(TEST_TOPIC_1);
+    testFetchOffsets(TEST_TOPIC_2);
+  }
+
+  private void testFetchOffsets(String topic)
+      throws Exception {
+    String streamType = "kafka";
+    String streamKafkaBrokerList = "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+    String streamKafkaConsumerType = "simple";
+    String clientId = "clientId";
+
+    Map<String, String> streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", topic);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    StreamConfig streamConfig = new StreamConfig(streamConfigMap);
+
+    int numPartitions =
+        new KafkaStreamMetadataProvider(clientId, streamConfig).fetchPartitionCount(10000);
+    for (int partition = 0; partition < numPartitions; partition++) {
+      KafkaStreamMetadataProvider kafkaStreamMetadataProvider =
+          new KafkaStreamMetadataProvider(clientId, streamConfig, partition);
+      Assert.assertEquals(0, kafkaStreamMetadataProvider
+          .fetchPartitionOffset(new OffsetCriteria.OffsetCriteriaBuilder().withOffsetSmallest(), 10000));
+      Assert.assertEquals(NUM_MSG_PRODUCED_PER_PARTITION, kafkaStreamMetadataProvider
+          .fetchPartitionOffset(new OffsetCriteria.OffsetCriteriaBuilder().withOffsetLargest(), 10000));
+    }
+  }
+
+  @Test
+  public void testConsumer()
+      throws Exception {
+    testConsumer(TEST_TOPIC_1);
+    testConsumer(TEST_TOPIC_2);
+  }
+
+  private void testConsumer(String topic)
+      throws TimeoutException {
+    String streamType = "kafka";
+    String streamKafkaBrokerList = "127.0.0.1:" + kafkaCluster.getKafkaServerPort(0);
+    String streamKafkaConsumerType = "simple";
+    String clientId = "clientId";
+
+    Map<String, String> streamConfigMap = new HashMap<>();
+    streamConfigMap.put("streamType", streamType);
+    streamConfigMap.put("stream.kafka.topic.name", topic);
+    streamConfigMap.put("stream.kafka.broker.list", streamKafkaBrokerList);
+    streamConfigMap.put("stream.kafka.consumer.type", streamKafkaConsumerType);
+    streamConfigMap.put("stream.kafka.consumer.factory.class.name", KafkaConsumerFactory.class.getName());
+    streamConfigMap.put("stream.kafka.decoder.class.name", "decoderClass");
+    StreamConfig streamConfig = new StreamConfig(streamConfigMap);
+
+    final StreamConsumerFactory streamConsumerFactory = StreamConsumerFactoryProvider.create(streamConfig);
+    int numPartitions =
+        new KafkaStreamMetadataProvider(clientId, streamConfig).fetchPartitionCount(10000);
+    for (int partition = 0; partition < numPartitions; partition++) {
+      final PartitionLevelConsumer consumer = streamConsumerFactory.createPartitionLevelConsumer(clientId, partition);
+
+      // Test consume a large batch, only 500 records will be returned.
+      final MessageBatch batch1 = consumer.fetchMessages(0, NUM_MSG_PRODUCED_PER_PARTITION, 10000);
+      Assert.assertEquals(batch1.getMessageCount(), 500);
+      for (int i = 0; i < batch1.getMessageCount(); i++) {
+        final byte[] msg = (byte[]) batch1.getMessageAtIndex(i);
+        Assert.assertEquals(new String(msg), "sample_msg_" + i);
+      }
+      // Test second half batch
+      final MessageBatch batch2 = consumer.fetchMessages(500, NUM_MSG_PRODUCED_PER_PARTITION, 10000);
+      Assert.assertEquals(batch2.getMessageCount(), 500);
+      for (int i = 0; i < batch2.getMessageCount(); i++) {
+        final byte[] msg = (byte[]) batch2.getMessageAtIndex(i);
+        Assert.assertEquals(new String(msg), "sample_msg_" + (500 + i));
+      }
+      // Some random range
+      final MessageBatch batch3 = consumer.fetchMessages(10, 35, 10000);
+      Assert.assertEquals(batch3.getMessageCount(), 25);
+      for (int i = 0; i < batch3.getMessageCount(); i++) {
+        final byte[] msg = (byte[]) batch3.getMessageAtIndex(i);
+        Assert.assertEquals(new String(msg), "sample_msg_" + (10 + i));
+      }
+    }
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfigTest.java b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfigTest.java
new file mode 100644
index 00000000000..f3b2d375036
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelStreamConfigTest.java
@@ -0,0 +1,162 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2;
+
+import java.util.HashMap;
+import java.util.Map;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStreamConfigProperties;
+import org.apache.pinot.core.realtime.stream.StreamConfig;
+import org.apache.pinot.core.realtime.stream.StreamConfigProperties;
+import org.testng.Assert;
+import org.testng.annotations.Test;
+
+
+public class KafkaPartitionLevelStreamConfigTest {
+
+  private KafkaPartitionLevelStreamConfig getStreamConfig(String topic, String bootstrapHosts, String buffer,
+      String socketTimeout) {
+    return getStreamConfig(topic, bootstrapHosts, buffer, socketTimeout, null, null);
+  }
+
+  private KafkaPartitionLevelStreamConfig getStreamConfig(String topic, String bootstrapHosts, String buffer,
+      String socketTimeout, String fetcherSize, String fetcherMinBytes) {
+    Map<String, String> streamConfigMap = new HashMap<>();
+    String streamType = "kafka";
+    String consumerType = StreamConfig.ConsumerType.LOWLEVEL.toString();
+    String consumerFactoryClassName = KafkaConsumerFactory.class.getName();
+    String decoderClass = "org.apache.pinot.core.realtime.impl.kafka.KafkaAvroMessageDecoder";
+    streamConfigMap.put(StreamConfigProperties.STREAM_TYPE, streamType);
+    streamConfigMap
+        .put(StreamConfigProperties.constructStreamProperty(streamType, StreamConfigProperties.STREAM_TOPIC_NAME),
+            topic);
+    streamConfigMap
+        .put(StreamConfigProperties.constructStreamProperty(streamType, StreamConfigProperties.STREAM_CONSUMER_TYPES),
+            consumerType);
+    streamConfigMap.put(StreamConfigProperties
+            .constructStreamProperty(streamType, StreamConfigProperties.STREAM_CONSUMER_FACTORY_CLASS),
+        consumerFactoryClassName);
+    streamConfigMap
+        .put(StreamConfigProperties.constructStreamProperty(streamType, StreamConfigProperties.STREAM_DECODER_CLASS),
+            decoderClass);
+    streamConfigMap.put("stream.kafka.broker.list", bootstrapHosts);
+    if (buffer != null) {
+      streamConfigMap.put("stream.kafka.buffer.size", buffer);
+    }
+    if (socketTimeout != null) {
+      streamConfigMap.put("stream.kafka.socket.timeout", socketTimeout);
+    }
+    if (fetcherSize != null) {
+      streamConfigMap.put("stream.kafka.fetcher.size", fetcherSize);
+    }
+    if (fetcherMinBytes != null) {
+      streamConfigMap.put("stream.kafka.fetcher.minBytes", fetcherMinBytes);
+    }
+    return new KafkaPartitionLevelStreamConfig(new StreamConfig(streamConfigMap));
+  }
+
+  @Test
+  public void testGetKafkaTopicName() {
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "", "", "");
+    Assert.assertEquals("topic", config.getKafkaTopicName());
+  }
+
+  @Test
+  public void testGetBootstrapHosts() {
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "host1", "", "");
+    Assert.assertEquals("host1", config.getBootstrapHosts());
+  }
+
+  @Test
+  public void testGetKafkaBufferSize() {
+    // test default
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "host1", null, "");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT,
+        config.getKafkaBufferSize());
+
+    config = getStreamConfig("topic", "host1", "", "");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT,
+        config.getKafkaBufferSize());
+
+    config = getStreamConfig("topic", "host1", "bad value", "");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT,
+        config.getKafkaBufferSize());
+
+    // correct config
+    config = getStreamConfig("topic", "host1", "100", "");
+    Assert.assertEquals(100, config.getKafkaBufferSize());
+  }
+
+  @Test
+  public void testGetKafkaSocketTimeout() {
+    // test default
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "host1", "", null);
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT_DEFAULT,
+        config.getKafkaSocketTimeout());
+
+    config = getStreamConfig("topic", "host1", "", "");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT_DEFAULT,
+        config.getKafkaSocketTimeout());
+
+    config = getStreamConfig("topic", "host1", "", "bad value");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_SOCKET_TIMEOUT_DEFAULT,
+        config.getKafkaSocketTimeout());
+
+    // correct config
+    config = getStreamConfig("topic", "host1", "", "100");
+    Assert.assertEquals(100, config.getKafkaSocketTimeout());
+  }
+
+  @Test
+  public void testGetFetcherSize() {
+    // test default
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "host1", "", "", "", null);
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_BUFFER_SIZE_DEFAULT,
+        config.getKafkaFetcherSizeBytes());
+
+    config = getStreamConfig("topic", "host1", "100", "", "", null);
+    Assert.assertEquals(100, config.getKafkaFetcherSizeBytes());
+
+    config = getStreamConfig("topic", "host1", "100", "", "bad value", null);
+    Assert.assertEquals(100, config.getKafkaFetcherSizeBytes());
+
+    // correct config
+    config = getStreamConfig("topic", "host1", "100", "", "200", null);
+    Assert.assertEquals(200, config.getKafkaFetcherSizeBytes());
+  }
+
+  @Test
+  public void testGetFetcherMinBytes() {
+    // test default
+    KafkaPartitionLevelStreamConfig config = getStreamConfig("topic", "host1", "", "", "", null);
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_MIN_BYTES_DEFAULT,
+        config.getKafkaFetcherMinBytes());
+
+    config = getStreamConfig("topic", "host1", "", "", "", "");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_MIN_BYTES_DEFAULT,
+        config.getKafkaFetcherMinBytes());
+
+    config = getStreamConfig("topic", "host1", "", "", "", "bad value");
+    Assert.assertEquals(KafkaStreamConfigProperties.LowLevelConsumer.KAFKA_FETCHER_MIN_BYTES_DEFAULT,
+        config.getKafkaFetcherMinBytes());
+
+    // correct config
+    config = getStreamConfig("topic", "host1", "", "", "", "100");
+    Assert.assertEquals(100, config.getKafkaFetcherMinBytes());
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/EmbeddedZooKeeper.java b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/EmbeddedZooKeeper.java
new file mode 100644
index 00000000000..47370aa76ae
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/EmbeddedZooKeeper.java
@@ -0,0 +1,60 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2.utils;
+
+import java.io.Closeable;
+import java.io.File;
+import java.io.IOException;
+import java.net.InetSocketAddress;
+import java.nio.file.Files;
+import org.apache.commons.io.FileUtils;
+import org.apache.zookeeper.server.NIOServerCnxnFactory;
+import org.apache.zookeeper.server.ZooKeeperServer;
+
+
+public class EmbeddedZooKeeper implements Closeable {
+
+  private static final int TICK_TIME = 500;
+  private final NIOServerCnxnFactory factory;
+  private final ZooKeeperServer zookeeper;
+  private final File tmpDir;
+  private final int port;
+
+  EmbeddedZooKeeper() throws IOException, InterruptedException {
+    this.tmpDir = Files.createTempDirectory(null).toFile();
+    this.factory = new NIOServerCnxnFactory();
+    this.zookeeper = new ZooKeeperServer(new File(tmpDir, "data"), new File(tmpDir, "log"),
+        TICK_TIME);
+    InetSocketAddress addr = new InetSocketAddress("127.0.0.1", 0);
+    factory.configure(addr, 0);
+    factory.startup(zookeeper);
+    this.port = zookeeper.getClientPort();
+  }
+
+  public int getPort() {
+    return port;
+  }
+
+  @Override
+  public void close() throws IOException {
+    zookeeper.shutdown();
+    factory.shutdown();
+    FileUtils.deleteDirectory(tmpDir);
+  }
+}
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/MiniKafkaCluster.java b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/MiniKafkaCluster.java
new file mode 100644
index 00000000000..3ec32fcfab4
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/test/java/org/apache/pinot/core/realtime/impl/kafka2/utils/MiniKafkaCluster.java
@@ -0,0 +1,175 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka2.utils;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.net.ServerSocket;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Properties;
+import java.util.concurrent.ExecutionException;
+import kafka.server.KafkaConfig;
+import kafka.server.KafkaServer;
+import org.apache.commons.io.FileUtils;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.CreateTopicsResult;
+import org.apache.kafka.clients.admin.DeleteTopicsResult;
+import org.apache.kafka.clients.admin.NewTopic;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.common.network.ListenerName;
+import org.apache.kafka.common.security.auth.SecurityProtocol;
+import org.apache.kafka.common.utils.Time;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import scala.Option;
+import scala.collection.Seq;
+
+
+public final class MiniKafkaCluster implements Closeable {
+
+  private static final Logger LOGGER = LoggerFactory.getLogger(MiniKafkaCluster.class);
+  private final EmbeddedZooKeeper zkServer;
+  private final ArrayList<KafkaServer> kafkaServer;
+  private final Path tempDir;
+  private final AdminClient adminClient;
+
+  @SuppressWarnings({"rawtypes", "unchecked"})
+  private MiniKafkaCluster(List<String> brokerIds)
+      throws IOException, InterruptedException {
+    this.zkServer = new EmbeddedZooKeeper();
+    this.tempDir = Files.createTempDirectory(Paths.get(System.getProperty("java.io.tmpdir")), "mini-kafka-cluster");
+    this.kafkaServer = new ArrayList<>();
+    int port = 0;
+    for (String id : brokerIds) {
+      port = getAvailablePort();
+      KafkaConfig c = new KafkaConfig(createBrokerConfig(id, port));
+      Seq seq =
+          scala.collection.JavaConverters.collectionAsScalaIterableConverter(Collections.emptyList()).asScala().toSeq();
+      kafkaServer.add(new KafkaServer(c, Time.SYSTEM, Option.empty(), seq));
+    }
+    Properties props = new Properties();
+    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:" + port);
+    adminClient = AdminClient.create(props);
+  }
+
+  static int getAvailablePort() {
+    try {
+      try (ServerSocket socket = new ServerSocket(0)) {
+        return socket.getLocalPort();
+      }
+    } catch (IOException e) {
+      throw new RuntimeException("Failed to find available port to use", e);
+    }
+  }
+
+  private Properties createBrokerConfig(String nodeId, int port)
+      throws IOException {
+    Properties props = new Properties();
+    props.put("broker.id", nodeId);
+    props.put("port", Integer.toString(port));
+    props.put("log.dir", Files.createTempDirectory(tempDir, "broker-").toAbsolutePath().toString());
+    props.put("zookeeper.connect", "127.0.0.1:" + zkServer.getPort());
+    props.put("replica.socket.timeout.ms", "1500");
+    props.put("controller.socket.timeout.ms", "1500");
+    props.put("controlled.shutdown.enable", "true");
+    props.put("delete.topic.enable", "true");
+    props.put("auto.create.topics.enable", "true");
+    props.put("offsets.topic.replication.factor", "1");
+    props.put("controlled.shutdown.retry.backoff.ms", "100");
+    props.put("log.cleaner.dedupe.buffer.size", "2097152");
+    return props;
+  }
+
+  public void start() {
+    for (KafkaServer s : kafkaServer) {
+      s.startup();
+    }
+  }
+
+  @Override
+  public void close()
+      throws IOException {
+    for (KafkaServer s : kafkaServer) {
+      s.shutdown();
+    }
+    this.zkServer.close();
+    FileUtils.deleteDirectory(tempDir.toFile());
+  }
+
+  public EmbeddedZooKeeper getZkServer() {
+    return zkServer;
+  }
+
+  public List<KafkaServer> getKafkaServer() {
+    return kafkaServer;
+  }
+
+  public int getKafkaServerPort(int index) {
+    return kafkaServer.get(index).socketServer()
+        .boundPort(ListenerName.forSecurityProtocol(SecurityProtocol.PLAINTEXT));
+  }
+
+  public AdminClient getAdminClient() {
+    return adminClient;
+  }
+
+  public boolean createTopic(String topicName, int numPartitions, int replicationFactor) {
+    NewTopic newTopic = new NewTopic(topicName, numPartitions, (short) replicationFactor);
+    CreateTopicsResult createTopicsResult = this.adminClient.createTopics(Arrays.asList(newTopic));
+    try {
+      createTopicsResult.all().get();
+    } catch (InterruptedException | ExecutionException e) {
+      LOGGER.error("Failed to create Kafka topic: {}, Exception: {}", newTopic.toString(), e);
+      return false;
+    }
+    return true;
+  }
+
+  public boolean deleteTopic(String topicName) {
+    final DeleteTopicsResult deleteTopicsResult = this.adminClient.deleteTopics(Collections.singletonList(topicName));
+    try {
+      deleteTopicsResult.all().get();
+    } catch (InterruptedException | ExecutionException e) {
+      LOGGER.error("Failed to delete Kafka topic: {}, Exception: {}", topicName, e);
+      return false;
+    }
+    return true;
+  }
+
+  public static class Builder {
+
+    private List<String> brokerIds = new ArrayList<>();
+
+    public Builder newServer(String brokerId) {
+      brokerIds.add(brokerId);
+      return this;
+    }
+
+    public MiniKafkaCluster build()
+        throws IOException, InterruptedException {
+      return new MiniKafkaCluster(brokerIds);
+    }
+  }
+}
\ No newline at end of file
diff --git a/pinot-connectors/pinot-connector-kafka-base/README.md b/pinot-connectors/pinot-connector-kafka-base/README.md
new file mode 100644
index 00000000000..15f8df55dff
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-base/README.md
@@ -0,0 +1,23 @@
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+# Base class and common utils for Pinot Kafka connector
+
+This is a shared module by all the Pinot Kafka Connector
\ No newline at end of file
diff --git a/pinot-connectors/pinot-connector-kafka-base/pom.xml b/pinot-connectors/pinot-connector-kafka-base/pom.xml
new file mode 100644
index 00000000000..6b714a28323
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-base/pom.xml
@@ -0,0 +1,38 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<!--
+
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+
+-->
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
+  <parent>
+    <artifactId>pinot-connectors</artifactId>
+    <groupId>org.apache.pinot</groupId>
+    <version>0.2.0-SNAPSHOT</version>
+    <relativePath>..</relativePath>
+  </parent>
+  <modelVersion>4.0.0</modelVersion>
+  <artifactId>pinot-connector-kafka-base</artifactId>
+  <name>Pinot Connector Kafka Base</name>
+  <url>https://pinot.apache.org/</url>
+  <properties>
+    <pinot.root>${basedir}/../..</pinot.root>
+  </properties>
+</project>
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaAvroMessageDecoder.java b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaAvroMessageDecoder.java
similarity index 100%
rename from pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaAvroMessageDecoder.java
rename to pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaAvroMessageDecoder.java
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaJSONMessageDecoder.java b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaJSONMessageDecoder.java
similarity index 100%
rename from pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaJSONMessageDecoder.java
rename to pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaJSONMessageDecoder.java
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/KafkaStarterUtils.java b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStarterUtils.java
similarity index 78%
rename from pinot-tools/src/main/java/org/apache/pinot/tools/KafkaStarterUtils.java
rename to pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStarterUtils.java
index 73da8dc91c3..4881caf0456 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/KafkaStarterUtils.java
+++ b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStarterUtils.java
@@ -16,12 +16,14 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pinot.tools;
+package org.apache.pinot.core.realtime.impl.kafka;
 
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Properties;
+import java.util.ServiceLoader;
 import org.apache.pinot.common.utils.ZkStarter;
+import org.apache.pinot.core.realtime.stream.StreamConsumerFactory;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 
@@ -38,9 +40,17 @@ public class KafkaStarterUtils {
   private static final String LOG_DIRS = "log.dirs";
 
   public static final String KAFKA_SERVER_STARTABLE_CLASS_NAME =
-      "org.apache.pinot.core.realtime.impl.kafka.server.KafkaDataServerStartable";
-  public static final String KAFKA_PRODUCER_CLASS_NAME =
-      "org.apache.pinot.core.realtime.impl.kafka.server.KafkaDataProducer";
+      getKafkaConnectorPackageName() + ".server.KafkaDataServerStartable";
+  public static final String KAFKA_PRODUCER_CLASS_NAME = getKafkaConnectorPackageName() + ".server.KafkaDataProducer";
+  public static final String KAFKA_STREAM_CONSUMER_FACTORY_CLASS_NAME =
+      getKafkaConnectorPackageName() + ".KafkaConsumerFactory";
+  public static final String KAFKA_STREAM_LEVEL_CONSUMER_CLASS_NAME =
+      getKafkaConnectorPackageName() + ".KafkaStreamLevelConsumer";
+
+  private static String getKafkaConnectorPackageName() {
+    return ServiceLoader.load(StreamConsumerFactory.class).iterator().next().getClass().getPackage().getName();
+  }
+
   public static final String KAFKA_JSON_MESSAGE_DECODER_CLASS_NAME =
       "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder";
 
@@ -52,7 +62,7 @@ public static Properties getDefaultKafkaConfiguration() {
 
     // Set host name
     configureHostName(configuration, "localhost");
-
+    configureOffsetsTopicReplicationFactor(configuration, (short) 1);
     configuration.put(PORT, DEFAULT_KAFKA_PORT);
     configuration.put(BROKER_ID, DEFAULT_BROKER_ID);
     configuration.put(ZOOKEEPER_CONNECT, DEFAULT_ZK_STR);
@@ -61,6 +71,10 @@ public static Properties getDefaultKafkaConfiguration() {
     return configuration;
   }
 
+  public static void configureOffsetsTopicReplicationFactor(Properties configuration, short replicationFactor) {
+    configuration.put("offsets.topic.replication.factor", replicationFactor);
+  }
+
   public static void configureTopicDeletion(Properties configuration, boolean topicDeletionEnabled) {
     configuration.put("delete.topic.enable", Boolean.toString(topicDeletionEnabled));
   }
@@ -89,6 +103,7 @@ public static StreamDataServerStartable startServer(final int port, final int br
       final Properties configuration) {
     StreamDataServerStartable kafkaStarter;
     try {
+      configureOffsetsTopicReplicationFactor(configuration, (short) 1);
       configuration.put(KafkaStarterUtils.PORT, port);
       configuration.put(KafkaStarterUtils.BROKER_ID, brokerId);
       configuration.put(KafkaStarterUtils.ZOOKEEPER_CONNECT, zkStr);
diff --git a/pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java
similarity index 96%
rename from pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java
rename to pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java
index c46f44abe65..03182ac046f 100644
--- a/pinot-connectors/pinot-connector-kafka-0.9/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java
+++ b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/KafkaStreamConfigProperties.java
@@ -29,7 +29,17 @@ public class KafkaStreamConfigProperties {
   public static final String DOT_SEPARATOR = ".";
   public static final String STREAM_TYPE = "kafka";
 
+  /**
+   * Helper method to create a property string for kafka stream
+   * @param property
+   * @return
+   */
+  public static String constructStreamProperty(String property) {
+    return Joiner.on(DOT_SEPARATOR).join(StreamConfigProperties.STREAM_PREFIX, property);
+  }
+
   public static class HighLevelConsumer {
+    public static final String KAFKA_HLC_BOOTSTRAP_SERVER = "kafka.hlc.bootstrap.server";
     public static final String KAFKA_HLC_ZK_CONNECTION_STRING = "kafka.hlc.zk.connect.string";
     public static final String ZK_SESSION_TIMEOUT_MS = "zookeeper.session.timeout.ms";
     public static final String ZK_CONNECTION_TIMEOUT_MS = "zookeeper.connection.timeout.ms";
@@ -52,14 +62,5 @@ public static class LowLevelConsumer {
   }
 
   public static final String KAFKA_CONSUMER_PROP_PREFIX = "kafka.consumer.prop";
-
-  /**
-   * Helper method to create a property string for kafka stream
-   * @param property
-   * @return
-   */
-  public static String constructStreamProperty(String property) {
-    return Joiner.on(DOT_SEPARATOR).join(StreamConfigProperties.STREAM_PREFIX, property);
-  }
 }
 
diff --git a/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/MessageAndOffset.java b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/MessageAndOffset.java
new file mode 100644
index 00000000000..81b389628a1
--- /dev/null
+++ b/pinot-connectors/pinot-connector-kafka-base/src/main/java/org/apache/pinot/core/realtime/impl/kafka/MessageAndOffset.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pinot.core.realtime.impl.kafka;
+
+import java.nio.ByteBuffer;
+
+
+public class MessageAndOffset {
+
+  private ByteBuffer _message;
+  private long _offset;
+
+  public MessageAndOffset(byte[] message, long offset) {
+    this(ByteBuffer.wrap(message), offset);
+  }
+
+  public MessageAndOffset(ByteBuffer message, long offset) {
+    _message = message;
+    _offset = offset;
+  }
+
+  public ByteBuffer getMessage() {
+    return _message;
+  }
+
+  public long getOffset() {
+    return _offset;
+  }
+
+  public long getNextOffset() {
+    return getOffset() + 1;
+  }
+
+  public int payloadSize() {
+    return getMessage().array().length;
+  }
+}
diff --git a/pinot-connectors/pom.xml b/pinot-connectors/pom.xml
index 36951899603..d45f1a77edd 100644
--- a/pinot-connectors/pom.xml
+++ b/pinot-connectors/pom.xml
@@ -32,12 +32,15 @@
   <artifactId>pinot-connectors</artifactId>
   <packaging>pom</packaging>
   <name>Pinot Connectors</name>
+  <url>https://pinot.apache.org/</url>
   <properties>
     <pinot.root>${basedir}/..</pinot.root>
   </properties>
 
   <modules>
+    <module>pinot-connector-kafka-base</module>
     <module>pinot-connector-kafka-0.9</module>
+    <module>pinot-connector-kafka-2.0</module>
   </modules>
 
   <dependencies>
@@ -45,11 +48,23 @@
       <groupId>org.apache.pinot</groupId>
       <artifactId>pinot-common</artifactId>
       <scope>provided</scope>
+      <exclusions>
+        <exclusion>
+          <groupId>org.scala-lang</groupId>
+          <artifactId>scala-library</artifactId>
+        </exclusion>
+      </exclusions>
     </dependency>
     <dependency>
       <groupId>org.apache.pinot</groupId>
       <artifactId>pinot-core</artifactId>
       <scope>provided</scope>
+      <exclusions>
+        <exclusion>
+          <groupId>org.scala-lang</groupId>
+          <artifactId>scala-library</artifactId>
+        </exclusion>
+      </exclusions>
     </dependency>
 
     <!-- test -->
diff --git a/pinot-distribution/pinot-assembly.xml b/pinot-distribution/pinot-assembly.xml
index dc8b8dbab2d..0c21f704624 100644
--- a/pinot-distribution/pinot-assembly.xml
+++ b/pinot-distribution/pinot-assembly.xml
@@ -81,6 +81,11 @@
       <directory>${pinot.root}/pinot-tools/src/main/resources/conf</directory>
       <outputDirectory>conf</outputDirectory>
     </fileSet>
+    <fileSet>
+      <useDefaultExcludes>false</useDefaultExcludes>
+      <directory>${pinot.root}/pinot-tools/src/main/resources/sample_data/kafka_${kafka.version}</directory>
+      <outputDirectory>sample_data/</outputDirectory>
+    </fileSet>
     <fileSet>
       <useDefaultExcludes>false</useDefaultExcludes>
       <directory>${pinot.root}/pinot-tools/src/main/resources/sample_data</directory>
diff --git a/pinot-integration-tests/pom.xml b/pinot-integration-tests/pom.xml
index 8277e84319e..00498383c12 100644
--- a/pinot-integration-tests/pom.xml
+++ b/pinot-integration-tests/pom.xml
@@ -191,7 +191,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.pinot</groupId>
-      <artifactId>pinot-connector-kafka-${kafka.lib.version}</artifactId>
+      <artifactId>pinot-connector-kafka-${kafka.version}</artifactId>
       <version>${project.version}</version>
       <scope>runtime</scope>
     </dependency>
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/BaseClusterIntegrationTest.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/BaseClusterIntegrationTest.java
index d94b7b60352..406f7abe8d6 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/BaseClusterIntegrationTest.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/BaseClusterIntegrationTest.java
@@ -38,9 +38,8 @@
 import org.apache.pinot.common.config.TagNameUtils;
 import org.apache.pinot.common.utils.TarGzCompressionUtils;
 import org.apache.pinot.common.utils.ZkStarter;
-import org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
-import org.apache.pinot.tools.KafkaStarterUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.util.TestUtils;
 import org.testng.Assert;
 
@@ -112,7 +111,7 @@ protected boolean useLlc() {
   }
 
   protected String getStreamConsumerFactoryClassName() {
-    return KafkaConsumerFactory.class.getName();
+    return KafkaStarterUtils.KAFKA_STREAM_CONSUMER_FACTORY_CLASS_NAME;
   }
 
   protected int getRealtimeSegmentFlushSize() {
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterIntegrationTestUtils.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterIntegrationTestUtils.java
index fe017b20e4e..4ea9167514e 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterIntegrationTestUtils.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterIntegrationTestUtils.java
@@ -41,9 +41,6 @@
 import java.util.concurrent.TimeUnit;
 import javax.annotation.Nonnull;
 import javax.annotation.Nullable;
-import kafka.javaapi.producer.Producer;
-import kafka.producer.KeyedMessage;
-import kafka.producer.ProducerConfig;
 import org.apache.avro.Schema;
 import org.apache.avro.file.DataFileStream;
 import org.apache.avro.generic.GenericData;
@@ -59,11 +56,14 @@
 import org.apache.pinot.common.utils.StringUtil;
 import org.apache.pinot.common.utils.TarGzCompressionUtils;
 import org.apache.pinot.core.indexsegment.generator.SegmentGeneratorConfig;
+import org.apache.pinot.core.realtime.stream.StreamDataProducer;
+import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.segment.creator.SegmentIndexCreationDriver;
 import org.apache.pinot.core.segment.creator.impl.SegmentIndexCreationDriverImpl;
 import org.apache.pinot.core.startree.v2.builder.StarTreeV2BuilderConfig;
 import org.apache.pinot.core.util.AvroUtils;
 import org.apache.pinot.server.util.SegmentTestUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.testng.Assert;
 
 
@@ -324,16 +324,13 @@ public static void pushAvroIntoKafka(@Nonnull List<File> avroFiles, @Nonnull Str
     properties.put("request.required.acks", "1");
     properties.put("partitioner.class", "kafka.producer.ByteArrayPartitioner");
 
-    ProducerConfig producerConfig = new ProducerConfig(properties);
-    Producer<byte[], byte[]> producer = new Producer<>(producerConfig);
+    StreamDataProducer producer = StreamDataProvider.getStreamDataProducer(KafkaStarterUtils.KAFKA_PRODUCER_CLASS_NAME, properties);
 
     try (ByteArrayOutputStream outputStream = new ByteArrayOutputStream(65536)) {
       for (File avroFile : avroFiles) {
         try (DataFileStream<GenericRecord> reader = AvroUtils.getAvroReader(avroFile)) {
           BinaryEncoder binaryEncoder = new EncoderFactory().directBinaryEncoder(outputStream, null);
           GenericDatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(reader.getSchema());
-
-          List<KeyedMessage<byte[], byte[]>> messagesToWrite = new ArrayList<>(maxNumKafkaMessagesPerBatch);
           for (GenericRecord genericRecord : reader) {
             outputStream.reset();
             if (header != null && 0 < header.length) {
@@ -345,19 +342,8 @@ public static void pushAvroIntoKafka(@Nonnull List<File> avroFiles, @Nonnull Str
             byte[] keyBytes = (partitionColumn == null) ? Longs.toByteArray(System.currentTimeMillis())
                 : (genericRecord.get(partitionColumn)).toString().getBytes();
             byte[] bytes = outputStream.toByteArray();
-            KeyedMessage<byte[], byte[]> data = new KeyedMessage<>(kafkaTopic, keyBytes, bytes);
-
-            messagesToWrite.add(data);
-
-            // Send a batch of messages
-            if (messagesToWrite.size() == maxNumKafkaMessagesPerBatch) {
-              producer.send(messagesToWrite);
-              messagesToWrite.clear();
-            }
+            producer.produce(kafkaTopic,keyBytes,bytes);
           }
-
-          // Send last batch of messages
-          producer.send(messagesToWrite);
         }
       }
     }
@@ -386,16 +372,12 @@ public static void pushRandomAvroIntoKafka(@Nonnull File avroFile, @Nonnull Stri
     properties.put("request.required.acks", "1");
     properties.put("partitioner.class", "kafka.producer.ByteArrayPartitioner");
 
-    ProducerConfig producerConfig = new ProducerConfig(properties);
-    Producer<byte[], byte[]> producer = new Producer<>(producerConfig);
-
+    StreamDataProducer producer = StreamDataProvider.getStreamDataProducer(KafkaStarterUtils.KAFKA_PRODUCER_CLASS_NAME, properties);
     try (ByteArrayOutputStream outputStream = new ByteArrayOutputStream(65536)) {
       try (DataFileStream<GenericRecord> reader = AvroUtils.getAvroReader(avroFile)) {
         BinaryEncoder binaryEncoder = new EncoderFactory().directBinaryEncoder(outputStream, null);
         Schema avroSchema = reader.getSchema();
         GenericDatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<>(avroSchema);
-
-        List<KeyedMessage<byte[], byte[]>> messagesToWrite = new ArrayList<>(maxNumKafkaMessagesPerBatch);
         GenericRecord genericRecord = new GenericData.Record(avroSchema);
 
         while (numKafkaMessagesToPush > 0) {
@@ -411,21 +393,10 @@ public static void pushRandomAvroIntoKafka(@Nonnull File avroFile, @Nonnull Stri
           byte[] keyBytes = (partitionColumn == null) ? Longs.toByteArray(System.currentTimeMillis())
               : (genericRecord.get(partitionColumn)).toString().getBytes();
           byte[] bytes = outputStream.toByteArray();
-          KeyedMessage<byte[], byte[]> data = new KeyedMessage<>(kafkaTopic, keyBytes, bytes);
-
-          messagesToWrite.add(data);
-
-          // Send a batch of messages
-          if (messagesToWrite.size() == maxNumKafkaMessagesPerBatch) {
-            producer.send(messagesToWrite);
-            messagesToWrite.clear();
-          }
 
+          producer.produce(kafkaTopic,keyBytes,bytes);
           numKafkaMessagesToPush--;
         }
-
-        // Send last batch of messages
-        producer.send(messagesToWrite);
       }
     }
   }
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterTest.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterTest.java
index 0ec8ef2f92b..a03c2547e16 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterTest.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ClusterTest.java
@@ -427,6 +427,9 @@ protected void addRealtimeTable(String tableName, boolean useLlc, String kafkaBr
       streamConfigs.put(KafkaStreamConfigProperties
               .constructStreamProperty(KafkaStreamConfigProperties.HighLevelConsumer.KAFKA_HLC_ZK_CONNECTION_STRING),
           kafkaZkUrl);
+      streamConfigs.put(KafkaStreamConfigProperties
+              .constructStreamProperty(KafkaStreamConfigProperties.HighLevelConsumer.KAFKA_HLC_BOOTSTRAP_SERVER),
+          kafkaBrokerList);
     }
     streamConfigs.put(StreamConfigProperties
             .constructStreamProperty(streamType, StreamConfigProperties.STREAM_CONSUMER_FACTORY_CLASS),
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ControllerPeriodicTasksIntegrationTests.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ControllerPeriodicTasksIntegrationTests.java
index 374740c2e19..fc147896221 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ControllerPeriodicTasksIntegrationTests.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/ControllerPeriodicTasksIntegrationTests.java
@@ -51,7 +51,7 @@
 import org.apache.pinot.controller.validation.OfflineSegmentIntervalChecker;
 import org.apache.pinot.controller.validation.RealtimeSegmentValidationManager;
 import org.apache.pinot.core.indexsegment.generator.SegmentVersion;
-import org.apache.pinot.tools.KafkaStarterUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.util.TestUtils;
 import org.testng.Assert;
 import org.testng.ITestContext;
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/FlakyConsumerRealtimeClusterIntegrationTest.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/FlakyConsumerRealtimeClusterIntegrationTest.java
index 6b652cac7f6..cba7238b35a 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/FlakyConsumerRealtimeClusterIntegrationTest.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/FlakyConsumerRealtimeClusterIntegrationTest.java
@@ -18,17 +18,18 @@
  */
 package org.apache.pinot.integration.tests;
 
+import java.lang.reflect.Constructor;
 import java.util.Random;
 import org.apache.pinot.common.data.Schema;
 import org.apache.pinot.common.metadata.instance.InstanceZKMetadata;
 import org.apache.pinot.common.metrics.ServerMetrics;
 import org.apache.pinot.core.data.GenericRow;
-import org.apache.pinot.core.realtime.impl.kafka.KafkaStreamLevelConsumer;
 import org.apache.pinot.core.realtime.stream.PartitionLevelConsumer;
 import org.apache.pinot.core.realtime.stream.StreamConfig;
 import org.apache.pinot.core.realtime.stream.StreamConsumerFactory;
 import org.apache.pinot.core.realtime.stream.StreamLevelConsumer;
 import org.apache.pinot.core.realtime.stream.StreamMetadataProvider;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.testng.annotations.BeforeClass;
 
 
@@ -55,8 +56,15 @@ public static class FlakyStreamLevelConsumer implements StreamLevelConsumer {
 
     public FlakyStreamLevelConsumer(String clientId, String tableName, StreamConfig streamConfig, Schema schema,
         InstanceZKMetadata instanceZKMetadata, ServerMetrics serverMetrics) {
-      _streamLevelConsumer =
-          new KafkaStreamLevelConsumer(clientId, tableName, streamConfig, schema, instanceZKMetadata, serverMetrics);
+      try {
+        final Constructor constructor = Class.forName(KafkaStarterUtils.KAFKA_STREAM_LEVEL_CONSUMER_CLASS_NAME)
+            .getConstructor(String.class, String.class, StreamConfig.class, Schema.class, InstanceZKMetadata.class,
+                ServerMetrics.class);
+        _streamLevelConsumer = (StreamLevelConsumer) constructor
+            .newInstance(clientId, tableName, streamConfig, schema, instanceZKMetadata, serverMetrics);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
     }
 
     @Override
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTest.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTest.java
index d1fd9ff706b..0983d4ea796 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTest.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTest.java
@@ -33,7 +33,7 @@
 import org.apache.pinot.common.utils.CommonConstants;
 import org.apache.pinot.common.utils.JsonUtils;
 import org.apache.pinot.controller.ControllerConf;
-import org.apache.pinot.tools.KafkaStarterUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.util.TestUtils;
 import org.testng.Assert;
 import org.testng.annotations.AfterClass;
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTestCommandLineRunner.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTestCommandLineRunner.java
index ec15591b30c..73bbfd937eb 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTestCommandLineRunner.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/HybridClusterIntegrationTestCommandLineRunner.java
@@ -41,7 +41,7 @@
 import org.apache.pinot.controller.ControllerConf;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.tools.query.comparison.QueryComparison;
-import org.apache.pinot.tools.KafkaStarterUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.util.TestUtils;
 import org.testng.Assert;
 import org.testng.ITestResult;
diff --git a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/RealtimeClusterIntegrationTest.java b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/RealtimeClusterIntegrationTest.java
index 6dc4fc6dead..8b4baebaae4 100644
--- a/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/RealtimeClusterIntegrationTest.java
+++ b/pinot-integration-tests/src/test/java/org/apache/pinot/integration/tests/RealtimeClusterIntegrationTest.java
@@ -26,7 +26,7 @@
 import java.util.concurrent.TimeUnit;
 import org.apache.commons.io.FileUtils;
 import org.apache.pinot.common.data.Schema;
-import org.apache.pinot.tools.KafkaStarterUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.util.TestUtils;
 import org.testng.Assert;
 import org.testng.annotations.AfterClass;
diff --git a/pinot-perf/pom.xml b/pinot-perf/pom.xml
index 3c0165e523f..7b53a61c63f 100644
--- a/pinot-perf/pom.xml
+++ b/pinot-perf/pom.xml
@@ -52,7 +52,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.pinot</groupId>
-      <artifactId>pinot-connector-kafka-${kafka.lib.version}</artifactId>
+      <artifactId>pinot-connector-kafka-${kafka.version}</artifactId>
       <version>${project.version}</version>
       <scope>runtime</scope>
     </dependency>
diff --git a/pinot-perf/src/main/java/org/apache/pinot/perf/BenchmarkRealtimeConsumptionSpeed.java b/pinot-perf/src/main/java/org/apache/pinot/perf/BenchmarkRealtimeConsumptionSpeed.java
index e623ecee1cd..8a9355f4262 100644
--- a/pinot-perf/src/main/java/org/apache/pinot/perf/BenchmarkRealtimeConsumptionSpeed.java
+++ b/pinot-perf/src/main/java/org/apache/pinot/perf/BenchmarkRealtimeConsumptionSpeed.java
@@ -26,9 +26,9 @@
 import java.util.Random;
 import java.util.concurrent.TimeUnit;
 import org.apache.pinot.common.utils.TarGzCompressionUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.integration.tests.ClusterIntegrationTestUtils;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.apache.pinot.integration.tests.RealtimeClusterIntegrationTest;
 import org.apache.pinot.util.TestUtils;
 
diff --git a/pinot-perf/src/main/java/org/apache/pinot/perf/RealtimeStressTest.java b/pinot-perf/src/main/java/org/apache/pinot/perf/RealtimeStressTest.java
index b945fc5c0ae..21fdca15a1e 100644
--- a/pinot-perf/src/main/java/org/apache/pinot/perf/RealtimeStressTest.java
+++ b/pinot-perf/src/main/java/org/apache/pinot/perf/RealtimeStressTest.java
@@ -26,9 +26,9 @@
 import java.util.Random;
 import java.util.concurrent.TimeUnit;
 import org.apache.pinot.common.utils.TarGzCompressionUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.integration.tests.ClusterIntegrationTestUtils;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.apache.pinot.integration.tests.OfflineClusterIntegrationTest;
 import org.apache.pinot.integration.tests.RealtimeClusterIntegrationTest;
 import org.apache.pinot.util.TestUtils;
diff --git a/pinot-tools/pom.xml b/pinot-tools/pom.xml
index cf9f5077534..43b7afa7b64 100644
--- a/pinot-tools/pom.xml
+++ b/pinot-tools/pom.xml
@@ -56,7 +56,12 @@
     </dependency>
     <dependency>
       <groupId>org.apache.pinot</groupId>
-      <artifactId>pinot-connector-kafka-${kafka.lib.version}</artifactId>
+      <artifactId>pinot-connector-kafka-base</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.pinot</groupId>
+      <artifactId>pinot-connector-kafka-${kafka.version}</artifactId>
       <version>${project.version}</version>
       <scope>runtime</scope>
     </dependency>
@@ -311,6 +316,31 @@
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-enforcer-plugin</artifactId>
       </plugin>
+      <plugin>
+        <artifactId>maven-resources-plugin</artifactId>
+        <version>2.6</version>
+        <executions>
+          <execution>
+            <id>copy-kafka-resources</id>
+            <phase>process-resources</phase>
+            <goals>
+              <goal>copy-resources</goal>
+            </goals>
+            <configuration>
+              <!-- this is important -->
+              <overwrite>true</overwrite>
+              <!-- target -->
+              <outputDirectory>${basedir}/target/classes/sample_data/</outputDirectory>
+              <resources>
+                <resource>
+                  <!-- source -->
+                  <directory>src/main/resources/sample_data/kafka_${kafka.version}</directory>
+                </resource>
+              </resources>
+            </configuration>
+          </execution>
+        </executions>
+      </plugin>
     </plugins>
   </build>
 </project>
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/HybridQuickstart.java b/pinot-tools/src/main/java/org/apache/pinot/tools/HybridQuickstart.java
index 08d9b2aeb92..259676425ac 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/HybridQuickstart.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/HybridQuickstart.java
@@ -23,11 +23,11 @@
 import java.io.File;
 import java.io.IOException;
 import java.net.URL;
-import java.util.Properties;
 import org.apache.commons.io.FileUtils;
 import org.apache.pinot.common.data.Schema;
 import org.apache.pinot.common.utils.ZkStarter;
 import org.apache.pinot.core.data.readers.FileFormat;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.tools.Quickstart.Color;
@@ -90,7 +90,8 @@ private QuickstartTableRequest prepareRealtimeTableRequest()
 
     File tableConfigFile = new File(_realtimeQuickStartDataDir, "airlineStats_realtime_table_config.json");
 
-    URL resource = Quickstart.class.getClassLoader().getResource("sample_data/airlineStats_realtime_table_config.json");
+    URL resource = Quickstart.class.getClassLoader().getResource(
+        "sample_data/airlineStats_realtime_table_config.json");
     Preconditions.checkNotNull(resource);
     FileUtils.copyURLToFile(resource, tableConfigFile);
 
@@ -99,12 +100,10 @@ private QuickstartTableRequest prepareRealtimeTableRequest()
 
   private void startKafka() {
     _zookeeperInstance = ZkStarter.startLocalZkServer();
-
-    String kafkaClazz = "org.apache.pinot.core.realtime.impl.kafka.server.KafkaDataServerStartable";
     try {
-      _kafkaStarter = StreamDataProvider.getServerDataStartable(kafkaClazz, KafkaStarterUtils.getDefaultKafkaConfiguration());
+      _kafkaStarter = StreamDataProvider.getServerDataStartable(KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, KafkaStarterUtils.getDefaultKafkaConfiguration());
     } catch (Exception e) {
-      throw new RuntimeException("Failed to start " + kafkaClazz, e);
+      throw new RuntimeException("Failed to start " + KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, e);
     }
     _kafkaStarter.start();
     _kafkaStarter.createTopic("airlineStatsEvents", KafkaStarterUtils.getTopicCreationProps(10));
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/RealtimeQuickStart.java b/pinot-tools/src/main/java/org/apache/pinot/tools/RealtimeQuickStart.java
index b97064856d3..7b0696ad4bc 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/RealtimeQuickStart.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/RealtimeQuickStart.java
@@ -22,9 +22,9 @@
 import com.google.common.collect.Lists;
 import java.io.File;
 import java.net.URL;
-import java.util.Properties;
 import org.apache.commons.io.FileUtils;
 import org.apache.pinot.common.utils.ZkStarter;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.tools.Quickstart.Color;
@@ -72,12 +72,10 @@ public void execute()
 
     printStatus(Color.CYAN, "***** Starting Kafka *****");
     final ZkStarter.ZookeeperInstance zookeeperInstance = ZkStarter.startLocalZkServer();
-
-    String kafkaClazz = "org.apache.pinot.core.realtime.impl.kafka.server.KafkaDataServerStartable";
     try {
-      _kafkaStarter = StreamDataProvider.getServerDataStartable(kafkaClazz, KafkaStarterUtils.getDefaultKafkaConfiguration());
+      _kafkaStarter = StreamDataProvider.getServerDataStartable(KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, KafkaStarterUtils.getDefaultKafkaConfiguration());
     } catch (Exception e) {
-      throw new RuntimeException("Failed to start " + kafkaClazz, e);
+      throw new RuntimeException("Failed to start " + KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, e);
     }
     _kafkaStarter.start();
     _kafkaStarter.createTopic("meetupRSVPEvents", KafkaStarterUtils.getTopicCreationProps(10));
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StartKafkaCommand.java b/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StartKafkaCommand.java
index 53a1d1ca139..36d58e5a87f 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StartKafkaCommand.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StartKafkaCommand.java
@@ -20,10 +20,10 @@
 
 import java.io.File;
 import java.io.IOException;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.realtime.stream.StreamDataServerStartable;
 import org.apache.pinot.tools.Command;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.kohsuke.args4j.Option;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -71,11 +71,10 @@ public String description() {
   @Override
   public boolean execute()
       throws IOException {
-    String kafkaClazz = "org.apache.pinot.core.realtime.impl.kafka.server.KafkaDataServerStartable";
     try {
-      _kafkaStarter = StreamDataProvider.getServerDataStartable(kafkaClazz, KafkaStarterUtils.getDefaultKafkaConfiguration());
+      _kafkaStarter = StreamDataProvider.getServerDataStartable(KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, KafkaStarterUtils.getDefaultKafkaConfiguration());
     } catch (Exception e) {
-      throw new RuntimeException("Failed to start " + kafkaClazz, e);
+      throw new RuntimeException("Failed to start " + KafkaStarterUtils.KAFKA_SERVER_STARTABLE_CLASS_NAME, e);
     }
     _kafkaStarter.start();
 
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StreamAvroIntoKafkaCommand.java b/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StreamAvroIntoKafkaCommand.java
index e9639a85306..66ae94450a4 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StreamAvroIntoKafkaCommand.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/admin/command/StreamAvroIntoKafkaCommand.java
@@ -27,11 +27,11 @@
 import org.apache.avro.file.DataFileStream;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.pinot.common.utils.HashUtil;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.util.AvroUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProducer;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.tools.Command;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.kohsuke.args4j.Option;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/streams/AirlineDataStream.java b/pinot-tools/src/main/java/org/apache/pinot/tools/streams/AirlineDataStream.java
index 1d6a4b11796..b3bb8d60dc0 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/streams/AirlineDataStream.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/streams/AirlineDataStream.java
@@ -34,9 +34,9 @@
 import org.apache.pinot.common.data.Schema;
 import org.apache.pinot.common.data.TimeFieldSpec;
 import org.apache.pinot.common.utils.JsonUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProducer;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.apache.pinot.tools.Quickstart;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
diff --git a/pinot-tools/src/main/java/org/apache/pinot/tools/streams/MeetupRsvpStream.java b/pinot-tools/src/main/java/org/apache/pinot/tools/streams/MeetupRsvpStream.java
index d0d9ed1587b..7aafa0517b6 100644
--- a/pinot-tools/src/main/java/org/apache/pinot/tools/streams/MeetupRsvpStream.java
+++ b/pinot-tools/src/main/java/org/apache/pinot/tools/streams/MeetupRsvpStream.java
@@ -32,17 +32,15 @@
 import javax.websocket.Session;
 import org.apache.pinot.common.data.Schema;
 import org.apache.pinot.common.utils.JsonUtils;
+import org.apache.pinot.core.realtime.impl.kafka.KafkaStarterUtils;
 import org.apache.pinot.core.realtime.stream.StreamDataProducer;
 import org.apache.pinot.core.realtime.stream.StreamDataProvider;
 import org.apache.pinot.core.realtime.stream.StreamMessageDecoder;
-import org.apache.pinot.tools.KafkaStarterUtils;
 import org.glassfish.tyrus.client.ClientManager;
 
 
 public class MeetupRsvpStream {
 
-  private static final String DEFAULT_KAFKA_BROKER = "localhost:19092";
-
   private Schema schema;
   private StreamDataProducer producer;
   private boolean keepPublishing = true;
@@ -52,7 +50,7 @@ public MeetupRsvpStream(File schemaFile)
       throws Exception {
     schema = Schema.fromFile(schemaFile);
     Properties properties = new Properties();
-    properties.put("metadata.broker.list", DEFAULT_KAFKA_BROKER);
+    properties.put("metadata.broker.list", KafkaStarterUtils.DEFAULT_KAFKA_BROKER);
     properties.put("serializer.class", "kafka.serializer.DefaultEncoder");
     properties.put("request.required.acks", "1");
     producer = StreamDataProvider.getStreamDataProducer(KafkaStarterUtils.KAFKA_PRODUCER_CLASS_NAME, properties);
@@ -60,6 +58,7 @@ public MeetupRsvpStream(File schemaFile)
 
   public void stopPublishing() {
     keepPublishing = false;
+    producer.close();
     client.shutdown();
   }
 
diff --git a/pinot-tools/src/main/resources/sample_data/kafka_0.9/airlineStats_realtime_table_config.json b/pinot-tools/src/main/resources/sample_data/kafka_0.9/airlineStats_realtime_table_config.json
new file mode 100644
index 00000000000..5098f6a17ef
--- /dev/null
+++ b/pinot-tools/src/main/resources/sample_data/kafka_0.9/airlineStats_realtime_table_config.json
@@ -0,0 +1,37 @@
+{
+  "tableName": "airlineStats",
+  "tableType": "REALTIME",
+  "segmentsConfig": {
+    "timeColumnName": "DaysSinceEpoch",
+    "timeType": "DAYS",
+    "retentionTimeUnit": "DAYS",
+    "retentionTimeValue": "5",
+    "segmentPushType": "APPEND",
+    "segmentAssignmentStrategy": "BalanceNumSegmentAssignmentStrategy",
+    "schemaName": "airlineStats",
+    "replication": "1",
+    "replicasPerPartition": "1"
+  },
+  "tenants": {
+    "broker": "airline_broker",
+    "server": "airline"
+  },
+  "tableIndexConfig": {
+    "loadMode": "MMAP",
+    "streamConfigs": {
+      "streamType": "kafka",
+      "stream.kafka.consumer.type": "simple",
+      "stream.kafka.topic.name": "flights-realtime",
+      "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+      "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+      "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+      "stream.kafka.broker.list": "localhost:19092",
+      "realtime.segment.flush.threshold.time": "3600000",
+      "realtime.segment.flush.threshold.size": "50000",
+      "stream.kafka.consumer.prop.auto.offset.reset": "smallest"
+    }
+  },
+  "metadata": {
+    "customConfigs": {}
+  }
+}
diff --git a/pinot-tools/src/main/resources/sample_data/kafka_0.9/meetupRsvp_realtime_table_config.json b/pinot-tools/src/main/resources/sample_data/kafka_0.9/meetupRsvp_realtime_table_config.json
new file mode 100644
index 00000000000..e8fe4e410e4
--- /dev/null
+++ b/pinot-tools/src/main/resources/sample_data/kafka_0.9/meetupRsvp_realtime_table_config.json
@@ -0,0 +1,28 @@
+{
+  "tableName": "meetupRsvp",
+  "tableType": "REALTIME",
+  "segmentsConfig": {
+    "timeColumnName": "mtime",
+    "timeType": "MILLISECONDS",
+    "segmentPushType": "APPEND",
+    "segmentAssignmentStrategy": "BalanceNumSegmentAssignmentStrategy",
+    "schemaName": "meetupRsvp",
+    "replication": "1"
+  },
+  "tenants": {},
+  "tableIndexConfig": {
+    "loadMode": "MMAP",
+    "streamConfigs": {
+      "streamType": "kafka",
+      "stream.kafka.consumer.type": "highLevel",
+      "stream.kafka.topic.name": "meetupRSVPEvents",
+      "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+      "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory",
+      "stream.kafka.zk.broker.url": "localhost:2191/kafka"
+    }
+  },
+  "metadata": {
+    "customConfigs": {}
+  }
+}
diff --git a/pinot-tools/src/main/resources/sample_data/kafka_2.0/airlineStats_realtime_table_config.json b/pinot-tools/src/main/resources/sample_data/kafka_2.0/airlineStats_realtime_table_config.json
new file mode 100644
index 00000000000..f4267dac99c
--- /dev/null
+++ b/pinot-tools/src/main/resources/sample_data/kafka_2.0/airlineStats_realtime_table_config.json
@@ -0,0 +1,38 @@
+{
+  "tableName": "airlineStats",
+  "tableType": "REALTIME",
+  "segmentsConfig": {
+    "timeColumnName": "DaysSinceEpoch",
+    "timeType": "DAYS",
+    "retentionTimeUnit": "DAYS",
+    "retentionTimeValue": "5",
+    "segmentPushType": "APPEND",
+    "segmentAssignmentStrategy": "BalanceNumSegmentAssignmentStrategy",
+    "schemaName": "airlineStats",
+    "replication": "1",
+    "replicasPerPartition": "1"
+  },
+  "tenants": {
+    "broker": "airline_broker",
+    "server": "airline"
+  },
+  "tableIndexConfig": {
+    "loadMode": "MMAP",
+    "streamConfigs": {
+      "streamType": "kafka",
+      "stream.kafka.consumer.type": "simple",
+      "stream.kafka.topic.name": "flights-realtime",
+      "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory",
+      "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+      "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+      "stream.kafka.broker.list": "localhost:19092",
+      "realtime.segment.flush.threshold.time": "3600000",
+      "realtime.segment.flush.threshold.size": "50000",
+      "stream.kafka.consumer.prop.auto.offset.reset": "smallest"
+    }
+  },
+  "metadata": {
+    "customConfigs": {}
+  }
+}
diff --git a/pinot-tools/src/main/resources/sample_data/kafka_2.0/meetupRsvp_realtime_table_config.json b/pinot-tools/src/main/resources/sample_data/kafka_2.0/meetupRsvp_realtime_table_config.json
new file mode 100644
index 00000000000..7d4b8e833c1
--- /dev/null
+++ b/pinot-tools/src/main/resources/sample_data/kafka_2.0/meetupRsvp_realtime_table_config.json
@@ -0,0 +1,29 @@
+{
+  "tableName": "meetupRsvp",
+  "tableType": "REALTIME",
+  "segmentsConfig": {
+    "timeColumnName": "mtime",
+    "timeType": "MILLISECONDS",
+    "segmentPushType": "APPEND",
+    "segmentAssignmentStrategy": "BalanceNumSegmentAssignmentStrategy",
+    "schemaName": "meetupRsvp",
+    "replication": "1"
+  },
+  "tenants": {},
+  "tableIndexConfig": {
+    "loadMode": "MMAP",
+    "streamConfigs": {
+      "streamType": "kafka",
+      "stream.kafka.consumer.type": "highLevel",
+      "stream.kafka.topic.name": "meetupRSVPEvents",
+      "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+      "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory",
+      "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+      "stream.kafka.hlc.bootstrap.server": "localhost:19092"
+    }
+  },
+  "metadata": {
+    "customConfigs": {}
+  }
+}
diff --git a/pinot-tools/src/main/resources/sample_data/meetupRsvp_realtime_table_config.json b/pinot-tools/src/main/resources/sample_data/meetupRsvp_realtime_table_config.json
index a6ddc97e995..e8fe4e410e4 100644
--- a/pinot-tools/src/main/resources/sample_data/meetupRsvp_realtime_table_config.json
+++ b/pinot-tools/src/main/resources/sample_data/meetupRsvp_realtime_table_config.json
@@ -18,7 +18,7 @@
       "stream.kafka.topic.name": "meetupRSVPEvents",
       "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
       "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
-      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka.SimpleConsumerFactory",
+      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory",
       "stream.kafka.zk.broker.url": "localhost:2191/kafka"
     }
   },
diff --git a/pom.xml b/pom.xml
index 0d007298559..fb926b595ab 100644
--- a/pom.xml
+++ b/pom.xml
@@ -141,10 +141,22 @@
     kafka dependency is still explicitly defined in pinot-integration-tests, pinot-tools and pinot-perf pom files.
     To change kafka connector dependency, we only need to update this version number config.
     TODO: figure out a way to inject kafka dependency instead of explicitly setting the kafka module dependency -->
-    <kafka.lib.version>0.9</kafka.lib.version>
+    <kafka.version>0.9</kafka.version>
   </properties>
 
   <profiles>
+    <profile>
+      <id>kafka-2.0</id>
+      <activation>
+        <property>
+          <name>kafka.version</name>
+          <value>2.0</value>
+        </property>
+      </activation>
+      <properties>
+        <kafka.version>2.0</kafka.version>
+      </properties>
+    </profile>
     <profile>
       <id>travis</id>
       <activation>
