diff --git a/pinot-connectors/pinot-connector-kafka-2.0/README.md b/pinot-connectors/pinot-connector-kafka-2.0/README.md
index f70b7c371e9..b28f8213b2a 100644
--- a/pinot-connectors/pinot-connector-kafka-2.0/README.md
+++ b/pinot-connectors/pinot-connector-kafka-2.0/README.md
@@ -18,8 +18,40 @@
     under the License.
 
 -->
-# Pinot connector for kafka 2.0.x
+# Pinot connector for kafka 2.x
 
-This is an implementation of the kafka stream for kafka versions 2.0.x The version used in this implementation is kafka 2.0.0.
+This is an implementation of the kafka stream for kafka versions 2.x The version used in this implementation is kafka 2.0.0.
 
 A stream plugin for another version of kafka, or another stream, can be added in a similar fashion. Refer to documentation on (Pluggable Streams)[https://pinot.readthedocs.io/en/latest/pluggable_streams.html] for the specfic interfaces to implement.
+
+* How to build and release Pinot package with Kafka 2.x connector
+```$xslt
+mvn clean package -DskipTests -Pbin-dist -Dkafka.version=2.0
+```
+
+* How to use Kafka 2.x connector
+Below is a sample `streamConfigs` used to create a realtime table with Kafka Stream(High) level consumer:
+```$xslt
+"streamConfigs": {
+  "streamType": "kafka",
+  "stream.kafka.consumer.type": "highLevel",
+  "stream.kafka.topic.name": "meetupRSVPEvents",
+  "stream.kafka.decoder.class.name": "org.apache.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder",
+  "stream.kafka.hlc.zk.connect.string": "localhost:2191/kafka",
+  "stream.kafka.consumer.factory.class.name": "org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory",
+  "stream.kafka.zk.broker.url": "localhost:2191/kafka",
+  "stream.kafka.hlc.bootstrap.server": "localhost:19092"
+}
+```
+
+* Upgrade from Kafka 0.9 connector to Kafka 2.x connector:
+
+  1. Update  table config:
+ `stream.kafka.consumer.factory.class.name` from `org.apache.pinot.core.realtime.impl.kafka.KafkaConsumerFactory` to `org.apache.pinot.core.realtime.impl.kafka2.KafkaConsumerFactory`.
+
+  1. If using Stream(High) level consumer, please also add config `stream.kafka.hlc.bootstrap.server` into `tableIndexConfig.streamConfigs`.
+This config should be the URI of Kafka broker lists, e.g. `localhost:9092`.
+
+* How to upgrade to Kafka version > `2.0.0`
+This connector is also suitable for Kafka lib version higher than `2.0.0`.
+In `pinot-connector-kafka-2.0/pom.xml` change the `kafka.lib.version` from `2.0.0` to `2.1.1` will make this Connector working with Kafka `2.1.1`.
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java
index df8f8cdd4c0..0a80a30f38e 100644
--- a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaPartitionLevelConnectionHandler.java
@@ -32,6 +32,11 @@
 import org.apache.pinot.core.realtime.stream.StreamConfig;
 
 
+/**
+ * KafkaPartitionLevelConnectionHandler provides low level APIs to access Kafka partition level information.
+ * E.g. partition counts, offsets per partition.
+ *
+ */
 public abstract class KafkaPartitionLevelConnectionHandler {
 
   protected final KafkaPartitionLevelStreamConfig _config;
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java
index b8c04ca1916..9a0eb958909 100644
--- a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumer.java
@@ -20,7 +20,6 @@
 
 import com.yammer.metrics.core.Meter;
 import java.time.Duration;
-import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
@@ -55,7 +54,7 @@ public class KafkaStreamLevelConsumer implements StreamLevelConsumer {
   private String _tableAndStreamName;
 
   private StreamConfig _streamConfig;
-  private KafkaHighLevelStreamConfig _kafkaHighLevelStreamConfig;
+  private KafkaStreamLevelStreamConfig _kafkaStreamLevelStreamConfig;
 
   private KafkaConsumer<Bytes, Bytes> consumer;
   private ConsumerRecords<Bytes, Bytes> consumerRecords;
@@ -74,7 +73,7 @@ public KafkaStreamLevelConsumer(String clientId, String tableName, StreamConfig
       InstanceZKMetadata instanceZKMetadata, ServerMetrics serverMetrics) {
     _clientId = clientId;
     _streamConfig = streamConfig;
-    _kafkaHighLevelStreamConfig = new KafkaHighLevelStreamConfig(streamConfig, tableName, instanceZKMetadata);
+    _kafkaStreamLevelStreamConfig = new KafkaStreamLevelStreamConfig(streamConfig, tableName, instanceZKMetadata);
     _serverMetrics = serverMetrics;
 
     _messageDecoder = StreamDecoderProvider.create(streamConfig, schema);
@@ -88,7 +87,7 @@ public KafkaStreamLevelConsumer(String clientId, String tableName, StreamConfig
   @Override
   public void start()
       throws Exception {
-    consumer = KafkaStreamLevelConsumerManager.acquireKafkaConsumerForConfig(_kafkaHighLevelStreamConfig);
+    consumer = KafkaStreamLevelConsumerManager.acquireKafkaConsumerForConfig(_kafkaStreamLevelStreamConfig);
   }
 
   private void updateKafkaIterator() {
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java
index 0f79b888830..9df8d336634 100644
--- a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelConsumerManager.java
@@ -69,10 +69,10 @@ public class KafkaStreamLevelConsumerManager {
       new HashMap<>();
   private static final IdentityHashMap<KafkaConsumer, Long> CONSUMER_RELEASE_TIME = new IdentityHashMap<>();
 
-  public static KafkaConsumer acquireKafkaConsumerForConfig(KafkaHighLevelStreamConfig kafkaHighLevelStreamConfig) {
+  public static KafkaConsumer acquireKafkaConsumerForConfig(KafkaStreamLevelStreamConfig kafkaStreamLevelStreamConfig) {
     final ImmutableTriple<String, String, String> configKey =
-        new ImmutableTriple<>(kafkaHighLevelStreamConfig.getKafkaTopicName(), kafkaHighLevelStreamConfig.getGroupId(),
-            kafkaHighLevelStreamConfig.getBootstrapServers());
+        new ImmutableTriple<>(kafkaStreamLevelStreamConfig.getKafkaTopicName(), kafkaStreamLevelStreamConfig.getGroupId(),
+            kafkaStreamLevelStreamConfig.getBootstrapServers());
 
     synchronized (KafkaStreamLevelConsumerManager.class) {
       // If we have the consumer and it's not already acquired, return it, otherwise error out if it's already acquired
@@ -88,13 +88,13 @@ public static KafkaConsumer acquireKafkaConsumerForConfig(KafkaHighLevelStreamCo
       }
 
       LOGGER.info("Creating new kafka consumer and iterator for topic {}",
-          kafkaHighLevelStreamConfig.getKafkaTopicName());
+          kafkaStreamLevelStreamConfig.getKafkaTopicName());
 
       // Create the consumer
 
       Properties consumerProp = new Properties();
-      consumerProp.putAll(kafkaHighLevelStreamConfig.getKafkaConsumerProperties());
-      consumerProp.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaHighLevelStreamConfig.getBootstrapServers());
+      consumerProp.putAll(kafkaStreamLevelStreamConfig.getKafkaConsumerProperties());
+      consumerProp.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaStreamLevelStreamConfig.getBootstrapServers());
       consumerProp.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
       consumerProp.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, BytesDeserializer.class.getName());
       if (consumerProp.containsKey(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG) && consumerProp
@@ -102,13 +102,13 @@ public static KafkaConsumer acquireKafkaConsumerForConfig(KafkaHighLevelStreamCo
         consumerProp.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
       }
       KafkaConsumer consumer = new KafkaConsumer<>(consumerProp);
-      consumer.subscribe(Collections.singletonList(kafkaHighLevelStreamConfig.getKafkaTopicName()));
+      consumer.subscribe(Collections.singletonList(kafkaStreamLevelStreamConfig.getKafkaTopicName()));
 
       // Mark both the consumer and iterator as acquired
       CONSUMER_FOR_CONFIG_KEY.put(configKey, consumer);
       CONSUMER_RELEASE_TIME.put(consumer, IN_USE);
 
-      LOGGER.info("Created consumer with id {} for topic {}", consumer, kafkaHighLevelStreamConfig.getKafkaTopicName());
+      LOGGER.info("Created consumer with id {} for topic {}", consumer, kafkaStreamLevelStreamConfig.getKafkaTopicName());
 
       return consumer;
     }
diff --git a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaHighLevelStreamConfig.java b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java
similarity index 93%
rename from pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaHighLevelStreamConfig.java
rename to pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java
index 9bbb5decd9f..062fa326454 100644
--- a/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaHighLevelStreamConfig.java
+++ b/pinot-connectors/pinot-connector-kafka-2.0/src/main/java/org/apache/pinot/core/realtime/impl/kafka2/KafkaStreamLevelStreamConfig.java
@@ -33,7 +33,7 @@
 /**
  * Wrapper around {@link StreamConfig} for use in the {@link KafkaStreamLevelConsumer}
  */
-public class KafkaHighLevelStreamConfig {
+public class KafkaStreamLevelStreamConfig {
   private static final String DEFAULT_AUTO_COMMIT_ENABLE = "false";
 
   private static final Map<String, String> defaultProps;
@@ -48,7 +48,7 @@ public class KafkaHighLevelStreamConfig {
    * @param tableName
    * @param instanceZKMetadata
    */
-  public KafkaHighLevelStreamConfig(StreamConfig streamConfig, String tableName,
+  public KafkaStreamLevelStreamConfig(StreamConfig streamConfig, String tableName,
       InstanceZKMetadata instanceZKMetadata) {
     Map<String, String> streamConfigMap = streamConfig.getStreamConfigsMap();
 
@@ -94,7 +94,7 @@ public Properties getKafkaConsumerProperties() {
 
   @Override
   public String toString() {
-    return "KafkaHighLevelStreamConfig{" + "_kafkaTopicName='" + _kafkaTopicName + '\'' + ", _groupId='" + _groupId
+    return "KafkaStreamLevelStreamConfig{" + "_kafkaTopicName='" + _kafkaTopicName + '\'' + ", _groupId='" + _groupId
         + '\'' + ", _bootstrapServers='" + _bootstrapServers + '\'' + ", _kafkaConsumerProperties="
         + _kafkaConsumerProperties + '}';
   }
@@ -109,7 +109,7 @@ public boolean equals(Object o) {
       return false;
     }
 
-    KafkaHighLevelStreamConfig that = (KafkaHighLevelStreamConfig) o;
+    KafkaStreamLevelStreamConfig that = (KafkaStreamLevelStreamConfig) o;
 
     return EqualityUtils.isEqual(_kafkaTopicName, that._kafkaTopicName) && EqualityUtils
         .isEqual(_groupId, that._groupId) && EqualityUtils.isEqual(_bootstrapServers, that._bootstrapServers)
