diff --git a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ParquetMetadata.java b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ParquetMetadata.java
index d35582af26..a645117505 100644
--- a/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ParquetMetadata.java
+++ b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ParquetMetadata.java
@@ -27,6 +27,7 @@
 import org.codehaus.jackson.JsonParseException;
 import org.codehaus.jackson.map.JsonMappingException;
 import org.codehaus.jackson.map.ObjectMapper;
+import org.codehaus.jackson.map.ObjectWriter;
 import org.codehaus.jackson.map.SerializationConfig.Feature;
 
 /**
@@ -38,11 +39,7 @@
  */
 public class ParquetMetadata {
 
-  private static ObjectMapper objectMapper = new ObjectMapper();
-  private static ObjectMapper prettyObjectMapper = new ObjectMapper();
-  static {
-    prettyObjectMapper.configure(Feature.INDENT_OUTPUT, true);
-  }
+  private static final ObjectMapper objectMapper = new ObjectMapper();
 
   /**
    *
@@ -50,7 +47,7 @@ public class ParquetMetadata {
    * @return the json representation
    */
   public static String toJSON(ParquetMetadata parquetMetaData) {
-    return toJSON(parquetMetaData, objectMapper);
+    return toJSON(parquetMetaData, false);
   }
 
   /**
@@ -59,13 +56,17 @@ public static String toJSON(ParquetMetadata parquetMetaData) {
    * @return the pretty printed json representation
    */
   public static String toPrettyJSON(ParquetMetadata parquetMetaData) {
-    return toJSON(parquetMetaData, prettyObjectMapper);
+    return toJSON(parquetMetaData, true);
   }
 
-  private static String toJSON(ParquetMetadata parquetMetaData, ObjectMapper mapper) {
+  private static String toJSON(ParquetMetadata parquetMetaData, boolean isPrettyPrint) {
     StringWriter stringWriter = new StringWriter();
     try {
-      mapper.writeValue(stringWriter, parquetMetaData);
+      if (isPrettyPrint) {
+        objectMapper.writerWithDefaultPrettyPrinter().writeValue(stringWriter, parquetMetaData);
+      } else {
+        objectMapper.writeValue(stringWriter, parquetMetaData);
+      }
     } catch (JsonGenerationException e) {
       throw new RuntimeException(e);
     } catch (JsonMappingException e) {
diff --git a/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java b/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java
index 1a740fe852..0ac7be7e3d 100644
--- a/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java
+++ b/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java
@@ -262,7 +262,7 @@ public void randomTestFilterMetaData() {
   }
 
   @Test
-  public void testNullFieldMetadataDebugLogging() throws NoSuchFieldException, IllegalAccessException, IOException {
+  public void testNullFieldMetadataDebugLogging() {
     MessageType schema = parseMessageType("message test { optional binary some_null_field; }");
     org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);
     List<BlockMetaData> blockMetaDataList = new ArrayList<BlockMetaData>();
@@ -273,6 +273,16 @@ public void testNullFieldMetadataDebugLogging() throws NoSuchFieldException, Ill
     ParquetMetadata.toJSON(metadata);
   }
 
+  @Test
+  public void testMetadataToJson() {
+    ParquetMetadata metadata = new ParquetMetadata(null, null);
+    assertEquals("{\"fileMetaData\":null,\"blocks\":null}", ParquetMetadata.toJSON(metadata));
+    assertEquals("{\n" +
+            "  \"fileMetaData\" : null,\n" +
+            "  \"blocks\" : null\n" +
+            "}", ParquetMetadata.toPrettyJSON(metadata));
+  }
+
   private ColumnChunkMetaData createColumnChunkMetaData() {
     Set<Encoding> e = new HashSet<Encoding>();
     PrimitiveTypeName t = PrimitiveTypeName.BINARY;
