project,pull_number,pull_type,id,text,classification,indicator
beam,1974,review,100887469,"What is the relationship between ""dataflow counters"" and ""metric updates""?  This comment refers to counters, while the code below refers to metric update protos.  Should this comment instead mention translating accumulators for metric updates so that this class should be renamed accordingly?
Also, the naming of the methods below is confusing: two arguments are passed, e.g. set_boolean(accumulator, metric_update_proto).  Just reading this signature suggests that the accumulator will somehow have a boolean set from metric_update_proto, which is the opposite of what is intended.  Can you instead rename these methods translate_*, e.g. for translate_boolean(accumulator, metric_update_proto)?  This would make it clear that we are translating accumulator into metric_update_proto.  If you do this, can you rename set_scalar and set_mean in this file as well?",code_debt,low_quality_code
spark,29795,review,493190061,nit: use local variable if possible,code_debt,low_quality_code
beam,13137,review,522334301,Maybe you also need to add null checks in process... up to you.,code_debt,low_quality_code
pulsar,3019,review,243864237,"yes, I think earlier code was not formatted with formatter that caused this reformatting change.",code_debt,low_quality_code
druid,3939,review,102355893,Use log message formatting `%d`,code_debt,low_quality_code
spark,21476,review,192476723,"This is so specific to the way YARN runs things that I don't think it would be useful anywhere else. If at some point it becomes useful, the code can be moved.
I think the tests I added are better than just unit testing this function, since that way the code is actually being run through YARN and bash.",code_debt,low_quality_code
flink,6159,review,195443441,"@Nonnegative int keyGroupId, for consistency",code_debt,low_quality_code
kafka,3325,review,128649545,"Instead of using this separate class and do the `instanceof` check on each call (which maybe expensive), maybe we could just have a `WrappedBatchingStateRestoreCallback` which only takes the non-batching `StateRestoreCallback` in constructor and then in `restoreAll` always do the for-loop, and in places that we need it (seems we only have two callers) we can do sth. like
Just once.",code_debt,low_quality_code
spark,8093,review,36782853,"Hmm... you'll also need to change `taskEndReasonFromJson`, otherwise the history server won't see the new information. Also, when adding new properties to serialized classes, we generally use `Option[Foo]` (see calls to `Utils.jsonOption` in this class).",code_debt,low_quality_code
superset,10728,review,479692093,"nit: commented out code bothers me. It's in a storybook, though, so I won't strongly object.",code_debt,low_quality_code
drill,1892,review,363140288,"You probably *really* don't want to do this. Converting the response to a string causes the client to download the **entire** response to memory, then convert it to a Java string. Doing so completely negates the disk-based buffering that seems to be implemented below.",code_debt,low_quality_code
spark,4363,review,24212525,nit: indent two spaces,code_debt,low_quality_code
spark,23181,review,237952522,nit: `clone()`,code_debt,low_quality_code
flink,7759,review,258458285,"consider converting the following conditions to methods, this would assign a name to the condition, and possibly would make it a bit easier to test.",code_debt,low_quality_code
spark,13565,review,66319324,"Pretty sure that such a long duration isn't really necessary, but I don't think it hurts to make it longer just in case.",code_debt,low_quality_code
incubator-doris,3587,review,425210765,The second parameter is better to be a complete sentence.,code_debt,low_quality_code
spark,20194,review,161000312,"Impala is the only reference I can find https://www.cloudera.com/documentation/enterprise/5-10-x/topics/impala_show.html
To be consistent with the other SHOW function, we can make LIKE optional?",code_debt,low_quality_code
spark,1218,review,15828037,"@mridulm yes I'm aware of that. But before, SparkContext was using reflection to instantiate two different classes in the yarn package, and then connect them manually. I removed one of those (see that there's still reflection code to load `YarnClusterScheduler`) because it seemed unnecessary.",code_debt,dead_code
flink,4364,review,128512830,"In very rare cases, it might. I want to change the `Execution` a bit on the `master` to make this unnecessary.
However, that is too much surgery in a critical part for a bugfix release, so I decided to be conservative in the runtime code and rather pay this price in the tests.",code_debt,complex_code
trafficserver,6379,review,373984797,i think this is already defined in #6317 so it should not be here again ?,code_debt,low_quality_code
incubator-heron,176,review,56283550,"instead of string formatter - can we use string builder so that we don't have to count the number of formatting arguments - often it takes a couple of iterations to get this right?  String Builder now will be good especially with the CLI options, thoughts?",code_debt,low_quality_code
spark,8531,review,48378751,I'm thinking of taking over this PR and am considering dropping this logic since it adds complexity and might not be a huge performance issue in practice. Let me know if you disagree.,code_debt,complex_code
superset,13210,review,582436382,"`scalar` in linear algebra, relative to `vector`. refer to SQLAlachemy and Pandas variable naming. 
`isArray` more straightforward, I changed this variable name.",code_debt,low_quality_code
spark,3651,review,21697554,"OK, in the name of keeping it simple I might not touch this this time. Since this occurs 2 places only, it doesn't save much.",code_debt,low_quality_code
flink,6173,review,196042694,"We have unmodifiable map alternatives in the JDK. If there is not a good reason why we need guava here, I would suggest to solve this without this import. Even in that case, I would use import and not fully qualified classname.",code_debt,low_quality_code
spark,23062,review,234404871,Spacing here. Is this simpler with .forall?,code_debt,low_quality_code
spark,11620,review,56418491,indentation,code_debt,low_quality_code
incubator-heron,1820,review,113760959,Is this the implementation class? If so can we use `class` instead of type to be more explicit?,code_debt,low_quality_code
orc,570,review,528911518,Could you add `@deprecated` to give a proper warning?,code_debt,low_quality_code
calcite,2286,review,534475582,"What about using `rules(RelTrait in, JdbcConvention out, RelBuilderFactory relBuilderFactory)` here, but with null `in` value and adding if statement in that other overloaded method?
It allows slightly avoid duplication of code",code_debt,duplicated_code
spark,15971,review,89209660,"This is pretty hacky. It makes assumptions about how things are formatted in the event log.
The previous assert should be enough (ignoring my previous comment about changing this test).",code_debt,complex_code
nifi-minifi-cpp,1040,review,605509404,"minor, but `org::apache::nifi::minifi::` is not needed
in a few other places, too, `org::apache::nifi::minifi::utils::StringUtils::toBool(...)` could be shortened to `utils::StringUtils::toBool(...)`",code_debt,dead_code
attic-stratos,192,review,23697450,I think its better to use org.wso2.carbon.utils.CarbonUtils.getCarbonHome() here.,code_debt,low_quality_code
nifi,4767,review,561227339,Perhaps renaming this variable to something like `DEFAULT_STORE_TYPE` would clarify the reason for declaring it here as opposed to just using the enum value where necessary.,code_debt,low_quality_code
beam,4910,review,175947778,At this pointm it may be worth wrapping the getXOrThrow calls to actually return a legible error message since the generated code does not print the input ids.,code_debt,low_quality_code
spark,23982,review,263647807,nit: `left_anti`,code_debt,low_quality_code
lucene-solr,2229,review,562524364,"Would it be acceptable to reformat the code when such a demand appears, or after I'm finished with these improvements? These ""some people"" might never need to look into Hunspell code.",code_debt,low_quality_code
hadoop,1881,review,388858948,nit: this line is long enough its time to split,code_debt,low_quality_code
druid,10070,review,445155089,minor nit:mark return type as nullable,code_debt,low_quality_code
tvm,1403,review,201886571,"this warning seems can be a bit frequent, instead of this, do
Do CHECK_GE(sorted_order_.size(), threads_.size());",code_debt,low_quality_code
kafka,7799,review,364476808,"We only needed it here because the we were trying to ensure we had a chance to send the Abort before going into poll. After thinking about it, it seemed a little simpler to move the `hasAbortableError` check to `maybeSendAndPollTransactionalRequest`. Then we no longer need this change.",code_debt,low_quality_code
netbeans-website,473,review,430013474,Mmm.... infra will complain if we use jenkins builds as an update center. Also note this is extremely slow...,code_debt,slow_algorithm
zookeeper,924,review,299043313,Unused default constructor.,code_debt,dead_code
phoenix,516,review,292095594,Do you mean I should remove the `addPKColumns` variable and change the if statement to something like the following:,code_debt,low_quality_code
hadoop,802,review,291163702,nit: stick at the top (line 20) with the gab above the org.apache stuff. We can at least try to not make import ordering worse,code_debt,low_quality_code
beam,10455,review,362682454,Is it possible to reverse this (`SqlTypeFamily.BINARY.equals(family)`)? That is the preferred style and also eliminates the need for a null check.,code_debt,low_quality_code
spark,5760,review,29388845,"OK, yeah I see that in the docs, though it's not set up that way in CDH (at least, maybe my installation never needed to configure that file a certain way, dunno). It seems bad to silently ignore unreadable files, so at least log it maybe? then... should it be a warning? because it sounds like there's one file that could reasonably be expected to be unreadable. Do we special case it and warn on anything else? fail on anything else?  i'd rather tighten this up in some way from doing this silently.",code_debt,low_quality_code
storm,215,review,15836939,"We should remove Config.UI_ACTIONS_ENABLED, or at least deprecate it.",code_debt,dead_code
openwhisk,1071,review,75486973,"As noted in personal discussion:
I found it hard to grasp the code because we use `Unit` returning functions which resolve a `Promise` at some point. This reads like it blocks the code until the result is there (which makes no sense at this point).
I'ld prefer to use `Future` composition if possible so it becomes more apparent that this is in fact returning a Future which is resolved as soon as the ack-message is there.",code_debt,low_quality_code
dubbo,3162,review,245860102,"Personally think that this place is worth discussing. Most of the exceptions have been handled in connect and disconnect, and there is a log. I think the log here can be removed.",code_debt,low_quality_code
spark,15627,review,85951050,"If we can cleanup the variables names above I think it would help a lot, the test is confusing.  I know you just copy and pasted but would be nice to clean up.
Also can we have 3 tests or 3 asserts,  
- one for same file in --files
- one for same file in --archives
- one for same file in --files and --archives",code_debt,low_quality_code
spark,204,review,11469526,"Very minor - but I'm guessing the first error condition will happen way more often than the second - and getting a message that says it's ""invalid"" is a lot less clear than just saying it doesn't exist. What about breaking these out and including more specific error messages in either case? Also it might be nice to print the value of `fileSystem` in the error message, so it knows what filesystem we inferred the path to be from.",code_debt,low_quality_code
attic-apex-core,479,review,108591304,"This method doesn't always create a new DeployRequest - can we name it appropriately? Say ""addOrModifyDeployRequest"" or ""getDeployRequestForContainerId"". By the way, would it be an error in some cases to find an existing DeployRequest for a given containerId? For example when you are calling this from StreamingContainerManager.scheduleContainerRestart(), wouldn't it be an error to find an existing request? If true, we should add code to detect such errors. The caller passes a parameter to indicate that only new request is expected to be created and so on...",code_debt,low_quality_code
spark,1222,review,15478958,"We shouldn't remove this... This is intended to be a general purpose class for logging that handles compression and buffering and other logic. I notice that you replaced this with PrintWriter elsewhere. Even with the new naming scheme you can still use this class for event logging, and it will simplify `EventLoggingListener` a bunch.",code_debt,low_quality_code
kafka,6536,review,275588869,"Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true; it's guaranteed that there is a headers object.
Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.",code_debt,complex_code
flink,1640,review,52993965,"I think you can create the callback once in the open method and then pass the instance to all async calls.
This way, you save a lot of instance creations",code_debt,low_quality_code
beam,5028,review,181501125,"yes, if max input is sys.maxint, then we will get 57. I increase bucket size by 1 just for safe purpose, maybe unnecessary. But I don't think one more bucket per counter will cost too many memory.",code_debt,low_quality_code
spark,20787,review,175982564,"Hm, this should have been caught by Scala linter because we follow Java style comment. See ""Code documentation style"" in http://spark.apache.org/contributing.html",code_debt,low_quality_code
flink,6236,review,241972720,Format the code to be consistent with old functions,code_debt,low_quality_code
calcite,2369,review,593096896,"Makes sense. I removed these assertions completely, as they are unrelated to the fix. Also added comments to the tests.",code_debt,low_quality_code
spark,10993,review,51332430,"AFAICT, this is never used, so I removed it.",code_debt,dead_code
flink,14839,review,586411292,Why `rethrow` as `RuntimeException`?,code_debt,low_quality_code
cloudstack,768,review,39469736,Why are deprecated methods being used?,code_debt,low_quality_code
drill,1251,review,187210052,I just removed this detail since we are saying these libraries are deprecated and we no longer want to use them.,code_debt,dead_code
spark,4690,review,25098634,"I think we can back that out if there's any question to keep this limited. @JoshRosen Yes that's where we ended up again, now that it's clear that there are several ways and several places this can happen. It's easiest just to ignore the exception.",code_debt,low_quality_code
kafka,2320,review,94887536,"I thought about it before, but it is a bit tricky to do since it could be dynamic based on which class the `toString` function is triggered first.  Let me think about it more and see if I can come with a clean solution.",code_debt,low_quality_code
beam,8270,review,274875921,Seems it'd be worth factoring this out into a context (including the expansion service jar test above).,code_debt,low_quality_code
airflow,2372,review,140193220,This is redundant.,code_debt,complex_code
spark,23747,review,269640058,Nit: Redundant conversion,code_debt,complex_code
cloudstack,1578,review,77754965,"Per our coding standards, please wrap all `if` blocks in curly braces.",code_debt,low_quality_code
spark,15237,review,90548133,"this seems a little more complicated than is really necessary for the what you're doing here.  couldn't you achieve the same thing by leaving the original code and changing the one line above the original to:
not exactly the same -- it also allows whitespace around the scheduling mode, but maybe a good thing?",code_debt,complex_code
incubator-mxnet,15277,review,296085060,nit: add this blank line back in.,code_debt,low_quality_code
ozone,1233,review,482676002,"need space before ""+""",code_debt,low_quality_code
carbondata,1321,review,138830621,"`null != sortScope` can be removed, it is checked inside `CarbonUtil.isValidSortOption`",code_debt,complex_code
openwhisk,4870,review,399976882,"I'd think we explicitly do not want to have the prewarm pool survive, if other actions would benefit from using that space. After all, it's a performance optimization, not a guarantee.
Wouldn't this also be plumbed into the controller for it to not send requests down a path where they might not get executed?
I think this warrants a dev-list discussion 🤔",code_debt,slow_algorithm
spark,5355,review,28042923,`np.uint64` -> `np.int32` (to be consistent with Scala implementation),code_debt,low_quality_code
trafficcontrol,3601,review,297263939,"maybe just add a ""finally"" instead of a ""then""?",code_debt,low_quality_code
jena,337,review,159239355,"Doing just one operation for fluent seems inconsistent. `Context` manipulation isn't (shouldn't) be that common an operation.
See also the quite recent `Context.mergeCopy`.",code_debt,low_quality_code
airflow,11132,review,495856534,"Yea default values can be hardcoded.
And also agree that showing a precise message for the used config will be better",code_debt,low_quality_code
iceberg,1793,review,553700047,Same with these other methods. Should these be primitives?,code_debt,low_quality_code
trafodion,1036,review,109077021,Would this change be needed if we touch the package name in T2 source to include apache ? Should we change the package name (org.apache.trafodion...) in the source and the file layout  to make it consistent ?,code_debt,low_quality_code
spark,1484,review,23672440,It might be easier to read if we use multiple lines:,code_debt,low_quality_code
spark,28952,review,447482241,"let's put `10` as a parameter, to make this method a bit more general.",code_debt,low_quality_code
spark,4588,review,26089697,super nit: order. swap with import above.,code_debt,low_quality_code
arrow,5451,review,329113827,"This is ""bad"", according to the tidyverse style guide, which I believed we were trying to follow: https://style.tidyverse.org/functions.html#long-lines-1
I can get used to whatever style conventions we decide, just want to make sure we're in agreement.",code_debt,low_quality_code
druid,50,review,2541441," Also, there's no need for the String.format(), preconditions will do %s interpolation for you (I realize the String.format()was there before, but let's take this chance to get rid of it).",code_debt,low_quality_code
flink,12042,review,422441179,"in fact, the table qualified name may be long. in addition, we may need to add catalog name and database name to distinguish between tables with the same name.",code_debt,low_quality_code
flink,12268,review,430016877,"hmm, not sure. Maybe this happens because the JVM writes that to the STDERR. That's annoying :/
This is probably fine then; I was initially worried we might get so long filenames that you wouldn't be able to extract the logs on Windows.",code_debt,low_quality_code
spark,6959,review,33198256,"AFAIK we don't have a style guide for Java code, but I think we should put spaces after the casts.",code_debt,low_quality_code
kafka,2405,review,97622033,Feels like this collection is redundant. You can get the name from `InternalTopicConfig` perhaps?,code_debt,complex_code
spark,27612,review,380444937,"~nit. If possible, can we have additional test statements for `minute` and `second` together below line 743 to make it sure?~
Never mind. I missed the other PR which landed already on `branch-2.4`.",code_debt,low_quality_code
kafka,1791,review,80134212,Rather than using `!muted` I think it would be much clearer to alias this to a boolean that is named: `guaranteeExpirationOrder` similar to the `guaranteeMessageOrder` boolean in `Sender`,code_debt,low_quality_code
spark,25626,review,320095359,"nit: we can use multiline string, e.g.",code_debt,low_quality_code
spark,15009,review,84790527,You can simplify all this by doing:,code_debt,complex_code
spark,25811,review,324969028,I've just renamed this as it made me confused - I imagined DB as LevelDB but there's separate suite for LevelDB. KVStore sounds better to me.,code_debt,low_quality_code
cloudstack,2578,review,225156473,"This message looks a little bit misleading, or am I mistaken?",code_debt,low_quality_code
spark,29992,review,504358081,Quick question: can we avoid catching `NullPointerException`? It's a bit odd that we catch `NullPointerException`. We could just switch to if-else I guess.,code_debt,low_quality_code
storm,2911,review,283128941,Unnecessary whitespace,code_debt,low_quality_code
kafka,6521,review,273258127,"Could you add a check to verify that the returned iterator is empty. Something along the lines of `assertThat(iterator.hasNext(), is(false))`?
Could you also add a test for a range query where the start key is equal to the end key? Such a unit test ensures correct behaviour for this special case.    
nit: I would rename the test to `shouldReturnEmptyIteratorForRangeQueryWithInvalidKeyRange`. Correct me, if I am wrong, but I think the empty iterator and the invalid key range are the points here, not the negative starting key. I would even change the range from (-1, 1) to (5, 3). It took me a bit to understand why (-1, 1) is an invalid range. 
These comments apply also to the unit tests below.",code_debt,low_quality_code
apisix-dashboard,979,review,543388564,hard code is not a good way,code_debt,low_quality_code
iceberg,933,review,411434730,I think it makes sense to reuse the definition.,code_debt,low_quality_code
drill,996,review,146815141,Could you please factor out this logic in a separate method?,code_debt,low_quality_code
nifi,914,review,75858615,"Might be able to create a utility method in the abstract class like `byte[] getRow(String row, String encoding)` since it looks PutHBaseCell and PutHBaseJson both need the same logic",code_debt,low_quality_code
druid,3928,review,106563776,"formatting, new lines would be easier to read",code_debt,low_quality_code
flink,11047,review,379996777,Can we introduce a `TableSinkFactoryContextImpl` class to reduce so many anonymous classes?,code_debt,low_quality_code
incubator-mxnet,13549,review,246983078,indent.,code_debt,low_quality_code
spark,15009,review,106230108,nit: move to previous line,code_debt,low_quality_code
flink,7123,review,236347738,missing space **...setup (data ...**,code_debt,low_quality_code
cloudstack,2058,review,139283005,"Fix indentations if you want to remove try-catch, also add a test?",code_debt,low_quality_code
netbeans,2324,review,511329375,Using `Logger` would be more standard. See [Logging in NetBeans](http://bits.netbeans.org/dev/javadoc/org-openide-util/org/openide/util/doc-files/logging.html) document.,code_debt,low_quality_code
ignite,8048,review,480182091,Bad formatting.,code_debt,low_quality_code
brooklyn-server,835,review,142912453,"TL;DR: I agree this is fine. Below are some notes from digging around in the synchronization code.
Looking at the other synchronization blocks, `emitInternal` will synchronize on `EntityManagementSupport` instance (when it calls `getSubscriptionContext()`. It will then call `publish` which will synchronize on `LocalSubscriptionManager` instance (in `getSubscriptionsForEntitySensor`).
However, I think we can trust both the `EntityManagementSupport` and the `LocalSubscriptionManager` to not call out to alien code while holding a lock on itself. Therefore we should be safe in that respect.
The code in `AbstractEntity` and `AbstractGroupImpl` looks a bit scary, where it first gets the lock on this `values` object and then on either `abstractGroup.members` or `abstractEntity.children` (but it's kind-of understandable why it does that and how that pattern avoids it; it would just be nicer if instead we didn't call out to alient code while holding the lock on `values` - but that's a bigger discussion than for this PR):",code_debt,low_quality_code
spark,448,review,11813558,"Yeah, you are going to end up getting the same thing.  I'd say we drop this one and leave the other.  Right now it probably doesn't matter, but the other one is lazy and gives the optimizer a chance to possibly improve things before actually executing.",code_debt,low_quality_code
flink,6178,review,196555570,maybe more descriptive name than `get`.,code_debt,low_quality_code
flink,4851,review,145917513,"ad 1. - that's why I have used `StateAssignmentOperation.operatorSubtaskStateFrom`, to share this logic. I didn't want to use all of the `StateAssignmentOperation`, because it's constructor is really annoying to fulfil.
ad 2. - that's a valid concern, however writing ITCases for this might be also an overkill. And wouldn't it be to necessary to test it against every state backend, to make sure that there are no introduced quirks during (de)serialisation?",code_debt,low_quality_code
spark,16706,review,98121245,"nit: remove ""of bytes"".",code_debt,low_quality_code
airflow,5343,review,288918215,"This overall feels like it makes the code cleaner, but I do wonder if it's useful to keep this config setting (so that people can tweak the behaviour of the Airflow without _having_ to go in to the UI and change behaviours.
If we do decide to delete these two config values we should add a note to UPDATING.md about it.",code_debt,low_quality_code
systemds,740,review,172401453,use `new MatrixBlock(((ScalarObject) newOutput).getDoubleValue())` instead.,code_debt,low_quality_code
incubator-mxnet,15371,review,302817980,Get rid of dead code or un-comment to add coverage.,code_debt,dead_code
spark,5511,review,28394051,Origin code ignore the `newName`. Is this intended?,code_debt,low_quality_code
kafka,5885,review,236424422,Can we name this `transactionGenerator` for simplicity?,code_debt,low_quality_code
flink,12900,review,454415036,"Is this interface really necessary? Especially with `@PublicEvolving` annotation? How are users supposed to use it? If I understand it correctly you need it for internal operations. Moreover you need it because the `WrapperTypeInfo` is in `blink-runtime`, right?
Can't we move the `WrapperTypeInfo` to the `table-common` instead?  The class itself has no runtime dependencies. Only the factory methods need some runtime classes.",code_debt,complex_code
spark,7181,review,33755269,how is this less code? It is just a bad idea to create unnecessary state. Just move both tables into two fields in Hex object.,code_debt,complex_code
tvm,5144,review,397530214,letlist is everywhere. can you put this in some common file?,code_debt,low_quality_code
spark,25823,review,330702021,"This is a single class name, variable name should reflect that.",code_debt,low_quality_code
flink,14734,review,569501903,nit: one step further would be to store `CheckpointBrief` in `PendingCheckpoint` instead of collections. But that's probably out of scope of this PR,code_debt,low_quality_code
hive,550,review,343202348,yeah...this seems to be one place where this tablename git confusing ... :+1:,code_debt,low_quality_code
cassandra,711,review,472502571,"nit: better to do `node1.nodetoolResult(""disableautocompaction"", ""netstats_test"").asserts().success();` rather than exec into the instance.",code_debt,low_quality_code
incubator-pinot,5853,review,476001577,Recommend inline these 2 methods for readability (as the existing code). Current way is not as readable,code_debt,low_quality_code
iceberg,2167,review,571340787,"Nit: it would be nice to keep the expression on one line, wrapping after `checkArgument(`",code_debt,low_quality_code
druid,7758,review,287539672,but why did you add the space between `undefined` and `)`,code_debt,low_quality_code
spark,22154,review,234385204,"The suggested change is only for making this test suite cleaner, right? In that case I'd +1 with the suggestion of being able to clearly check we're catching the exception we know we're throwing.
Would you like to submit a PR for it?
The intent is never to actually reach the null-return, but always cause an exception to be thrown at `CodeGenerator.compile()` and abruptly return to the caller with the exception. To make the compiler happy you'll have to have some definite-returning statement to end the function, so a useless null-return would probably have to be there anyway (since the compiler can't tell you'll always be throwing an exception unless you do a throw inline)",code_debt,low_quality_code
beam,10367,review,364531963,"Yes, here:
It would be nice if we could get rid of this case somehow, because by making this optional we have to deal with the possibility of `pvalue.pipeline` being `None` throughout the code base.  I went back and forth on whether to make the arg optional or simply ignore the error in the method above, but I think I decided that the method above was a common case and thus we needed the protection against None-values throughout the code.",code_debt,low_quality_code
kafka,9007,review,454569003,"That's great point. At the moment, I think that we are not consistent about this. Some are package private and some are not. The advantage of keeping it public is that it allows to use the class in unit tests which resides in other packages.",code_debt,low_quality_code
incubator-dolphinscheduler,4896,review,584050628,keep old-style maybe better,code_debt,low_quality_code
incubator-pinot,4777,review,344013968,"try to make comparisons more specific. Since you know there are only 2 parts and parts cannot be negative, avoid the ""less than"" comparison and instead use `parts.length != 2`",code_debt,low_quality_code
ozone,1503,review,506851310,Can we reuse an existed method OzoneFSUtils#addTrailingSlashIfNeeded(keyName) instead of?,code_debt,low_quality_code
spark,28038,review,402528402,"Yes. Talking with @dbtsai he wanted to add a lock on the blocks inside of `doCleanupShuffle`, but given that the only price is duplicated messages to the executors I'm not sure its worth the overhead of keeping track of that many locks.",code_debt,low_quality_code
airflow,14219,review,584775664,"Ideally you should get the token by calling a python method directly -- by calling this endpoint you are ""retesting"" the login endpoint, which it would be better to avoid.",code_debt,low_quality_code
dubbo,4526,review,303318487,"it is a bad idea to have two *isSetter* methods, pls. consider to combine both into one single isSetter method",code_debt,low_quality_code
ignite,7851,review,430399878,"It is not correct javadoc ""list"", it should be written in HTML style like
So, it is a test though... who cares.",code_debt,low_quality_code
hudi,1274,review,370818776, I thought having all metadata constants in one place would make it simpler. This is used in reading archived commit. I can move the constant to ArchivedTimeline if you think thats a better place.,code_debt,low_quality_code
incubator-mxnet,12918,review,227170725,"Can you use something like this instead?
Otherwise it throws an uncaught exception on CPU only machines.",code_debt,low_quality_code
spark,26682,review,371318137,"it's for correctness.  to allow users to manipulate the ExecutorResourceRequests multiple times.  The original intent was all of these classes would be immutable, but that made things less user friendly so in the original PR this class got created and users are allowed to modify multiple times and they could do it from multiple threads.  It's something I missed in the original pr rework.",code_debt,low_quality_code
spark,19911,review,155335791,"@dongjoon-hyun to be clear, I think there are 2 problems:
1. The PostgresDialect indicates that `CASCADE` is enabled by default for Postgres. This isn't the case as the Postgres docs show. 
2. As you correctly mention (this is what in my previous comment), Spark doesn't use `CASCADE` at all, which, especially considering the method this PR edits, is a bit odd I think. I plan to open a different JIRA ticket for this, and add it. This will be more work, and is outside the scope of the current JIRA.",code_debt,low_quality_code
reef,1473,review,198332781,"I did -- so setting SkipMessage=null has the same effect as the SkipTestException since our version of XUnit does not support it. But I think the comment of ""// Use null to run tests"" could be a bit more explicit. This is more what I was thinking:",code_debt,low_quality_code
spark,31296,review,562868528,"This sounds a little risky to me, `A process is invoked even for empty partitions`.
This may cause a hang situation if the command is expecting input.
For example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.
If we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",code_debt,low_quality_code
arrow,2890,review,230019538,There's some other cruft here that ought to be removed. I'll make some additional changes and then merge this,code_debt,dead_code
druid,3570,review,87488947,"TreeMap was used in the original GroupByEngine code being moved, my goal with this PR was to create the interface and move type-specific code but not change functionality generally/target performance optimizations",code_debt,slow_algorithm
incubator-heron,1969,review,122385823,"There was an intentional optimization to reduce one  `System.nanoTime()` call per tuple execution. In the new commit this optimization is removed to simplify the reasoning. 
Now it is much easier to understand.",code_debt,complex_code
daffodil,408,review,488125346,"I think it would be more clear to just do string matching. E.g.
The regex just adds extra complexity. Same with setProperty.",code_debt,complex_code
calcite,1584,review,347069363,"BTW, this code `this.getRoot().getCluster().getMetadataQuerySupplier().get()` confused me a lot, can you please explain why we need a fresh new `RelMetadataQuery` instance here ? Couldn't we use the instance already existing there with `RelOptCluster#getMetadataQuery` ? (I have checked that almost each `isValid` case is in the `RelOptRuleCall` circle).
Even we have to got a fresh new instance here, why we just add a new interface `RelOptCluster#getMetadataQuerySupplier` just for debugging ? Can you refactor that out ?",code_debt,low_quality_code
spark,158,review,11264777,"Instead of doing this, just prevent users from creating a StorageLevel with offHeap = true and replication = 1. Add a check in the StorageLevel constructor and throw an exception if they make one. Otherwise nobody will understand why this code was added here.",code_debt,low_quality_code
spark,23943,review,264538235,"Since we don't need the old index, shall we remove the obsolete indexes?",code_debt,dead_code
flink,13135,review,475413540,"We are no longer referring to field names here. Honestly I am not sure if the point makes sense with the `KeySelector` only. The way I read this point is tells you can use field names, which we discourage nowadays.
I'd rather remove the point whatsoever.",code_debt,complex_code
spark,8744,review,39581040,`en` is unused.,code_debt,dead_code
spark,16944,review,103047268,useless import?,code_debt,low_quality_code
hudi,2127,review,499304054,we can reuse some code in this file by pulling the common structure into a helper function ?,code_debt,low_quality_code
guacamole-client,122,review,161413980,"If this is an upper limit, I would suggest something like `radius-max-retries` to make this more clear.",code_debt,low_quality_code
flink,10454,review,355092999,also useless,code_debt,complex_code
spark,15963,review,88906249,Nit: the second brace and its match are redundant,code_debt,complex_code
spark,22006,review,207876249,nit: 2 space indentation,code_debt,low_quality_code
druid,4754,review,152706384,"I think it should be computed on higher level, producing two different classes. The lambda below is capturing, i. e. it's an allocation on each iteration",code_debt,low_quality_code
flink,11403,review,392897158,nit: indent,code_debt,low_quality_code
kafka,3575,review,129764370,Nit: should we include bytes like we did `ms` for the other case?,code_debt,low_quality_code
cassandra,730,review,485793488,"this is `Class.toString` which is harder to read for arrays, so did this so I could read the output without googling =D.
primitives and objects will normally have valid string names, but arrays will be like `J[` which require knowing what that means or googling it.",code_debt,low_quality_code
camel,3171,review,324231977,Maybe this at debug. Too noisy.,code_debt,low_quality_code
airflow,1830,review,90948950,"I find the else to be more readable, but happy to drop it and the indent...",code_debt,low_quality_code
incubator-pinot,2068,review,149484084,"todo before release: 
Document this cp as what's it's doing is a little bit obscure",code_debt,low_quality_code
arrow,467,review,109291184,"There's https://github.com/apache/arrow/blob/master/cpp/cmake_modules/FindPythonLibsNew.cmake, is one or the other made redundant by the other?",code_debt,complex_code
arrow,898,review,130206222,I would prefer doing this change differently. Maybe by allowing allocator to return an empty buffer even if closed. This is because it makes bugs/issues much easier to understand than getting an NPE.,code_debt,low_quality_code
reef,1054,review,68419655,This looks 100% the same as in the local runtime. Can we reuse that code instead of duplicating it?,code_debt,duplicated_code
daffodil,291,review,348540375,"These ``_fileOS == null`` checks are kindof ugly. Thoughts on making this so this class just extends OutputStream instead of ByteArrayOutputStream, and then make it a wrapper for an output stream, which might change from ByteArrayOutputStream to FileOutputStream? I'm thinking something like:
Makes it so all the overrirde functions are basically just stream.whatever(), except for write which just calls the switch thing. Another benefit is once the switch happens, the old ByteArrayOutputStream can be garbage collected, whereas before it couldn't.",code_debt,low_quality_code
arrow,2623,review,220653441,This automatic fix isn't my favorite though I see that it is necessary to not have a braking change in the api for  ParquetDataset (with the filters argument). Perhaps though it would be better to throw an error here and have this fix in that specific case instead of allowing a wrong nesting level in all cases.,code_debt,low_quality_code
kafka,6163,review,252942686,"nit: since it's just an accessor, maybe we could drop the parenthesis?",code_debt,low_quality_code
beam,2330,review,112816993,.toString() is unnecessary,code_debt,complex_code
spark,19460,review,143571046,I get declaring a constant though it doesn't just pertain to byte arrays. I couldn't find a good place for it. I don't know if its reused so much that it needs this,code_debt,low_quality_code
kafka,10039,review,571271791,"Feels slightly odd to pass in the set of topics to load here, but I can't think of a good way to avoid it. Perhaps we could pass MetadataCache into LogManager and let startup call MetadataCache#getAllTopics? That might be more risky though since it changes the startup order in KafkaServer, maybe we can look into this as a follow-up.
Besides that, the name here seems strange. Maybe something like ""topicsToLoad""?",code_debt,low_quality_code
pulsar,2101,review,202403205,"if using an Optional, this becomes:
Looks cleaner to me.",code_debt,low_quality_code
spark,6558,review,31485428,"oh, I just realized that when we reuse the Decimal value, we do not really need to use the returned value. But, we have another place that needs the returned value. Can we add a comment at here?",code_debt,low_quality_code
pulsar,6196,review,374325392,Instead of `3.7` we should use `PYTHON_VERSION` variable,code_debt,low_quality_code
tvm,2292,review,241858156,using `unwrap` in libraries is generally a bad idea if it's possible to panic. please use `?` since you're already returning a `Result`,code_debt,low_quality_code
gobblin,1106,review,70877402,"Is there any reason to `catch Throwable`, instead of specific exceptions?",code_debt,low_quality_code
spark,30204,review,516533952,nit: Can we unify this with `createYarnResourceForResourceProfile` in YarnAllocator ?,code_debt,low_quality_code
qpid-dispatch,1097,review,608990009,"Can we drop ""Adaptor"" from this class name?  When I first read this I was assuming it was a set of tests that pertained only to the HTTP1 Adaptor.
Also should this class be a subclass of ""object""?",code_debt,low_quality_code
lucene-solr,2423,review,581247428,"what is this? is it doing a union? For a union, it would be better to use BasicOperations.union on Automaton objects rather than mess around with regexes as strings",code_debt,low_quality_code
arrow,66,review,64469901,"This is actually somewhat problematic, since two arrays might be unequal but their unequal parts are ""masked"" by the parent bitmap. For example
so the data is technically equal, even though the children are different when you examine them without the ""mask"" of the parent bitmap
this suggests that the `Equals` method should accept an inclusion bitmap, which adds a lot of complexity. @emkornfield what do you think?",code_debt,complex_code
incubator-pinot,141,review,65645131,"Sharing is fine for children of the same parent. It makes the 'remove' redundant, but saves a lot of garbage.",code_debt,low_quality_code
kafka,3662,review,132859349,"I tried breaking up the new test method in `WorkerSinkTaskTest` into multiple methods, but it actually just made it harder to read.",code_debt,low_quality_code
druid,2524,review,81665494,"Would prefer `List<KeyValueMap>` here, it's generally easier to work with.",code_debt,low_quality_code
iceberg,1128,review,443691512,Why is `name` needed? The `MessageType` has a name that can be used instead of passing it in separately.,code_debt,complex_code
nifi-minifi-cpp,788,review,428031207,Unnecessary semicolon at the end of the line.,code_debt,low_quality_code
helix,478,review,325376537,"Let's pass the map only, Map<GlobalRebalancePreferenceKey, Integer>.
ClusterConfig is too much for the rebalancer.",code_debt,complex_code
beam,12938,review,550847398,nitpick - there should be blank line above the comment like before,code_debt,low_quality_code
druid,5418,review,179293634,"Removed `reportParseExceptions` here, it should always be true (always thrown, handling depends on config)",code_debt,low_quality_code
airflow,12383,review,524536973,"I don't think we really need this -- with multiple webserver worker processes any refresh would lead to confusing state (some on new list, some on old, and no way to tell which is which) unless we have some way to make _all_ workers perform it.",code_debt,complex_code
spark,7842,review,43212805,The curly braces are not needed. The same for the following `case` statement.,code_debt,complex_code
spark,8785,review,48715367,"This call is effectively just cloning the properties, but we're already doing the clone inside of `.jdbc()` itself, so we don't need this.",code_debt,complex_code
spark,21200,review,185664585,"It's not okay catch and ignore all throwables. E.g. OOMs should NEVER be ignored as it leads absolutely unexpected situations.
At best, you can catch `NonFatal(ex)` and ignore those (only after logging as a warning). For other throwables, log as a warning, and rethrow.",code_debt,low_quality_code
nifi,4753,review,561119292,"For consistency, `Zookeeper` should be replaced with `ZooKeeper` to match the official naming.",code_debt,low_quality_code
spark,9867,review,46597229,"You should be able to have a single run with ""-Pkinesis-asl -Pyarn -Phive -Phive-thriftserver"" - I even think ""-Phive"" is unnecessary, I think it only affects packaging right now.
""-Phadoop2.2"" is unnecessary, that's the default.",code_debt,complex_code
kafka,5068,review,191512386,Do we actually want this to be `Long.MAX_VALUE`? Seems error prone since the normal thing to do with the TTL is add it to the current time and that will cause overflow. Should we have some sentinel value for infinite TTL?,code_debt,low_quality_code
spark,22878,review,241273157,nit: use the api using `jsonFormatSchema`,code_debt,low_quality_code
kafka,6363,review,278752888,This is getting a bit confusing -- this seems the same as `currentWorkerAssignment`. What mutates that causes this to be different?,code_debt,low_quality_code
hawq,392,review,54617226,indentation is off the hook,code_debt,low_quality_code
helix,1663,review,588774064,"This test should probably be named as TestZkClientAsyncFailureMetric since your new code is only adding metrics. The aync function should be already covered in existing tests. 
With that said, have you checked whether it's possible to only add metrics testing to existing tests by mocking the results? That would save us one additional test file. If there is no available code path or too many efforts involved, I'm fine with having a separate test too. But please check the zkclient and monitor test.",code_debt,low_quality_code
spark,19763,review,152914613,"After rethinking about this, I think it is better to indicate this threshold also determines the number of threads in parallelism. So it should not be set to zero or negative number.",code_debt,low_quality_code
hbase,1627,review,418984035,Is this really needed? Isn't BULK_LOAD_HFILES_BY_FAMILY loaded as false by default if not set on LoadIncrementalHFiles#195? Or perhaps you can move this for a separate method that could be overridden by  TestLoadIncrementalHFilesByFamily to avoid duplicating whole setUpBeforeClass in the child class?,code_debt,complex_code
incubator-pinot,5700,review,459633576,"Unused, please remove",code_debt,dead_code
accumulo,1330,review,316892853,"Regarding this naming convention, should these be `*.bind.address` or `*.address.bind`? What do you think? `master.bind.address` reads more naturally, but `master.address.bind` supports logically grouping address-related configs in a configuration hierarchy, which can be better for parsing, and more easily incorporate future changes.
An example of a future change which could benefit from the `*.address.bind` form could include work that address an explicit public advertisement address, in the case of the bind address not being publicly reachable (in the case of more complicated networking setups, such as those on some cloud services' infrastructure):
Example:
I'm leaning towards the `<service>.address.<addressType>` naming convention, rather than the `<service>.<addressType>.address` convention that you currently have here, but am open to discussion... because naming is hard. What do you think?",code_debt,low_quality_code
iceberg,2294,review,596510752,nit: also `equals` here,code_debt,low_quality_code
carbondata,3584,review,389603171,"Suppose to be && instead of || ?
line 123: 
boolean usePartitionInfoForDataMapPruning = table.isHivePartitionTable() && filter != null && !filter.isEmpty() && partitions != null
and use the same flag to reset in line 138, it is confusing now",code_debt,low_quality_code
spark,31721,review,589378403,"nit: please use the upper case where possible, `LIST ARCHIVES`.",code_debt,low_quality_code
storm,2433,review,152902841,Two lines looks duplicated. We could remove duplicated two lines via following:,code_debt,duplicated_code
incubator-heron,81,review,51090091,Can we get rid of this?,code_debt,low_quality_code
spark,29728,review,487921098,"Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?",code_debt,low_quality_code
beam,3936,review,142818337,This will slow down this performance critical code.,code_debt,slow_algorithm
flink,9890,review,335896695,Keep four spaces indentation for these parameters. Same for method `createProjectionRexProgram`,code_debt,low_quality_code
arrow,9356,review,580411319,This cast is unnecessary since the `Sum` method already returns a `double`.,code_debt,complex_code
accumulo,1995,review,607994959,I think these error message would be more useful if they included the full path of the client props file that was used.,code_debt,low_quality_code
spark,5400,review,27923456,nit: spaces around `{`,code_debt,low_quality_code
spark,9193,review,42587435,"nit: this is kinda hard to read with the double negatives. consider:
?",code_debt,low_quality_code
beam,6707,review,229053783,Is this a debugging println? Should this line be removed?,code_debt,low_quality_code
kafka,2492,review,100844023,No sure -- we also have upgrade info for 0.9. and 0.10 -- maybe this need a general clean-up (so not part of this PR)?,code_debt,low_quality_code
apisix,2329,review,495666587,"my fault,I should remove irrelevant code,actually,test_base.py has nothing to do with this PR",code_debt,dead_code
spark,17130,review,104006906,"~I think it is better to explicitly declare the data instead of manipulating strings, that way it is very clear what the input data is for the example.~ On second thought, never mind this comment - it's pretty clear the way it is",code_debt,low_quality_code
openwhisk,4031,review,255882825,Note that the `namespace` and `action` would remain same for all test. Though we do a cleanup after each run it may be better to use different name for each test run,code_debt,low_quality_code
spark,22960,review,231413853,I think we can just get rid of it. I can't imagine both functions are specifically broken alone in `selectExpr`.,code_debt,complex_code
flink,12176,review,426165664,do not need `ifPresent`,code_debt,complex_code
incubator-mxnet,10900,review,188720460,I would suggest to simply use `mx.image.imread()` and then call `.asnumpy()` before plotting. There shouldn't be a need of a dependency on `cv2` that way.,code_debt,complex_code
nifi,1036,review,81411506,"Not an incredibly big deal, but getSupportedPropertyDescriptors() is called very often (most often in validation during a UI Refresh) which causes lots of ArrayLists to be created.  I prefer the way DistributeLoad handles this, by creating one ArrayList and reusing it, while also using an AtomicBoolean to know when it should be recreated.",code_debt,low_quality_code
nifi,1918,review,122836798,I knew that code should have been simpler :) updated it to use the readValueAsTree,code_debt,complex_code
phoenix,933,review,510479056,nit: 0L,code_debt,low_quality_code
systemds,204,review,73768209,Would you want to check if sc is a SparkContext instance to raise a more meaningful error?,code_debt,low_quality_code
beam,92,review,58796896,"This particular test (and probably others in this file) could be made runner-agnostic. That would make them much better.
However, this is an underlying problem here. Ok to ignore for now.",code_debt,low_quality_code
spark,15172,review,83513326,nit: space after `if`,code_debt,low_quality_code
airflow,13929,review,570530609,This exception is too broad,code_debt,low_quality_code
geode-native,288,review,185342808,variable name could be better perhaps,code_debt,low_quality_code
flink,6170,review,195824665,indentation,code_debt,low_quality_code
ambari,1646,review,199117523,This is unnecessary.,code_debt,complex_code
trafficserver,6609,review,419565115,indentation here is not multiple of four,code_debt,low_quality_code
druid,1265,review,27927153,"Aren't 1, 2 and 4 the same as the 1, Shorts.BYTES and Ints.BYTES from the part above?
This code is really confusing me...",code_debt,low_quality_code
kafka,9998,review,569278561,Nit: seems a bit redundant. Can we not assign the size in the line below?,code_debt,complex_code
kafka,3765,review,144588126,"Btw, there are some really long lines in this PR. Our convention is that lines should not be longer than the GitHub review window.",code_debt,low_quality_code
lucene-solr,1303,review,387662567,"This is fine with me but FWIW I wouldn't even bother defining it.  It has no value set aside like this; I doubt any user code would want to refer to it.  If we want to document what the default cost is, we should say so in cost()'s javadoc.  I know some devs like to make static constants for everything but IMO it's sometimes wasted ceremony.",code_debt,complex_code
kafka,5379,review,204563539,"Personally, I would get rid of this and use `extensionValue` and `extensionNames`. Otherwise, as @rondagostino said below, we should remove `extensionValue`.",code_debt,low_quality_code
drill,520,review,106658488,"This ` <exclude>javax/**</exclude>` exludes validation-api as well. Therefore it should be deleted. 
To avoid including unnecessary libraries I decided to add:
`               <exclude>javax/activation/**</exclude>`
`               <exclude>javax/annotation-api/**</exclude>`
`               <exclude>javax/inject/**</exclude>`
`               <exclude>javax/servlet-api/**</exclude>`
`               <exclude>javax/json/**</exclude>`
`               <exclude>javax/ws/**</exclude>`",code_debt,low_quality_code
beam,1706,review,95843283,"This worries me.
I'm ok with having the tests run with `Parameterized`, but if we do so we should still be able to determine which case we're in and set the expected exception per-test within the test based on the value of the parameters, rather than inspecting the name of the test. For example, `testConflict_runnableOnServiceAnnotation_expectIllegalStateException` doesn't demonstrate by reading the test code directly that it expects an exception.",code_debt,low_quality_code
airflow,12096,review,517611582,"It's a more efficient way to get the last item from a generator - no need to iterate over every item from the generator just to get to the last one.
It might make a difference in case of lots of events.",code_debt,slow_algorithm
incubator-doris,5219,review,559096882,better to add the value of max journal id and image id in response msg for easy debugging.,code_debt,low_quality_code
trafficcontrol,4959,review,470844261,I hope someone can figure out why I couldn't get that to work. It was *very* confusing.,code_debt,low_quality_code
spark,17459,review,118818160,"Nit: can the last two args simply be m, n for clarity?",code_debt,low_quality_code
hbase,809,review,344314816,"nit: Always better to use parameterized logging for performance [1]. Here info is the default, so probably doesn't matter as much. 
[1] https://logging.apache.org/log4j/2.x/performance.html",code_debt,slow_algorithm
spark,21356,review,189349209,Not used anymore.,code_debt,dead_code
flink,9082,review,340395215,throw `IllegalArgumentException` or `UnsupportedOperatorException` makes more sense to me.,code_debt,low_quality_code
kafka,7984,review,372683286,As above. Simplify to:,code_debt,complex_code
druid,2806,review,59071300,the default should be made a constant somewhere instead of being defined in multiple places,code_debt,low_quality_code
spark,19495,review,144992379,nit: extra line,code_debt,low_quality_code
flink,8446,review,285160885,"I think you are right that in order to break up the cyclic dependency we would need to decouple the `SchedulingResultPartition` from the `SchedulingExecutionVertex` (via the `ExecutionVertexID` for example).
I'm wondering how bad this cyclic dependency and the need for mutable state is, though. From a user's perspective, the existing interfaces are a bit easier and more convenient to use. On the down side, it makes the implementation a bit harder and harder to test in isolation. However, do we want to test the `Scheduling*` implementations in isolation? Moreover, you always have this problem in graph structures which have bidirectional edges or loops.",build_debt,under-declared_dependencies
kafka,2929,review,113856373,remove. this import already exists.,build_debt,over-declared_dependencies
incubator-mxnet,15161,review,292706143,Are you using any functions in this library? if not please get rid of this line.,build_debt,over-declared_dependencies
flink,2363,comment,240791329,"Thanks for your contribution @zentol. I've gone over the code and made some inline comments. My main concern/question is actually the representation of metric's type and hierarchy information. I think that encoding it in a string and then re-parsing it on the receiver side to reconstruct the information is rather fragile and error-prone especially wrt maintainability. Maybe you can give me some background why you decided to do it so.
Apart from that, I think the code contains many tests, which I really like :-)",design_debt,non-optimal_design
accumulo,332,comment,368365644,"Agreed.
You may have gotten that impression because I'm against bundling. But, I'm also against using Hadoop's bundled libs for same reason I'm against bundling our own. I'm in favor of intentional and thoughtful dependency convergence, as a downstream activity. I'd actually prefer we not ship any binary tarball packaging... but since we do, we might as well do it in a way that works well for most users. In any case, I agree with you, this can be improved once we get the basics in. :smile_cat:",design_debt,non-optimal_design
jena,151,comment,252431461,"Should we take a step back and reconsider whether updating the current API in HttpOp is the right thing to do.
Maybe we ought to
- introduce a different style more fluid
- reconsider a design where the caller is responsible for setting up more of the `HttpClient` and `HttpOp` provides operations for nothing special (no auth) + ops that use a redefined `HttpClient`. This is to reduce method bloat.
On (1): something like (quick sketch)
where there are implicit builder objects from, `.get(url)`, `.post(url)`, `.put(url)`, `'delete(url)`.",design_debt,non-optimal_design
activemq-artemis,2012,comment,380818801,"As noted my biggest concern is that message refs need to be as light as humanly possible as they’re all in memory and affects greatly the scaling.
I would personally prefer the refactor if needed, than take this hit. 
Especially as this is only needed by someone wanting to use this in a plugin. Which means everyone else has to suffer",design_debt,non-optimal_design
spark,2685,comment,61052515,"Okay I think the issue is pretty tough. Unfortunately hive is directly using the shaded objenesis classes. However, Spark needs Kryo 2.21 which depends on the original objenesis classes.
Here is the hive code that uses it:
https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L186
So we can't just remove kryo that hive uses. This is pretty ugly. One solution might be to update chill in Spark so that Spark is using the same Kryo version as Hive.",design_debt,non-optimal_design
spark,15148,comment,259376702,"I tend to agree that the terminology used here is a little confusing, and doesn't seem to match up with the ""general"" terminology (I use that term loosely however).
In my dealings with LSH, I too have tended to come across the version that @sethah mentions (and @karlhigley's package, and others such as https://github.com/marufaytekin/lsh-spark, implement). that is, each input vector is hashed into `L` ""tables"" of hash signatures of ""length"" or ""dimension"" `d`. Each hash signature is created by concatenating the result of applying `d` ""hash functions"".
I agree what's effectively implemented here is `L = outputDim` and `d=1`. What I find a bit troubling is that it is done ""implicitly"", as part of the `hashDistance` function. Without knowing that is what is happening, it is not clear to a new user - coming from other common LSH implementations - that `outputDim` is not the ""number of hash functions"" or ""length of the hash signatures"" but actually the ""number of hash tables"".
In terms of `transform` - I disagree somewhat that the main use case is ""dimensionality reduction"". Perhaps there are common examples of using the hash signatures as a lower-dim representation as a feature in some model (e.g. in a similar way to say a PCA transform), but I haven't seen that. In my view, the real use case is the approximate nearest neighbour search.
I'll give a concrete example for the `transform` output. Let's say I want to export recommendation model factor vectors (from ALS), or Word2Vec vectors, etc, to a real-time scoring system. I have many items, so I'd like to use LSH to make my scoring feasible. I do this by effectively doing a real-time version of OR-amplification. I store the hash tables (`L` tables of `d` hash signatures) with my vectors. When doing ""similar items"" for a given item, I retrieve the hash sigs of the query item, and use these to filter down the candidate item set for my scoring. This is in fact something I'm working on in a demo project currently. So if we will support the OR/AND combo, then it will be very important to output the full `L x d` set of hash sigs in `transform`.
My recommendation is: 
1. future proof the API by returning `Array[Vector]` in `transform` (as mentioned above by others);
2. we need to update the docs / user guide to make it really clear what the implementation is doing;
3. I think we need to make it clear that the implied `d` value here is `1` - we can mention that AND amplification will be implemented later and perhaps even link to a JIRA.
4. rename `outputDim` to something like `numHashTables`.
5. when we add AND-amp, we can add the parameter `hashSignatureLength` or `numHashFunctions`.
6. make as much private as possible to avoid being stuck with any implementation detail in future releases (e.g. I also don't see why `randUnitVectors` or `randCoefficients` needs to be public).
One issue I have is that currently we would output a `1 x L` set of hash values. But it actually should be `L x 1` i.e. a set of signatures of length `1`. I guess we can leave it as is, but document what the output actually is.
I believe we should support OR/AND in future. If so, then to me many things need to change - `hashFunction`, `hashDistance` etc will need to be refactored. Most of the implementation is private/protected so I think it will be ok. Let's just ensure we're not left with an API that we can't change in future. Setting `L` and `d=1` must then yield the same result as current impl to avoid a behavior change (I guess this will be ok since current default for `L` is `1`, and we can make the default for `d` when added also `1`).
Finally, my understanding was results from some performance testing would be posted. I don't believe we've seen this yet.",design_debt,non-optimal_design
spark,24807,comment,500879466,"I think the SparkSession.close() behavior is on purpose, and that's a coherent behavior (i.e. just don't shut anything down until you're done, and then everything shuts down). What's not consistent with that is maintaining some state in the session that can't be cleared. 
I think the ways forward are probably:
- A new lifecycle method like `clear()`? more user burden but at least provides _some_ means of doing cleanup without changing `close()`
- Figure out how to automatically dispose of those resources or not hold them
- Just change the behavior of session's `close()` to not shut down the context. Behavior change, yes, but perhaps less surprising than anything.
Eh, do people like @cloud-fan or @gatorsmile or @HyukjinKwon or @dongjoon-hyun have thoughts on this? I feel like reference counting is going to end in tears here eventually, but, it's not crazy",design_debt,non-optimal_design
parquet-mr,808,comment,672774756,"@shangxinli,
If we will agree on the extending of the schema with metadata is a good idea and as you said the serialization/deserialization is also required we need to change the format first. The schema objects in parquet-mr are only exist in the parquet-mr runtime. To have them serialized we need to convert this object structure to the thrift object structure defined in the format. If we don't have the new metatdata fields in the format we cannot serialize/deserialize them. So it is a much bigger topic. Also, I'd like to see this feature separated from the encryption as it would be general approach for storing metadata in the schema. Meanwhile, I am not convinced that we need to have such extension.
About the namespace prefix etc. I don't agree this is not user friendly. That's why I've suggested to implement a helper API so the user doesn't need to deal with the conf keys (and values) directly. 
@ggershinsky,
I don't agree we cannot have a meeting about this topic in terms of transparency. What we have to do is to document here about what we have discussed and what are the conclusions. Meanwhile, I am not sure if a meeting would help but I am happy to participate if anyone thinks otherwise.
Also, if we think we are getting stuck with this issue I would suggest involving other members of the community. Maybe draw their attention on the dev list about this PR or bring up the topic on the next parquet sync.",design_debt,non-optimal_design
phoenix,936,comment,724872173,"1. Not sure about it but we can introduce additional constraints like the total size of scanned bytes as you suggested to further improve this feature later. 
2. This is correct. By itself, it does not eliminate. However, the client can wait for all the page operation to complete or fail before returning to the application, as an additional improvement. This will further reduce the race conditions. I think we have to enforce the client side timestamp to make the race almost impossible.
3. I expect this feature will improve the overall performance and availability since paging limits the memory usage and the time to hold server resources. My experience with paging on a real cluster is very positive.  I have not seen any negative impact yet as long as the page size is not very small (e.g., less than 1000).",design_debt,non-optimal_design
superset,5445,comment,457472346,"This PR fell in a crack. Let's revive it. Seems like the long term solution is not using WTForms at all as we rip out more of the MVC and FAB scaffolding over time.
So I get the `NULL` vs `''` problem with unique constraints not applying properly, but other than that, does it affect users directly? I'm guessing it may affect the filtering functionality in the CRUD list view UI?",design_debt,non-optimal_design
fineract,1458,comment,718052221,"I just feel increasing timeouts won't cut it. Doesn't sound like a real solution. Unfortunately, I don't understand much about schedular jobs to have a proper say in this but I think we have some performance issues. Need to figure out how to deal with that.",design_debt,non-optimal_design
beam,10227,comment,558882339,"We should still get rid of setup_requires, but this just might not be the _complete_ solution.",design_debt,non-optimal_design
storm,1781,comment,262494323,"That should do it - the batching option has been removed from MapState, in favor of parallel processing with existing opaque and transactional logic to handle consistency. Cassandra batch statements are more trouble than they're worth.",design_debt,non-optimal_design
spark,30392,comment,729168362,"I think so. In some cases, unnecessary executor-side reduce might invoke an additional map task although it just returns the single element. So this is just a minor concern for me.
The other thing is, `takeOrdered` is introduced into RDD API earlier than `treeReduce`. So I guess we may not consider to use `treeReduce` in this place before. For the nature of `takeOrdered` that sequences of elements are collected to the driver and reduced. It sounds a good fit for `treeReduce` as we can partially reduce before collecting to the driver. There is an overhead but sounds like a trade-off. Driver side usually a bottleneck for such case. The driver-side reduce is not parallel and also bound to local memory for all data.
I'm totally okay to close this if it doesn't sound good direction to go.",design_debt,non-optimal_design
spark,4593,comment,75540678,"I have the same concern as @dbtsai in his comment. Most consumers of this API will already be caching their dataset before the learning phase. Without user care, this will introduce effectively double caching (in terms of data size of cached RDDs) and will cause many jobs to fail after upgrading by exceeding available heap for RDD cache. Furthermore, we are making assumptions about how to cache -- in-memory only in this case. Should we parameterise this? Perhaps that will help send the message in the API that there is caching also done before learning. (FWIW, in-memory is definitely the right default choice here.)
See email thread on dev for my specific encountering of this bug: 
http://mail-archives.apache.org/mod_mbox/spark-dev/201502.mbox/%3CCAH5MZvMBjqOST-9Nr9k1z1rUODfSiczr_fV9kwqDFqAMNLC2Zw%40mail.gmail.com%3E",design_debt,non-optimal_design
incubator-heron,1728,comment,281558138,"@objmagic - Thanks for reviewing. I'm agree with you. At first, I think putting `ByteAmountUnit` inside of `ByteAmount` is a better approach and using enum to instead static `MB` and `GB` inside of `ByteAmount` seems nice too. But It might trigger a large refactor in `ByteAmount`. So, I decided to put `ByteAmountUnit` outside of `ByteAmount` temporarily.",design_debt,non-optimal_design
storm,1406,comment,222930875,"@abhishekagarwal87 @knusbaum 
Not at all. It shouldn't hurt much so let's apply this as it is. We can address broader considerations from another issues.
One thing I'd like to see is the status of backpressure for each queue. We can show the percentage, or just show whether this queue meets condition to trigger backpressure or not.",design_debt,non-optimal_design
spark,25785,comment,531497209,"I see, can that thread be a daemon? If System.exit is viable (i.e. immediately stopping daemon threads) then it should be. But if not, then yeah such a thread needs to be shut down cleanly somehow during the shutdown process. This could be a shutdown hook.",design_debt,non-optimal_design
helix,1472,comment,710682396,"This PR is ready to be merged, approved by @kaisun2000 
There is connection leakage in CustomRestClientImpl and causes timeout waiting for connection.
Fix this issue by consuming entity and releasing the content stream and connection.",design_debt,non-optimal_design
spark,12474,comment,211507927,"Thanks for the pull request. We can't just change a private to public like this, because it can make the API more difficult to maintain in the long run. Can you justify more why this is needed, and why you can't work around it from the user side?",design_debt,non-optimal_design
spark,1541,comment,49855865,"Instead of a ConcurrentHashMap, we should actually move it to a disk backed Map - the cleanup of this datastructure is painful - which it can become extremely large; particularly for iterative algo's.
Fortunately, most cases, we just need the last few entries - and so LRU scheme by most disk backed map's work beautifully.
We have been using mapdb for this in MapOutputTrackerWorker  - and it has worked beautifully.
@rxin might be particularly interested since he is looking into reduce memory footprint of spark
CC @mateiz - this is what I had mentioned about earlier.",design_debt,non-optimal_design
flink,938,comment,125751865,"Out of curiosity: why was it failing sometimes on Travis and not locally? And how did you discover this? From the program level logs?
Another thing that came to my mind: in the long run, do we need a more complex way of configuring the retry policy? In my understanding, the number of retries is fixed. I can see an issue for very long run programs, which fail once in a while, but operate normally most of the time -- then at some point they will fail because of the fixed number of retries.",design_debt,non-optimal_design
hadoop,840,comment,499279608,The implementation is different from the description.  It would be easier to call it docker instead of docker-build to make the command less characters to type.,design_debt,non-optimal_design
incubator-heron,3479,comment,626232335,"One ""short term workaround""™ for getting the `pex_pytest` to work was to change it so the pex binaries+tests defaulted to be non-zip safe so they are extracted. There were still failures later, which I think is from pex_library consumption. I'll see if I can patch that too, then hopefully come up with a neater solution for all of the issues",design_debt,non-optimal_design
storm,1480,comment,226404728,"@abhishekagarwal87: Ok, didn't spot that. Was only looking for ""guava"" relocations.
So, the dependency could be additionally included relocated into storm-redis. Or is it possible to use the already relocated package which is provided via storm-core? This would save some resources. I'm not very experienced with the `maven-shade-plugin` yet, unfortunately.
What is preferred?",design_debt,non-optimal_design
cloudstack,1224,comment,164271374,"@bhaisaab I don't like maven but we are using it! cherry-picking is really not an argument and backporting is difficult for worse reasons then this one.
Using maven we better adhere to the conventions in the maven world as keeping our diversions from it correct will become increasingly difficult over time. I will meet you half way so we can abandon 4.5 first and continue to prove our fwd-merge schedule over several versions. We will face issues in this respect as well, btw, if at the time of 4.11 we will be fixing things in 4.6 ;)",design_debt,non-optimal_design
thrift,448,comment,183605715,"@nsuke, I switched it so we now consider a `long` like two `int`s (the high bits and low bits) and we combine their values like we combine `hashCodes` (just with a different multiplicative factor). For `double`s we get the `long` represented by their bytes and treat as before. Since we're changing the values, I also took the chance to pick arbitrary constants to combine with.
For what it's worth, though, the previous `hashCode` helper functions are pretty much what are described in Effective Java. Anyway, I'll rebase and squash after you have a look.
@jsirois That's a good idea. Unfortunately, I don't have the time at the moment to do that :(",design_debt,non-optimal_design
incubator-mxnet,3781,comment,260116824,@sxjscience Yeah this will cut out a lot of code in our framework that were just there to redundantly track tensor dims for the purposes of feeding them to Reshape layer. I'm very excited about this. A small increase in (optional) complexity in Reshape layer pays off with a large decrease in complexity in our framework code.,design_debt,non-optimal_design
nifi,1800,comment,303611024,"couple of quick thoughts.
- we'll need to get all the version numbers aligned with whatever nifi version this would be committed into.  Currently that would be 1.3.0-SNAPSHOT.
- It would probably be a good idea to have the notion of 'nifi-leaderelection-api' which is not about zookeeper but rather just generic election/tracking of a leader for a given thing (a partition?) Then there would be zookeeper based implementations of those.  Processors then can leverage the api for their code but users can select whichever types of services exist (zookeeper being the obvious initial example).  The structure appears already in place for this other than the current naming and perhaps the API referencing zookeeper.  Thoughts?
- It would be good to have a processor which leverages this or some docs that go along with it to show suggested usage.",design_debt,non-optimal_design
cloudstack,4448,comment,724618330,@ravening I don't see the probably with requiring an internal DNS entry.  The operator can always enter the external DNS IP for it if they need to.  I can only see this causing pain elsewhere in the code.,design_debt,non-optimal_design
spark,25651,comment,534813380,"@rdblue yea it's possible. In this PR, I try to adopt your suggestion to make it clear that `TableProvider.getTable` should take all the table metadata, so the method signature becomes
`TableProvider` has another `getTable` method which needs to infer schema/partitioning, and previously the method signature was
To make it consistent, I change it to use `properties: Map[String, String]`, also rename it to `loadTable` since we need to touch many files anyway.
We can still keep the old method signature with a TODO to change it later, so that this PR can be much smaller.",design_debt,non-optimal_design
cloudstack,1740,comment,353858935,"@yvsubhash you are right... My problem is not related to this one here. The snapshots in XenServer is working as you said. However, sometimes if exceptions happen during copy of the snapshot to the secondary storage, snapshots in the primary storage are not cleaned. I am still investigating and trying to find a way to solve this problem.
BTW thanks for the reply!",design_debt,non-optimal_design
druid,2231,comment,173443345,"Actually , looking at com.alibaba.rocketmq.common.ServiceThread from http://grepcode.com/file/repo1.maven.org/maven2/com.alibaba.rocketmq/rocketmq-common/3.2.6/com/alibaba/rocketmq/common/ServiceThread.java/ it looks like ServiceThread does a lot of synchronization and notifying on itself. That makes this impl much harder to review without an intimate understanding of ServiceThread.
Regarding testing, The general recommendation if the framework does not have a unit-test friendly tool, is to use EasyMock to force the behaviors you are testing for.",design_debt,non-optimal_design
beam,7237,comment,447180932,"I am just starting to review, but I want to get some principles out beforehand:
1. The shaded path should absolutely never be used. It is derived from the module name just to make it unique. The reason we shade is as a way to isolate ""implementation detail"" dependencies. If this ends up on an API surface that is a bug. We had some tests for this, but they have rotted.
2. We work pretty hard to avoid Guava on the API surface, since the risk of diamond dependency conflicts is very high.
3. It is OK for an IO to have its own esoteric dependencies - including Guava - if the thing it is connecting to requires it. So if it is _Cassandra_ that requires Gauva on the API surface, then it can be included in the deps.
4. For those situations where Beam wants to use Guava internally, we are (slowly) moving to depend on `beam-vendored-guava-20_0`",design_debt,non-optimal_design
spark,228,comment,39641216,"Hi, @pwendell , Thank you for your comments, here is my reply
First, ""whether accumulator value should be subject to change in the case of failure""...I think no, though during a long period, Spark runs in this way (this patch is actually resolving a very old TODO in DAGScheduler.scala)...I think accumulator is usually used to take the task which is more complicate than rdd.count/rdd.filter.count (Maybe I'm wrong), e.g. counting the sum of distance from the points to a potential center in K-means (see mllib), I think in this case, the health status of the cluster should be transparent to the user, i.e. the final result of K-means should be irrelevant to whether executor is lost, etc....
Second, Good point, I can understand what the use scenario is, but do you mind providing more details like how to implement this in Spark? I guess this can be solved by providing a approximateValue API in Accumulator or SparkContext....
Third, actually, this patch ensures that the value of the accumulator in a stage will only be available when this stage becomes independent (means that no job needs it any more, https://github.com/apache/spark/pull/228/files#diff-6a9ff7fb74fd490a50462d45db2d5e11L400), if a job finishes, and the other job still needs the certain stage, the accumulator value calculated in that stage will not be counted...",design_debt,non-optimal_design
cloudstack,1705,summary,0,Made the changes to improve logging.,code_debt,low_quality_code
arrow,6103,summary,0,ARROW-7473: [C++][Gandiva] Improve error message for locate function,code_debt,low_quality_code
trafodion,115,summary,0,"Adding .rat-excludes, readme for rat and remove unneeded files lacking copyrights.Also hive/TEST009 fix.",code_debt,dead_code
kafka,2937,summary,0,Also add tests and a few clean-ups.,code_debt,low_quality_code
superset,12626,summary,0,docs: Updates to Superset Site for 1.0,code_debt,low_quality_code
hawq,1321,summary,0,"Rewrite the tuple construct and consume work flow to improve the read and write performance in pluggable storage framework. To be specific, it uses virtual tuple in tuple table slot instead of heap tuple between storage and executor to avoid tuple data copy during query processing.
@huor @jiny2",code_debt,slow_algorithm
zeppelin,2753,summary,0,ZEPPELIN-3138. checkstyle for zeppelin-interpreter,code_debt,low_quality_code
hbase,2167,summary,0,HBASE-24791 Improve HFileOutputFormat2 to avoid always call getTableRelativePath method,code_debt,low_quality_code
tvm,1905,summary,0,[Relay][Test] remove redundant test cases in test_op_level4.py,code_debt,dead_code
flink,15389,summary,0,[FLINK-21609][tests] Remove usage of LocalCollectionOutpuFormat from SimpleRecoveryITCaseBase,code_debt,low_quality_code
spark,13729,summary,0,[SPARK-16008][ML] Remove unnecessary serialization in logistic regression,code_debt,complex_code
trafficserver,7138,summary,0,Remove useless shortopt,code_debt,dead_code
usergrid,180,summary,0,[USERGRID-415] Pushing changes to make duplicate properties more readabl...,code_debt,duplicated_code
trafficserver,347,summary,0,TS-4038: Redundant `isdigit(b)` in `LogFormat::parse_escape_string,code_debt,complex_code
spark,9682,summary,0,[SPARK-11719] [ML] Remove duplicate DecisionTreeExample under examples/ml,code_debt,duplicated_code
spark,708,summary,0,Converted bang to ask to avoid scary warning when a block is removed,code_debt,low_quality_code
airflow,10346,summary,0,Breeze was slightly too chatty when there was no dirs created,code_debt,low_quality_code
dubbo,4655,summary,0,optimize some code styles,code_debt,low_quality_code
zookeeper,1328,summary,0,Extra horizontal lines,code_debt,low_quality_code
parquet-mr,227,summary,0,PARQUET-318: Remove unnecessary object mapper,code_debt,complex_code
carbondata,2878,summary,0,[CARBONDATA-3107] Optimize error/exception coding for better debugging,code_debt,low_quality_code
flink,14084,review,554882968,"Just not supported yet, the type TIME_STAMP_WITH_LOCAL_TIME_ZONE is rarely used. But I think we should both support them in this version.",requirement_debt,requirement_partially_implemented
tinkerpop,712,review,140818483,"Even though `Bindings` is based on a `ThreadLocal<T>` instance, I think the implementation is not thread-safe.
For example: Multiple tasks executed serially on the same thread, modifying the same bindings dictionary.
Besides not being thread-safe, it doesn't support defining a binding on 1 thread and adding the step on another, sample:",requirement_debt,non-functional_requirements_not_fully_satisfied
phoenix,508,review,285231247,"if all is null --> code will use indexes list
if all is not null --> code will get all the indexes on that table. 
will be implemented in the upcoming PR.",requirement_debt,requirement_partially_implemented
incubator-doris,4925,review,528201502,I think `DEL_NOT_SATISFIED` is more safe. Or you'd better change DCHECK to CHECK,requirement_debt,non-functional_requirements_not_fully_satisfied
spark,25628,review,319461063,Seems like it doesn't support special characters in the column name now. Can we keep the support?,requirement_debt,requirement_partially_implemented
carbondata,240,review,84475433,Map is not supported yet so thrown unsupported exception for Map,requirement_debt,requirement_partially_implemented
spark,16280,review,92527834,"oh i know why. The `InMemoryExternalCatalog` hasn't implemented all the interfaces(e.g. some partition related ones), so we did it intentionally.",requirement_debt,requirement_partially_implemented
incubator-mxnet,9552,review,176825148,"Just tried. That function for FP32 convolution seems not implemented appropriately. The data member `layout` of struct `ConvolutionParam` is a `option<int>` type, but it was used with the assumption that `layout` contains a real value in that function. This will cause my test case to fail. Making that function properly implemented needs another PR. For now, I will just keep the shape inference for quantized_conv op. Already deleted the TODO comment.",requirement_debt,requirement_partially_implemented
storm,2241,review,129494647,"Looks like you're removing max spout pending and also configuration for backpressure. Does this patch also touch the mechanism of backpressure?
And let's address this in patch or file an issue and remove TODO.",requirement_debt,requirement_partially_implemented
spark,25048,review,300427664,"Thank you for investigating this, @HeartSaVioR .
The original logic looks not safe. Do you think if there is any other better way?",requirement_debt,non-functional_requirements_not_fully_satisfied
spark,31905,comment,811106021,"Regarding your questions:
Can you provide a common streaming use case where a non-event-base processing of metrics is useful? The `Observation` proposes a ""get me the metrics"" API that hides listeners and multi-threading. In streaming context, there are no finite metrics.
Can you elaborate on this, please? Do you mean an async `get`? Can you provide some pseudo-code on how to use the metrics async.
It is not thread-safe in a scenario where the action on the observed dataset is called in a different thread than where the metrics are retrieved. I'd say such a multi-thread driver process is not the general use case for batch processing. Even if, the proposed `Observation` API aims at hiding multi-threading complexity so that users can use `Dataset.observe` in single-threaded scenarios. Users that are experience with multi-threading have no problem in using the existing listeners approach. They are not the target user of this extension.
Agreed.",requirement_debt,non-functional_requirements_not_fully_satisfied
spark,12241,comment,207105529,"To fix MiMa, you need to add a line to the `mimaProjects` definition to temporarily exclude the new project: https://github.com/dbtsai/spark/blob/b9870d6c62d75cd59698bd72751bc4f7854cd2e3/project/SparkBuild.scala#L254
Also, add a TODO and followup task for post-2.0-release to remove the exclude once 2.0 has been published and there is a previous artifact for MiMa to compare against.",requirement_debt,requirement_partially_implemented
druid,3956,comment,281423634,"Well, it's definitely sketchy to be calling aggregate and get concurrently. HyperLogLogCollector isn't thread safe. It's possible that you'll get bizarre values from time to time, like if the offset of an HLLC is in the process of being incremented in one thread while it's being read in another thread. So I think this PR has value.",requirement_debt,non-functional_requirements_not_fully_satisfied
tomee,713,comment,734780620,"I gave it a try at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 but I am quite unsure if this is what we were talking about. Consequently, I dropped some FIXME/TODO questions.
For this reason, I didn't open a related (new) PR yet. Maybe you can have a look at it?",requirement_debt,requirement_partially_implemented
airflow,1352,comment,309399231,"I'm sorry for commenting on such an old (and closed) PR.
Is this feature completely implemented?  It seems that the SchedulerJob class uses a completely different method for loading DAGs than the webserver etc. and it will not schedule tasks from a packaged dag since it only considers raw python files.",requirement_debt,requirement_partially_implemented
cloudstack,4574,comment,763102959,"@DaanHoogland I would limit it to ""4.15.0.0 to 4.15.1.0"", because adding it also to 4.14.1 will conflict with the the insert's in table `cloud.guest_os` during upgrade to 4.15.0.
I noticed that there is an `com.cloud.upgrade.dao` package still needed, but I hope, someone other will implement this 🤞 😄",requirement_debt,requirement_partially_implemented
nifi,71,comment,130314880,"Why is propertyMap marked volatile?  The value is only ever set once at construction time.
If the answer is because of thread safety, the contents of the HashMap are not ""protected"" just because the reference to the map is marked volatile.  puts/gets to the map do not inherit the memory barrier protections associated to the volatile reference.  c.f.  http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap
Maybe a review of the concurrency issues of this processor is in order before accepting this merge request?  I'm pretty sure that, even though the class will mostly behave correctly since values are set during OnScheduled and OnStopped, these are not ""safely published"" to the map.  While unlikely, other threads could potentially see stale values in this map.
Either this class should likely be using ConcurrentHashMap here, or it should republish an entirely fresh map by calling ""new HashMap()"" instead of ""clear()"".",requirement_debt,non-functional_requirements_not_fully_satisfied
groovy,480,comment,275655612,"I think no documentation and no test. I extracted the example from the code you changed actually to show you what your change will affect. In my opinion the get/set path needs to be removed, but that is another topic. The trouble is that get/set in the MOP is in general very badly documented. So it might or it might not be, that your change is a breaking one. Well no, it is a breaking change in terms of semantics, but if real world examples will be badly affected by this? no idea. But I have another example:
    public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
the last three asserts will all fail.",test_debt,lack_of_tests
zeppelin,2706,comment,353205540,@zjffdu can u confirm it's a flaky test?,test_debt,flaky_test
incubator-heron,3032,comment,437498318,"Overall is ok for me. 
**IT** coverage will be useful so i will be addressing through #3106",test_debt,low_coverage
zeppelin,3497,comment,548180234,"Thanks for the contribution, @amakaur  Could you add unit test  ?",test_debt,lack_of_tests
spark,19143,comment,327427666,"You haven't actually added tests, and that's not all this PR does. At the least, this doesn't match the intent you describe, and should be closed. I'd back up and describe the test you want in the JIRA.",test_debt,lack_of_tests
flink,966,comment,126706999,Ah yes. I'll update them in a while. There's actually some problem with the unit test I've written too. Travis fails sporadically.,test_debt,flaky_test
flink,5901,comment,387376186,"By the way, the test still doesn't catch the bug that you're fixing (https://issues.apache.org/jira/browse/FLINK-8286). I think we need proper end-to-end tests that really test Flink-Kerberos integration on an actual YARN cluster. I have started looking onto using Docker Compose for that, i.e. bringing up a hadoop cluster in docker with Kerberos and then running Flink on that as an end-to-end test.",test_debt,low_coverage
helix,1447,comment,704707243,"minor: suggest to change the PR (commit) title to ""Replace Thread.sleep() with TestHelper.verify() to fix the flaky unit tests"".",test_debt,flaky_test
cloudstack,1572,comment,253562333,"1. the current set of integration tests doesnt test the new functionality added in this PR. Its merely a check to see nothing else is broken.
2. when did we freeze? I skimmed through the mails didnt see anything about master frozen. I do not see any blocker defects for 4.10.0.0. The latest understanding I have is anybody can merge with required code LGTMs and a BVT run.
3. Which smoke tests are broken? Is it you environment issue you are talking about? If not, are these new or did we release 4.9.0 with these broken tests/features?",test_debt,lack_of_tests
camel-quarkus,1526,comment,672675824,"The test in
org.apache.camel.quarkus.core.CoreTest#testLookupRoutes
has a TODO as the 2nd route from RouteBuilderConfigurer is not discovered.
It uses @Produces annotation from JEE but Camel cannot discover it. Not sure how we can make this possible.
The regular RouteBuilder classes are discovered via jandex index and added during recorder magic.
I would assume a @Produces annotation from CDI/JEE would also work. But since the bean is not injected somewhere then arc may not trigger it. So maybe we need some jandex magic to discover all methods that returns a RouteBuilderConfigurer and are annotated with @Produces should then record the method, or whatever needs to be done.",test_debt,low_coverage
commons-lang,261,comment,289800588,@yasserzamani I thinks because you added more conditional branches in the latest commit and that is why less LOC are covered,test_debt,low_coverage
kafka,841,comment,177319309,@ijuma Do you mean adding it to the comments or update the ticket description? I did not test Java 1.7 CRC32 performance. I only tested the one we used to use and the CRC32 performance in Java 1.8.,test_debt,lack_of_tests
hudi,1115,comment,567470696,Will add more test cases to cover complex dag.,test_debt,low_coverage
zookeeper,1614,comment,785912733,"using the link on the failed GitHub CI job (the one with label ""Details""), you can select ""Rerun Jobs"". But that would re-run all GitHub based CI jobs for this PR, and that potentially can cause more flaky tests. So I think this is good enough.",test_debt,flaky_test
flink,10388,comment,561631987,"Thank for the PR @HuangZhenQiu , the PR overall looks good, can you also add the cases for Blink planner ? Even though the code path looks the same, we still need the tests to make sure the logic works correctly.",test_debt,lack_of_tests
geode,4189,comment,544634796,"CI is failing due to flaky test, for which I created ticket GEODE-7319, and proposed solution.",test_debt,flaky_test
brooklyn-server,144,comment,222313156,"Worth adding test case(s).
Also note the related discussion the mailing list. An alternative suggestion is that we pick up _all_ files with the given name on the classpath (rather than all those in a given directory). I personally prefer the approach of all files in the directory. That would allow us to more easily incrementally add things (e.g. have separate files for upgrading between versions).
---
I wonder about a nicer package name than `org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames`. I imagine many people will just put this in their `./conf/` directory, so we don't want them to have to create a really deep nested directory.
---
Another thing we could add (in the future?) is if there are conflicting changes - e.g. A is renamed to B in the first file, and B is renamed to C in the second file. Currently, the result would depend on the other the files were processed: i.e. it could be ""B"" or ""C"". It would be good to be more predictable.
I'm fine with that being deferred for now.",test_debt,lack_of_tests
kafka,8402,comment,609450055,test this please,test_debt,lack_of_tests
spark,1285,comment,47998047,Good catch! Can you add a unit test for queue input stream? It could be in the InputStreamsSuite.scala.,test_debt,lack_of_tests
incubator-mxnet,12768,comment,428218284,"See my comment in the issue, were you able to reproduce? Do you have some evidence to declare it as a flaky test?  Unless there's more data I would be against disabling this test with the information that we have so far to prevent lowering the quality bar.",test_debt,flaky_test
druid,3788,comment,268051850,"@erikdubbelboer ok that makes sense.
Can you please:
1. Add a comment to the code as to why this does not use `_`
2. Add unit tests to verify functionality.",test_debt,lack_of_tests
incubator-mxnet,17808,comment,605590914,"We can get rid of WIN_GPU_MKLDNN tests altogether but that still leaves us with the flakiness of WIN_GPU as can be seen in these builds
For roughly same code of this PR & same windows AMI, below are the results so far
WIN_GPU | WIN_GPU_MKLDNN | Build Number | Link
-- | -- | -- | -- |
✖︎| ✔︎|15 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/15/pipeline
✖︎| ✔︎| 14 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/14/pipeline
✔︎|✔︎| 12 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/12/pipeline
✔︎| ✖︎| 13 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/13/pipeline
Ofcourse your tests on local have a different story to tell...",test_debt,flaky_test
flink,439,comment,76031299,"I like the idea - that means that the webfrontend now shows what values are actually used, even if they are not in the config.
To make it even nicer, it would be cool to:
- Show which values come from the config (top) and show for which ones what default is used (in a second section below)
- add a Unit test that checks that all keys from the `ConfigConstants` are converted. I think it is easy to check this via enumerating all fields in the class using reflection.",test_debt,lack_of_tests
thrift,1039,comment,238129818,"The `thrift` directory makes sense.
The test # 4 was relying on `sleep` in test script that is inherently flaky.
I've removed a double pclose in parent process code so we'll see if it fixes the problem in #368 result.
Another problem is that Appveyor CI is failing due to include error.
Not sure why it's only happening on Windows.",test_debt,flaky_test
spark,9773,comment,157865029,I will add a test case.,test_debt,lack_of_tests
nutch,484,comment,555061506,"Well, the previous non-REST test implemented a client which did not send anything to the server but just returned a successful response or (if `clusterSaturated` was set to true) a temporary failure.
But I'm ok to remove the Test class if it's too much work to rewrite it for the REST client.
I've tested the PR but the initial rounds failed for about 50% of the pages/documents:
I got it fixed by using XContentBuilder to pass document as JSON to ES client, you'll find the necessary changes in [this branch](https://github.com/sebastian-nagel/nutch/tree/NUTCH-2739). Also:
- updated the description how to upgrade the dependencies in the plugin.xml and added few exclusions of dependencies already provided by Nutch core.
- changed the default properties in index-writers.xml.template so that the indexer-elastic plugin works out-of-the-box with default settings
So far, I didn't run any tests at scale. Should be to make sure we are able to index millions of documents with the given settings.",test_debt,lack_of_tests
kafka,6234,comment,462503916,"ping. it'd be good to get this in since it'll help with debugging tests that flake pretty consistently on jenkins (though we may also need some follow up to turn up logging output during tests, I think a bunch of modules don't even have logging turned on).",test_debt,flaky_test
incubator-pinot,5769,review,467200428,"I think both ways have some pros and cons. I personally think that using a single endpoint is easier for users?
* The main purpose of online AD endpoint is to provide a convenient way to run AD tasks so the endpoint should be as simple as possible. Using a single endpoint is more user-friendly. Using a separate CRUD endpoint does allow more flexibility but it will ask users to firstly register their data before using this service.
* Secondly, online service will not allow much too large size of data so in my sense, sending data in the request every time is not a bottleneck?
I think we could provide both ways. This is phase 1 for this feature. In phase 2, we probably could support another two endpoints to support what you suggested. 
Regarding the cleanup, I think currently in phase 1, it is just a one-call request so I mentioned this as stateless because users will not have a separate endpoint to retrieve anomalies afterwards and hence we could clean up them. In phase 2, another two endpoints will be provided and for those endpoints, we do not need to do the cleanup.
Thanks for your suggestions!",design_debt,non-optimal_design
trafodion,644,review,73953757,Doesn't look like columnReference is used after this. Is this a memory leak?,design_debt,non-optimal_design
incubator-mxnet,8015,review,141497578,"yes, let's worry about this later.",design_debt,non-optimal_design
spark,3246,review,22343430,"There are _12_ new overloads of `createStream` on top of the existing 4. This seems like big overkill. There should be one version in Java/Scala that takes all arguments, one each that takes minimal arguments, and any others needed to retain binary compatibility. The rest seem superfluous.",design_debt,non-optimal_design
spark,7626,review,35370290,Yea this is right now used by a lot of test code. Somewhat annoying to update those.,design_debt,non-optimal_design
incubator-pinot,6004,review,496927468,I tried and don't see a better way to organize the code. We need to have slightly different logic for each primitive type,design_debt,non-optimal_design
pulsar,5032,review,317796702,"This will require to update this file for each release which is not ideal. We could get the list of versions from `versions.json` though the maven install might take a while to complete.
Another option would be to just build the latest version and make sure we add the version tag, so that we just build 1 version (since the older docs will not change).",design_debt,non-optimal_design
shardingsphere,3158,review,329463176,"We may not add a class only for convenience, may need more design or inline into original class",design_debt,non-optimal_design
flink,705,review,30817476,Actually no. I think it's safe to have it fixed. We only need to adjust the values for tests. What is the easiest way to only allow internal configuration? The problem is that for integration tests it's hard to set configuration values for runtime components otherwise.,design_debt,non-optimal_design
samza,230,review,124421458,"@sborya I agree it is pretty convoluted how we are doing this. However, @prateekm and I have discussed this option extensively in the past when we are working on [SAMZA-1212](https://issues.apache.org/jira/browse/SAMZA-1212). 
As I explained before , it is not fully clear how to wire-in the source of the stop and cause of the stop of a streamprocessor across the components. I don't believe it is as straightforward as passing in a `Throwable`. It could work. It becomes particularly tricky for bounded jobs or jobs that decide to stop themselves using TaskCoordinator. The approach you are suggesting is equivalent to threading a needle , where we will end up passing around the state/source of the `stop` across the components' api and callbacks. 
The alternative and more straightforward solution, imo, is to clearly define a state model for each components and persist/manage the status in the streamprocessor. That way, it will make it easier for streamprocessor to take remediation steps.
We can discuss more on this. However, this PR is simply a bug-fix. We should scrutinize the refactoring/behavior of the APIs in a separate thread/JIRA. What do you think?",design_debt,non-optimal_design
beam,1278,review,86889887,"Fair. Added that. Still not perfect, but at least it will be perfect in pipelines without bundle failures, which is, hopefully, most pipelines.",design_debt,non-optimal_design
gobblin,2015,review,129176332,"The configuration options are not symmetric here, which can lead to confusion and does not allow all override possibilities. The code is ignoring the config store whitelist tag when a job-level whitelist is specified, but it is adding the job-level blacklist to the tag-based blacklist.",design_debt,non-optimal_design
kafka,764,review,50764320,"@apovzner Personally I think the timestamp should be accurate. Modifying the timestamp sounds very hacky and creates extra complexity. Please also notice that the timestamp index built by the followers will be purely depending on the timestamp in outer message of compressed messages. The followers will not even decompress the messages. If we play the trick here, the time index on follower will also be affected. 
If we want to make things right, then producer should be able to get the necessary topic configuration info from broker, either from TopicMetadataRequest or some other requests. So the producer can set the timestamp correctly to avoid server side recompression. But like you said this is a bigger change and it is unnecessary to block on that change.
I think the current solution is reasonably clean as of the moment.
Once the producer is able to get the topic configuration from broker, we can simply migrate to use that. Since everything is purely internal, the migration is very simple and transparent to users.",design_debt,non-optimal_design
samza,103,review,110061958,I prefer addressing non SEP-1 related suggestions in a separate PR. This is an awfully large change and should not be bloated with other important changes.,design_debt,non-optimal_design
beam,6981,review,233161421,"Regarding the operator as a whole and as discussed in the past, constraints are mostly inherited and would need to be addressed by refactoring the base. I'm not sure about the timing for this though, if it should wait until legacy runner goes away (there is still significant work for portable runner to become adequate replacement for Java pipelines).",design_debt,non-optimal_design
spark,9297,review,46203981,"Also, is the only purpose of `sqlListener` to prevent multiple listeners from being created at the same time? If so, I think it would be better to use an AtomicBoolean so that we don't create another strong reference to a SQLListener, which might have a lot of internal state that could lead to memory leaks.",design_debt,non-optimal_design
kafka,8105,review,379594069,When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.,design_debt,non-optimal_design
spark,2933,review,19377630,"I think it's better to keep this internal, it's a tradeoff between 1.0 and 1.1, most of the users do need to touch this.
We could document it later if user really need it.",design_debt,non-optimal_design
spark,1270,review,17257206,"`RDD[(Set[Double], Set[Double])]` may be hard for Java users. We can ask users to input `RDD[(Array[Double], Array[Double])]`, requiring that the labels are ordered. It is easier for Java users and faster to compute intersection and other set operations.",design_debt,non-optimal_design
ignite,5635,review,241036804,"Hmm... We will avoid boilerplate, but seems like it will make code more error prone. Developer can forget to override this method for trainer which potentially supports updating and get an error while trying to update this model in the future whereas keeping it abstract forces developer to think if this trainer supports update and insert `NotImplementedException` more cautiously.",design_debt,non-optimal_design
spark,22847,review,228600757,"it will not be JITted; it also should not not be too small, otherwise there will be many function calls.",design_debt,non-optimal_design
pulsar,43,review,80817924,"Here, to avoid creation of new ByteBuf we modify same DoubleByteBuf of the message with newly computed checksum.
However, while message creation if we see memory-leak then we create `SimpleLeakAwareByteBuf` or `AdvancedLeakAwareByteBuf` (based on `ResourceLeak Level`) instead   `DoubleByteBuf`. So, should we keep this check.",design_debt,non-optimal_design
spark,2907,review,19663363,Thanks for the much better solution. That works just fine.,design_debt,non-optimal_design
arrow,9626,review,587325457,"Does this mean that conversion could give the wrong results (in addition to being leaky)? If so, can you add a test showcasing that? (I believe you need the different chunks to be unequal...).",design_debt,non-optimal_design
samza,235,review,124903942,"It will be cleaner to simply pass the metrics registry associated with this component and register more granular group of metrics under `ZkUtilsMetrics`. Overloading it with `ZkJobCoordinatorMetrics` is confusing as this component - ZkUtils is also accessed from CoordinationService. 
HTH. Thanks!",design_debt,non-optimal_design
activemq-artemis,2427,review,232558150,"It may have them from legacy someone put it there, but if you were designing a collections class, you'd design it in a fashion so it focussed just on the logic it needs to have. And any interaction needed is supplied generically, e.g. for your case, you;d need generic method: remove(Predicate<SimpleString> predicate). This is a collection class in reality so i apply the same rules of engagement..
If anything that field, the flag and the method check hasInternalProperties really should move up to CoreMessage, as its only used there. Doing that would mean you can still get your benefit as then the check stays in CoreMessage. 
This would really clean up TypedProperties to just having fields it really should only care for, keeping it clean.",design_debt,non-optimal_design
incubator-brooklyn,910,review,41485190,"No particularly strong feelings from me, but I'd lean towards including the no-arg constructor for two reasons:
1. It's consistent with the other enrichers (we could change them all, but having some of each pattern seems more confusing).
2. The contract for enrichers/policies/entities/locations to be instantiated through the `EnricherSpec` etc is that the class must have a no-arg constructor. We don't expect people to call this constructor directly.
The second point means we probably should include the constructor with a comment. We could maybe even change `InternalPolicyFactory` etc so that it can handle calling protected constructors, which would enforce that more.
Anyway, I'm happy to ignore it in this PR.",design_debt,non-optimal_design
ignite,6420,review,273863106,Nanoseconds is too much. Milliseconds will be enough.,design_debt,non-optimal_design
kafka,9318,review,495932581,"This check makes sure the code always stays in sync. However, if we can make great code reviews (like yours), this static check should be unnecessary :)
I will remove this painful check (to me also) in next commit.",design_debt,non-optimal_design
camel-quarkus,570,review,360695073,"I think we really need to consider option 1, eventually we may need to clean up a bit the camel's bom but I think that as long term we'll end up supporting most of the component camel supports and the chance to hit this issue again is in my opinion high",design_debt,non-optimal_design
apisix,2036,review,487762501,why call `res_fun` twice?,documentation_debt,low_quality_documentation
beam,6676,review,233138480,"Looks like doc was outdated, corrected it, batchSize controls elements used in batch INSERT SQL, i.e INSERT INTO t(a, b, c) VALUES(x1, y1, z1), ..., (xN, yN, yN); Batching can speed up loading data greatly, but not all SQL database may support it, so controlling batch size gives some lever for various SQL vendors.",documentation_debt,outdated_documentation
nifi,4303,review,432322027,"This looks a little strange to my be retrospect, but the documentation says “To reduce the amount of time admins spend on authorization management, policies are inherited from parent resource to child resource”, so I think in practice the case where you cannot access the process group but you can access a resource within should not happen.",documentation_debt,low_quality_documentation
hive,2111,review,607667423,"Since it's a new interface method, can you add some javadoc please?",documentation_debt,outdated_documentation
hbase,1482,review,416140357,What does this function do? Favor SSD? Needs comment.,documentation_debt,low_quality_documentation
flink,9748,review,328404835,"Add comment for this in this commit otherwise, other people would be confused.",documentation_debt,low_quality_documentation
superset,8867,review,360617020,"It's not super clear here what this does or why you'd want to use this config element. I had to read a bit of code to understand it.
Then add a proper example with type annotation",documentation_debt,low_quality_documentation
spark,3099,review,20111063,"Doc needed!  Something like:
""A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer.  When [[fit()]] and [[transform()]] are called, the stages are executed in order, and each stage may modify the dataset before it is passed to the next stage.""
Maybe say something about what happens when there are no stages too.",documentation_debt,outdated_documentation
kafka,241,review,40615287,"Typo. ""Unless the..."" Don't leave us hanging!",documentation_debt,low_quality_documentation
incubator-mxnet,11502,review,200206922,Needs more explanation on what this is used for and how do users use this,documentation_debt,low_quality_documentation
airflow,5010,review,272785021,Can you share how you derive `yesterday_ds` with `execution_date` with macros in the docs?,documentation_debt,outdated_documentation
airflow,5661,review,307279581,"Enter is required. Otherwise, documentation is not rendered correctly.",documentation_debt,low_quality_documentation
nifi,1553,review,103938291,Are these comments necessary?,documentation_debt,low_quality_documentation
kafka,386,review,43579299,"Rep @onurkaraman : `removeGroup` should always be guarded by the group lock inside the `GroupCoordinator`, while `getGroup` and `addGroup` are not since the group object is not available yet. I will make that more clear in the comments.
Rep @junrao : ack.",documentation_debt,low_quality_documentation
flink,12260,review,429716465,"Could you add a short comment for this field to explain why we need this? The same to `CatalogManagerCalciteSchema`, `CatalogSchemaTable`, `DatabaseCalciteSchema`. For example:",documentation_debt,low_quality_documentation
arrow,8648,review,567582052,"1.  Input first,
2. Input output next.
3. Output variables.",documentation_debt,outdated_documentation
incubator-pinot,4397,review,307084550,please add some javadoc,documentation_debt,outdated_documentation
kafka,158,review,38986690,"just to circle back here - I asked @benstopford to remove the test and doc and do it in a new PR, since they are a bit out of context here.",documentation_debt,outdated_documentation
tvm,2773,review,283191720,Doc formatting.,documentation_debt,low_quality_documentation
calcite,782,review,209381477,"``
// below might change the type of the call (e.g. from nullable to non-nullable)
// however simplify(..) is allowed to return node with different type
// if the type should match, then `simplifyPreservingType` should be used",documentation_debt,low_quality_documentation
flink,4143,review,123715389,"In general, I would not use the ""kleene"" term, as we do not use it throughout the rest of the docs.
You can say sth like ""looping"" or simply oneOrMore.
In addition, it would also be more consistent to not use ""operator"" but ""pattern"". In the docs we have ""pattern sequences"" composed of ""patterns"" so we should stick to that.
So ""Kleene Operator"" -> ""looping pattern"" or ""oneOrMore pattern""",documentation_debt,low_quality_documentation
beam,12645,review,486156062,@abhiy13 short javadoc please.,documentation_debt,outdated_documentation
spark,20303,review,249798529,"I can answer that myself, the countdown stuff is useful to figure out if the computation has completed. Please add some doc here.",documentation_debt,outdated_documentation
incubator-brooklyn,910,review,39962111,"Worth having some javadoc (even though the method is small, it's behaviour is non-obvious). It wasn't clear until looking carefully why would increment the iterator sometimes (i.e. call `next()`) and not other times.",documentation_debt,low_quality_documentation
tinkerpop,1308,review,512632352,"nit: Capital ""A"" in ""Authorization"" please since it's a title. 
nit: There's a bit more formatting to do in the text like enclosing class names in backticks.
I think it would be worth adding some note here to providers to say that while Gremlin Server supports this authorization feature it is not a feature that TinkerPop requires of graph providers as part of the agreement between client and server. Graph providers may choose to implement their own methods for authorization in the manner they see fit.  I would say a similar ""IMPORTANT"" callout box should probably be added to the reference documentation to alert users to this notion. Finally, as you draw closer to a final body of work, this is a neat new feature that should have upgrade documentation. (and perhaps more user facing documentation?))
UPDATE: I read a bit further on and saw you linked from the user documentation to this page....that could suffice, but if I'm thinking of this feature right I sense that users will write these authorizors and i think it could be a popular feature which means more front facing documentation.",documentation_debt,low_quality_documentation
flink,10017,review,349058826,To check whether a file is zip-format we must read its actual content. If user specifies a DFS url the check will introduce additional IO. Suffix checking is also hard because there are too many file formats are actually zip format. Maybe rewriting the doc string in detail to tell users what file format are actually supported is a better choice?,documentation_debt,low_quality_documentation
helix,1208,review,464529958,"Typo.
Also, why do you ever need to set the task version after-the-fact?",documentation_debt,low_quality_documentation
hudi,2106,review,503737912,"please keep this javadoc to be just about the preCombine() method, without any context from this PR;s scenarios.",documentation_debt,low_quality_documentation
kafka,6762,review,286572482,"The comment above doesn't seem to fit the case, perhaps the result of a refactor somewhere along the way. Shall we move it below to the actual `Dead` case?",documentation_debt,outdated_documentation
kafka,764,review,51615946,"Could we put those comments in a more prominent place like the beginning of the class? With v1 message format, we are adding a timestamp, a timestamp type attribute, and are using a relative for inner message. It would be useful to document the format in a bit more details for both the outer and the inner message. For example, should the timestamp type attribute be set for inner messages?",documentation_debt,low_quality_documentation
hadoop,575,review,264257746,"""if the stream is not in state Open"" would be clearer.",documentation_debt,low_quality_documentation
incubator-mxnet,8302,review,155828922,The documentation need to be updated. Does copying from mkl-dnn array to sparse ndarray work??,documentation_debt,outdated_documentation
cloudstack,2699,review,194376281,"Well, I did not see anything yet in the Github. However, if you add just a snippet of comments explaining why we need such method, I am fine with it.",documentation_debt,low_quality_documentation
gobblin,2656,review,292076607,"The reason is I get rid of `this.dags.get(dagId);` and there's no reference to a `dag` associated with this `dagId`. So clean up has to happen before `remove`, or `dag` object won't be fetched. 
Will add comment to make it clear",documentation_debt,outdated_documentation
beam,9730,review,331592994,"use `re.search()`.  also, I don't think this needs to be private, so remove the leading underscore.
maybe add a comment like:  ""we don't use json.loads to test validity because we don't want to propagate json syntax errors downstream to the runner""",documentation_debt,outdated_documentation
hbase,1077,review,368823636,May be good to update doc for `hbase.coprocessor.regionserver.classes` in section `Restricting Coprocessor Usage` of cp.adoc?,documentation_debt,outdated_documentation
couchdb,3015,review,488882226,"It was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl 
https://github.com/apache/couchdb-couch-replicator/commit/b48d7bdc49d107f33d96f08603006a1c9edc322f#diff-ba1cca81bdc216835256f72cc6a72fa5R374
Beyond that not sure about its origin. Technically it is possible, say, for a couch_replicator_auth plugin to link to a process that then exit normally so we end up in this part of the code and it won't be a worker, changes reader or any other known process in this module. So perhaps we should change to a comment indicating that instead of leaving the commented code in there?",documentation_debt,low_quality_documentation
arrow,5213,review,318643171,"It would be nice to have some javadocs for the new methods, although they are fairly self-explanatory",documentation_debt,outdated_documentation
flink,6076,review,190841926,"+1 for getting this into the docs. I offer some grammatical improvements. Also, is it correct to describe ""operators are required to completely process a given watermark before forwarding it downstream"" as a general rule, meaning that it might have exceptions, or should we simply say ""operators are required ..."" without adding this caveat?
I changed behaviour to behavior because most of the docs seem to be using American spellings rather than English ones, but I'm not sure if we have a policy regarding this.",documentation_debt,low_quality_documentation
zookeeper,684,review,238789004,"Yes, I think we randomly chose one, it depends on how much data you have, we'll update the doc for the best practice. Also there is metric to show the cache hit rate, if it's too low, maybe we need to raise the cache size.",documentation_debt,outdated_documentation
incubator-pinot,4387,review,300516190,Would be nice to add a comment on why TreeMap (ordering) is needed.,documentation_debt,low_quality_documentation
spark,29104,review,459576703,"nit: Should we add a comment here that lookupKey contains only a single column ? It will make understanding ""allNull"" easier.",documentation_debt,low_quality_documentation
spark,9399,review,43590281,Let's add a comment?,documentation_debt,outdated_documentation
beam,4185,review,153627791,Can you add a comment with an example of an actual value here?,documentation_debt,low_quality_documentation
tinkerpop,141,review,44267313,"Typo.""the the"" => ""then the""",documentation_debt,low_quality_documentation
nifi,4369,review,452304677,Typo: configureSasToken(),documentation_debt,low_quality_documentation
gobblin,2788,review,351428413,typo: Gobblin config path,documentation_debt,low_quality_documentation
incubator-pinot,4047,review,288787662,"Instead of these 3 comment lines, just point to the design document",documentation_debt,low_quality_documentation
spark,4214,review,25109930,can you add a quick comment of what `true` means here:,documentation_debt,low_quality_documentation
beam,12241,review,454528596,I would add a comment above to state the assumption. But it's minor though.,documentation_debt,low_quality_documentation
storm,128,comment,45091287,"I finally made it through all of the code.  It looks good for the most part.  Just a few minor comments.  I also am wiling to maintain/support this code. I have a umber of customers who I know would be very interested in using this, so I would be on the hook for supporting it anyways :)",code_debt,low_quality_code
trafficserver,2300,comment,318132384,"It seems possible to collapse at least some of this class hierarchy, but I was told that there's future plans for making subclasses of the PrinterIface class. There just seems like a lot of class hierarchy (up to 4) for such a small amount of code, and I really don't think we should make provisioning now, for something that *might* be done later. If it becomes an issue later, it's easy to refactor (don't do premature ""abstractions"" :-).",code_debt,low_quality_code
trafficserver,866,comment,240594556,"I'd say it is not sufficient.
As a workaround for most cases, it works, and personally I'm OK with it as is. However, I think the behavior of `TSStringPercentEncode` shouldn't be changed because it's just a workaround for logging issue.
So, if the API keeps current behavior, then I'm fine with landing this change.",code_debt,low_quality_code
hive,1780,comment,761269576,Updated PR to improve readability (https://github.com/apache/hive/pull/1780/commits/37e707705c6cc7c025bbe7fb1b67682062dc84c7) and at the same time address HIVE-24646.,code_debt,low_quality_code
kafka,1908,comment,250894831,"LGTM. Merging to trunk and 0.10.1. Thanks @edoardocomar and @mimaison for the hard work (and patience) on this patch! I have some minor cleanups/improvements on the client and in testing, which I will submit in a follow-up PR.",code_debt,low_quality_code
carbondata,4078,comment,787036665,"@VenuReddy2103
 I checked all places, but 14 places still keep the same with previous.
1) in 9 places, it uses only one event, fireEvent is ok. 
2) in 4 places, preEvent and postEvent are in the different code blocks.
3) in 1 place, there is a large code block between preEvent and postEvent.",code_debt,low_quality_code
activemq-artemis,2558,comment,472853473,"I see you added a test, but you still haven't addressed my question about synchronized on .class.
Why is that needed?",code_debt,complex_code
openwhisk,3449,comment,373702523,"What a mess...
On Mar 16, 2018 5:12 AM, ""Carlos Santana"" <notifications@github.com> wrote:",code_debt,low_quality_code
cxf,561,comment,498837564,"I'm wondering if it would make sense to insert a layer between AbstractFeature and the actual features like:  
and then each of the subclasses would look something like:
and a lot of the duplicate code in each of the subclasses goes away. 
Thoughts?",code_debt,duplicated_code
pulsar,2888,comment,441582678,"The ""data:"" parser you wrote would be a duplicate code because it's already implemented in the in-house URL class I wrote.  I'm not going to make a change request because I don't want to delay this feature, but either of the parsers should be removed eventually to eliminate duplicate code.
Also, because environment variables are general enough as data sources, I think ""env:"" parser should be added as a `URLStreamHandler` so that other places can use it too via the in-house URL class.
As for ""token:"", I'm ok with having it in this plugin as a special case. But if you renamed it to ""raw:"", it could be used on other places.",code_debt,duplicated_code
pulsar,366,comment,296908563,"@rdhabalia Maybe the name `NonDurableCursor` doesn't fully convey the intended semantic for the new class. The context here is just to have a way to read through a topic (eg: support `TopicReader`) and reuse as much code as possible from the regular cursor implementation. Basically all the cache code and the logic for how to switch to ""next valid position"", plus the asyncReadOrWait stuff.
About non impeding messages to be deleted, consider that in case of non-durable topic, during a disconnection the cursor will go away and data will get potentially deleted anyway. So I prefer it to be explicitly ingrained into the API. Same thing about naming the cursor. It will go away in any case after a restart, so I don't see the advantage of naming it.
When data gets deleted, the cursor will just skip over it. The intended usage for the non-durable cursor and topic reader is in conjunction with the message retention, to make sure data sticks around for the intended amount of time",code_debt,low_quality_code
spark,17582,comment,294933906,"so we should definitely fix the /api/v1/applications/<app-id>/logs to go through the acls.  It looks like it should be protected in ApiRootResource.java. You have the app id so it needs to do something like the withSparkUI to get the acls included in that application.
Like I mentioned the listing (/api/v1/applications) and /api/v1/applications/<app-id> (which is same info I believe as listing) were intentionally left open.  I don't really see a reason to change that but if other people have a use case for it then perhaps we should make which pages are protected by acls configurable.  
on the history server I would expect spark.acls.enable=false and spark.history.ui.acls.enable=true, I can see where that could be confusing, perhaps we should document this better. spark.acls.enable on the history UI really is protecting the root UI, not the app level ui's.  We could explicitly turn this off.",code_debt,low_quality_code
kafka,6517,comment,484309340,"@hzxa21 Thanks for the code review.
1)	This consistently happens on Windows platform. I have seen few people complaining similar issues for some Linux flavors, but consensus is this is not an issue for most of the popular Linux flavors. This is a classic windows problem where if a handle is open for a file, NTFS does not allow it to be renamed / moved / deleted. All the file handles open on the file are required to be closed before renaming them. This is not much of an issue with Linux because of the way Linux file system is implemented. The way files are implemented in Linux, it allows to rename / move / delete files even if there are open handles to them.
The way I think about this patch is, before renaming any files / directories, we should always close the file channels. This is applicable to Linux as well. There are 2 fundamental scenarios. 
a)	Deletion of directories since the topic is deleted and deletion of segment files by cleaner thread. We believe, we should close the log file in case of log deletion and close the segment in case of segment file cleanup. Since no one should be reading these files, it is safe to close.
b)	 During swapping flow, where ‘.cleaned’ files are renamed to ‘.swap’ files and then they are renamed to actual log files. Here also we believe segment should be closed during renaming and re-opened after the swap.
Overall gist is, files should be closed before renaming them and should be opened back if required. This should be applicable to Linux as well
2)	Yes, initially we implemented the change in rename files method. But that change turned out to be 
ugly. Here is the abandoned pull request done such way.
https://github.com/Microsoft/kafka/pull/17/files
Problem was, there was no need to re-open ‘.deleted’ files. So we had put an if check to not to re-open ‘.deleted’ file. Also, it resulted in changes at many places.
Also, I felt like it was not a cleaner fix where we are not solving the root problem. Which is to close the segment before they are renamed / deleted. Also, we wanted this to be a generic fix, rather than specific to Windows.
3)	Yes, I can add some test cases. We wanted to get a initial feedback on the idea before adding tests.",code_debt,low_quality_code
flink,1052,comment,134567455,"Really good work @r-pogalz. I had only some minor comments concerning style and test cases. 
I like your approach to split the implementation of FLINK-687 into multiple parts. This makes it far easier to review. Concerning the description of FLINK-2106, you haven't integrated the outer sort merge join into the optimizer and the API, yet. I guess this will happen as a next step. Maybe you can update the description of FLINK-2106 accordingly.
Other than that, the PR looks good to me :-)",code_debt,low_quality_code
spark,4105,comment,103159431,"This is reasonable, but it doesn't handle `Error`. I don't think you need a new 'inner' method? it also duplicates the job cleanup code.",code_debt,low_quality_code
tajo,308,comment,67804082,"Hi @sirpkt ,
+1
The patch looks good to me. I have one suggestion. Each test method name should have the prefix 'test'. For example, `lastValue1` should be `testLastValue1`. It's trivial, so you can immediately commit the patch after fixing them.",code_debt,low_quality_code
trafodion,604,comment,234079094,"The code seems fine, however, it seems like it would be good to at least log the transid with the exceptions so we can better track down/correlate issues after the fact.",code_debt,low_quality_code
kafka,4001,comment,333581688,Am I missing something or you're using raw types in several places now? Raw types only exist for migration compatibility purposes and should never be used in new code IMO.,code_debt,low_quality_code
airflow,1488,comment,222606343,Ah. The hive-hook shouldn't be there. I think I must have included it from someone elses commit when doing a rebase. Will tidy it up and do a force push later today.,code_debt,low_quality_code
spark,28661,comment,635693433,"Well, I'd say it differently. A Python person may not know what a JVM stack trace means. Taking it away doesn't itself do much except shorten a big dump of output, which doesn't really simplify much. Taking away important information that perhaps someone _else_ can make sense of isn't making usage (debugging) harder. if this is only removing ""unhelpful"" stack traces from the console, I can see it.",code_debt,low_quality_code
tvm,4847,comment,584462639,CSourceModule with an empty string looks to me as well. @kumasento could you do that instead of creating a dummy llvm module? Thanks.,code_debt,low_quality_code
tvm,7021,comment,797529527,"@ANSHUMAN87 Hi, thanks for trying this! Could you be more specific about your setting? The adaptive evaluator works in the occasion when the repeat/number of the measure_option is a big number (like 500), and according to our experiment results in the paper, the search efficiency outperforms than the base AutoTVM in the Transformer encoder tuning case.",code_debt,slow_algorithm
beam,5049,comment,379410687,"Note that this change is not Python 3 compatible since `long` was removed in Python 3.  Can you add something similar to the following?
https://github.com/apache/beam/blob/8d854d4ce0365a8e201b388618d7732f000c65b9/sdks/python/apache_beam/transforms/combiners.py#L39",code_debt,low_quality_code
spark,1379,comment,71531266,@dbtsai I did batching for artificial neural networks and the performance improved ~5x https://github.com/apache/spark/pull/1290#issuecomment-70313952,code_debt,slow_algorithm
spark,4533,comment,73925690,"Sounds correct. The subsequent tries do try in parallel. So, I suppose that's pretty good evidence it's parallelized. Unless anyone else speaks up I think this sentence can be removed.",code_debt,complex_code
activemq-artemis,3455,comment,779848243,"@michaelandrepearce this is just code cleanup. Nothing that would bring any value to a release notes...
The commit itself would be enough record of the change here.
If someone is creating a JIRA, it would be a task, as the is not an improvement, not a feature, not a bug fx.. no value on the release notes.",code_debt,low_quality_code
airflow,3090,comment,370109428,"nit: i think line 68-70 can be removed (to avoid confusion), looks like abandoned legacy code:
thanks for fixing this!",code_debt,dead_code
spark,12836,comment,216750713,"Ok - if the behavior we get from `dapply(repartition(df, cols))` is the same as `groupByKey().flatMap` then I'm fine with going with the simpler implementation. 
But I think we should have a high level `gapply(df, cols, function(group))` API in SparkR that is clearly specified. The internal implementation we should go with whatever is simpler.",code_debt,complex_code
trafficserver,3903,comment,402768113,"I think it's good to take opportunities like this to do more general code cleanup. I'm on vacation this week, I will look at splitting it in to two commits when I get  back.",code_debt,low_quality_code
systemds,1023,comment,728301245,"LGTM - thanks for the cleanup @Shafaq-Siddiqi. The scenario with 10 components was still failing, but after some debugging it turned out this was due to Kmeans not converging. During the merge I fixed the hard-coded maximum iterations for Kmeans, some formatting issues, and vectorized part of the cholesky computation. With those changes it ran fine.",code_debt,low_quality_code
pulsar,9246,comment,775491410,"sorry for the late reply. @sijie 
I just notice this change and am wondering the necessity of exposing the whole admin client from functions to users. Maybe we can have some discussion when you are available",code_debt,low_quality_code
beam,13435,comment,747291549,"@rHermes yes general good practice is to separate PRs that deal with different subjects. But for cleaning PRs, it is the same subject. There a lot of leftovers in nexmark what I would like to avoid is tens of PRs that remove only a couple of fields.",code_debt,dead_code
hadoop,2674,comment,781282914,"+1 LGTM , @cyrus-jackson  please to take care of the checkstyle too",code_debt,low_quality_code
arrow,1804,comment,377119109,"I'm sorta ambivalent on the package name -- I looked at crates.io and there are some other ASF projects with packages that just use the Foo in Apache Foo. If ""arrow"" is shorter and sweeter, that's no problem",code_debt,low_quality_code
skywalking,4987,comment,652475825,"local compile and package speed is so slow,because  need to download so much dependency jar file from maven depository",code_debt,slow_algorithm
rocketmq,64,comment,280278200,"Can you provide some test data, say before/after applying this patch, how many duplications are found respectively?
IMHO, we should make the API as concise as possible.",code_debt,duplicated_code
arrow,5508,comment,535832777,"@emkornfield I created a new benchmark to evaluate the performance of consumer directly. The improvement is not significant:
after: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  77326.747 ± 218.829  ns/op
before: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  79007.087 ± 63.994  ns/op
I think there are two reasons for this:
1. in our benchmark, the jdbc implementation is based on h2, and for this library the wasNull method implementation was simple: 
    @Override
    public boolean wasNull() throws SQLException {
        try {
            debugCodeCall(""wasNull"");
            checkClosed();
            return wasNull;
        } catch (Exception e) {
            throw logAndConvert(e);
        }
    }
It can be seen that the implementation is based on a simple flag, plus some simple checks. For other implementations with other RDBs, the implementation can be more heavy. For example, the following code gives the implementation of MySQL JDBC, which may involve megamorphic virtual calls:
  public boolean wasNull() throws SQLException {
    try {
      return this.thisRow.wasNull();
    } catch (CJException var2) {
      throw SQLExceptionsMapping.translateException(var2, this.getExceptionInterceptor());
    }
  }
2. The time for wasNull method was insignificant compared with the Arrow set methods.",code_debt,slow_algorithm
arrow,4322,comment,493617041,"For posterity, I came across this discussion around the original hard-coding of the rpath: https://github.com/apache/arrow/pull/2489#discussion_r215664651.",code_debt,low_quality_code
spark,2761,comment,58776141,"I also think, it's difficult to apply new style checker only to new codes. I cleaned up codes in origin/master for the style checker suggested in this PR.  So, if this PR is merged, then we can enforce the new style to developers and all developers have to do is to check the style of the code changed by them.",code_debt,low_quality_code
spark,1561,comment,50182962,"Looks good to me, this is definitely cleaner",code_debt,low_quality_code
spark,11006,comment,178279347,"LGTM, other than the naming issues (StandingQuery, etc. in the code)",code_debt,low_quality_code
zookeeper,1417,comment,665952705,"okay, so I'll have to fix a few checkstyle issues :)",code_debt,low_quality_code
spark,224,comment,38748839,"Hi Cheng Hao,  Thanks for implementing these functions.  I bet they will be much faster than using HiveUDFs as we do now!
Based on the number of comments on the test case refactoring, I think it would be easiest to try and commit just the LIKE and RLIKE with a few simple test cases, and then take on the rest in a separate PR.",code_debt,slow_algorithm
spark,9432,comment,154740151,"Oops my bad, also removed the unnecessary sbin/../ from the other tachyon paths",code_debt,complex_code
flink,11794,comment,619004787,@godfreyhe I think that this is a valid tradeoff for now. In the future we may have an exception that says that `CollectionEnviroment not allowed to be used with TableEnvironment. Please use XXXX instead`. For `DataSet` we do not have this problem because the execution logic is hardcoded in the environment.,code_debt,low_quality_code
lucene-solr,1802,comment,683374849,"I ran a quick benchmark of just the javadoc task of `master` vs `LUCENE-9215` branch:
`./gradlew clean compileJava && time ./gradlew javadoc`
master: BUILD SUCCESSFUL in 3m 21s
LUCENE-9215: BUILD SUCCESSFUL in 3m 22s
So this check pass is effectively free and doesn't slow down javadoc processing at all (the hard part is already done), and I think it is a lot easier dealing with the errors directly from `gradlew javadoc`: things like filenames and line numbers really help. Plus we remove the previous python script which was recursively parsing HTML, and that thing was never instant.",code_debt,low_quality_code
airflow,5847,comment,523869438,"It is, but the less load we can place on the reviewers the better. If it's possible without lots of effort anyway.
Everything in `tests/utils` is badly named btw -- they are utils _for_ tests, not tests themselves.",code_debt,low_quality_code
spark,5434,comment,92291279,"Also, could you make the stats at the top of the page ""Waiting batches"" and ""Processed batches"" links to the corresponding sections? And the names should be consistent, so please rename them here, keep them as ""Active Batches"" and ""Completed Batches"".",code_debt,low_quality_code
arrow,4258,comment,490781968,"+1 lgtm
@liyafan82 , thanks for the contribution. I suggest you add a similar flag for isSet() to be optionally skipped in a separate jira to improve performance",code_debt,slow_algorithm
iceberg,1669,comment,718305151,"In the case of a large amount of data, in order to ensure the performance of normal writing,
1. Should we use asynchronous Rewrite? (use AsyncWaitOperator)
2. Should you add a switch to control whether to  enable rewrite",code_debt,slow_algorithm
spark,28370,comment,628792196,Could you add the code to avoide the infinite retry loop on error & also checking thread interrupted incase something else swallows the thread interruption exception in the future?,code_debt,low_quality_code
kafka,5221,comment,399157359,"here are the results on my workstation:
the update path is somewhat slower (because the update must start by 1st copying the latest snapshot and then applying the new data to the copy), but starvation is gone.
also note that metadata codepaths in general do an awful lot of copying - seems to me that converting the entire class to java (or at least making it use 100% java collections) would avoid multiple copies going to/from KafkaApi (all those toJava() calls).",code_debt,slow_algorithm
airflow,1435,comment,215288030,"Much thanks for the in-depth review!
Is the scenario you are worrying about (two workers running the same task instance) already possible? For example if a worker's communication with the DB gets interrupted, then the scheduler assigns the task instance to a new worker, and then the communication between the initial worker and the DB resumes.
This makes sense. I misspoke in the PR description though, SLAs should still be sent, the difference would be the SLA email would now omit task instances in the dagrun that didn't succeed for reasons other than depends_on_past not being met (e.g. a task that couldn't run because it's pool was full won't get reported in the email). I think I'm going to just include all tasks that don't have a successful status in the SLA miss email, even those stuck on depends_on_past to align with your criteria (if the task caused core_data to not be delivered by 9AM the task caused the DAG to miss it's SLA regardless of it's depends_on_past_dependency), plus is stops treating depends_on_past differently from the other dependencies like the pool being full. LMK what you think.
Agreed about the efficiency, was going to look into caching if this causes perf issues.
The newfound power of the force flag could be used instead of ignore_depends_on_past, but making ""force"" the default for every backfill could potentially be a bit dangerous as users could e.g. unintentionally force run over a large range of already successful tasks in a backfill or violate a pool constraint. If you have any ideas let me know.
Agreed about not passing in a bunch of different flags. There is actually a TODO above that part of the code in the PR to use a context parameter instead (it will be addressed in this PR).
For the flag upstream_failed I would prefer to leave the fix for another PR since it was an existing hack and the cope of this PR is already a bit dangerously large.",code_debt,low_quality_code
hudi,360,comment,377576044,"@n3nash : Thanks. Added both Unused and Redundant Imports in checkstyle and corresponding code-style. If there are any other rules missing, we can add them in future PRs.",code_debt,low_quality_code
spark,23260,comment,446345304,"My understanding is that this allows pointing the Spark UI directly at the history server (old JHS or new ATS) instead of hardcoding the NM URL and relying on the NM redirecting you, since the NM may not exist later on.
That does open up some questions though. The code being modified is in the AM, which means that the user needs to opt into this when he submits the app, when perhaps if there was a way to hook this up on the Spark history server side only, that may be more useful.
I think someone tried that in the past but the SHS change was very YARN-specific, which made it kinda sub-optimal.",code_debt,low_quality_code
carbondata,432,summary,0,"Carbon-core module should not depend on spark, this PR removes this dependency",build_debt,under-declared_dependencies
druid,1535,summary,0,Update alphanumeric sort docs + more tests / examples,test_debt,low_coverage
dubbo,2741,summary,0,Fixing flaky tests in PortTelnetHandlerTest,test_debt,flaky_test
nifi,3260,summary,0,NIFI-5790 removed the last test as it's causing a race condition intermittently,test_debt,flaky_test
ignite,6069,summary,0,ignite-11261: [ML] Flaky test(testNaiveBaggingLogRegression),test_debt,flaky_test
incubator-mxnet,10819,summary,0,[MXNET-367] update mkldnn to v0.14 and disable building test examples,test_debt,lack_of_tests
ignite,4970,summary,0,9769 test flakiness,test_debt,flaky_test
shardingsphere-elasticjob,1608,summary,0,For checkstyle & fix typo,documentation_debt,low_quality_documentation
tvm,4144,summary,0,Fix typo,documentation_debt,low_quality_documentation
dubbo,2520,summary,0, typo: leastIndexs->leastIndexes,documentation_debt,low_quality_documentation
trafficserver,5380,summary,0,Fixes spelling in src,documentation_debt,low_quality_documentation
druid,1899,summary,0,Docs improved: more details about caching and memory for segments on historicals,documentation_debt,low_quality_documentation
kafka,7908,summary,0,KAFKA-9068: Fix incorrect JavaDocs for `Stores.xxxSessionStore(...)`,documentation_debt,low_quality_documentation
spark,7046,summary,0,[SPARK-8639] [Docs] Fixed Minor Typos in Documentation,documentation_debt,low_quality_documentation
ignite,2941,summary,0,"ignite-6774 Java doc is broken: ""LUDecomposition.java:40: warning - T…",documentation_debt,low_quality_documentation
tvm,4251,summary,0,Fix typo in err msg,documentation_debt,low_quality_documentation
tvm,1716,summary,0,[NNVM][KERAS] Fix keras model converter and improve tutorial,documentation_debt,low_quality_documentation
flink,5663,comment,371969548,Currently in flink connector we are depending only on aws-sdk-kinesis and not on aws-java-sdk-bundle and also don't depend on kinesisvideo. So by default the dependency on kinesisvideo is not included in the connector which means we don't have to exclude any dependencies. I also verified that there is no unwanted netty dependencies by running mvn dependency:tree. The only instance of netty is this: `[INFO] |  +- org.apache.flink:flink-shaded-netty:jar:4.0.27.Final-2.0:provided` in accordance to the value in flink-parent pom.,build_debt,over-declared_dependencies
beam,539,comment,228871382,"My fault. I should open a JIRA issue before I start on this. I thought it is a quick change.
For the WordCount part, I like to move it to a top level class, and document why Spark runners need its own copy. (+@dhalperi for Read.from doesn't work for HDFS)
For the Tfidf part, I think I am close to run it with Spark runner in beam-examples module. #533 
I am having a spark dependency problem in the test (JavaSparkContext class not found):
https://builds.apache.org/job/beam_PreCommit_MavenVerify/1908/console
Could you help me to take a look? And, you can make a call on whether you want to include the Tfidf in this PR or left to me to handle in #533.",documentation_debt,outdated_documentation
skywalking,5293,comment,673764279,Really detected one besides the mock. You need some fix on the doc this time. :),documentation_debt,low_quality_documentation
trafodion,294,comment,178076645,Looks good to me. Thanks for fixing all those typos.,documentation_debt,low_quality_documentation
netbeans,968,comment,430955215,"@geertjanw The XSDs/DTDs are used to generate code and my interpretation of the reply from apache legal is, that the license transcends to the generated code. So for weblogic we would distribute unlicensed code (i.e. we have no right to distribute) and for jboss/wildfly we would distribute LGPL code, which is against the ASF rules.
Getting the XSDs/DTDs at runtime would be perfectly doable, but the modules would need to be reworked to no rely on code generated at build-time.
@vikasprabhakar the DTD files are present here:
https://www.jboss.org/j2ee/dtd/
The files are missing an explicit license and as such I would assume they are covered by the projects LGPL license. That at least is reflected in some of the XSDs I saw. Asking redhat would most probably reveal the same. That still leaves us with the ""wrong"" license.",documentation_debt,low_quality_documentation
skywalking,5685,comment,711190648,"@wu-sheng  thx for check the structure this time.  If this version looks good, I will modify the doc and add the unit test later.",documentation_debt,outdated_documentation
storm,1783,comment,263910453,"@ambud 
The code looks good except what @vesense commented. 
Two things more to address:
1. It would be better to document new configurations. Without documentation, end-users have no idea about added feature. `external/storm-hbase/README.md` and `docs/storm-hbase.md`.
2. The code already uses JDK 8 API (Map.getOrDefault()), so can't get it as it is for 1.x. Could you provide a new PR for 1.x branch?
It would be also great if you can test it (with Caffeine) on JRE7 (expected to not work but we can document the precondition for JRE version) and JRE8 (expected to work).
cc. @ben-manes Is my expectation right?
Thanks in advance!",documentation_debt,outdated_documentation
fineract,810,comment,623127986,"@maektwain just iteratively follow the process described on https://github.com/apache/fineract#pull-requests ... :smiling_imp: (so add that to this PR as well). If this could be written more clearly in the README, then improve it. smiley_cat",documentation_debt,low_quality_documentation
spark,27165,comment,572924850,"For fixing examples and documentation, I will do it separately.",documentation_debt,low_quality_documentation
incubator-brooklyn,83,comment,49760955,"A few failing tests, and a couple of (very minor) comments, but other than that, looks good (once tests are passing again :-))",documentation_debt,low_quality_documentation
spark,16892,comment,280117833,Ok I added back the other test but improved the commenting there.,documentation_debt,low_quality_documentation
zookeeper,567,comment,405419213,Thanks for the review @anmolnar. Please take a look at unit tests when you get a chance. I have addressed the comments. I will also add documentation in a separate commit.,documentation_debt,low_quality_documentation
spark,3165,comment,62288954,I'm actually happy to just drop this though if we can update the documentation in our wiki to suggest people use hub. @JoshRosen or @rxin would one of you guys be able to put a few lines in https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools with the process you use? I can then verify and if it's all good I can just drip this PR.,documentation_debt,outdated_documentation
carbondata,3136,comment,468114436,"@tianyouyangying +1 for @qiuchenjian 's suggestion.
1. Please change all chinese to english.
2. Please change PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity'
@qiuchenjian The origin SQL in doc actually are two statements, we should seperate them to avoid ambiguity.
Thanks.",documentation_debt,low_quality_documentation
hadoop,1900,comment,601190652,"Thanx @vinayakumarb  for checking. Seems comment isn't coming, some problem, will check. it isn't coming for patches too.
Anyway the build seems cleans.",documentation_debt,outdated_documentation
madlib,425,comment,519652224,We also need to add user docs along with examples for the new function,documentation_debt,outdated_documentation
gobblin,104,comment,95396477,LGTM barring the typo,documentation_debt,low_quality_documentation
flink,1813,comment,229693884,@subhankarb We should also add Redis Sink to the fault tolerance guarantee table for the connectors in the documentation. It can be found at `flink/docs/apis/streaming/fault_tolerance.md`.,documentation_debt,outdated_documentation
trafficserver,6001,comment,539771774,"Can you add a doc ?
Also can you update the release notes if we add this new feature to the master - https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html ?
How about remap plugin? can you check the corresponding symbols for remap plugin as well?
What other features are you planning for this ?",documentation_debt,outdated_documentation
druid,2683,comment,198480289,Are these documented?,documentation_debt,outdated_documentation
druid,6230,description,0,"This patch includes the following bug fixes:
- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type
  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.
  This fixes a bug where, for example, grouping on doubles-cast-to-longs would
  fail to merge two doubles that should have been combined into the same long value.
- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns
  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long
  would fail to merge two strings that should have been combined.
- GroupByQuery: Cast numeric types to the expected output type before comparing them
  in compareDimsForLimitPushDown. This fixes #6123.
- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into
  the proper output type. This fixes an inconsistency between results that came
  from cache vs. not-cache: for example, Jackson sometimes deserializes integers
  as Integers and sometimes as Longs.
And the following code-cleanup changes, related to the fixes above:
- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,
  and converterFromTypeToType to make it easier to handle casting operations.
- TopN in general: Rename various ""dimName"" variables to ""dimValue"" where they
  actually represent dimension values. The old names were confusing.
* Remove unused imports.",build_debt,over-declared_dependencies
ignite,6392,summary,0,IGNITE-11654: [ML] Memory leak in KNNClassificationModel,design_debt,non-optimal_design
hadoop,2556,summary,0,HDFS-15731. Reduce threadCount for unit tests to reduce the memory usage,design_debt,non-optimal_design
lucene-solr,133,summary,0,LUCENE-9069: Prevent memory leaks in PerFieldAnalyzerWrapper,design_debt,non-optimal_design
couchdb,2426,description,0,"Design doc writes could fail on the target when replicating if with non-admin
credentials. Typically the replicator will skip over them and bump the
`doc_write_failures` counter. However, that relies on the POST request
returning a `200 OK` response. If the authentication scheme is implemented such
that the whole request fails if some docs don't have enough permission to be
written, then the replication job ends up crashing with an ugly exception and
gets stuck retrying forever. In order to accomodate that scanario write _design
docs in their separate requests just like we write attachments.
Fixes: #2415",code_debt,low_quality_code
airflow,1270,description,0,pypi can use categories for better description and version number was out of sync,code_debt,low_quality_code
superset,5946,description,0,"- Remove `lodash.throttle` from dependency since there is now `lodash`.
- Add `babel-plugin-lodash` that helps optimize bundle output by taking only necessary part from lodash instead of the entire bundle.
https://github.com/lodash/babel-plugin-lodash
- Replace `underscore` calls with `lodash` where applicable. 
@williaster @xtinec @conglei",code_debt,complex_code
geode-native,56,description,0,"- section title is Interoperability of Language Classes and Types
- corrected namespaces (packages)
- removed duplicate table captions
@davebarnes97 @joeymcallister @dgkimura @mmartell @echobravopapa @PivotalSarge Can you please review these (mostly namespace) changes in the docs?",code_debt,duplicated_code
spark,15573,description,0,"Jira: https://issues.apache.org/jira/browse/SPARK-18035
In HiveInspectors, I saw that converting Java map to Spark's `ArrayBasedMapData` spent quite sometime in buffer copying : https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala#L658
The reason being `map.toSeq` allocates a new buffer and copies the map entries to it: https://github.com/scala/scala/blob/2.11.x/src/library/scala/collection/MapLike.scala#L323
This copy is not needed as we get rid of it once we extract the key and value arrays.
Here is the call trace:
Also, earlier code was populating keys and values arrays separately by iterating twice. The PR avoids double iteration of the map and does it in one iteration.
EDIT: During code review, there were several more places in the code which were found to do similar thing. The PR dedupes those instances and introduces convenient APIs which are performant and memory efficient
The number is subjective and depends on how many map columns are accessed in the query and average entries per map. For one the queries that I tried out, I saw 3% CPU savings (end-to-end) for the query.
This does not change the end result produced so relying on existing tests.",code_debt,duplicated_code
cloudstack,2594,description,0,"While working on other issues, I found this empty class. I am proposing its removal as it is not used and can only cause confusion.
Locally
Testing",code_debt,dead_code
spark,15272,description,0,"Jira : https://issues.apache.org/jira/browse/SPARK-17698
`ExtractEquiJoinKeys` is incorrectly using filter predicates as the join condition for joins. `canEvaluate` [0] tries to see if the an `Expression` can be evaluated using output of a given `Plan`. In case of filter predicates (eg. `a.id='1'`), the `Expression` passed for the right hand side (ie. '1' ) is a `Literal` which does not have any attribute references. Thus `expr.references` is an empty set which theoretically is a subset of any set. This leads to `canEvaluate` returning `true` and `a.id='1'` is treated as a join predicate. While this does not lead to incorrect results but in case of bucketed + sorted tables, we might miss out on avoiding un-necessary shuffle + sort. See example below:
[0] : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L91
eg.
BEFORE: This is doing shuffle + sort over table scan outputs which is not needed as both tables are bucketed and sorted on the same columns and have same number of buckets. This should be a single stage job.
AFTER :
- Added a new test case for this scenario : `SPARK-17698 Join predicates should not contain filter clauses`
- Ran all the tests in `BucketedReadSuite`",code_debt,complex_code
trafficcontrol,4821,description,0,"There are a couple of CDN-in-a-Box services that take much longer to build than they ought to because of how long it takes to send the build context to the daemon - but don't actually need a context of that size. This reduces the services' contexts to only what is necessary for them to build.
- CDN in a Box
Build CDN-in-a-Box",code_debt,slow_algorithm
flink,4426,description,0,"Remove unnecessary include-elasticsearch5 profile since its activation condition (usage of jdk8) is always true.
Run any mvn command in `flink` or `flink-connectors` and check that the reactor includes ES5.",code_debt,dead_code
spark,24300,description,0,"This PR aims to clean up package name mismatches.
Pass the Jenkins.",code_debt,low_quality_code
spark,5511,description,0,"Even if we wrap column names in backticks like ``a#$b.c``,  we still handle the ""."" inside column name specially. I think it's fragile to use a special char to split name parts, why not put name parts in `UnresolvedAttribute` directly?",code_debt,low_quality_code
zeppelin,3897,description,0,"Travis-CI:
 - remove all environment variables from the build matrix, to use the same cache for all jobs
 - activate the mysql service only when this service is needed
 - activate the xvfb service, when is necessary and possible
 - removed Bower Caching to remove too many complicated lines in Travis-ci
 - Giving the test names
 - Installing R only once with conda, previously it was installed twice with 'testing/install_R.sh' and 'testing/install_external_dependencies.sh
 - remove the R-Cache, because the installation with conda is quite fast
 - Delete 'test/install_R.sh' because it is no longer used
Other:
 - Ignore the tests in 'HeliumApplicationFactoryTest.java' to get the JUnit tests running in the IDE + remove exclude in 'travis.yml
 - Remove deprecation warning in maven-surefire-plugin
 - Helium works better with an absolute path, because a relative path in PATH, is not a good idea for local testing
 - Remove JVM language dependent asserts
Improvement
* https://issues.apache.org/jira/browse/ZEPPELIN-5024
* Travis-CI: https://travis-ci.org/github/Reamer/zeppelin/builds/723116251",code_debt,dead_code
spark,28163,description,0,"In the PR, I propose to optimise the `DateTimeUtils`.`rebaseJulianToGregorianMicros()` and `rebaseGregorianToJulianMicros()` functions, and make them faster by using pre-calculated rebasing tables. This approach allows to avoid expensive conversions via local timestamps. For example, the `America/Los_Angeles` time zone has just a few time points when difference between Proleptic Gregorian calendar and the hybrid calendar (Julian + Gregorian since 1582-10-15) is changed in the time interval 0001-01-01 .. 2100-01-01:
The difference in microseconds between Proleptic and hybrid calendars for any local timestamp in time intervals `[local timestamp(i), local timestamp(i+1))`, and for any microseconds in the time interval `[Gregorian micros(i), Gregorian micros(i+1))` is the same. In this way, we can rebase an input micros by following the steps:
1. Look at the table, and find the time interval where the micros falls to
2. Take the difference between 2 calendars for this time interval
3. Add the difference to the input micros. The result is rebased microseconds that has the same local timestamp representation.
Here are details of the implementation:
- Pre-calculated tables are stored to JSON files `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json` in the resource folder of `sql/catalyst`. The diffs and switch time points are stored as seconds, for example:
  The JSON files are generated by 2 tests in `RebaseDateTimeSuite` - `generate 'gregorian-julian-rebase-micros.json'` and `generate 'julian-gregorian-rebase-micros.json'`. Both tests are disabled by default. 
  The `switches` time points are ordered from old to recent timestamps. This condition is checked by the test `validate rebase records in JSON files` in `RebaseDateTimeSuite`. Also sizes of the `switches` and `diffs` arrays are the same (this is checked by the same test).
The hash maps store the switch time points and diffs in microseconds precision to avoid conversions from microseconds to seconds in the runtime.
- I moved the code related to days and microseconds rebasing to the separate object `RebaseDateTime` to do not pollute `DateTimeUtils`. Tests related to date-time rebasing are moved to `RebaseDateTimeSuite` for the same reason.
- I placed rebasing via local timestamp to separate methods that require zone id as the first parameter assuming that the caller has zone id already. This allows to void unnecessary retrieving the default time zone. The methods are marked as `private[sql]` because they are used in `RebaseDateTimeSuite` as reference implementation.
- Modified the `rebaseGregorianToJulianMicros()` and `rebaseJulianToGregorianMicros()` methods in `RebaseDateTime` to look up the rebase tables first of all. If hash maps don't contain rebasing info for the given time zone id, the methods falls back to the implementation via local timestamps. This allows to support time zones specified as zone offsets like '-08:00'.
To make timestamps rebasing faster:
- Saving timestamps to parquet files is ~ **x3.8 faster**
- Loading timestamps from parquet files is ~**x2.8 faster**.
- Loading timestamps by Vectorized reader ~**x4.6 faster**.
No
- Added the test `validate rebase records in JSON files` to `RebaseDateTimeSuite`. The test validates 2 json files from the resource folder - `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json`, and it checks per each time zone records that
  - the number of switch points is equal to the number of diffs between calendars. If the numbers are different, this will violate the assumption made in `RebaseDateTime.rebaseMicros`.
  - swith points are ordered from old to recent timestamps. This pre-condition is required for linear search in the `rebaseMicros` function.
- Added the test `optimization of micros rebasing - Gregorian to Julian` to `RebaseDateTimeSuite` which iterates over timestamps from 0001-01-01 to 2100-01-01 with the steps 1 ± 0.5 months, and checks that optimised function `RebaseDateTime`.`rebaseGregorianToJulianMicros()` returns the same result as non-optimised one. The check is performed for the UTC, PST, CET, Africa/Dakar, America/Los_Angeles, Antarctica/Vostok, Asia/Hong_Kong, Europe/Amsterdam time zones.
- Added the test `optimization of micros rebasing - Julian to Gregorian` to `RebaseDateTimeSuite` which does similar checks as the test above but for rebasing from the hybrid calendar (Julian + Gregorian) to Proleptic Gregorian calendar.
- Re-run `DateTimeRebaseBenchmark` at the America/Los_Angeles time zone (it is set explicitly in the PR #28127):",code_debt,slow_algorithm
gobblin,1862,description,0,"This PR makes various changes to improve how throttling server uses config library.
* Add a policies endpoint to throttling server to query the policy for a particular resource.
* Add expiring resources to broker so policies can be reloaded (to always get the newest one from config library).
* Refactor config library client to make loading of few configs more efficient.
* Refactor Hadoop fs config store to make it less confusing.",code_debt,low_quality_code
flink,8136,description,0,"*This work is a preparation for FLINK-11726.*
*In `SingleInputGate#create`, we could remove unused parameter `ExecutionAttemptID`.
And for the constructor of `SingleInputGate`, we could remove unused parameter `TaskIOMetricGroup`.
Then we introduce `createSingleInputGate` for reusing the process of creating `SingleInputGate` in related tests.*
This change is a trivial rework / code cleanup without any test coverage.",code_debt,dead_code
pulsar,2241,description,0,"In #2237, `administration-auth` is removed. However it is not removed from sidebard. It is causing failures on building website.
Remove `administration-auth` from the sidebar. Also remove codebase page, which doesn't make sense to be there because code changes are happening very frequently. The page quickly becomes out-of-dated.
Additionally cleanup a few things in the sidebar.",code_debt,dead_code
tvm,7314,description,0,"This PR improves the implementation of GPU `argwhere` added in https://github.com/apache/tvm/pull/6868, using exclusive scan (see https://github.com/apache/tvm/pull/7303). 
The current implementation of `argwhere` is very inefficient, because it uses atomic to update the write location. Since all threads compete for the single location, this effectively makes it a sequential kernel. Moreover, since the output indices need to be lexicographically sorted, the current implementation involves sorting along each axis.
Since `argwhere` is literally an instance of stream compaction, this is a perfect application of exclusive scan. Now, `argwhere` simply consists of 
* A single call to exclusive scan on a boolean flag array to compute the write indices.
* Compaction using the write indices (just copying elements with nonzero condition).
both of which are highly parallel operation. Thus, both atomic and sort are gone, vastly simplifying the implementation. Moreover, it also brings huge speed up, as shown below.
All numbers in milli sec
please review @zhiics @Laurawly @mbrookhart @tkonolige @anijain2305 @trevor-m",code_debt,slow_algorithm
jmeter,337,description,0,"Fixed bug whereby calling `registerError` with the following data set `[""A"", ""B"", ""C"", ""D"", ""E"", ""F""]` would return `[[""A"", 1], [null, null], [null, null], [null, null], [null, null]]` instead of `[[""A"", 1], [""B"", 1], [""C"", 1], [""D"", 1], [""E"", 1]]`.
Improved JavaDoc for `registerError`
Also removed JavaDoc which did not add anything to the method names.
Made the code more readable and at the same time fixed a subtle error.
On my spock branch:
- Bug fix (non-breaking change which fixes an issue)
[style-guide]: https://wiki.apache.org/jmeter/CodeStyleGuidelines",code_debt,low_quality_code
flink,9363,description,0,"-->
Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion.
1. Introduces a RelWriter `RelDisplayNameWriterImpl` to reuse code of ""explainTerms"" to generate operator names
2. Fix some operator names are not set in blink planner
This change is a trivial rework / code cleanup without any test coverage.",code_debt,low_quality_code
kafka,6680,description,0,"* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`",code_debt,complex_code
kafka,4691,description,0,A few small logging improvements which help debugging replication issues.,code_debt,low_quality_code
cloudstack,1295,description,0,"Added quotes to prevent syntax errors in weird situations.
Error seen in mgt server:
Cause:
Somehow a nic was missing.
After fix the script can handle this:
The other states are also reported fine:
While at it, I also removed the INTERFACES variable/constant as it was only used once and hardcoded the second time. Now both are hardcoded and easier to read.
This is the same as PR #1249 except it is against `4.7`.",code_debt,low_quality_code
flink,10753,description,0,"-->
To complete finish the translation work of this page.
This page is about Flink's Table API & SQL. Mainly showing the usage of Table API & SQL, how Flink optimize SQL queries and the difference between Blink and Flink planner. Now it has been translate into Chinese.
This change is a trivial rework / code cleanup without any test coverage.",code_debt,low_quality_code
accumulo,221,description,0,"* Removed log4j config being done in Java but some remains
* Logging is now configured using standard log4j JVM property
  'log4.configuration' in accumulo-env.sh
* Tarball ships with less log4j config files (3 rather than 6)
  which are all log4j properties files.
* Log4j XML can still be used by editing accumulo-env.sh
* Moved AsyncSocketAppend to start module due to classpath issues
* Removed auditLog.xml and added audit log configuration to
  log4j-service & log4j-monitor properties files
* Accumulo conf/ directory no longer has an examples/ directory.
  Configuration files ship in conf/ and are used by default.
* Accumulo monitor by default will bind to 0.0.0.0 but will
  advertise hostname looked up in Java for log forwarding
* Shortened names of logging system properties
* Removed MonitorLoggingIT as it is now difficult to setup log
  forwarding using MiniAccumuloCluster.",code_debt,low_quality_code
madlib,243,description,0,"JIRA: MADLIB-1206
This commit adds support for mini-batch based gradient descent for MLP.
If the input table contains a 2D matrix for independent variable,
minibatch is automatically used as the solver. Two minibatch specific
optimizers are also introduced: batch_size and n_epochs.
- batch_size is defaulted to min(200, buffer_size), where buffer_size is
  equal to the number of original input rows packed into a single row in
  the matrix.
- n_epochs is the number of times all the batches in a buffer are
  iterated over (default 1).
Other changes include:
- dependent variable in the minibatch solver is also a matrix now. It
  was initially a vector.
- Randomize the order of processing a batch within an epoch.
- MLP minibatch currently doesn't support weights param, an error is
  thrown now.
- Delete an unused type named mlp_step_result.
- Add unit tests for newly added functions in python file.
Co-authored-by: Rahul Iyer <riyer@apache.org>
Co-authored-by: Nikhil Kak <nkak@pivotal.io>
Closes #243",code_debt,low_quality_code
pulsar,5592,description,0,"Fixes #5589 
It seems that there is a memory leak in the pulsar-function-go library.
I implemented a simple pulsar function worker that just write logs using pulsar-function-go/logutil for sending logs to log topic. I tried to long-term test by sending request messages consecutively to the input topic to check the feasibility.
During the test, I faced `ProducerQueueIsFull` error with `--log-topic` option. And I observed indefinitely grown memory usage of the pulsar function worker process.
Clear the `StrEntry` variable after finish addLogTopicHandler() function regardless of the log messages are appended to logger or not. If it is not cleared, it causes memory leak because StrEntry has grown indefinitely. Moreover, if the function set --log-topic, then the topic could get accumulated huge messages that cause ProducerQueueIsFull error.
Verified it by reproducing step described in the issue #5589.",design_debt,non-optimal_design
hudi,1664,description,0,"Default value of number of delta commits required to do inline compaction (hoodie.compact.inline.max.delta.commits) is currently 1. Because of this by default every delta commit to MERGE ON READ table is doing inline compaction as well automatically.
I think default value of 1 is little overkill and also it will make MERGE ON READ work like COPY ON WRITE with compaction on every run. I am propsing to increaset the value to old default of 10.
This pull request is a trivial rework / code cleanup without any test coverage.",design_debt,non-optimal_design
qpid-dispatch,1047,description,0,"* Do not write new buffers if connection is CLOSED_WRITE
* Do not call connection_wake if CLOSED_READ or CLOSED_WRITE
This fixes crashes but there is still work left with leaking messages and buffers when server connections close before client connections.",design_debt,non-optimal_design
airflow,7085,description,0,"Most of the executors run local task jobs by running `airflow tasks run ...`. This is achieved by passing  `['airflow', 'tasks', 'run', ...]` object to subprocess.check_call. 
This is very limiting when creating new executors that do not necessarily want to start a new process when starting a local task job, e.g. fork a process instead of create.
We could achieve a similar effect if we process back the argument list, but this is an ugly and hack solution, so I did refactor the code and now the executor passes the LocalTaskJobDeferredRun object that contains all the detailed information. A particular executor could create a command if it needs it.
This will facilitate the development of other executors:
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-29%3A+AWS+Fargate+Executor (@aelzeiny)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option (@dazza-codes)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-25+The+Knative+Executor (@dimberman 
https://github.com/apache/airflow/pull/6750 (@nuclearpinguin )
This also made the DebugExecutor code simpler. (@nuclearpinguin )
---
Issue link: [AIRFLOW-6334](https://issues.apache.org/jira/browse/AIRFLOW-6334)
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.",design_debt,non-optimal_design
incubator-heron,2295,description,0,"New commands:
  heron examples list
  heron examples run <cluster> <example-id>
This patch is to provide easier commands to get the heron examples up and running.",design_debt,non-optimal_design
incubator-mxnet,11076,description,0,"this pull request is based on https://github.com/apache/incubator-mxnet/pull/10804
with the following further changes:
1. reduce ident changes
2. prefer cudnn depthwise convolution over mxnet implementation
still use the explicit #if #else #endif statement over
the new variable effective_num_group solution for backward code path compability
because the new variable effective_num_group may confuse readers with standard group convolution",design_debt,non-optimal_design
spark,26138,description,0,"What changes were proposed in this pull request?
Some of the columns of JDBC/ODBC tab  Session info in Web UI are hard to understand.
Add tool tip for Start time, finish time , Duration and Total Execution
Why are the changes needed?
To improve the understanding of the WebUI
Does this PR introduce any user-facing change?
No
How was this patch tested?
manual test",design_debt,non-optimal_design
spark,20619,description,0,"ParquetFileFormat leaks opened files in some cases. This PR prevents that by registering task completion listers first before initialization.
Manual. The following test case generates the same leakage.",design_debt,non-optimal_design
hive,767,description,0,"LockedDriverState is a nested class within Driver, while it is used outside of it as well, and it is complex enough to be a class on it's own. DriverState should be it's nested class, and transitions / locking should be facilitated by functions within it.",design_debt,non-optimal_design
kafka,4138,description,0,"Even though this class is internal, it's widely
used by other projects and it's better to avoid
breaking them until we have a publicly supported
test library.",design_debt,non-optimal_design
trafficserver,7279,description,0,"Some messages get excessively noisy under high traffic conditions if
something about their mechanism goes wrong. The pipe logging feature,
for instance, will emit warning and error messages on every single log
event if the reader goes down or the pipe buffer fills up. This can
result in thousands of log messages being emitted per second, which
makes reading the logs difficult and causes disk space issues.
This commit addresses this issue by adding throttled versions of the
common logging messages so they only emit a message on some set
interval (60 seconds as a default). The following functions are added:
SiteThrottledStatus
SiteThrottledNote
SiteThrottledWarning
SiteThrottledError
SiteThrottledFatal
SiteThrottledAlert
SiteThrottledEmergency
As a bonus, these are implemented using a generic Throttler class which may also be
useful in other applications where throttling is desired.",design_debt,non-optimal_design
bookkeeper,1059,description,0,"Descriptions of the changes in this PR:
This is cherry-pick from yahoo repo of branch yahoo-4.3.
original commit is:
https://github.com/yahoo/bookkeeper/commit/42bdc083
Release addEntry-Bytebuf on readOnlyBookie to prevent memory-leak",design_debt,non-optimal_design
arrow,7885,review,465480283,Added more test cases for negative numbers and boundary conditions.,test_debt,low_coverage
druid,4815,review,146068538,Can you add some tests around early publishing and loading sequence data from disk?,test_debt,lack_of_tests
spark,24043,review,292472811,We should at least add a test case for this new option,test_debt,lack_of_tests
flink,9977,review,338932359,"Add a UT test for these coders. For example, we can add a test in `pyflink/fn_execution/tests`.",test_debt,lack_of_tests
daffodil,289,review,346101694,"Need to explain this algorithm more thoroughly around what is done with sequences specifically.
Also unit tests focusing specifically on this algorithm, to strongly characterize proper behavior and insure full coverage.",test_debt,low_coverage
spark,28841,review,463960973,"Could you add tests for multiple file cases? Probably, I think you might be able to use `(new File(""/tmp/file.csv"")).setLastModified(xxx)` to control timestamp.",test_debt,lack_of_tests
flink,8438,review,287425587,I think we don't need this test here.,test_debt,expensive_tests
kafka,3530,review,130078682,It would be nice to allow the staging receives to complete and verify that the closing channel is also empty like in the other test.,test_debt,expensive_tests
kafka,6239,review,256163993,minor - no test coverage for lines 221 - 225 in the unit test,test_debt,low_coverage
guacamole-client,511,review,602658052,"Sure, makes sense. I've got the changes almost done - just need to clean up the unit tests.",test_debt,expensive_tests
spark,18887,review,140699378,The test cases in `kvstore` are just unit test cases. We also need integration tests for ensuring they work as expected.,test_debt,lack_of_tests
lucene-solr,2200,review,556698348,"Classloading won't help, because we still need a separate JVM. When JVM loads classes, it trys in parent classloader first. If the class is already there it won't load. So we need at least one separate process.
I don't like to try many times each with separate JVM. Maybe only try once (like in the other test with codecs). It may not fail every time, but sometimes test fails.
I am also not sure if we really need a test for this. If we may get a static checker that finds classes that initialize their subclasses in their own static initializer, we can prevent similar cars in future.",test_debt,flaky_test
spark,29403,review,496341174,"Also, if I'm correct, it would be good to include a test case for this case.  What happens if you actually do operations on the null value stored as an enum?  Is the nullability of the resulting schema correct. Do operations like `.isNotNull` work correctly?",test_debt,lack_of_tests
kafka,6363,review,280294974,"It'd be really great to have unit tests for many of these methods. The `performTaskAssignment(...)` method is already pretty lengthy, and there are just a few unit tests whereas there seem to be lots of permutations and branches. Not only would they help with confidence, but they'd help with regression testing if/when we have to get back into this code.",test_debt,lack_of_tests
incubator-mxnet,13654,review,241911517,"Can you add comment saying why we have excluded these test cases?
Do we have custom tests for these somewhere?",test_debt,lack_of_tests
samza,347,review,180646742,"I haven't measured how much time increase it causes. The purpose of this change is to make each test independent of each other. If we use BeforeClass and AfterClass tags, then these methods need to be static and the variables instantiated (i.e. producer, systemAdmin) needs to be static as well. These variables will then be instantiated exactly once for the test suite and used by all unit tests in this test suite. Thus kind allows interference between tests and make make test development difficult.
During development of this patch the interference must have caused some problem which required me to make this change. I don't exactly remember what that problem is now. But in general it seems like good practice to make unit test independent of each other. What do you think?",test_debt,expensive_tests
samza,103,review,109215517,"This is a bit concerning that we are commenting out a good number of tests here. I would prefer to fix them, before check-in.",test_debt,lack_of_tests
kafka,2143,review,94173222,"Besides not removing this, it seems we should also probably have a unit test validating the new behavior for naming consumer groups in sinks?",test_debt,lack_of_tests
drill,159,review,39804781,"please add a test where both _SYSTEM_ and _SESSION_ options are changed, and confirm the reset is working as expected.",test_debt,lack_of_tests
pulsar,7601,description,0,"-->
Fixes #<xyz>
Master Issue: #<xyz>
This mr adds an api to check if the worker is ready to serve requests.
*Describe the modifications you've done.*
This change is a trivial rework / code cleanup without any test coverage.
This change is already covered by existing tests, such as *(please describe tests)*.
This change added tests and can be verified as follows:
  - If a feature is not applicable for documentation, explain why?
  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation",documentation_debt,low_quality_documentation
arrow,4515,description,0,"At this point the function is not exported or documented and threads are always used, users would need to set `options(arrow.use_threads)` to turn them off.",documentation_debt,outdated_documentation
airflow,2816,description,0,"Dear Airflow maintainers,
    - https://issues.apache.org/jira/browse/AIRFLOW-1848
Dataflow Python operator takes in a filename without `.py` extension, which was incorrectly documented previously.
N/A, just a doc change.
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood (""add"", not ""adding"")
    5. Body wraps at 72 characters
    6. Body explains ""what"" and ""why"", not ""how""",documentation_debt,low_quality_documentation
superset,12739,description,0,"""Environment"" was misspelled on line 348, I have corrected this typo.",documentation_debt,low_quality_documentation
flink,15045,description,0,"fix some typo errors to make the context consistent:
some are ""streamOfWords"" but some are ""dataStreamOfWords""",documentation_debt,low_quality_documentation
incubator-mxnet,12426,description,0,"The PR applies the website theme to each version. The navigation will be the same, so the option for Clojure needs to be handled properly for old versions. For this I use the .htaccess file to redirect users to an API error page. For good measure, I also added a custom 404 error page.
This PR stacks on #12413 (has the same changes in build_all_version.sh), plus a change to copy the theme, and fixes my concerns there with the theme.
http://34.201.8.176/
You can test the redirect if you switch to an old version like 1.0.0 and go to API --> Clojure
You can look at the 404:
http://34.201.8.176/error/404.html 
I'm sure there's probably some fancy regex that would collapse the clojure rules to one line, but I'll let someone else get fancy.",documentation_debt,low_quality_documentation
spark,6593,description,0,"This also helps us get rid of the sparkr-docs maven profile as docs are now built by just using -Psparkr when the roxygen2 package is available
Related to discussion in #6567 
cc @pwendell @srowen -- Let me know if this looks better",documentation_debt,low_quality_documentation
spark,29402,description,0,"This PR proposes to `include` `_images` and `_sources` directories, generated from Sphinx, in Jekyll build.
**For `_images` directory,**
After SPARK-31851, now we add some images to use within the pages built by Sphinx. It copies and images into `_images` directory. Later, when Jekyll builds, the underscore directories are ignored by default which ends up with missing image in the main doc.
Before:
After:
**For `_sources` directory,**
To show the images correctly in PySpark documentation.
No, only in unreleased branches.
Manually tested via:",documentation_debt,low_quality_documentation
carbondata,3769,description,0," - No
 - Yes. (please explain the change and update document)
 - No
 - Yes",documentation_debt,outdated_documentation
dubbo,1906,description,0,"#1682: Enhance the test coverage part-4 : dubbo-common/src/main/java/com/alibaba/dubbo/common/status(store|threadpoolutils) modules
dubbo-common/src/test/java/com/alibaba/dubbo/common/utils/UrlUtilsTest.java
XXXXX",test_debt,low_coverage
kafka,6475,description,0,"Fix flaky test cases in `WorkerTest` by mocking the `ExecutorService` in `Worker`.  Previously, when using a real thread pool executor, the task may or may not have been run by the executor until the end of the test.
Related JIRA issues:
Ran all tests (`./gradlew test`).
Ran unit tests in `connect/runtime` repeatedly.",test_debt,flaky_test
incubator-mxnet,18560,description,0,"Currently, 1.6.x branch is failing on multiple pipelines
1. centos-cpu & centos-gpu pipelines due to 
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline
2. Flaky test on unix-cpu mkldnn
3. Edge pipeline",test_debt,flaky_test
camel,4496,description,0,"#4490 does not cover all the cases. While performing end-to-end tests with this snapshot, I found that traces were still not aggregating correctly. This PR fixes the problem.
I am not sure why the tests did not catch this issue, so unfortunately cannot provide better automatic coverage.
Below are the contribution guidelines:
https://github.com/apache/camel/blob/master/CONTRIBUTING.md",test_debt,low_coverage
incubator-heron,1893,description,0,These tests all have to duplicate the same verbose boilerplate. Centralizing that in `SlaveTester`.,test_debt,expensive_tests
druid,5102,review,158433249,"@jihoonson If you mean the actual ""CompactionTask"" etc classes, I think probably moving something so heavy from druid-indexing-service all the way down to druid-api would probably require collapsing a ton of druid modules into one giant druid-core module. I guess we could do that but it seems like a big change. Do you think it's worth it?",architecture_debt,violation_of_modularity
kafka,9001,review,462716040,Could be moved to `UpdateFeaturesResponse` as a utility.,architecture_debt,violation_of_modularity
airflow,730,review,85289615,We already have a test plugin in `tests/plugins/`. This is less complete than that one and should probably not be added here. We can consider moving the other one (in a separate PR) to serve as a sample.,architecture_debt,violation_of_modularity
kafka,1664,review,77421658,Can you restructure this to use `val` - it helps to have a single block on the RHS that encapsulates the full assignment logic (as opposed to being exposed to the method's entire scope).,architecture_debt,violation_of_modularity
trafficserver,3322,review,176559777,Do me a favor and move both `MemSpan` and `MemArena` so the test source files are in alphabetically order.,architecture_debt,violation_of_modularity
cloudstack,4490,description,0,"Fixes https://github.com/apache/cloudstack/issues/4481
TODO",requirement_debt,requirement_partially_implemented
airflow,13728,comment,761947640,Temprary failures - it's good to go.,defect_debt,uncorrected_known_defects
spark,20923,comment,383571843,"@vanzin : The followup to this is #21066; I could move the compile time changes there but if you are going to have POMs playing with dependencies, seems best to have it all in one place...the other one just setting up the compile and tests
@jerryshao what do you suggest? It was your proposal to split things into pom and source for ease of reviewal, after all?",architecture_debt,violation_of_modularity
spark,18986,comment,323618879,"Yea, since this topic is important for some users, I mean we better move the doc into `./docs/` ( I feel novices dont seem to check the code documents).",architecture_debt,violation_of_modularity
activemq-artemis,2241,comment,412575863,"IMO, the JMS pool from 5.x should not be migrated to Artemis.  It belongs in it's own project with it's own release cycle.  Also, it makes sense for it to *not* be in the ActiveMQ project to make clear that the pool is generic and isn't tied to any ActiveMQ broker.
The pool on messaginghub has JMS 2.0 support.",architecture_debt,violation_of_modularity
