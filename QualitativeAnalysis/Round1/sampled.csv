project,pull_number,pull_type,id,text,classification,indicator
incubator-brooklyn,131,review,16893613,No point in duplicating IfFunctionBuilderApplying's code just for one differing line. Better instantiate IfFunctionBuilderApplying with Null object for the input argument (i.e. empty IfFunction).,code_debt,duplicated_code
tvm,3531,review,305483155,Moving to C++ and better error message,code_debt,low_quality_code
calcite,1825,review,383130171,No need to use ```LinkedHashMultimap``` because the key should be unique.,code_debt,low_quality_code
calcite,2076,review,457807922,Remove this sentence. It is not needed anymore.,code_debt,dead_code
zookeeper,904,review,274872856,This method is only used in the test currently. Please add it to `shutdown()` methods of `ZooKeeperServer` and `QuorumPeer` classes.,code_debt,low_quality_code
kafka,6977,review,298727205,This function seems not used?,code_debt,dead_code
incubator-heron,3123,review,242657195,+1 for reducing the redundancy.,code_debt,complex_code
trafficserver,834,review,78213185,"There is a `MIN()` macro you can use for this.
Also, I would use `sizeof(buffer)` instead of magical numbers.",code_debt,low_quality_code
nifi-minifi-cpp,74,review,109758278,i want to return the same so that it can be reused in next trigger.,code_debt,low_quality_code
spark,19041,review,163041732,"It's very rare for us to mark a class as final, especially a private one. Is there a reason for that?
It makes other things (like mocking the class in tests) more complicated, for example.",code_debt,low_quality_code
kylin,961,review,352444230,It is a `Singleton`. Should be `final` instead.,code_debt,low_quality_code
kafka,8706,review,432145121,"As above: `stores` should never be null, and thus we don't need this change? Also the check for `isEmpty` does give us much, we can still call `addAll` even it `stores` is empty?",code_debt,low_quality_code
superset,10473,review,466556166,"Typing here seems not needed. If source is optional, you can also give it a default value, otherwise maybe make it non-optional. There is actually an [eslint rule](https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/require-default-props.md) for this in the PropTypes world.",code_debt,low_quality_code
spark,22197,review,212812755,nit: We can have just `exception when duplicate fields in case-insensitive mode` as test title. Original one is too verbose.,code_debt,low_quality_code
spark,28641,review,435012863,Is this TaskSet index really needed? We should avoid rely on the ordering of TaskSets to verify the results.,code_debt,complex_code
nifi,3536,review,300057978,Is this commented out code no longer needed?,code_debt,dead_code
spark,26594,review,353931395,nit. `//  Need` -> `// Need`. (reducing one space between `//` and `Need`.).,code_debt,low_quality_code
flink,5367,review,165039357,I think this method is not really necessary. We already have logic for accessing composite types (see `visitFieldAccess()`). Maybe we just have to make the methods there a bit more generic.,code_debt,complex_code
airflow,4751,review,346970934,"This works, but it asks for a lot more columns and rows than we need.
We could try changing the return inside this function from `return tis.all()` to just `return tis`, and this line could become:
https://docs.sqlalchemy.org/en/13/orm/loading_columns.html#load-only-and-wildcard-options
Do you think this is worth it or not worth it?",code_debt,low_quality_code
kafka,9100,review,477786630,I still think we need a better name for `pendingInSyncReplicaIds` since it is misleading in this case. Maybe we could call it `overrideInSyncReplicaIds` or something like that?,code_debt,low_quality_code
airflow,11541,review,507190029,"Same here  - there are possibly few hundreds of places all over our code where "" no quote on the left side inside [[ ]] "" is  followed. I'd prefer to keep it - it makes the code a bit more readable.",code_debt,low_quality_code
accumulo,484,review,188314105,It seems like this method is no longer needed.,code_debt,dead_code
parquet-mr,456,review,170675141,Why is nullCounts checked for being null but minValues and maxValues being used without a similar check?,code_debt,low_quality_code
calcite,1075,review,261510267,"I would suggest to change the name `PRETTY_JSONISE` to `JSON_PRETTY`, also the string value and test method names below. Using a unified name convention will benefit new developers when they do a code searching.",code_debt,low_quality_code
beam,4080,review,150063020,I believe setting the compile scope is redundant,code_debt,complex_code
daffodil,3,review,148564978,"I understand this is hardcoded now, but expected this naming convention would change. So are we going to continue with the NNN-SNAPSHOT convention for naming?",code_debt,low_quality_code
spark,4525,review,24634640,"So, this feels a little racy, in that this thread might miss things added by the log checking thread. I'd suggest the following:
- Create a single-threaded executor for running the replay tasks
- Create a list of app infos to parse in the log checking thread, break it down into batches.
- Submit each batch to the executor
Basically, instead of having `logLazyReplay`, you'd have something like `replay(apps: Seq[LazyAppInfo])`. You don't need `lazyApplications` because that becomes part of the task being submitted to the executor, so you solve another source of contention in the code. And since it's a single-threaded executor, you know there's only a single thread touching `apps`, so it should all be thread-safe.
For testing, you can use Guava's `sameThreadExecutor()` as I mentioned, instead of the single-threaded executor.",code_debt,low_quality_code
spark,13410,review,65192185,"`seed` is not necessary since we use `approxQuantile`. It was added after Spark 1.6, so removing it will not involve breaking change.",code_debt,low_quality_code
flink,6076,review,192981069,`TwoInputStreamOperator` and `OneInputStreamOperator` are internal classes. We should not mention them here but use a generic `operator with one input` term.,code_debt,low_quality_code
incubator-pinot,2228,review,158572949,I actually don't need the .valueOf(). Removing it.,code_debt,low_quality_code
spark,610,review,12292824,"As per our offline discussion, let's just change this to `unused/class/path` or something.",code_debt,low_quality_code
nifi,4824,review,576641177,Minor: unnecessary space after `put(`,code_debt,low_quality_code
arrow,4542,review,293670228,it might be slightly faster to take the minimum of length and only compare that in the loop check?,code_debt,slow_algorithm
incubator-mxnet,16790,review,345358591,This sync is not necessary.,code_debt,complex_code
kafka,7994,review,373361209,"nit: ""an none"" seems ungrammatical",code_debt,low_quality_code
flink,9866,review,333911282,"The scala code is almost the same as Java, so no need to create a separate tab for scala , we have also the case that Java/Scala share the same code demo, such as  Filesystem connectors..",code_debt,low_quality_code
spark,22646,review,223168936,This exception message looks a bit confusing. We can say the given type is not supported and we only support the certain type (`java.util.List` and `java.util.Map`).,code_debt,low_quality_code
spark,19901,review,155169114,"nit: `val tmpIsNull = ctx.freshName(""coalesceTmpIsNull"")`, to be consistent with https://github.com/apache/spark/pull/19901/files#diff-a966ee88604a834221e82916ec051d7dR190",code_debt,low_quality_code
hadoop,885,review,289591884,whitespace:tabs in line,code_debt,low_quality_code
spark,9353,review,43474236,This is not necessary. See discussion at https://issues.apache.org/jira/browse/SPARK-11337. You can include all imports in a single block or move unused imports to a separate group. We only need to keep imports ordered in each group. Use empty lines to separate import groups.,code_debt,complex_code
cloudstack,1953,review,102184780,"Not part of your changes but... variable `devIds` should have type `List<Long>` instead of `List<String>`.
All that conversion from `int` to `String` and then converting from `String` to `long` seems unnecessary.
Should simply be able to do:
Note: my code above includes fixes to two other comments I made further down in the code.",code_debt,complex_code
nifi,3894,review,365769826,Unused import.,build_debt,over-declared_dependencies
spark,11236,comment,189069233,"LGTM,
thanks for writing these benchmarks. 
I think moving forward, I agree that ColumnVector is a natural data structure to decode into, but we should probably not add this logic directly into those classes just from a code maintenance point of view. I think exploring the parquet encodings makes sense but let's start by benchmarking those and see if they have the right performance characteristics.",design_debt,non-optimal_design
spark,29715,comment,692354511,"No. That's a side improvement which can be dropped, not a major goal.
As I commented, fixing the problems on DataStreamWriter isn't the purpose of introducing DataStreamWriterV2. This is rather providing symmetric user experience between batch and streaming, as with DataFrameWriterV2 end users can go through running batch query with **catalog table** on writer side, whereas streaming query doesn't have something to enable this.
The problems I described in previous comment are simply the problems on Structured Streaming - let me explain at the end of comment, as it might be going to be out of topic.
I see DataFrameWriterV2 has integrated lots of other benefits (more fluent, logical plan on write node, etc.) which should be great to have in DataStreamWriterV2, but I think they're not a key part of *WriterV2. Supporting catalog table is simply the major reason to have it.
Regarding the problems on Structured Streaming - 
I kicked the incomplete state support on continuous mode out from Structured Streaming, but I basically concerns about ""continuous mode"" itself, as it's rather applying hacks to workaround architectural limitation. (+ No one cares about it in community.) 
And as I had initiated discussion earlier (and has been commented in various PRs), I think complete mode should be kicked out as well. The mode addresses some limited cases but is treated as one of valid modes which adds much complexity - some operations which basically shouldn't be supported in streaming query are supported under complete mode, and vice versa. Because the mode doesn't fit naturally.
It's useful for now because Spark doesn't support true update mode on sink - and once Spark can support update mode on sink, content in external storage should be just equivalent to what the complete mode provides, without having to dump all of the outputs. (Or that's just because of missing feature - queryable state.) Probably we can simulate complete mode via having a special stateful operator which only works with update mode.
Specific to micro-batch, supporting DSv1 is also a major headache - lots of pattern matchings in MicroBatchExecution are to support DSv1, and even there're workarounds applied for DSv1 (e.g #29700). I remember the answer in discussion thread that DSv1 for streaming data source is not exposed to the public API which is great news, but I see no action/plan to get rid of it. Is there something DSv2 cannot cover the functionality which is possible in DSv1? If then why not prioritize to address the problem?",design_debt,non-optimal_design
zookeeper,1257,comment,593204387,"looks good. I still have another thought(no binding for the following, you can ignore it):
- Since the root cause is a narrow windows between `queuePacket` and `cleanup`, so synchronized `objectLock` is also an alternative way? which one is better? Since `outgoingQueue` is a critical Queue for client to talk with server, synchronized `outgoingQueue` will have performance and future program extensibility issue?
Haha, I also test for the `global` and `inner` wording by the following way:
- new two different zookeeper clients, create some znodes, printing the `hashcode` of `outgoingQueue` and `state`. They really hold different `outgoingQueue`, but the same hashcode of `state`. it really confuses me.
- I believe different clients will have different `state` instance, otherwise when one client calls `close()`(set `state` to `CLOSE`), it will affect another client. However, using following ways cannot reason about it.
`javap` to see the bytecode, the value set to `state` is `public static final`, so it's really global-shared by multi-clients.
- synchronized `Enum` is also not thread-safe. Look at my demo attached in JIRA.
- In a word, `Enum` is a heresy:)",design_debt,non-optimal_design
kafka,683,comment,167010840,"@benstopford @fpj This looks good to merge aside from the comment I left about the `use_authorizer` parameter to `KafkaService`. We only have one `Authorizer` implementation, so the parameter kind of makes sense in the scope of only Kafka's tests, but we should think of the `Service` classes as semi-public interfaces we want to be careful about changing -- they should be reusable by others to write system tests. If anyone wanted to test other authorizer implementations, I think the current `use_authorizer` parameters wouldn't be a great solution -- they'd have to add still another option (or make assumptions about the implementation by setting the `authorizer_class_name` field on `KafkaService` directly).",design_debt,non-optimal_design
bookkeeper,888,comment,352890209,"We assume that, for a given version, the notice and license files will not change, which is a safe assumption to make.
I'm dubious as to whether it will be possible to pull in NOTICE files automatically. We would have to pull in every NOTICE file, which isn't really necessary. And then someone would have to check the contents of the pulled in NOTICE file to ensure everything is ok.
If we make the pulling of licenses automatic, then they will only ever be checked at release time. At release time, all dependencies need to be checked, and when there's so much to check, people are likely to just give it a quick glance, and +1 it, without actually checking each dependency.
I would prefer that the work in manually checking dependencies occurs as part of the development process, each time we update a dependency. At this time, there will be a smaller subset of the dependencies changing, so it can be reviewed more carefully. The submitter will be able to take their time with it, and the reviewer will be able to give each dependency their full attention. Once a license/notice has been updated for a version of the dependency, it shouldn't need to be looked at again (as licenses/notices don't change within a single version).",design_debt,non-optimal_design
carbondata,3800,summary,0,[CARBONDATA-3877] Reduce read tablestatus overhead during inserting into partition table,code_debt,low_quality_code
tinkerpop,144,summary,0,TINKERPOP3-957: Improve speed of addV(),code_debt,slow_algorithm
tajo,674,summary,0,TAJO-1736: Remove and improvement a unnecessary method(getMountPath()).,code_debt,dead_code
apisix,336,summary,0,change(CLI): removed duplicated `include` option.,code_debt,duplicated_code
arrow,9397,review,571604769,"Fixed Length Byte Array makes sense to me (as the interpretation of the bytes for decimal is different than either i32 or i64). 
Reasons I could imagine using i32 or i64 to write decimals into Parquet would be 
1. ecosystem compatibility (aka that the pandas parquet reader assumed decimals were stored using those types) 
2.possibly so better / more performant encodings could be used.
But I am just SWAG'ing it here",requirement_debt,non-functional_requirements_not_fully_satisfied
flink,10358,comment,561655258,"@shuttie Got it, makes a lot of sense to me.
I was wondering at some point if it would be worthwhile to just ""unsafe arraycopy"" the char[] from the string to the byte[] in the memory segments or stream buffers. So basically no byte-wise logic at all in the serialization. That would increase the state size (all chars would have two bytes), but might save CPU resources.
Have you ever experimented with something that?",requirement_debt,non-functional_requirements_not_fully_satisfied
spark,18567,comment,313762412,"BTW, do we need a new test case for `correctly set the active session`?",test_debt,low_coverage
cloudstack,1519,comment,215347296,"@dsclose I think it is better to split this PR into some isolated PRs, as the issues are isolated.
to be honest, some commits looks good to me ( as we have similar fix in our production), others need testing.",test_debt,lack_of_tests
spark,6059,comment,101015161,"Sorry I wasn't being clear, but my point is that it doesn't fail all the time, and when it does fail it delays other unrelated patches being merged. This is why we ignored it for now. We will reenable it later before we actually ship the release but we need to find a way to actually fix the flakiness before we do that.",test_debt,flaky_test
fineract,726,comment,596234596,"This is just a rebase of  #719 from @xurror and @percyashu and I'm just curious if this passes... I ran this locally x3 times, and it failed with https://issues.apache.org/jira/browse/FINERACT-855 every time - may be that is not just a flaky test, but really something that this PR breaks, for some (strange, yes) reason.
NB that other PRs passed today (e.g. #723 and #725) so if this still fails, perhaps the new RestAssured version is somehow changing some timing or something which causes IT test failures?!",test_debt,flaky_test
flink,13595,comment,719467007,"Thanks for the updates @wsry ! 
I think most of my previous comments were addressed except for https://github.com/apache/flink/pull/13595#discussion_r514988635. And as @gaoyunhaii also mentioned above, it is better to supplement some tests for covering empty subpartition case.",test_debt,low_coverage
spark,26108,review,334704586,"I'm a little confused about why this needs to be a config. Isn't this just metadata written to the state store? Why would a user ever need to set this?
I have to go over the rest of the code with more care, since I'm really not familiar with it. But it looks like an insane amount of code just to add one single field to some state object...",design_debt,non-optimal_design
spark,22375,review,217495874,"I see, so the example above passes in codegen off and fails with codegen on with this fix, while using `Map(3 -> 7, 6 -> -1)` passes codegen on and fails codegen off, am I right?
What I am thinking about (but I have not yet found a working implementation) is: since the problem arise when we say we expect `null` in a non-nullable datatype, can we add such a check? I mean, instead of pretending the expected value to be nullable, can't we add a check in case it is not nullable for being sure that it does not contain `null`? I think it would be better, because we would be able to distinguish a failure caused by a bad test, ie. a test written wrongly, from a UT failure caused by a bug in what we are testing. What do you think?",design_debt,non-optimal_design
spark,22071,review,209714362,"Got it, my reasoning is that it could be harder for someone looking at the code to figure out why this is not allowed, since we don't really mention about the rest server which is really the one requiring security to be turned off. Another reason it will be beneficial to have the check in the MesosRestServer is that the MesosClusterDispatcher framework could technically be decoupled from the MesosRestServer and allow another way to receive requests, so to increase flexibility and avoid someone forgetting about why we put this here, my suggestion is to move the check closer to where it's being required will help maintain this a bit better.",design_debt,non-optimal_design
spark,5350,review,28019159,"We need more thinking about reuse the UTF8String object, it's not a trivial decision, so I'd like to leave this out of this PR.",design_debt,non-optimal_design
airflow,7516,review,383040768,"I want to use this decorator also to prevent regression in scheduler performance. Some methods are critical and I have optimized it to use very few queries, but it can be easily broken.  This context manager will allow us to detect a regression regarding it.",design_debt,non-optimal_design
reef,234,review,32866959,Remove the commented out code.,documentation_debt,low_quality_documentation
drill,1953,review,379551973,"Thanks, replaced as you proposed, but also left mentioning that we have metadata about segments, files, row groups, partitions since it wasn't described in this doc yet.",documentation_debt,low_quality_documentation
beam,5384,review,189163677,"Let's drop this comment for two reasons:
1. First part of the comment is describing the same thing as the code in English, it is not adding additional context.
2. Second part is dataflow runner specific, and the example does not even run on dataflow runner yet.",documentation_debt,low_quality_documentation
incubator-pagespeed-ngx,1028,review,43148618,comment is out of date now,documentation_debt,outdated_documentation
druid,5632,review,180936341,"This deserves a comment so someone doesn't ""simplify"" it back into the old code.",documentation_debt,low_quality_documentation
spark,10152,review,46763785,"will add java doc to explain it.
for backward compatibility, I can add default value in this function instead of the newly introduced one.",documentation_debt,outdated_documentation
druid,7647,review,283550456,Is `lenghth` a typo here?,documentation_debt,low_quality_documentation
airflow,8875,review,426287475,"It _may_ be doable, but is not documented/clear. I think when I added timezones I got it working for display side (rather i got it using my custom control to decorate the fields so they get converted)",documentation_debt,low_quality_documentation
superset,6519,comment,448402012,"i agree no matter gunicorn/nginx support very long header or not, carry 8k referrer request header is too much. I think add an upper limit is necessary.
About losing query state, @mistercrunch do you think use localStorage to save query state is a good idea? we can use js (already existed) generate an `impression` key, unique for every page load. With same `impression` key, we store query state, so we can support redo and undo like dashboard edit.",code_debt,low_quality_code
tinkerpop,1309,comment,677678867,"Thanks for this - a few minor comments/nits:
1. Could you please rename tests that start with ""Test*"" to our more standard ""should*""
2. Have a look at where you might add more `final` declarations to match the code style.
3. I like the idea of integration tests with the `SimpleWebSocketServer` - very smart
4. I'm surprised you found as many places as you did where `Cluster.close()` wasn't called.
5. I don't see where the semantics of any of the `GremlinDriverIntegrateTest` tests changed so it seems you accomplished this fix without breaking behavioral changes in the driver...that's nice.
6. Maybe I missed it but was there a test in Gremlin Server to validate any of this change - perhaps it was already better tested by way of your `SimpleWebSocketServer` tests?
7. I assume you will polish up the commit history a bit on merge and squash things down to a few (or one) commits?",code_debt,low_quality_code
flink,879,comment,118008845,"Also maybe it is completely unnecessary to automatically attach a source timestamp if we don't have any windowing operators. 
One other thing that came into my mind: in order to keep ""deterministic"" results after failure we should persist data with the timestamp attached at the sources. Are we planning to do this? I guess this question goes hand in hand with the automatic source level backup even without kafka. I just wanted to bring it up.",code_debt,complex_code
superset,6663,comment,453598576,"The issue I had with RAT is it wouldn't deal with py and js/jsx files in a way that our linters liked, and there was lots of files to touchup... rodent is a very small app (haven't pushed it to pypi yet), but could become an alternative to RAT.
Also it wasn't aware of my .gitignore, but I see you took care of this here with a rat-ignore...",code_debt,low_quality_code
beam,13592,comment,749735258,"That should not be a problem as in most cases, processing a single `element` in this case means emitting quite many elements downstream. The cost of the structuralValue should be pretty much amortized I would say. We could do something to calculate it only once per element?
I don't think we can use a reference either, because the UnboundedSource is Serializable and any runner is free to clone it (which is what DirectRunner does, afaik). The best solution would seem to be to mark each initially split (via @SplitRestriction) restriction with unique ID and then transfer this ID to all residual restrictions. There should be always be at most one ""active"" (either currently being processed or having non-null residual) restriction, so we could use that for identifying the reader without referencing Source (which might be problematic, as implementing hashCode and/or equals for UnboundedSource will not be a common practice, I'm afraid).",code_debt,low_quality_code
spark,13967,comment,229733872,"I added a `null` map testcase and remove redundant implementation.
For the removal from `sql/functions.scala`, there is no problem to remove that. But, could you check that again?",code_debt,complex_code
carbondata,2836,comment,431731434,so we should not change this value without better knowledge about it.,code_debt,low_quality_code
openwhisk,1063,comment,243067533,"Adressed issues in comments and added 3 more commits. They should be squashed to their respective ""parent"" commit.
1. ""Adding withClue statements to action tests, dropping unneeded tests"" -> ""Rewrite CLIActionTests in Scala, refactor withActivation helper""
2. ""Formatting nit in TestUtils"" -> ""Refactor runCmd, remove obsolete helperclasses""
3. ""Renaming some ruletests, adding comments"" -> ""Rewriting CLIRuleTests in Scala, adding a new testhelper""",code_debt,dead_code
beam,1230,comment,260433002,"Added a test for the builders for Read, and cleaned up the code a bit for Write (since topic should never be null there).",code_debt,low_quality_code
kafka,8883,summary,0,KAFKA-9896: fix flaky StandbyTaskEOSIntegrationTest,test_debt,flaky_test
spark,13078,summary,0,[MINOR] Fix Typos,documentation_debt,low_quality_documentation
nifi-minifi-cpp,924,comment,732347261,I'm pretty sure the answer is yes: we need to include all transitive dependencies' licenses and notices. Otherwise it would be trivial to circumvent these requirements by creating a wrapper project that depends on the software and exposes all of its components.,build_debt,under-declared_dependencies
parquet-mr,150,comment,90990009,"could you also update this doc with the new API? https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md
It could serve as a tutorial for users to use it",documentation_debt,outdated_documentation
camel,3381,comment,561470632,"Thanks for keep working on this.
Thanks so what I think is really taking people with some surprise is that it registers Camel routes into the **same single camel context** from any bundles. The point of Apache Karaf was to be like an app server where each bundle is isolated. So this goes against this practice. This should be documented much much more clearly. And also it lacks features with the ease of use how to configure camel context itself (you end up with its defaults) and how would people do dependency injections for beans etc. 
Also it should be renamed to `camel-osgi-activator`, and moved to components (as its not a core piece, eg not used by other osgi like osgi blueprint which is the main osgi support in Camel).",documentation_debt,low_quality_documentation
flink,592,comment,92341849,"Looks good.
I would expect a change in the documentation when you add support for a new feature ;)",documentation_debt,outdated_documentation
pulsar,9751,summary,0,[Issue 9725][Transaction] - Fix deleteTransactionMarker memory leak,design_debt,non-optimal_design
flink,15051,description,0,"The current `KafkaRecordDesrializer` has the following problems:
- Missing an `open()` method with context for serialization and deserialization.
The purpose of the change is to fix the above issues. 
- Renamed `KafkaRecordDeserializer` to `KafkaRecordDeserializationSchema` to follow the naming convention.
- Added the method `getUserCodeClassLoader()` to `SourceReaderContext` so the `SourceReader` implementation can construct the `SerializationDeserializationContext`.
- Added methods `valueOnly(...)` and `open(..)` in the `KafkaRecordDeserializationSchema` interface to enable the reuse of the `DeserializationSchema` and `KafkaDeserializationSchema`.
- Added the method `setValueOnlyDeserializer(...)` in `KafkaSourceBuilder` class to make it easy to set value-only deserializer.
- Added unit tests in `TestingDeserializationContext` and `KafkaRecordDeserializationSchemaTest`.
- Added tests in `KafkaRecordDeserializationSchemaTest` to verify the changes made in KafkaRecordDeserializationSchema",code_debt,low_quality_code
spark,16152,description,0,"Our existing withColumn for adding metadata can simply use the existing public withColumn API.
The existing test cases cover it.",code_debt,low_quality_code
ambari,2707,description,0,"Improve `KerberosDescriptorResourceProvider`:
 * Return HTTP 409 if trying to create duplicate `kerberos_descriptor` instead of HTTP 500 with ugly stack trace
 * Clarify message for incomplete request
 * Clean up the unit test
 * Minor clean-up in `KerberosDescriptorResourceProvider`
https://issues.apache.org/jira/browse/AMBARI-25025
Added test case in unit test.
Tested manually:",code_debt,low_quality_code
flink,13571,description,0,"# What is the purpose of the change
Currently the error thrown from `runAsync()` method will be swallowed because Flink didn't handle all throwables with `AkkaRpcActor`. Here is a temporary fix for such cases in `YarnResourceManager`.
* Use try-catch to wrap the runnable that was invoked in `runAsync()` method, and reuse the `FatalErrorHandler` to handle the error.
* Add a new unit test",design_debt,non-optimal_design
spark,22094,description,0,"With code changes in https://github.com/apache/spark/pull/21847 , Spark can write out to Avro file as per user provided output schema.
To make it more robust and user friendly, we should validate the Avro schema before tasks launched.
Also we should support output logical decimal type as BYTES (By default we output as FIXED)
Unit test",design_debt,non-optimal_design
drill,819,review,113834302,Seems to be no test for the Decimal precision mapping. That may be why the issue mentioned in the code slipped through: there is no unit test to catch it...,test_debt,lack_of_tests
geode-native,615,review,443683084,"I tend to always work in an IDE that supports project view (Visual Studio, Xcode), wherein I already know I'm working with a test project. Hence the Test suffix is a bit redundant. Also, we already have 4 tests in the new framework that don't use the Test suffix.",test_debt,expensive_tests
daffodil,407,review,470752272,I'd also like to see a test for leading space,test_debt,low_coverage
openwhisk,484,review,65249201,"I was thinking for the good test, if somehow there could be leaking across connections.",test_debt,low_coverage
trafficserver,326,description,0,I fix one spelling mistake,documentation_debt,low_quality_documentation
incubator-mxnet,13145,description,0,"Add documentation on GPU performance on Quantization example so end user knows that GPU performance is expected to be slower than CPU
Fixes https://github.com/apache/incubator-mxnet/issues/10897
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here
@ThomasDelteil @reminisce 
@mxnet-label-bot [pr-awaiting-review]",test_debt,lack_of_tests
arrow,7507,review,562065051,"It doesn't seem necessary to put this in `RecordBatchStreamReader`, you can move it to the implementation class.",architecture_debt,violation_of_modularity
openwhisk-wskdeploy,103,description,0,"This PR is for code review, **this should not be merged yet.** It still needs to be debugged and the actual deployment of entities is not completed yet.  Also, it **may be missing some files** since I tore everything down and put it back together.  I'll add back the missing files (like report.go and version.go).  Sorry about that :-)
The goal here is to refactor the code to modularize it better so we can add ""big"" features.  
The major refactors are:
- refactor utils.go into separate classes
- refactor manifest and deployment functions into parsers and readers
- refactor classes into packages that are more descriptive of their functions, e.g. parsers, utils, deployers
The features added in this code:
- support for multiple packages in the service deployer.  This is mainly targeted towards multiple packages in the deployment.yaml, not the manifest.yaml
- add parameter and annotation binding from the deployment file into the deployment plan.
- add support for sequence notation per use case` openstack.`
- add placeholder for dependency specification in a package.",architecture_debt,violation_of_modularity
