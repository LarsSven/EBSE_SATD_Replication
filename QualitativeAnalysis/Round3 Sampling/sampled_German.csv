project,pull_number,pull_type,id,text,classification,indicator
nifi,4767,review,561227339,Perhaps renaming this variable to something like `DEFAULT_STORE_TYPE` would clarify the reason for declaring it here as opposed to just using the enum value where necessary.,code_debt,low_quality_code
incubator-doris,3587,review,425210765,The second parameter is better to be a complete sentence.,code_debt,low_quality_code
cloudstack,768,review,39469736,Why are deprecated methods being used?,code_debt,low_quality_code
accumulo,1995,review,607994959,I think these error message would be more useful if they included the full path of the client props file that was used.,code_debt,low_quality_code
netbeans-website,473,review,430013474,Mmm.... infra will complain if we use jenkins builds as an update center. Also note this is extremely slow...,code_debt,slow_algorithm
kafka,7799,review,364476808,"We only needed it here because the we were trying to ensure we had a chance to send the Abort before going into poll. After thinking about it, it seemed a little simpler to move the `hasAbortableError` check to `maybeSendAndPollTransactionalRequest`. Then we no longer need this change.",code_debt,low_quality_code
spark,7181,review,33755269,how is this less code? It is just a bad idea to create unnecessary state. Just move both tables into two fields in Hex object.,code_debt,complex_code
spark,20787,review,175982564,"Hm, this should have been caught by Scala linter because we follow Java style comment. See ""Code documentation style"" in http://spark.apache.org/contributing.html",code_debt,low_quality_code
helix,1663,review,588774064,"This test should probably be named as TestZkClientAsyncFailureMetric since your new code is only adding metrics. The aync function should be already covered in existing tests. 
With that said, have you checked whether it's possible to only add metrics testing to existing tests by mocking the results? That would save us one additional test file. If there is no available code path or too many efforts involved, I'm fine with having a separate test too. But please check the zkclient and monitor test.",code_debt,low_quality_code
spark,3651,review,21697554,"OK, in the name of keeping it simple I might not touch this this time. Since this occurs 2 places only, it doesn't save much.",code_debt,low_quality_code
flink,9082,review,340395215,throw `IllegalArgumentException` or `UnsupportedOperatorException` makes more sense to me.,code_debt,low_quality_code
kafka,6363,review,278752888,This is getting a bit confusing -- this seems the same as `currentWorkerAssignment`. What mutates that causes this to be different?,code_debt,low_quality_code
flink,12268,review,430016877,"hmm, not sure. Maybe this happens because the JVM writes that to the STDERR. That's annoying :/
This is probably fine then; I was initially worried we might get so long filenames that you wouldn't be able to extract the logs on Windows.",code_debt,low_quality_code
iceberg,2167,review,571340787,"Nit: it would be nice to keep the expression on one line, wrapping after `checkArgument(`",code_debt,low_quality_code
spark,19460,review,143571046,I get declaring a constant though it doesn't just pertain to byte arrays. I couldn't find a good place for it. I don't know if its reused so much that it needs this,code_debt,low_quality_code
camel,3171,review,324231977,Maybe this at debug. Too noisy.,code_debt,low_quality_code
incubator-pinot,2068,review,149484084,"todo before release: 
Document this cp as what's it's doing is a little bit obscure",code_debt,low_quality_code
orc,570,review,528911518,Could you add `@deprecated` to give a proper warning?,code_debt,low_quality_code
spark,21200,review,185664585,"It's not okay catch and ignore all throwables. E.g. OOMs should NEVER be ignored as it leads absolutely unexpected situations.
At best, you can catch `NonFatal(ex)` and ignore those (only after logging as a warning). For other throwables, log as a warning, and rethrow.",code_debt,low_quality_code
attic-apex-core,479,review,108591304,"This method doesn't always create a new DeployRequest - can we name it appropriately? Say ""addOrModifyDeployRequest"" or ""getDeployRequestForContainerId"". By the way, would it be an error in some cases to find an existing DeployRequest for a given containerId? For example when you are calling this from StreamingContainerManager.scheduleContainerRestart(), wouldn't it be an error to find an existing request? If true, we should add code to detect such errors. The caller passes a parameter to indicate that only new request is expected to be created and so on...",code_debt,low_quality_code
spark,28952,review,447482241,"let's put `10` as a parameter, to make this method a bit more general.",code_debt,low_quality_code
storm,215,review,15836939,"We should remove Config.UI_ACTIONS_ENABLED, or at least deprecate it.",code_debt,dead_code
flink,12042,review,422441179,"in fact, the table qualified name may be long. in addition, we may need to add catalog name and database name to distinguish between tables with the same name.",code_debt,low_quality_code
qpid-dispatch,1097,review,608990009,"Can we drop ""Adaptor"" from this class name?  When I first read this I was assuming it was a set of tests that pertained only to the HTTP1 Adaptor.
Also should this class be a subclass of ""object""?",code_debt,low_quality_code
kafka,2405,review,97622033,Feels like this collection is redundant. You can get the name from `InternalTopicConfig` perhaps?,code_debt,complex_code
daffodil,408,review,488125346,"I think it would be more clear to just do string matching. E.g.
The regex just adds extra complexity. Same with setProperty.",code_debt,complex_code
kafka,6521,review,273258127,"Could you add a check to verify that the returned iterator is empty. Something along the lines of `assertThat(iterator.hasNext(), is(false))`?
Could you also add a test for a range query where the start key is equal to the end key? Such a unit test ensures correct behaviour for this special case.    
nit: I would rename the test to `shouldReturnEmptyIteratorForRangeQueryWithInvalidKeyRange`. Correct me, if I am wrong, but I think the empty iterator and the invalid key range are the points here, not the negative starting key. I would even change the range from (-1, 1) to (5, 3). It took me a bit to understand why (-1, 1) is an invalid range. 
These comments apply also to the unit tests below.",code_debt,low_quality_code
kafka,3575,review,129764370,Nit: should we include bytes like we did `ms` for the other case?,code_debt,low_quality_code
spark,7842,review,43212805,The curly braces are not needed. The same for the following `case` statement.,code_debt,complex_code
incubator-mxnet,10900,review,188720460,I would suggest to simply use `mx.image.imread()` and then call `.asnumpy()` before plotting. There shouldn't be a need of a dependency on `cv2` that way.,code_debt,complex_code
spark,15009,review,106230108,nit: move to previous line,code_debt,low_quality_code
drill,1251,review,187210052,I just removed this detail since we are saying these libraries are deprecated and we no longer want to use them.,code_debt,dead_code
airflow,12383,review,524536973,"I don't think we really need this -- with multiple webserver worker processes any refresh would lead to confusing state (some on new list, some on old, and no way to tell which is which) unless we have some way to make _all_ workers perform it.",code_debt,complex_code
kafka,5379,review,204563539,"Personally, I would get rid of this and use `extensionValue` and `extensionNames`. Otherwise, as @rondagostino said below, we should remove `extensionValue`.",code_debt,low_quality_code
flink,4851,review,145917513,"ad 1. - that's why I have used `StateAssignmentOperation.operatorSubtaskStateFrom`, to share this logic. I didn't want to use all of the `StateAssignmentOperation`, because it's constructor is really annoying to fulfil.
ad 2. - that's a valid concern, however writing ITCases for this might be also an overkill. And wouldn't it be to necessary to test it against every state backend, to make sure that there are no introduced quirks during (de)serialisation?",code_debt,low_quality_code
daffodil,291,review,348540375,"These ``_fileOS == null`` checks are kindof ugly. Thoughts on making this so this class just extends OutputStream instead of ByteArrayOutputStream, and then make it a wrapper for an output stream, which might change from ByteArrayOutputStream to FileOutputStream? I'm thinking something like:
Makes it so all the overrirde functions are basically just stream.whatever(), except for write which just calls the switch thing. Another benefit is once the switch happens, the old ByteArrayOutputStream can be garbage collected, whereas before it couldn't.",code_debt,low_quality_code
cloudstack,2578,review,225156473,"This message looks a little bit misleading, or am I mistaken?",code_debt,low_quality_code
spark,10993,review,51332430,"AFAICT, this is never used, so I removed it.",code_debt,dead_code
incubator-mxnet,12918,review,227170725,"Can you use something like this instead?
Otherwise it throws an uncaught exception on CPU only machines.",code_debt,low_quality_code
kafka,6536,review,275588869,"Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true; it's guaranteed that there is a headers object.
Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.",code_debt,complex_code
spark,4363,review,24212525,nit: indent two spaces,code_debt,low_quality_code
spark,6959,review,33198256,"AFAIK we don't have a style guide for Java code, but I think we should put spaces after the casts.",code_debt,low_quality_code
arrow,9356,review,580411319,This cast is unnecessary since the `Sum` method already returns a `double`.,code_debt,complex_code
beam,3936,review,142818337,This will slow down this performance critical code.,code_debt,slow_algorithm
beam,8270,review,274875921,Seems it'd be worth factoring this out into a context (including the expansion service jar test above).,code_debt,low_quality_code
drill,520,review,106658488,"This ` <exclude>javax/**</exclude>` exludes validation-api as well. Therefore it should be deleted. 
To avoid including unnecessary libraries I decided to add:
`               <exclude>javax/activation/**</exclude>`
`               <exclude>javax/annotation-api/**</exclude>`
`               <exclude>javax/inject/**</exclude>`
`               <exclude>javax/servlet-api/**</exclude>`
`               <exclude>javax/json/**</exclude>`
`               <exclude>javax/ws/**</exclude>`",code_debt,low_quality_code
spark,23181,review,237952522,nit: `clone()`,code_debt,low_quality_code
flink,14839,review,586411292,Why `rethrow` as `RuntimeException`?,code_debt,low_quality_code
druid,2806,review,59071300,the default should be made a constant somewhere instead of being defined in multiple places,code_debt,low_quality_code
cassandra,730,review,485793488,"this is `Class.toString` which is harder to read for arrays, so did this so I could read the output without googling =D.
primitives and objects will normally have valid string names, but arrays will be like `J[` which require knowing what that means or googling it.",code_debt,low_quality_code
spark,5400,review,27923456,nit: spaces around `{`,code_debt,low_quality_code
airflow,2372,review,140193220,This is redundant.,code_debt,complex_code
nifi,914,review,75858615,"Might be able to create a utility method in the abstract class like `byte[] getRow(String row, String encoding)` since it looks PutHBaseCell and PutHBaseJson both need the same logic",code_debt,low_quality_code
carbondata,1321,review,138830621,"`null != sortScope` can be removed, it is checked inside `CarbonUtil.isValidSortOption`",code_debt,complex_code
arrow,467,review,109291184,"There's https://github.com/apache/arrow/blob/master/cpp/cmake_modules/FindPythonLibsNew.cmake, is one or the other made redundant by the other?",code_debt,complex_code
druid,50,review,2541441," Also, there's no need for the String.format(), preconditions will do %s interpolation for you (I realize the String.format()was there before, but let's take this chance to get rid of it).",code_debt,low_quality_code
phoenix,933,review,510479056,nit: 0L,code_debt,low_quality_code
airflow,11132,review,495856534,"Yea default values can be hardcoded.
And also agree that showing a precise message for the used config will be better",code_debt,low_quality_code
cloudstack,2058,review,139283005,"Fix indentations if you want to remove try-catch, also add a test?",code_debt,low_quality_code
iceberg,1793,review,553700047,Same with these other methods. Should these be primitives?,code_debt,low_quality_code
flink,13135,review,475413540,"We are no longer referring to field names here. Honestly I am not sure if the point makes sense with the `KeySelector` only. The way I read this point is tells you can use field names, which we discourage nowadays.
I'd rather remove the point whatsoever.",code_debt,complex_code
accumulo,1330,review,316892853,"Regarding this naming convention, should these be `*.bind.address` or `*.address.bind`? What do you think? `master.bind.address` reads more naturally, but `master.address.bind` supports logically grouping address-related configs in a configuration hierarchy, which can be better for parsing, and more easily incorporate future changes.
An example of a future change which could benefit from the `*.address.bind` form could include work that address an explicit public advertisement address, in the case of the bind address not being publicly reachable (in the case of more complicated networking setups, such as those on some cloud services' infrastructure):
Example:
I'm leaning towards the `<service>.address.<addressType>` naming convention, rather than the `<service>.<addressType>.address` convention that you currently have here, but am open to discussion... because naming is hard. What do you think?",code_debt,low_quality_code
spark,15237,review,90548133,"this seems a little more complicated than is really necessary for the what you're doing here.  couldn't you achieve the same thing by leaving the original code and changing the one line above the original to:
not exactly the same -- it also allows whitespace around the scheduling mode, but maybe a good thing?",code_debt,complex_code
lucene-solr,2229,review,562524364,"Would it be acceptable to reformat the code when such a demand appears, or after I'm finished with these improvements? These ""some people"" might never need to look into Hunspell code.",code_debt,low_quality_code
incubator-mxnet,15277,review,296085060,nit: add this blank line back in.,code_debt,low_quality_code
hbase,809,review,344314816,"nit: Always better to use parameterized logging for performance [1]. Here info is the default, so probably doesn't matter as much. 
[1] https://logging.apache.org/log4j/2.x/performance.html",code_debt,slow_algorithm
kafka,5885,review,236424422,Can we name this `transactionGenerator` for simplicity?,code_debt,low_quality_code
druid,10070,review,445155089,minor nit:mark return type as nullable,code_debt,low_quality_code
spark,4588,review,26089697,super nit: order. swap with import above.,code_debt,low_quality_code
spark,22960,review,231413853,I think we can just get rid of it. I can't imagine both functions are specifically broken alone in `selectExpr`.,code_debt,complex_code
spark,448,review,11813558,"Yeah, you are going to end up getting the same thing.  I'd say we drop this one and leave the other.  Right now it probably doesn't matter, but the other one is lazy and gives the optimizer a chance to possibly improve things before actually executing.",code_debt,low_quality_code
hudi,2127,review,499304054,we can reuse some code in this file by pulling the common structure into a helper function ?,code_debt,low_quality_code
flink,8446,review,285160885,"I think you are right that in order to break up the cyclic dependency we would need to decouple the `SchedulingResultPartition` from the `SchedulingExecutionVertex` (via the `ExecutionVertexID` for example).
I'm wondering how bad this cyclic dependency and the need for mutable state is, though. From a user's perspective, the existing interfaces are a bit easier and more convenient to use. On the down side, it makes the implementation a bit harder and harder to test in isolation. However, do we want to test the `Scheduling*` implementations in isolation? Moreover, you always have this problem in graph structures which have bidirectional edges or loops.",build_debt,under-declared_dependencies
spark,24807,comment,500879466,"I think the SparkSession.close() behavior is on purpose, and that's a coherent behavior (i.e. just don't shut anything down until you're done, and then everything shuts down). What's not consistent with that is maintaining some state in the session that can't be cleared. 
I think the ways forward are probably:
- A new lifecycle method like `clear()`? more user burden but at least provides _some_ means of doing cleanup without changing `close()`
- Figure out how to automatically dispose of those resources or not hold them
- Just change the behavior of session's `close()` to not shut down the context. Behavior change, yes, but perhaps less surprising than anything.
Eh, do people like @cloud-fan or @gatorsmile or @HyukjinKwon or @dongjoon-hyun have thoughts on this? I feel like reference counting is going to end in tears here eventually, but, it's not crazy",design_debt,non-optimal_design
spark,4593,comment,75540678,"I have the same concern as @dbtsai in his comment. Most consumers of this API will already be caching their dataset before the learning phase. Without user care, this will introduce effectively double caching (in terms of data size of cached RDDs) and will cause many jobs to fail after upgrading by exceeding available heap for RDD cache. Furthermore, we are making assumptions about how to cache -- in-memory only in this case. Should we parameterise this? Perhaps that will help send the message in the API that there is caching also done before learning. (FWIW, in-memory is definitely the right default choice here.)
See email thread on dev for my specific encountering of this bug: 
http://mail-archives.apache.org/mod_mbox/spark-dev/201502.mbox/%3CCAH5MZvMBjqOST-9Nr9k1z1rUODfSiczr_fV9kwqDFqAMNLC2Zw%40mail.gmail.com%3E",design_debt,non-optimal_design
spark,15148,comment,259376702,"I tend to agree that the terminology used here is a little confusing, and doesn't seem to match up with the ""general"" terminology (I use that term loosely however).
In my dealings with LSH, I too have tended to come across the version that @sethah mentions (and @karlhigley's package, and others such as https://github.com/marufaytekin/lsh-spark, implement). that is, each input vector is hashed into `L` ""tables"" of hash signatures of ""length"" or ""dimension"" `d`. Each hash signature is created by concatenating the result of applying `d` ""hash functions"".
I agree what's effectively implemented here is `L = outputDim` and `d=1`. What I find a bit troubling is that it is done ""implicitly"", as part of the `hashDistance` function. Without knowing that is what is happening, it is not clear to a new user - coming from other common LSH implementations - that `outputDim` is not the ""number of hash functions"" or ""length of the hash signatures"" but actually the ""number of hash tables"".
In terms of `transform` - I disagree somewhat that the main use case is ""dimensionality reduction"". Perhaps there are common examples of using the hash signatures as a lower-dim representation as a feature in some model (e.g. in a similar way to say a PCA transform), but I haven't seen that. In my view, the real use case is the approximate nearest neighbour search.
I'll give a concrete example for the `transform` output. Let's say I want to export recommendation model factor vectors (from ALS), or Word2Vec vectors, etc, to a real-time scoring system. I have many items, so I'd like to use LSH to make my scoring feasible. I do this by effectively doing a real-time version of OR-amplification. I store the hash tables (`L` tables of `d` hash signatures) with my vectors. When doing ""similar items"" for a given item, I retrieve the hash sigs of the query item, and use these to filter down the candidate item set for my scoring. This is in fact something I'm working on in a demo project currently. So if we will support the OR/AND combo, then it will be very important to output the full `L x d` set of hash sigs in `transform`.
My recommendation is: 
1. future proof the API by returning `Array[Vector]` in `transform` (as mentioned above by others);
2. we need to update the docs / user guide to make it really clear what the implementation is doing;
3. I think we need to make it clear that the implied `d` value here is `1` - we can mention that AND amplification will be implemented later and perhaps even link to a JIRA.
4. rename `outputDim` to something like `numHashTables`.
5. when we add AND-amp, we can add the parameter `hashSignatureLength` or `numHashFunctions`.
6. make as much private as possible to avoid being stuck with any implementation detail in future releases (e.g. I also don't see why `randUnitVectors` or `randCoefficients` needs to be public).
One issue I have is that currently we would output a `1 x L` set of hash values. But it actually should be `L x 1` i.e. a set of signatures of length `1`. I guess we can leave it as is, but document what the output actually is.
I believe we should support OR/AND in future. If so, then to me many things need to change - `hashFunction`, `hashDistance` etc will need to be refactored. Most of the implementation is private/protected so I think it will be ok. Let's just ensure we're not left with an API that we can't change in future. Setting `L` and `d=1` must then yield the same result as current impl to avoid a behavior change (I guess this will be ok since current default for `L` is `1`, and we can make the default for `d` when added also `1`).
Finally, my understanding was results from some performance testing would be posted. I don't believe we've seen this yet.",design_debt,non-optimal_design
fineract,1458,comment,718052221,"I just feel increasing timeouts won't cut it. Doesn't sound like a real solution. Unfortunately, I don't understand much about schedular jobs to have a proper say in this but I think we have some performance issues. Need to figure out how to deal with that.",design_debt,non-optimal_design
flink,938,comment,125751865,"Out of curiosity: why was it failing sometimes on Travis and not locally? And how did you discover this? From the program level logs?
Another thing that came to my mind: in the long run, do we need a more complex way of configuring the retry policy? In my understanding, the number of retries is fixed. I can see an issue for very long run programs, which fail once in a while, but operate normally most of the time -- then at some point they will fail because of the fixed number of retries.",design_debt,non-optimal_design
superset,5445,comment,457472346,"This PR fell in a crack. Let's revive it. Seems like the long term solution is not using WTForms at all as we rip out more of the MVC and FAB scaffolding over time.
So I get the `NULL` vs `''` problem with unique constraints not applying properly, but other than that, does it affect users directly? I'm guessing it may affect the filtering functionality in the CRUD list view UI?",design_debt,non-optimal_design
incubator-mxnet,3781,comment,260116824,@sxjscience Yeah this will cut out a lot of code in our framework that were just there to redundantly track tensor dims for the purposes of feeding them to Reshape layer. I'm very excited about this. A small increase in (optional) complexity in Reshape layer pays off with a large decrease in complexity in our framework code.,design_debt,non-optimal_design
parquet-mr,808,comment,672774756,"@shangxinli,
If we will agree on the extending of the schema with metadata is a good idea and as you said the serialization/deserialization is also required we need to change the format first. The schema objects in parquet-mr are only exist in the parquet-mr runtime. To have them serialized we need to convert this object structure to the thrift object structure defined in the format. If we don't have the new metatdata fields in the format we cannot serialize/deserialize them. So it is a much bigger topic. Also, I'd like to see this feature separated from the encryption as it would be general approach for storing metadata in the schema. Meanwhile, I am not convinced that we need to have such extension.
About the namespace prefix etc. I don't agree this is not user friendly. That's why I've suggested to implement a helper API so the user doesn't need to deal with the conf keys (and values) directly. 
@ggershinsky,
I don't agree we cannot have a meeting about this topic in terms of transparency. What we have to do is to document here about what we have discussed and what are the conclusions. Meanwhile, I am not sure if a meeting would help but I am happy to participate if anyone thinks otherwise.
Also, if we think we are getting stuck with this issue I would suggest involving other members of the community. Maybe draw their attention on the dev list about this PR or bring up the topic on the next parquet sync.",design_debt,non-optimal_design
activemq-artemis,2012,comment,380818801,"As noted my biggest concern is that message refs need to be as light as humanly possible as they’re all in memory and affects greatly the scaling.
I would personally prefer the refactor if needed, than take this hit. 
Especially as this is only needed by someone wanting to use this in a plugin. Which means everyone else has to suffer",design_debt,non-optimal_design
spark,1541,comment,49855865,"Instead of a ConcurrentHashMap, we should actually move it to a disk backed Map - the cleanup of this datastructure is painful - which it can become extremely large; particularly for iterative algo's.
Fortunately, most cases, we just need the last few entries - and so LRU scheme by most disk backed map's work beautifully.
We have been using mapdb for this in MapOutputTrackerWorker  - and it has worked beautifully.
@rxin might be particularly interested since he is looking into reduce memory footprint of spark
CC @mateiz - this is what I had mentioned about earlier.",design_debt,non-optimal_design
hawq,1321,summary,0,"Rewrite the tuple construct and consume work flow to improve the read and write performance in pluggable storage framework. To be specific, it uses virtual tuple in tuple table slot instead of heap tuple between storage and executor to avoid tuple data copy during query processing.
@huor @jiny2",code_debt,slow_algorithm
trafficserver,347,summary,0,TS-4038: Redundant `isdigit(b)` in `LogFormat::parse_escape_string,code_debt,complex_code
parquet-mr,227,summary,0,PARQUET-318: Remove unnecessary object mapper,code_debt,complex_code
tvm,1905,summary,0,[Relay][Test] remove redundant test cases in test_op_level4.py,code_debt,dead_code
spark,9682,summary,0,[SPARK-11719] [ML] Remove duplicate DecisionTreeExample under examples/ml,code_debt,duplicated_code
kafka,2937,summary,0,Also add tests and a few clean-ups.,code_debt,low_quality_code
spark,25048,review,300427664,"Thank you for investigating this, @HeartSaVioR .
The original logic looks not safe. Do you think if there is any other better way?",requirement_debt,non-functional_requirements_not_fully_satisfied
flink,14084,review,554882968,"Just not supported yet, the type TIME_STAMP_WITH_LOCAL_TIME_ZONE is rarely used. But I think we should both support them in this version.",requirement_debt,requirement_partially_implemented
incubator-doris,4925,review,528201502,I think `DEL_NOT_SATISFIED` is more safe. Or you'd better change DCHECK to CHECK,requirement_debt,non-functional_requirements_not_fully_satisfied
spark,31905,comment,811106021,"Regarding your questions:
Can you provide a common streaming use case where a non-event-base processing of metrics is useful? The `Observation` proposes a ""get me the metrics"" API that hides listeners and multi-threading. In streaming context, there are no finite metrics.
Can you elaborate on this, please? Do you mean an async `get`? Can you provide some pseudo-code on how to use the metrics async.
It is not thread-safe in a scenario where the action on the observed dataset is called in a different thread than where the metrics are retrieved. I'd say such a multi-thread driver process is not the general use case for batch processing. Even if, the proposed `Observation` API aims at hiding multi-threading complexity so that users can use `Dataset.observe` in single-threaded scenarios. Users that are experience with multi-threading have no problem in using the existing listeners approach. They are not the target user of this extension.
Agreed.",requirement_debt,non-functional_requirements_not_fully_satisfied
tomee,713,comment,734780620,"I gave it a try at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 but I am quite unsure if this is what we were talking about. Consequently, I dropped some FIXME/TODO questions.
For this reason, I didn't open a related (new) PR yet. Maybe you can have a look at it?",requirement_debt,requirement_partially_implemented
flink,10388,comment,561631987,"Thank for the PR @HuangZhenQiu , the PR overall looks good, can you also add the cases for Blink planner ? Even though the code path looks the same, we still need the tests to make sure the logic works correctly.",test_debt,lack_of_tests
cloudstack,1572,comment,253562333,"1. the current set of integration tests doesnt test the new functionality added in this PR. Its merely a check to see nothing else is broken.
2. when did we freeze? I skimmed through the mails didnt see anything about master frozen. I do not see any blocker defects for 4.10.0.0. The latest understanding I have is anybody can merge with required code LGTMs and a BVT run.
3. Which smoke tests are broken? Is it you environment issue you are talking about? If not, are these new or did we release 4.9.0 with these broken tests/features?",test_debt,lack_of_tests
incubator-mxnet,17808,comment,605590914,"We can get rid of WIN_GPU_MKLDNN tests altogether but that still leaves us with the flakiness of WIN_GPU as can be seen in these builds
For roughly same code of this PR & same windows AMI, below are the results so far
WIN_GPU | WIN_GPU_MKLDNN | Build Number | Link
-- | -- | -- | -- |
✖︎| ✔︎|15 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/15/pipeline
✖︎| ✔︎| 14 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/14/pipeline
✔︎|✔︎| 12 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/12/pipeline
✔︎| ✖︎| 13 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/13/pipeline
Ofcourse your tests on local have a different story to tell...",test_debt,flaky_test
kafka,6234,comment,462503916,"ping. it'd be good to get this in since it'll help with debugging tests that flake pretty consistently on jenkins (though we may also need some follow up to turn up logging output during tests, I think a bunch of modules don't even have logging turned on).",test_debt,flaky_test
geode,4189,comment,544634796,"CI is failing due to flaky test, for which I created ticket GEODE-7319, and proposed solution.",test_debt,flaky_test
incubator-mxnet,12768,comment,428218284,"See my comment in the issue, were you able to reproduce? Do you have some evidence to declare it as a flaky test?  Unless there's more data I would be against disabling this test with the information that we have so far to prevent lowering the quality bar.",test_debt,flaky_test
zookeeper,1614,comment,785912733,"using the link on the failed GitHub CI job (the one with label ""Details""), you can select ""Rerun Jobs"". But that would re-run all GitHub based CI jobs for this PR, and that potentially can cause more flaky tests. So I think this is good enough.",test_debt,flaky_test
spark,9297,review,46203981,"Also, is the only purpose of `sqlListener` to prevent multiple listeners from being created at the same time? If so, I think it would be better to use an AtomicBoolean so that we don't create another strong reference to a SQLListener, which might have a lot of internal state that could lead to memory leaks.",design_debt,non-optimal_design
spark,2907,review,19663363,Thanks for the much better solution. That works just fine.,design_debt,non-optimal_design
pulsar,43,review,80817924,"Here, to avoid creation of new ByteBuf we modify same DoubleByteBuf of the message with newly computed checksum.
However, while message creation if we see memory-leak then we create `SimpleLeakAwareByteBuf` or `AdvancedLeakAwareByteBuf` (based on `ResourceLeak Level`) instead   `DoubleByteBuf`. So, should we keep this check.",design_debt,non-optimal_design
beam,6981,review,233161421,"Regarding the operator as a whole and as discussed in the past, constraints are mostly inherited and would need to be addressed by refactoring the base. I'm not sure about the timing for this though, if it should wait until legacy runner goes away (there is still significant work for portable runner to become adequate replacement for Java pipelines).",design_debt,non-optimal_design
kafka,8105,review,379594069,When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.,design_debt,non-optimal_design
arrow,9626,review,587325457,"Does this mean that conversion could give the wrong results (in addition to being leaky)? If so, can you add a test showcasing that? (I believe you need the different chunks to be unequal...).",design_debt,non-optimal_design
activemq-artemis,2427,review,232558150,"It may have them from legacy someone put it there, but if you were designing a collections class, you'd design it in a fashion so it focussed just on the logic it needs to have. And any interaction needed is supplied generically, e.g. for your case, you;d need generic method: remove(Predicate<SimpleString> predicate). This is a collection class in reality so i apply the same rules of engagement..
If anything that field, the flag and the method check hasInternalProperties really should move up to CoreMessage, as its only used there. Doing that would mean you can still get your benefit as then the check stays in CoreMessage. 
This would really clean up TypedProperties to just having fields it really should only care for, keeping it clean.",design_debt,non-optimal_design
trafodion,644,review,73953757,Doesn't look like columnReference is used after this. Is this a memory leak?,design_debt,non-optimal_design
kafka,9318,review,495932581,"This check makes sure the code always stays in sync. However, if we can make great code reviews (like yours), this static check should be unnecessary :)
I will remove this painful check (to me also) in next commit.",design_debt,non-optimal_design
spark,9399,review,43590281,Let's add a comment?,documentation_debt,outdated_documentation
gobblin,2656,review,292076607,"The reason is I get rid of `this.dags.get(dagId);` and there's no reference to a `dag` associated with this `dagId`. So clean up has to happen before `remove`, or `dag` object won't be fetched. 
Will add comment to make it clear",documentation_debt,outdated_documentation
superset,8867,review,360617020,"It's not super clear here what this does or why you'd want to use this config element. I had to read a bit of code to understand it.
Then add a proper example with type annotation",documentation_debt,low_quality_documentation
kafka,386,review,43579299,"Rep @onurkaraman : `removeGroup` should always be guarded by the group lock inside the `GroupCoordinator`, while `getGroup` and `addGroup` are not since the group object is not available yet. I will make that more clear in the comments.
Rep @junrao : ack.",documentation_debt,low_quality_documentation
spark,20303,review,249798529,"I can answer that myself, the countdown stuff is useful to figure out if the computation has completed. Please add some doc here.",documentation_debt,outdated_documentation
calcite,782,review,209381477,"``
// below might change the type of the call (e.g. from nullable to non-nullable)
// however simplify(..) is allowed to return node with different type
// if the type should match, then `simplifyPreservingType` should be used",documentation_debt,low_quality_documentation
spark,29104,review,459576703,"nit: Should we add a comment here that lookupKey contains only a single column ? It will make understanding ""allNull"" easier.",documentation_debt,low_quality_documentation
couchdb,3015,review,488882226,"It was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl 
https://github.com/apache/couchdb-couch-replicator/commit/b48d7bdc49d107f33d96f08603006a1c9edc322f#diff-ba1cca81bdc216835256f72cc6a72fa5R374
Beyond that not sure about its origin. Technically it is possible, say, for a couch_replicator_auth plugin to link to a process that then exit normally so we end up in this part of the code and it won't be a worker, changes reader or any other known process in this module. So perhaps we should change to a comment indicating that instead of leaving the commented code in there?",documentation_debt,low_quality_documentation
nifi,4303,review,432322027,"This looks a little strange to my be retrospect, but the documentation says “To reduce the amount of time admins spend on authorization management, policies are inherited from parent resource to child resource”, so I think in practice the case where you cannot access the process group but you can access a resource within should not happen.",documentation_debt,low_quality_documentation
hive,2111,review,607667423,"Since it's a new interface method, can you add some javadoc please?",documentation_debt,outdated_documentation
hudi,2106,review,503737912,"please keep this javadoc to be just about the preCombine() method, without any context from this PR;s scenarios.",documentation_debt,low_quality_documentation
hadoop,575,review,264257746,"""if the stream is not in state Open"" would be clearer.",documentation_debt,low_quality_documentation
beam,6676,review,233138480,"Looks like doc was outdated, corrected it, batchSize controls elements used in batch INSERT SQL, i.e INSERT INTO t(a, b, c) VALUES(x1, y1, z1), ..., (xN, yN, yN); Batching can speed up loading data greatly, but not all SQL database may support it, so controlling batch size gives some lever for various SQL vendors.",documentation_debt,outdated_documentation
incubator-pinot,4387,review,300516190,Would be nice to add a comment on why TreeMap (ordering) is needed.,documentation_debt,low_quality_documentation
spark,1379,comment,71531266,@dbtsai I did batching for artificial neural networks and the performance improved ~5x https://github.com/apache/spark/pull/1290#issuecomment-70313952,code_debt,slow_algorithm
carbondata,4078,comment,787036665,"@VenuReddy2103
 I checked all places, but 14 places still keep the same with previous.
1) in 9 places, it uses only one event, fireEvent is ok. 
2) in 4 places, preEvent and postEvent are in the different code blocks.
3) in 1 place, there is a large code block between preEvent and postEvent.",code_debt,low_quality_code
spark,4533,comment,73925690,"Sounds correct. The subsequent tries do try in parallel. So, I suppose that's pretty good evidence it's parallelized. Unless anyone else speaks up I think this sentence can be removed.",code_debt,complex_code
arrow,4258,comment,490781968,"+1 lgtm
@liyafan82 , thanks for the contribution. I suggest you add a similar flag for isSet() to be optionally skipped in a separate jira to improve performance",code_debt,slow_algorithm
cxf,561,comment,498837564,"I'm wondering if it would make sense to insert a layer between AbstractFeature and the actual features like:  
and then each of the subclasses would look something like:
and a lot of the duplicate code in each of the subclasses goes away. 
Thoughts?",code_debt,duplicated_code
kafka,1908,comment,250894831,"LGTM. Merging to trunk and 0.10.1. Thanks @edoardocomar and @mimaison for the hard work (and patience) on this patch! I have some minor cleanups/improvements on the client and in testing, which I will submit in a follow-up PR.",code_debt,low_quality_code
zookeeper,1417,comment,665952705,"okay, so I'll have to fix a few checkstyle issues :)",code_debt,low_quality_code
iceberg,1669,comment,718305151,"In the case of a large amount of data, in order to ensure the performance of normal writing,
1. Should we use asynchronous Rewrite? (use AsyncWaitOperator)
2. Should you add a switch to control whether to  enable rewrite",code_debt,slow_algorithm
spark,28370,comment,628792196,Could you add the code to avoide the infinite retry loop on error & also checking thread interrupted incase something else swallows the thread interruption exception in the future?,code_debt,low_quality_code
skywalking,4987,comment,652475825,"local compile and package speed is so slow,because  need to download so much dependency jar file from maven depository",code_debt,slow_algorithm
tvm,4847,comment,584462639,CSourceModule with an empty string looks to me as well. @kumasento could you do that instead of creating a dummy llvm module? Thanks.,code_debt,low_quality_code
airflow,1488,comment,222606343,Ah. The hive-hook shouldn't be there. I think I must have included it from someone elses commit when doing a rebase. Will tidy it up and do a force push later today.,code_debt,low_quality_code
trafficserver,2300,comment,318132384,"It seems possible to collapse at least some of this class hierarchy, but I was told that there's future plans for making subclasses of the PrinterIface class. There just seems like a lot of class hierarchy (up to 4) for such a small amount of code, and I really don't think we should make provisioning now, for something that *might* be done later. If it becomes an issue later, it's easy to refactor (don't do premature ""abstractions"" :-).",code_debt,low_quality_code
spark,224,comment,38748839,"Hi Cheng Hao,  Thanks for implementing these functions.  I bet they will be much faster than using HiveUDFs as we do now!
Based on the number of comments on the test case refactoring, I think it would be easiest to try and commit just the LIKE and RLIKE with a few simple test cases, and then take on the rest in a separate PR.",code_debt,slow_algorithm
kafka,4001,comment,333581688,Am I missing something or you're using raw types in several places now? Raw types only exist for migration compatibility purposes and should never be used in new code IMO.,code_debt,low_quality_code
nifi,3260,summary,0,NIFI-5790 removed the last test as it's causing a race condition intermittently,test_debt,flaky_test
ignite,4970,summary,0,9769 test flakiness,test_debt,flaky_test
druid,1899,summary,0,Docs improved: more details about caching and memory for segments on historicals,documentation_debt,low_quality_documentation
tvm,1716,summary,0,[NNVM][KERAS] Fix keras model converter and improve tutorial,documentation_debt,low_quality_documentation
dubbo,2520,summary,0, typo: leastIndexs->leastIndexes,documentation_debt,low_quality_documentation
zookeeper,567,comment,405419213,Thanks for the review @anmolnar. Please take a look at unit tests when you get a chance. I have addressed the comments. I will also add documentation in a separate commit.,documentation_debt,low_quality_documentation
carbondata,3136,comment,468114436,"@tianyouyangying +1 for @qiuchenjian 's suggestion.
1. Please change all chinese to english.
2. Please change PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity'
@qiuchenjian The origin SQL in doc actually are two statements, we should seperate them to avoid ambiguity.
Thanks.",documentation_debt,low_quality_documentation
gobblin,104,comment,95396477,LGTM barring the typo,documentation_debt,low_quality_documentation
skywalking,5293,comment,673764279,Really detected one besides the mock. You need some fix on the doc this time. :),documentation_debt,low_quality_documentation
skywalking,5685,comment,711190648,"@wu-sheng  thx for check the structure this time.  If this version looks good, I will modify the doc and add the unit test later.",documentation_debt,outdated_documentation
druid,6230,description,0,"This patch includes the following bug fixes:
- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type
  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.
  This fixes a bug where, for example, grouping on doubles-cast-to-longs would
  fail to merge two doubles that should have been combined into the same long value.
- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns
  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long
  would fail to merge two strings that should have been combined.
- GroupByQuery: Cast numeric types to the expected output type before comparing them
  in compareDimsForLimitPushDown. This fixes #6123.
- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into
  the proper output type. This fixes an inconsistency between results that came
  from cache vs. not-cache: for example, Jackson sometimes deserializes integers
  as Integers and sometimes as Longs.
And the following code-cleanup changes, related to the fixes above:
- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,
  and converterFromTypeToType to make it easier to handle casting operations.
- TopN in general: Rename various ""dimName"" variables to ""dimValue"" where they
  actually represent dimension values. The old names were confusing.
* Remove unused imports.",build_debt,over-declared_dependencies
geode-native,56,description,0,"- section title is Interoperability of Language Classes and Types
- corrected namespaces (packages)
- removed duplicate table captions
@davebarnes97 @joeymcallister @dgkimura @mmartell @echobravopapa @PivotalSarge Can you please review these (mostly namespace) changes in the docs?",code_debt,duplicated_code
madlib,243,description,0,"JIRA: MADLIB-1206
This commit adds support for mini-batch based gradient descent for MLP.
If the input table contains a 2D matrix for independent variable,
minibatch is automatically used as the solver. Two minibatch specific
optimizers are also introduced: batch_size and n_epochs.
- batch_size is defaulted to min(200, buffer_size), where buffer_size is
  equal to the number of original input rows packed into a single row in
  the matrix.
- n_epochs is the number of times all the batches in a buffer are
  iterated over (default 1).
Other changes include:
- dependent variable in the minibatch solver is also a matrix now. It
  was initially a vector.
- Randomize the order of processing a batch within an epoch.
- MLP minibatch currently doesn't support weights param, an error is
  thrown now.
- Delete an unused type named mlp_step_result.
- Add unit tests for newly added functions in python file.
Co-authored-by: Rahul Iyer <riyer@apache.org>
Co-authored-by: Nikhil Kak <nkak@pivotal.io>
Closes #243",code_debt,low_quality_code
cloudstack,1295,description,0,"Added quotes to prevent syntax errors in weird situations.
Error seen in mgt server:
Cause:
Somehow a nic was missing.
After fix the script can handle this:
The other states are also reported fine:
While at it, I also removed the INTERFACES variable/constant as it was only used once and hardcoded the second time. Now both are hardcoded and easier to read.
This is the same as PR #1249 except it is against `4.7`.",code_debt,low_quality_code
superset,5946,description,0,"- Remove `lodash.throttle` from dependency since there is now `lodash`.
- Add `babel-plugin-lodash` that helps optimize bundle output by taking only necessary part from lodash instead of the entire bundle.
https://github.com/lodash/babel-plugin-lodash
- Replace `underscore` calls with `lodash` where applicable. 
@williaster @xtinec @conglei",code_debt,complex_code
flink,4426,description,0,"Remove unnecessary include-elasticsearch5 profile since its activation condition (usage of jdk8) is always true.
Run any mvn command in `flink` or `flink-connectors` and check that the reactor includes ES5.",code_debt,dead_code
spark,28163,description,0,"In the PR, I propose to optimise the `DateTimeUtils`.`rebaseJulianToGregorianMicros()` and `rebaseGregorianToJulianMicros()` functions, and make them faster by using pre-calculated rebasing tables. This approach allows to avoid expensive conversions via local timestamps. For example, the `America/Los_Angeles` time zone has just a few time points when difference between Proleptic Gregorian calendar and the hybrid calendar (Julian + Gregorian since 1582-10-15) is changed in the time interval 0001-01-01 .. 2100-01-01:
The difference in microseconds between Proleptic and hybrid calendars for any local timestamp in time intervals `[local timestamp(i), local timestamp(i+1))`, and for any microseconds in the time interval `[Gregorian micros(i), Gregorian micros(i+1))` is the same. In this way, we can rebase an input micros by following the steps:
1. Look at the table, and find the time interval where the micros falls to
2. Take the difference between 2 calendars for this time interval
3. Add the difference to the input micros. The result is rebased microseconds that has the same local timestamp representation.
Here are details of the implementation:
- Pre-calculated tables are stored to JSON files `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json` in the resource folder of `sql/catalyst`. The diffs and switch time points are stored as seconds, for example:
  The JSON files are generated by 2 tests in `RebaseDateTimeSuite` - `generate 'gregorian-julian-rebase-micros.json'` and `generate 'julian-gregorian-rebase-micros.json'`. Both tests are disabled by default. 
  The `switches` time points are ordered from old to recent timestamps. This condition is checked by the test `validate rebase records in JSON files` in `RebaseDateTimeSuite`. Also sizes of the `switches` and `diffs` arrays are the same (this is checked by the same test).
The hash maps store the switch time points and diffs in microseconds precision to avoid conversions from microseconds to seconds in the runtime.
- I moved the code related to days and microseconds rebasing to the separate object `RebaseDateTime` to do not pollute `DateTimeUtils`. Tests related to date-time rebasing are moved to `RebaseDateTimeSuite` for the same reason.
- I placed rebasing via local timestamp to separate methods that require zone id as the first parameter assuming that the caller has zone id already. This allows to void unnecessary retrieving the default time zone. The methods are marked as `private[sql]` because they are used in `RebaseDateTimeSuite` as reference implementation.
- Modified the `rebaseGregorianToJulianMicros()` and `rebaseJulianToGregorianMicros()` methods in `RebaseDateTime` to look up the rebase tables first of all. If hash maps don't contain rebasing info for the given time zone id, the methods falls back to the implementation via local timestamps. This allows to support time zones specified as zone offsets like '-08:00'.
To make timestamps rebasing faster:
- Saving timestamps to parquet files is ~ **x3.8 faster**
- Loading timestamps from parquet files is ~**x2.8 faster**.
- Loading timestamps by Vectorized reader ~**x4.6 faster**.
No
- Added the test `validate rebase records in JSON files` to `RebaseDateTimeSuite`. The test validates 2 json files from the resource folder - `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json`, and it checks per each time zone records that
  - the number of switch points is equal to the number of diffs between calendars. If the numbers are different, this will violate the assumption made in `RebaseDateTime.rebaseMicros`.
  - swith points are ordered from old to recent timestamps. This pre-condition is required for linear search in the `rebaseMicros` function.
- Added the test `optimization of micros rebasing - Gregorian to Julian` to `RebaseDateTimeSuite` which iterates over timestamps from 0001-01-01 to 2100-01-01 with the steps 1 ± 0.5 months, and checks that optimised function `RebaseDateTime`.`rebaseGregorianToJulianMicros()` returns the same result as non-optimised one. The check is performed for the UTC, PST, CET, Africa/Dakar, America/Los_Angeles, Antarctica/Vostok, Asia/Hong_Kong, Europe/Amsterdam time zones.
- Added the test `optimization of micros rebasing - Julian to Gregorian` to `RebaseDateTimeSuite` which does similar checks as the test above but for rebasing from the hybrid calendar (Julian + Gregorian) to Proleptic Gregorian calendar.
- Re-run `DateTimeRebaseBenchmark` at the America/Los_Angeles time zone (it is set explicitly in the PR #28127):",code_debt,slow_algorithm
airflow,1270,description,0,pypi can use categories for better description and version number was out of sync,code_debt,low_quality_code
bookkeeper,1059,description,0,"Descriptions of the changes in this PR:
This is cherry-pick from yahoo repo of branch yahoo-4.3.
original commit is:
https://github.com/yahoo/bookkeeper/commit/42bdc083
Release addEntry-Bytebuf on readOnlyBookie to prevent memory-leak",design_debt,non-optimal_design
hive,767,description,0,"LockedDriverState is a nested class within Driver, while it is used outside of it as well, and it is complex enough to be a class on it's own. DriverState should be it's nested class, and transitions / locking should be facilitated by functions within it.",design_debt,non-optimal_design
spark,20619,description,0,"ParquetFileFormat leaks opened files in some cases. This PR prevents that by registering task completion listers first before initialization.
Manual. The following test case generates the same leakage.",design_debt,non-optimal_design
pulsar,5592,description,0,"Fixes #5589 
It seems that there is a memory leak in the pulsar-function-go library.
I implemented a simple pulsar function worker that just write logs using pulsar-function-go/logutil for sending logs to log topic. I tried to long-term test by sending request messages consecutively to the input topic to check the feasibility.
During the test, I faced `ProducerQueueIsFull` error with `--log-topic` option. And I observed indefinitely grown memory usage of the pulsar function worker process.
Clear the `StrEntry` variable after finish addLogTopicHandler() function regardless of the log messages are appended to logger or not. If it is not cleared, it causes memory leak because StrEntry has grown indefinitely. Moreover, if the function set --log-topic, then the topic could get accumulated huge messages that cause ProducerQueueIsFull error.
Verified it by reproducing step described in the issue #5589.",design_debt,non-optimal_design
lucene-solr,2200,review,556698348,"Classloading won't help, because we still need a separate JVM. When JVM loads classes, it trys in parent classloader first. If the class is already there it won't load. So we need at least one separate process.
I don't like to try many times each with separate JVM. Maybe only try once (like in the other test with codecs). It may not fail every time, but sometimes test fails.
I am also not sure if we really need a test for this. If we may get a static checker that finds classes that initialize their subclasses in their own static initializer, we can prevent similar cars in future.",test_debt,flaky_test
daffodil,289,review,346101694,"Need to explain this algorithm more thoroughly around what is done with sequences specifically.
Also unit tests focusing specifically on this algorithm, to strongly characterize proper behavior and insure full coverage.",test_debt,low_coverage
kafka,2143,review,94173222,"Besides not removing this, it seems we should also probably have a unit test validating the new behavior for naming consumer groups in sinks?",test_debt,lack_of_tests
guacamole-client,511,review,602658052,"Sure, makes sense. I've got the changes almost done - just need to clean up the unit tests.",test_debt,expensive_tests
kafka,6239,review,256163993,minor - no test coverage for lines 221 - 225 in the unit test,test_debt,low_coverage
incubator-mxnet,12426,description,0,"The PR applies the website theme to each version. The navigation will be the same, so the option for Clojure needs to be handled properly for old versions. For this I use the .htaccess file to redirect users to an API error page. For good measure, I also added a custom 404 error page.
This PR stacks on #12413 (has the same changes in build_all_version.sh), plus a change to copy the theme, and fixes my concerns there with the theme.
http://34.201.8.176/
You can test the redirect if you switch to an old version like 1.0.0 and go to API --> Clojure
You can look at the 404:
http://34.201.8.176/error/404.html 
I'm sure there's probably some fancy regex that would collapse the clojure rules to one line, but I'll let someone else get fancy.",documentation_debt,low_quality_documentation
flink,15045,description,0,"fix some typo errors to make the context consistent:
some are ""streamOfWords"" but some are ""dataStreamOfWords""",documentation_debt,low_quality_documentation
carbondata,3769,description,0," - No
 - Yes. (please explain the change and update document)
 - No
 - Yes",documentation_debt,outdated_documentation
incubator-mxnet,18560,description,0,"Currently, 1.6.x branch is failing on multiple pipelines
1. centos-cpu & centos-gpu pipelines due to 
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline
2. Flaky test on unix-cpu mkldnn
3. Edge pipeline",test_debt,flaky_test
druid,5102,review,158433249,"@jihoonson If you mean the actual ""CompactionTask"" etc classes, I think probably moving something so heavy from druid-indexing-service all the way down to druid-api would probably require collapsing a ton of druid modules into one giant druid-core module. I guess we could do that but it seems like a big change. Do you think it's worth it?",architecture_debt,violation_of_modularity
cloudstack,4490,description,0,"Fixes https://github.com/apache/cloudstack/issues/4481
TODO",requirement_debt,requirement_partially_implemented
spark,18986,comment,323618879,"Yea, since this topic is important for some users, I mean we better move the doc into `./docs/` ( I feel novices dont seem to check the code documents).",architecture_debt,violation_of_modularity
